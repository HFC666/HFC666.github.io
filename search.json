[{"title":"高斯混合模型和EM算法","url":"/2021/08/31/高斯混合模型和EM算法/","content":"## Gaussian mixture models and the EM algorithm\n\n我们使用简写符号$X_1^n$来表示$X_1,X_2,\\cdots,X_n$，相似地，$x_1^n$表示$x_1,x_2,\\cdots,x_n$。\n\n### The model\n\n假设我们有有编号的人$i=1,\\cdots,n$。我们观察表示每个人的身高的随机变量$Y_i\\in \\mathbb{R}$，同时假设有一个观测不到的标签$C_i\\in \\{\\operatorname{M,F}\\}$表示人的性别。在这了，小写字母$c$代表\"class\"。我们也假设两组具有相同的已知方差$\\sigma^2$，但不同的未知均值$\\mu_M$和$\\mu_F$。类标签符合伯努利分布：\n$$\np_{C_i}(c_i) = q^{\\mathbb{1}(c_i=M)}(1-q)^{\\mathbb{1}(c_i=F)}\n$$\n我们也假设$q$是已知的。为了简化符号，我们令$\\pi_M=q$和$\\pi_F=1-q$，因此我们可以写作：\n$$\np_{C_i}(c_i) = \\prod_{c\\in \\{M,F\\}}\\pi_c^{\\mathbb{1}(c_i=c)}\n$$\n每一类的条件分布都为高斯分布：\n$$\np_{Y_i|C_i}(y_i|c_i) = \\prod_c\\mathcal{N}(y_i;\\mu_c,\\sigma^2)^{\\mathbb{1}(c_i=c)}\n$$\n\n### Parameter estimation: a first attempt\n\n假设我们观测到独立同分布的身高$Y_1=y_1,\\cdots,Y_n=y_n$，并且我们想要去找到参数$\\mu_M,\\mu_F$的极大似然估计。这是一个非监督问题：我们不知道我们数据的性别标签，但是我们想根据这些标签学习参数。\n\n根据上文提到的模型设计，计算所有数据点$P_{Y_1,\\cdots,Y_n}$的联合密度，以$\\mu_M,\\mu_F,\\sigma,q$表示。取$\\log$后计算$\\log$似然，然后对$\\mu_M$进行求导。为什么优化这么困难？\n\n我们先对单个数据点$Y_i=y_i$求密度：\n$$\n\\begin{aligned}\nP_{Y_i}(y_i) &= \\sum_{c_i}p_{C_i}(c_i)p_{Y_i|C_i}(y_i|c_i)\\\\\n&= \\sum_{c_i}(\\pi_c\\mathcal{N}(y_i;\\mu_C,\\sigma^2))^{\\mathbb{1}(c_i=c)}\\\\\n&= q\\mathcal{N}(y_i;\\mu_M,\\sigma^2) + (1-q)\\mathcal{N}(y_i;\\mu_F,\\sigma^2)\n\\end{aligned}\n$$\n现在，所有观测的联合分布为\n$$\nP_{Y_1^n}(y_1^n) = \\prod_{i=1}^n(q\\mathcal{N}(y_i;\\mu_M,\\sigma^2) + (1-q)\\mathcal{N}(y_i;\\mu_F,\\sigma^2))\n$$\n则$\\log$似然函数为：\n$$\n\\ln p_{Y_1^n}(y_1^n) = \\sum_{i=1}^n\\ln(\\pi_M\\mathcal{N}(y_i;\\mu_M,\\sigma^2) + \\pi_F\\mathcal{N}(y_i;\\mu_F,\\sigma^2))\n$$\n我们已经遇到了一个问题，和的形式阻止我们将$\\log$应用于内部的正态分布密度函数。通过对称性，我们只需要看其中一个均值，另一个将遵循相同的过程。在我们深入区分之前，我们注意到：\n$$\n\\begin{aligned}\n\\frac{d}{d\\mu}\\mathcal{N}(x;\\mu,\\sigma^2) &= \\frac{d}{d\\mu}[\\frac{1}{\\sigma\\sqrt{2\\pi}}e^{-\\frac{(x-\\mu)^2}{2\\sigma^2}}]\\\\\n&= \\frac{1}{\\sigma\\sqrt{2\\pi}}e^{-\\frac{-(x-\\mu)^2}{2\\sigma^2}}\\cdot\\frac{2(x-\\mu)}{2\\sigma^2}\\\\\n&= \\mathcal{N}(x;\\mu,\\sigma^2)\\cdot\\frac{(x-\\mu)}{\\sigma^2}\n\\end{aligned}\n$$\n对数似然函数对$\\mu_M$进行求导，得\n\n[^1]: $\\sum_{i=1}^n\\frac{1}{\\pi_M\\mathcal{N}(y_i;\\mu_M,\\sigma^2)+\\pi_F\\mathcal{N}(y_i;\\mu_F,\\sigma^2)}\\pi_M\\mathcal{N}(y_i;\\mu_M,\\sigma^2)\\frac{y_i-\\mu_M}{\\sigma^2}=0$\n\n$$\n\\sum_{i=1}^n\\frac{1}{\\pi_M\\mathcal{N}(y_i;\\mu_M,\\sigma^2)+\\pi_F\\mathcal{N}(y_i;\\mu_F,\\sigma^2)}\\pi_M\\mathcal{N}(y_i;\\mu_M,\\sigma^2)\\frac{y_i-\\mu_M}{\\sigma^2}=0\n$$\n\n对于这个表达式我们无法求解。\n\n### Using hidden variables and the EM Algorithm\n\n退一步，什么会使这个计算容易。如果我们知道隐变量$C_i$的值，则对参数们做最大似然估计就会很容易：我们将会取所有$C_i=M$的点用来估计$\\mu_M$，然后对所有$C_i=F$的点重复此过程来估计$\\mu_F$。受此启发，我们尝试计算给定观测下$C_i$的分布。我们将从贝叶斯规则开始：\n$$\n\\begin{aligned}\np_{C_i|Y_i}(c_i|y_i) &= \\frac{p_{Y_i|C_i}(y_i|c_i)p_{C_i}(c_i)}{p_{Y_i}(y_i)}\\\\\n&= \\frac{\\prod_{c\\in \\{M,F\\}}(\\pi_c\\mathcal{N}(y_i;\\mu_c,\\sigma^2))^{\\mathbb{1}(c=c_i)}}{\\pi_M\\mathcal{N}(y_i;\\mu_M,\\sigma^2)+\\pi_F\\mathcal{N}(y_i;\\mu_F,\\sigma^2)} = q_{C_i}(c_i)\n\\end{aligned}\n$$\n我们看一下$C_i=M$的后验概率：\n$$\np_{C_i|Y_i}(M|y_i) = \\frac{\\pi_M\\mathcal{N}(y_i;\\mu_M,\\sigma^2)}{\\pi_M\\mathcal{N}(y_i;\\mu_M,\\sigma^2)+\\pi_F\\mathcal{N}(y_i;\\mu_F,\\sigma^2)} = q_{C_i}(M)\n$$\n这个看起来很熟悉，这是式[^1]中的一部分，我们可以将式[^1]用$q_{C_i}$重写，并且假定其跟$\\mu_M$无关\n$$\n\\sum_{i=1}^nq_{C_i}(M)\\frac{y_i-\\mu_M}{\\sigma^2}=0\\\\\n\\mu_M = \\frac{\\sum_{i=1}^nq_{C_i}(M)y_i}{\\sum_{i=1}^nq_{C_i}(M)}\n$$\n这样就看起来好多了：$\\mu_M$是身高的加权平均值，其中每个身高都根据该人是男性的可能性进行加权。\n\n因此现在我们形成了一个循环，如果我们知道参数我们将会很容易计算出$C_1^n$的后验概率，并且如果我们知道后验概率我们将很容易地估计出参数，这就陷入了死循环。这就暗示了以下策略，我们可以固定一个来求解另一个。这种方法通常被称为`EM`算法。它的工作原理大致如下：\n\n+ 首先，我们固定参数(在这种情况下为高斯分布的均值$\\mu_M$和$\\mu_F$)并且求解隐变量的后验分布(在这种情况下记为$q_{C_i}$)。\n+ 之后，我们固定隐变量的后验分布，利用隐变量的期望值来最优化参数。\n+ 重复两个步骤直到收敛。\n\n### The EM Algorithm: a more formal look\n\n正如我们将在几个短步骤中展示的那样，EM算法实际上是最大化对数似然的下限(换句话说，每一步都保证改进我们的答案直到收敛)。\n\n假设我们观测到了随机变量$Y$。现在假设我们也有一些隐变量$C$并且$Y$依赖于$C$。我们可以说$C$和$Y$的分布中有一些我们不知道的参数$\\theta$，并且我们有兴趣找到它们。\n\n在我们上一个例子中，我们观测到有隐变量(性别)$C=\\{C_1,\\cdots,C_n\\}$的身高变量$Y=\\{Y_1,\\cdots,Y_n\\}$，并且$Y$和$C$是独立同分布的，我们的参数是$\\mu_M$和$\\mu_F$。\n\n在我们真正推导算法之前，我们需要一个关键结论：`Jensen's inequation`(琴声不等式)。在这个例子中我们需要的琴声不等式的特例：\n$$\n\\log(\\mathbb{E}[X])\\ge \\mathbb{E}[\\log(X)]\n$$\n下图是关于琴声不等式的几何直观：\n\n![](https://raw.githubusercontent.com/HFC666/image/master/img/EM.png)\n\n> 琴声不等式的特例说明：对于任何随机变量$X$，$\\mathbb{E}[\\log X]\\le\\log\\mathbb{E}[X]$。令$X$的PDF为红色的曲线。令$Z=\\log X$。中间和右边的图展示了怎么构建$Z$的PDF(蓝色曲线)：因为$\\log$，与$X$的PDF相比，它倾向于更小的值。$\\log\\mathbb{E}(X)$是由中间黑色曲线或$\\mathbb{E}(Z)$给出的点。但是，$\\mathbb{E}[\\log X]$或者$\\mathbb{E}[Z]$，将始终较小(或至少永远不会较大)因为对数挤压了分布较大的一端(其中$Z$比较大)并拉伸了较小的一端(其中$Z$较小)。\n\n> 关于琴声不等式\n>\n> 对于一个实函数$\\phi(x)$，在区间$I$内它是凸的($\\frac{d^2\\phi(x)}{d^2x}>0, \\forall x\\in I$)，那么它满足下面关系：\n> $$\n> \\phi(\\sum_{i=1}^Np_ix_i)\\le \\sum_{i=1}^Np_i\\phi(x_i)\n> $$\n> 其中$p_i\\ge0,\\sum_{i=1}^Np_i=1$，且$x_i\\in I,(i=1,\\cdots,N)$。\n>\n> **证明**：令$A=\\sum_{i=1}^Np_ix_i$，显然$A\\in I$。取\n> $$\n> \\begin{aligned}\n> S &= \\sum_{i=1}^N\\phi(x_i) - \\phi(A)\\\\\n> &= \\sum_{i=1}^Np_i[\\phi(x_i)-\\phi(A)]\\\\\n> &= \\sum_{i=1}^N p_i\\int_A^{x_i}\\phi^{\\prime}(x)dx\n> \\end{aligned}\n> $$\n> 若$A\\le x_i$，因为$\\phi^{\\prime}(x)$在区间$I$上是递增的，所以\n> $$\n> \\int_A^{x_i}\\phi^{\\prime}(x)dx\\ge \\phi^{\\prime}(A)(x_i-A)\n> $$\n> 若$A_i>x_i$，则\n> $$\n> \\int_A^{x_i}\\phi^{\\prime}(x)dx = -\\int_{x_i}^A\\phi^{\\prime}(x)dx\\ge-(A-x_i)\\phi(A) = (x_i-A)\\phi^{\\prime}(A)\n> $$\n> 所以：\n> $$\n> \\begin{aligned}\n> S &\\ge \\sum_{i=1}^Np_i\\phi^{\\prime}(A)(x_i-A)\\\\\n> &= \\phi^{\\prime}(A)[\\sum_{i=1}^Np_i(x_i-A)]\\\\\n> &= \\phi^{\\prime}(A)(A-A)\\\\\n> &=0\n> \\end{aligned}\n> $$\n> 证毕。\n\n在本文中，求期望相当于加权平均，而$\\ln(x)$的二阶导小于$0$，故与上文提到的不等式符号相反。\n\n#### The EM Algorithm\n\n我们想要去最大化似然函数。我们通过边缘化$C$来计算对数似然函数：\n$$\n\\log p_Y(y;\\theta) = \\log\\left(\\sum_cp_{Y,C}(y,c)\\right)\n$$\n我们现在也遭受了困境，我们无法对和取对数。如果我们交换它们的顺序不是更好吗？对，期望是一种特殊的求和，并且琴声不等式可以让我们互换它们如果我们有期望的话。因此，我们将要引进一个隐变量$C$的新的分布$q_C$：\n$$\n\\begin{aligned} \\log p_{Y}(y ; \\theta) & \\\\ \\left.\\text { (Marginalizing over } C \\text { and introducing } q_{C}(c) / q_{C}(c)\\right) &=\\log \\left(\\sum_{c} q_{C}(c) \\frac{p_{Y, C}(y, c ; \\theta)}{q_{C}(c)}\\right) \\\\ \\text { (Rewriting as an expectation) } &=\\log \\left(\\mathbb{E}_{q_{C}}\\left[\\frac{p_{Y, C}(y, C ; \\theta)}{q_{C}(C)}\\right]\\right) \\\\ \\text { (Using Jensen's inequality) } & \\geq \\mathbb{E}_{q_{C}}\\left[\\log \\frac{p_{Y, C}(y, C ; \\theta)}{q_{C}(C)}\\right](2) \\\\ \\text { Using definition of conditional probability } &=\\mathbb{E}_{q_{C}}\\left[\\log \\frac{p_{Y}(y ; \\theta) p_{C \\mid Y}(C \\mid y ; \\theta)}{q_{C}(C)}\\right](3) \\end{aligned}\n$$\n\n\n现在我们有可以很容易优化的$\\log p_Y(y;\\theta)$的下界了。因为我们已经引入了$q_C$，我们想要在$\\theta$和$q_C$上实施最大化。\n\n我们将使用(2)和(3)分别进行优化。首先先用(2)寻找最好的参数：\n$$\n\\mathbb{E}_{q_{C}}\\left[\\log \\frac{p_{Y, C}(y, C ; \\theta)}{q_{C}(C)}\\right] = \\mathbb{E}_{q_C}[\\log p_{Y, C}(y, C ; \\theta)] - \\mathbb{E}_{q_C}[\\log q_C(C)]\n$$\n因为$q_C$不依赖于$\\theta$，因此我们可以只优化第一项：\n$$\n\\widehat{\\theta} \\leftarrow \\underset{\\theta}{\\operatorname{argmax}} \\mathbb{E}_{q_{C}}\\left[\\log p_{Y, C}(y, C ; \\theta)\\right]\n$$\n这被称作`M-step`：`M`代表最大化，因为我们正在最大化参数。现在，我们用(3)来找到最好的$q_C$：\n$$\n\\mathbb{E}_{q_{C}}\\left[\\log \\frac{p_{Y}(y ; \\theta) p_{C \\mid Y}(C \\mid y ; \\theta)}{q_{C}(C)}\\right]=\\mathbb{E}_{q_{C}}\\left[\\log p_{Y}(y ; \\theta)\\right]+\\mathbb{E}_{q_{C}}\\left[\\log \\frac{p_{C \\mid Y}(C \\mid y ; \\theta)}{q_{C}(C)}\\right]\n$$\n第一项不依赖于$c$，并且第二项看起来像KL散度：\n$$\n\\begin{aligned}\n&= \\log p_Y(y;\\theta) - \\mathbb{E}_{q_C}[\\log\\frac{q_C(C)}{p_{C|Y}(C|y;\\theta)}]\\\\\n&= \\log p_Y(y;\\theta) - D(q_C(\\cdot)||p_{C|Y}(\\cdot|y;\\theta))\n\\end{aligned}\n$$\n因此，当最大化上述值时，我们想要最小化KL散度。KL散度总是大于等于零，当两个分布完全一样时取零。因此，最优化的$q_C$即为$p_{C|Y}(c|y;\\theta)$：\n$$\n\\hat{q}_C(c) \\leftarrow p_{C|Y}(c|y;\\theta)\n$$\n这被称为`E-step`：`E`代表期望，因为我们正在计算$q_C$以便我们可以将其用于期望值。\n\n#### The algorithm\n\n![](https://raw.githubusercontent.com/HFC666/image/master/img/EM_a.png)\n\n> 信息熵可以表达数据的信息量大小，是信息处理的一个非常重要的概念。对于离散型随机变量，信息熵公式如下：\n> $$\n> H(p) = H(X) = \\mathrm{E}_{x\\sim p(x)}[-\\log p(x)] = -\\sum_{i=1}^n p(x)\\log p(x)\n> $$\n> 对于连续性随机变量，信息熵公式如下：\n> $$\n> H(p) = H(X) = E_{x\\sim p(x)}[-\\log p(x)] = -\\int p(x)\\log p(x)dx\n> $$\n> 接下来我们来说一下相对熵，又被称为KL散度或信息散度，是两个概率分布差异的非对称度量。在信息论中，相对熵等价于两个概率分布的信息熵的插值，若其中一个概率分布为真实分布，另一个为理论分布，则此时相对熵等于交叉熵与真实分布的信息熵之差，表示使用理论分布拟合真实分布时产生的信息损耗。公式为：\n> $$\n> D_{\\operatorname{KL}}(p||q) = \\sum_{i=1}^N[p(x_i)\\log p(x_i) - p(x_i)\\log q(x_i)]\n> $$\n> 上面的$p(x_i)$为真实事件的概率分布，$q(x_i)$为理论拟合出来的该事件的概率分布。当拟合出来的事件概率分布跟真实的一模一样时，相对熵等于零，而拟合出来不一样时，相对熵大于零。\n>\n> 最后我们来证明一下相对熵公式只有在$p(x_i)$等于$q(x_i)$的时候等于$0$，其他时候大于$0$。\n>\n> 要证：\n> $$\n> D_{\\operatorname{KL}}(p||q) = \\sum_{i=1}^N[p(x_i)\\log p(x_i)-p(x_i)\\log q(x_i)]\\ge 0\n> $$\n> 即证：\n> $$\n> \\sum_{i=1}^Np(x_i)\\log\\frac{q(x_i)}{p(x_i)}\\le 0\n> $$\n> 又$\\ln(x)\\le x-1$，当且仅当$x=1$时等号成立\n>\n> 故\n> $$\n> \\sum_{i=1}^Np(x_i)\\log\\frac{q(x_i)}{p(x_i)}\\le \\sum_{i=1}^Np(x_i)(\\frac{q(x_i)}{p(x_i)}-1) = \\sum_{i=1}^N[p(x_i)-q(x_i)]=0\n> $$\n> 上面式子中$=$只在$p(x_i)=q(x_i)$时成立。\n\n### Example: Applying the general algorithm to GMMS\n\n现在，让我们重现看一下关于身高的GMM模型，看看我们如何应用这两个步骤。我们有观测变量$Y=\\{Y_1,\\cdots,Y_n\\}$和隐变量$C=\\{C_1,\\cdots,C_n\\}$。对于`E-Step`，我们计算后验概率$p_{C|Y}(c|y)$，之前已经计算过了。对于`M-Step`，我们得去计算联合概率：\n$$\n\\begin{aligned}\n\\mathbb{E}_{q_C}[\\ln p_{Y,C}(y,C)] &= \\mathbb{E}[\\ln p_{Y|C}(y|C)p_C(C)]\\\\\n&= \\mathbb{E}_{q_C}\\left[\\ln \\prod_{i=1}^n\\prod_{c\\in \\{M,F\\}}(\\pi_c\\mathcal{N}(y_i;\\mu_c,\\sigma^2))^{\\mathbb{1}(C_i=c)}\\right]\\\\\n&= \\mathbb{E}_{q_C}\\left[\\sum_{i=1}^n\\sum_{c\\in \\{M,F\\}}\\mathbb{1}(C_i=c)(\\ln\\pi_c+\\ln\\mathcal{N}(y_i;\\mu_c,\\sigma^2))\\right]\\\\\n&= \\sum_{i=1}^n\\sum_{c\\in \\{M,F\\}}\\mathbb{E}_{q_C}[\\mathbb{1}(C_i=c)]\\left(\\ln\\pi_c + \\ln\\frac{1}{\\sigma\\sqrt{2\\pi}}-\\frac{(y_i-\\mu_c)^2}{2\\sigma^2}\\right)\n \\end{aligned}\n$$\n$\\mathbb{E}_{q_C}[\\mathbb{1}(C_i=c)]$是$C_i$为$c$的概率。现在我们对$\\mu_M$求导：\n$$\n\\frac{d}{d \\mu_{M}} \\mathbb{E}_{q_{C}}\\left[\\ln p_{Y \\mid C}(y \\mid C) p_{C}(C)\\right]=\\sum_{i=1}^{n} q_{C_{i}}(M)\\left(\\frac{y_{i}-\\mu_{M}}{\\sigma^{2}}\\right)=0\n$$\n得：\n$$\n\\mu_{M}=\\frac{\\sum_{i=1}^{n} q_{C_{i}}(M) y_{i}}{\\sum_{i=1}^{n} q_{C_{i}}(M)}\n$$\n\n同理可得：\n$$\n\\mu_{F}=\\frac{\\sum_{i=1}^{n} q_{C_{i}}(F) y_{i}}{\\sum_{i=1}^{n} q_{C_{i}}(F)}\n$$\n","tags":["机器学习"],"categories":["算法"]},{"title":"贝叶斯数据分析","url":"/2021/08/11/贝叶斯数据分析/","content":"\n当初为什么要用英文写笔记？？\n{% pdf https://hfcouc.work/pdfs/Bayesian_Data_Analysis.pdf %}","tags":["贝叶斯数据分析"],"categories":["贝叶斯机器学习"]},{"title":"微分方程","url":"/2021/08/09/微分方程/","content":"\n微分方程学习笔记\n\n{% pdf https://hfcouc.work/pdfs/Differential_equation.pdf %}\n","tags":["微分方程"],"categories":["数学"]},{"title":"数值分析笔记","url":"/2021/07/30/数值分析/","content":"\n学习数值分析的笔记\n\n{% pdf https://hfcouc.work/pdfs/Numerical_analysis.pdf %}\n\n","tags":["数值分析"],"categories":["数学"]},{"title":"统计学习基础","url":"/2021/07/28/统计学习精要笔记/","content":"\n自己学习统计学习基础的笔记。\n\n{% pdf https://hfcouc.work/pdfs/ESLII.pdf %}\n\n","tags":["机器学习"],"categories":["机器学习"]},{"title":"只争朝夕，不负韶华","url":"/2021/07/13/index/","content":"\n{% meting \"33378114\" \"netease\" \"song\" \"theme:#555\" \"mutex:true\" \"listmaxheight:340px\" \"preload:auto\" %}\n\n<img src=\"https://cdn.jsdelivr.net/gh/HFC666/image@master/20210713/3.178msyh9rg68.png\" style=\"zoom:80%;\" />\n\n\n\n> 现状是没那么容易改变的\n> 即便是足够努力\n> 也很难在短时间内看出效果\n> 所以有时你认为的无法改变\n> 也可能只是暂时没看出效果而己\n> 而不是不够努力\n\n<img src=\"https://cdn.jsdelivr.net/gh/HFC666/image@master/20210713/4.36iwffkaky00.png\" style=\"zoom:80%;\" />\n\n\n\n> 不能再浑浑噩噩\n> 如果不把眼皮用力抬起看个真切\n> 或许就会错过人生中按下快门的良机\n\n\n\n**即使是一个人，也需要好好吃饭，这是治愈自己的一种方式**\n\n\n\n“不被时间和社会所束缚，幸福的填饱肚子，短时间内变得随心所欲，变得自由，不被打扰；毫不费神的吃东西的这种孤高行为，是现代人，都平等的拥有的最高治愈。”\n\n{% raw %}\n\n<div style=\"position: relative; width: 100%; height: 0; padding-bottom: 75%;\"><iframe \nsrc=\"//player.bilibili.com/player.html?\taid=73923411&bvid=BV1AE411a7Nw&cid=126453908&page=1\" scrolling=\"no\" border=\"0\" \nframeborder=\"no\" framespacing=\"0\" allowfullscreen=\"true\" style=\"position: absolute; width: 100%; \nheight: 100%; left: 0; top: 0;\"> </iframe></div>\n\n\n{% endraw %}\n\n\n\n<div style=\"position: relative; width: 100%; height: 0; padding-bottom: 75%;\"><iframe \nsrc=\"//player.bilibili.com/player.html?aid=52761403&bvid=BV1r4411776v&cid=92328539&page=1\" scrolling=\"no\" border=\"0\" \nframeborder=\"no\" framespacing=\"0\" allowfullscreen=\"true\" style=\"position: absolute; width: 100%; \nheight: 100%; left: 0; top: 0;\"> </iframe></div>\n\n\n\n\n"}]