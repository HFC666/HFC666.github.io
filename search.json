[{"title":"极限","url":"/2021/12/19/极限/","content":"\n### 数列极限\n\n#### 数列极限的定义\n\n定义(数列极限)。设$\\{a_n\\}$为数列，$A\\in \\mathbb{R}$.如果任给$\\epsilon>0$,都存在正整数$N(\\epsilon)$，使得当$n>N$时，有\n$$\n\t|a_n-A|<\\epsilon\n$$\n则称$\\{a_n\\}$以$A$为极限，或称$\\{a_n\\}$收敛于$A$，记为\n$$\n\t\\lim_{n\\rightarrow\\infty}a_n=A\\text{ 或 }a_n\\rightarrow A(n\\rightarrow \\infty)\n$$\n当然我们也可以用$\\epsilon-N$语言给出数列$\\{a_n\\}$不以$A$为极限的定义：如果存在$\\epsilon_0>0$，使得任给正数$N$，均存在$n_0>N$满足不等式$|a_{n_0}-A|\\ge\\epsilon_0$，则$\\{a_n\\}$不以$A$为极限。\n\n命题：如果数列$\\{a_n\\}$有极限，则其极限是唯一的。\n\n定理(夹逼定理).设$\\{a_n\\},\\{b_n\\},\\{c_n\\}$均为数列，且\n$$\n\ta_n\\le b_n\\le c_n,\\forall n\\ge N_0\n$$\n其中$N_0$为一整数，如果\n$$\n\t\\lim_{n\\rightarrow\\infty}a_n=A=\\lim_{n\\rightarrow\\infty}c_n\n$$\n则$\\lim_{n\\rightarrow\\infty}b_n=A$。\n\n例题：\n考虑无限循环小数$A=0.99999\\cdots$，问：$A$是否小于$1$？\n解：我们可以将$A$视为一列有限小数$\\{a_n\\}$的极限，其中$a_n = 0.99\\cdots9(n\\text{个}9)$。由于：\n$$\n\t|a_n-1| = 10^{-n}\n$$\n根据夹逼定理\n$$\na_n\\le A\\le 1\n$$\n而\n$$\n\\lim_{n\\rightarrow\\infty}a_n=1\n$$\n所以\n$$\nA=1\n$$\n\n例：\n设$0<\\alpha<1$，证明$\\lim_{n\\rightarrow\\infty}[(n+1)^{\\alpha}-n^{\\alpha}]=0$\n证明：当$n\\ge1$时，有\n$$\n\t\\begin{aligned}\n\t\t0<(n+1)^{\\alpha}-n^{\\alpha} &= n^{\\alpha}[(1+\\frac{1}{n})^{\\alpha}-1]\\\\\n\t\t&\\le n^{\\alpha}[(1+\\frac{1}{n})-1]=\\frac{1}{n^{1-\\alpha}}\n\t\\end{aligned}\n$$\n根据夹逼定理我们有$\\lim_{n\\rightarrow\\infty}[(n+1)^{\\alpha}-n^{\\alpha}]=0$。\n\n例：设$\\alpha>0,a>1$，则$\\lim_{n\\rightarrow\\infty}\\frac{n^{\\alpha}}{a^n}=0$\n\n思路：由于分子分母同时含有$n$，因此我们很难进行判断，我们想要做的是根据放缩消去一个$n$，而留下的$n$很容易处理，因此我们对$a^n$进行放缩处理。\n我们记$a^{\\frac{1}{\\alpha}}=1+\\beta,\\beta>0$。由于$n>1$，有\n$$\n(1+\\beta)^n = 1 + n\\beta+\\frac{1}{2}n(n-1)\\beta^2+\\cdots+\\beta^n>\\frac{1}{2}n(n-1)\\beta^2\n$$\n故\n$$\n0<\\frac{n^{\\alpha}}{a^n} = \\left[\\frac{n}{(1+\\beta)^n}\\right]^{\\alpha} < \\left[\\frac{2}{(n-1)\\beta^2}\\right]^\\alpha\n$$\n由夹逼原理可知$\\lim_{n\\rightarrow\\infty}\\frac{n^{\\alpha}}{a^n}=0$。\n\n例：证明$\\lim_{n\\rightarrow\\infty}\\frac{1}{\\sqrt[n]{n!}}=0$\n\n注意到当$1\\le k\\le n$时$(k-1)(n-k)\\ge0$，从而$k(n-k+1)\\ge n$，我们就有：\n$$\n\t(n!)^2 = (1\\cdot n)(2(n-1))\\cdots(k(n-k+1))\\cdots(n\\cdot1)\\ge n^n,\\forall n\\ge1\n$$\n因此\n$$\n\t0<\\frac{1}{\\sqrt[n]{n!}}\\le\\frac{1}{\\sqrt{n}},\\forall n\\ge1\n$$\n由夹逼原理可得：$\\lim_{n\\rightarrow\\infty}\\frac{1}{\\sqrt[n]{n!}}=0$\n\n\n例：证明$\\lim_{n\\rightarrow\\infty}\\sqrt[n]{n}=1$\n证明：记$\\sqrt[n]{n}=1+\\alpha_n$，当$n>1$时，\n$$\n\tn = (1+\\alpha_n)^n=1+n\\alpha_n+\\frac{1}{2}n(n-1)\\alpha_n^2+\\cdots+\\alpha_n^n > \\frac{1}{2}n(n-1)\\alpha_n^2\n$$\n从而有估计\n$$\n0<\\alpha_n<\\sqrt{\\frac{2}{n-1}}\n$$\n因此，当$n>1$时，有\n$$\n1<\\sqrt[n]{n} = 1+\\alpha_n<1+\\sqrt{\\frac{2}{n-1}}\n$$\n由夹逼原理即得：$\\lim_{n\\rightarrow\\infty}\\sqrt[n]{n}=1$。\n\n下面两个为比较重要的例题：\n\n设$\\lim_{n\\rightarrow\\infty}a_n=A$，证明$\\lim_{n\\rightarrow\\infty}\\frac{a_1+a_2+\\cdots+a_n}{n}=A$。\n证明：任给$\\epsilon>0$，因为$\\lim_{n\\rightarrow\\infty}a_n=A$，故存在$N_0$，使得当$n>N_0$时，有\n$$\n\t|a_n-A|<\\frac{\\epsilon}{2}\n$$\n令\n$$\nN>\\max\\{N_0,2\\epsilon^{-1}|a_1+\\cdots+a_{N_0}-N_0A|\\}\n$$\n则当$n>N$时，有\n$$\n\\begin{aligned}\n&\\left|\\frac{a_{1}+\\cdots+a_{n}}{n}-A\\right|=\\left|\\frac{a_{1}+\\cdots+a_{N_{0}}-N_{0} A}{n}+\\frac{\\left(a_{N_{0}+1}-A\\right)+\\cdots+\\left(a_{n}-A\\right)}{n}\\right| \\\\\n\n&\\leqslant \\frac{\\left|a_{1}+\\cdots+a_{N_{0}}-N_{0} A\\right|}{n}+\\frac{\\left|a_{N_{0}+1}-A\\right|+\\cdots+\\left|a_{n}-A\\right|}{n} \\\\\n\n&\\leqslant \\frac{\\left|a_{1}+\\cdots+a_{N_{0}}-N_{0} A\\right|}{n}+\\frac{n-N_{0}}{n} \\frac{\\varepsilon}{2} \\\\\n\n&<\\frac{\\varepsilon}{2}+\\frac{\\varepsilon}{2}=\\varepsilon\n\n\\end{aligned}\n$$\n这说明：$\\lim_{n\\rightarrow\\infty}\\frac{a_1+a_2+\\cdots+a_n}{n}=A$。\n\n\n\n我们再来证明一个跟这个差不多的例题：\n\n$\\lim_{n\\rightarrow\\infty}a_n=A$，则$\\sqrt[n]{a_1\\cdots a_n}=A$。\n\n证明：\n\n我们可以取对数利用上面那个题的结论即可证明。\n\n\n\n例：**任何实数都是某个有理数列的极限**。\n证明：设$A$为实数。如果$A$为有理数，则令$a_n=A(n\\ge1)$即可。如果$A$为无理数，令\n$$\na_n = \\frac{[nA]}{n},\\forall n\\ge1\n$$\n其中$[x]$表示不超过$x$的最大整数，因此$a_n$都是有理数。因为$A$不是有理数，故：\n$$\nnA-1<[nA]<nA,\\forall n\\ge1\n$$\n即\n$$\nA-\\frac{1}{n}<a_n=\\frac{[nA]}{n}<A,\\forall n\\ge1\n$$\n由夹逼定律可知$\\lim_{n\\rightarrow\\infty} a_n=A$\n\n#### 数列极限的基本性质\n\n命题(有界性)：设数列$\\{a_n\\}$收敛，则$\\{a_n\\}$有界\n由此命题立知，无界数列必定发散。如果$\\{a_n\\}$发散到$+\\infty$，则称$\\{a_n\\}$发散到$\\infty$，记为\n$$\n\t\\lim_{n\\rightarrow\\infty}a_n=\\infty,\\text{ 或}a_n\\rightarrow\\infty(n\\rightarrow\\infty)\n$$\n\n命题(绝对值性质)。设数列$\\{a_n\\}$收敛到$A$，则$\\{|a_n|\\}$收敛到$|A|$。\n\n推论：数列$\\{a_n\\}$收敛到$0$当且仅当$|a_n|$收敛到$0$；数列$\\{a_n\\}$收敛到$A$当且仅当$|a_n-A|$收敛到$0$。\n\n命题(保序性质)。设数列$\\{a_n\\}$收敛到$A$，$\\{b_n\\}$收敛到$B$，则有\n\n1. 如果存在$N_0$，当$n>N_0$时$a_n\\ge b_n$，则$A\\ge B$。\n2. 反之，如果$A>B$，则存在$N$，使得当$n>N$时$a_n>b_n$。\n\n推论：设$\\lim_{n\\rightarrow\\infty}a_n=A$，如果$A\\neq0$，则存在$N$，使得当$n>N$时，有\n$$\n\t\\frac{1}{2}|A|<|a_n|<\\frac{3}{2}|A|\n$$\n\n例：设$q>1$，则$\\lim_{n\\rightarrow\\infty}\\frac{\\log_qn}{n}=0$\n解：任给$\\epsilon>0$，利用之前的结论，有\n$$\n\\lim_{n\\rightarrow\\infty}\\sqrt[n]{n}=1<q^{\\epsilon}\n$$\n由极限的保序性质，存在$N$，当$n>N$时，有\n$$\n\t\\sqrt[n]{n}<q^{\\epsilon}\n$$\n即\n$$\n\t\\frac{\\log_qn}{n}<\\epsilon,\\forall n>N\n$$\n这说明：$\\lim_{n\\rightarrow\\infty}a_n=A$.\n\n命题(四则运算)。设数列$\\{a_n\\}$收敛到$A$，$\\{b_n\\}$收敛到$B$，则有：\n\n1. $\\{\\alpha a_n+\\beta b_n\\}$收敛到$\\alpha A+\\beta B$，其中$\\alpha,\\beta$为常数\n2. $\\{a_nb_n\\}$收敛到$AB$\n3. 当$B\\neq0$时，$\\{a_n/b_n\\}$收敛到$A/B$\n\n\n下面我们引入数列的子列的概念，并研究数列的极限和其子列的极限之间的关系，设\n$$\n\ta_1,a_2,\\cdots,a_n,\\cdots\n$$\n是数列，如果\n$$\n\t1\\le n_1<n_2<\\cdots<n_k<\\cdots\n$$\n是一列严格递增的正整数，则称数列\n$$\n\ta_{n_1},a_{n_2},\\cdots,a_{n_k},\\cdots\n$$\n为原数列$\\{a_n\\}$的子列，记为$\\{a_{n_k}\\}$。两个特殊的子列$\\{a_{2k}\\}$和$\\{a_{2k-1}\\}$分别为偶子列和奇子列。\n\n命题\n\n1. 设$\\{a_n\\}$收敛到$A$，则它的任何子列$\\{a_{n_k}\\}$也收敛到$A$\n2. 如果$\\{a_n\\}$的偶子列与奇子列收敛到$A$，则$\\{a_n\\}$也收敛到$A$\n\n\n\n#### 例题\n\n设$\\lim_{n\\rightarrow \\infty}(a_{n+1}-a_n)=A$，则$\\lim_{n\\rightarrow\\infty}\\frac{a_n}{n}=A$\n$$\n\\frac{a_n}{n} = \\frac{a_1+(a_2-a_1)+(a_3-a_2)+\\cdots+(a_n-a_{n-1})}{n}\n$$\n因为$\\lim_{n\\rightarrow \\infty}(a_{n+1}-a_n)=A$，利用已知例题的结论即可证明得到。\n\n\n\n设$\\lim_{n\\rightarrow\\infty}a_n=A$，证明：\n$$\n\\lim_{n\\rightarrow\\infty}\\frac{1}{n^2}(a_1+2a_2+\\cdots+na_n)=\\frac{1}{2}A\n$$\n对上式变换得到\n$$\n\\frac{1}{n^2}(a_1+2a_2+\\cdots+na_n) = \\sum_{i=1}^n\\frac{i(a_i-A)}{n^2}+\\frac{n(n+1)}{2n^2}A\n$$\n\n\n这样就好证明了：\n\n因为：\n$$\n\\lim_{n\\rightarrow\\infty}\\frac{i}{n}=0\\quad\\lim_{n\\rightarrow\\infty}(a_i-A)=0\\Rightarrow\\lim_{n\\rightarrow\\infty}\\frac{i(a_i-A)}{n}=0\n$$\n所以：\n$$\n\\lim_{n\\rightarrow0}\\sum_{i=1}^n\\frac{i(a_i-A)}{n^2}=0\n$$\n有因为：\n$$\n\\lim_{n\\rightarrow0}\\frac{n(n+1)}{2n^2}A=\\frac{1}{2}A\n$$\n得证。\n\n#### 单调数列的极限\n\n确定原理：非空数集如果有上界则必有上确界，如果有下界则必有下确界。\n\n设$\\{a_n\\}$为实数列，如果\n$$\na_1\\le a_2\\le\\cdots\\le a_n\\le \\cdots\n$$\n则称$\\{a_n\\}$是单调递增序列，当上式中的$\\le$号换成$<$时称$\\{a_n\\}$是严格单调递增的。\n\n\n\n定理(单调数列的极限)：设$\\{a_n\\}$为单调数列\n\n1. 如果$\\{a_n\\}$为单调递增的数列，则$\\lim_{n\\rightarrow\\infty}a_n=\\sup\\{a_k|k\\ge1\\}$\n2. 如果$\\{a_n\\}$为单调递减序列，则$\\lim_{n\\rightarrow\\infty}a_n=\\inf\\{a_k|k\\ge1\\}$\n\n证明：记$M=\\sup\\{a_k|k\\ge1\\}$，先考虑$M$有限的情形。任给$\\epsilon>0$，存在$a_N$，使得\n$$\nM-\\epsilon<a_N\\le M\n$$\n因为$\\{a_n\\}$是单调递增序列，故当$n>N$时\n$$\nM-\\epsilon<a_N\\le a_n\\le M<M+\\epsilon\n$$\n由数列的极限定义可知：\n$$\n\\lim_{n\\rightarrow\\infty}a_n=M=\\sup\\{a_k|k\\ge1\\}\n$$\n如果$M=+\\infty$，则任给$A>0$，由上确界的定义，存在$a_N$，使得$a_N>A$。由于$\\{a_n\\}$是单调递增序列，故当$n>N$时有$a_n\\ge a_N>A$，从而\n$$\n\\lim_{n\\rightarrow\\infty}a_n=+\\infty=\\sup\\{a_k|k\\ge1\\}\n$$\n由上面的推理我们也可以得到：单调有界的数列必有极限。\n\n\n\n任何收敛数列都有单调的收敛子列。\n\n\n\n例：设$a_1>0,a_{n+1}=\\frac{1}{2}(a_n+\\frac{1}{a_n}),n\\ge1$。研究数列$\\{a_n\\}$的极限。\n\n对于$a_n>0,\\forall n\\ge1$。我们有：\n$$\na_{n+1} = \\frac{1}{2}(a_n+\\frac{1}{a_n})\\ge\\frac{1}{2}\\cdot2(a_n\\cdot\\frac{1}{a_n})=1\n$$\n故我们有：\n$$\na_{n+1} = \\frac{1}{2}(a_n+\\frac{1}{a_n})\\le\\frac{1}{2}(a_n+a_n)=a_n\n$$\n所以单调递减，而又有下界。因此收敛，记其极限为$A$，则$A\\ge1$。另一方面：\n$$\nA = \\lim_{n\\rightarrow\\infty}a_{n+1}=\\lim_{n\\rightarrow\\infty}\\frac{1}{2}(a_n+\\frac{1}{a_n}) = \\frac{1}{2}(A+\\frac{1}{A})\n$$\n故$A=1$.\n\n\n\n我们现在讨论几个重要的极限：\n$$\na_n = (1+\\frac{1}{n})^n, b_n = (1+\\frac{1}{n})^{n+1},n\\ge1\n$$\n\n> 详细证明过程见49页\n\n我们可以证明$\\{a_n\\}$单调递增，$\\{b_n\\}$单调递减。两者均收敛于$e$。\n$$\n\\lim_{n\\rightarrow\\infty}b_n = \\lim_{n\\rightarrow\\infty}a_n(1+\\frac{1}{n})=\\lim_{n\\rightarrow\\infty}a_n=e\n$$\n我们可以得到下述重要的不等式：\n$$\n\\left(1+\\frac{1}{n}\\right)^{n}<\\left(1+\\frac{1}{n+1}\\right)^{n+1}<e<\\left(1+\\frac{1}{n+1}\\right)^{n+2}<\\left(1+\\frac{1}{n}\\right)^{n+1}, \\quad \\forall n \\geqslant 1\n$$\n我们令$c_n=1+\\frac{1}{2}+\\cdots+\\frac{1}{n}-\\ln n$，可以证明$\\{c_n\\}$收敛，其极限为$\\gamma$，称为Euler常数，计算表明\n$$\n\\gamma = 0.5772156649\\cdots\n$$\n\n\n下面，我们利用单调数列来研究一般的有界数列。设$\\{a_n\\}$为有界数列，我们要研究它的收敛性。我们不知道$a_n$是否逐渐趋于某个数，一个好的想法就是取考虑$n$很大时$\\{a_n\\}$中最大的最小的项，看看它们是否相近。当然，最大和最小项不一定存在，但是我们可以利用上确界和下确界来分别代替它们。为此，令：\n$$\n\\underline{a}_{n}=\\inf \\left\\{a_{k} \\mid k \\geqslant n\\right\\}, \\quad \\bar{a}_{n}=\\sup \\left\\{a_{k} \\mid k \\geqslant n\\right\\}\n$$\n$\\{\\underline{a}_n\\}$和$\\{\\bar{a}_n\\}$分别是单调递增和单调递减的序列，且：\n$$\n\\underline{a}_n\\le a_n\\le \\bar{a}_n\n$$\n单调数列$\\{\\underline{a}_n\\}$和$\\{\\bar{a}_n\\}$的极限分别称为$\\{a_n\\}$的**下极限**和**上极限**，记为：\n$$\n\\varliminf_{n \\rightarrow \\infty} a_{n}=\\lim _{n \\rightarrow \\infty} \\underline{a}_{n}, \\quad \\varlimsup_{n \\rightarrow \\infty} a_{n}=\\lim _{n \\rightarrow \\infty} \\bar{a}_{n}\n$$\n定理：设$\\{a_n\\}$为有界数列，则下列命题等价：\n\n1. $\\{a_n\\}$收敛\n2. $\\{a_n\\}$的上极限和下极限相等\n3. $\\lim_{n\\rightarrow\\infty}(\\bar{a}_n-\\underline{a}_n)=0$\n\n\n\n命题：设$\\{a_n\\},\\{b_n\\}$为有界数列\n\n1. 如果存在$N_0$，当$n>N_0$时$a_n\\ge b_n$，则$$\\varliminf_{n \\rightarrow \\infty} a_{n} \\geqslant \\varliminf_{n \\rightarrow \\infty} b_{n}, \\quad \\varlimsup_{n \\rightarrow \\infty} a_{n} \\geqslant \\varlimsup_{n \\rightarrow \\infty} b_{n}$$\n2. $$\\overline{\\lim}_{n\\rightarrow\\infty}(a_n+b_n)\\le\\bar{\\lim}_{n\\rightarrow\\infty}a_n+\\bar{\\lim}_{n\\rightarrow\\infty}b_n$$\n\n\n\n设$a_n>0,a_n\\rightarrow A(n\\rightarrow \\infty)$。记$b_n = \\sqrt[n]{a_1a_2\\cdots a_n}$，则\n$$\n\\lim_{n\\rightarrow\\infty}b_n=A\n$$\n由已知条件，任取$\\epsilon>0$，则存在$N$，当$n>N$时，\n$$\n0<a_n<A+\\epsilon\n$$\n于是当$n>N$时，有\n$$\nb_n\\le\\sqrt[n]{a_1a_2\\cdots a_N}(A+\\epsilon)^{\\frac{n-N}{n}} = \\sqrt[n]{a_1a_2\\cdots a_N(A+\\epsilon)^{-N}}(A+\\epsilon)\n$$\n令$n\\rightarrow\\infty$，得\n$$\n\\overline{\\lim}_{n\\rightarrow\\infty}\\le A+\\epsilon\n$$\n同理可证\n$$\n\\underline{\\lim}_{n\\rightarrow\\infty}b_n\\ge A-\\epsilon\n$$\n因为$\\epsilon$是任取的，从而必有\n$$\n\\varlimsup_{n \\rightarrow \\infty} b_{n}=A=\\varliminf_{n \\rightarrow \\infty} b_{n}\n$$\n这说明$\\{b_n\\}$收敛到$A$。\n\n\n\n#### Cauchy准则\n\n定义：设$\\{a_n\\}$为数列，如果任给$\\epsilon>0$，均存在$N=N(\\epsilon)$，当$m,n>N$时，有\n$$\n|a_m-a_n|<\\epsilon\n$$\n则$\\{a_n\\}$为Cauchy数列或基本列。\n\n\n\n例：对于$n\\ge1$，定义\n$$\na_n =1+\\frac{1}{2}+\\cdots+\\frac{1}{n}\n$$\n则$\\{a_n\\}$不是Cauchy列。\n\n证明：对于$n\\ge1$，我们有：\n$$\n\\begin{aligned}\na_{2n}-a_n &= \\frac{1}{n+1}+\\cdots+\\frac{1}{2n}\\\\&\\ge\\frac{1}{2n}+\\cdots+\\frac{1}{2n}=n\\frac{1}{2n}=\\frac{1}{2} \n\\end{aligned}\n$$\n由此定义即知$\\{a_n\\}$不是Cauchy数列。\n\n\n\n命题：Cauchy数列必是有界数列。\n\n证明：按定义，取$\\epsilon=1$，则存在$N$，当$m,n>N$时，有\n$$\n|a_m-a_n|<1\n$$\n令$M=\\max\\{|a_k|+1|1\\le k\\le N+1\\}$，则当$n\\le N$时显然$|a_n|\\le M$；而当$n>N$时，有：\n$$\n|a_n|\\le |a_n-a_{N+1}|+|a_{N+1}|<1+|a_{N+1}|\\le M\n$$\n说明$\\{a_n\\}$是有界数列。\n\n\n\n定理(Cauchy准则)：$\\{a_n\\}$为Cauchy数列当且仅当它是收敛的。\n\n证明：\n\n充分性：设$\\{a_n\\}$收敛到$A$。则任给$\\epsilon>0$，存在$N$，当$n>N$时，有：\n$$\n|a_n-A|<\\frac{1}{2}\\epsilon\n$$\n因此，当$m,n>N$时，有\n$$\n|a_m-a_n|\\le |a_m-A|+|A-a_n|<\\frac{1}{2}\\epsilon+\\frac{1}{2}\\epsilon = \\epsilon\n$$\n这说明$\\{a_n\\}$为Cauchy数列。\n\n必要性：设$\\{a_n\\}$为Cauchy数列，由上面的命题可知$\\{a_n\\}$是有界数列，记$A$为其上极限。我们来说明$\\{a_n\\}$收敛到$A$。\n\n事实上，由于$\\{a_n\\}$为Cauchy数列，任给$\\epsilon>0$，存在$N$，当$m,n>N$时，有\n$$\n|a_m-a_n|<\\frac{1}{2}\\epsilon\n$$\n即\n$$\n-\\frac{1}{2}\\epsilon<a_m-a_n<\\frac{1}{2}\\epsilon,\\forall m,n>N\n$$\n在上式中令$m\\rightarrow\\infty$，利用上极限的保序性，得\n$$\n-\\frac{1}{2}\\epsilon\\le \\overline{\\lim_{m\\rightarrow\\infty}}a_m-a_n\\le\\frac{1}{2}\\epsilon,\\forall n>N\n$$\n即\n$$\n|A-a_n|\\le\\frac{1}{2}\\epsilon<\\epsilon,\\forall n>N\n$$\n这说明，$\\{a_n\\}$收敛到$A$。\n\n\n\n例：\n\n设数列$\\{a_n\\}$满足，存在正数$M$，对于一个$n$都有：\n$$\nA_n = |a_2-a_1|+|a_3-a_2|+\\cdots+|a_{n}-a_{n-1}|\\le M\n$$\n证明数列$\\{a_n\\}$是Cauchy数列，从而是收敛的。\n\n证明：\n$$\nA_{n+1}-A_n=|a_{n+1}-a_{n}|\\ge0\n$$\n所以数列$A_n$为递增数列，又因为$0\\le A_n \\le M$，故$A_n$单调有界，所以数列$A_n$一定有极限(收敛)。因为$A_n$是收敛的，那么它一定是cauthy数列，根据cauthy收敛准则：$\\forall \\epsilon>0,\\exists N$当$m,n>N$时，有：\n$$\n|A_{m}-A_{n}|<\\epsilon\n$$\n因为\n$$\n\\epsilon>|A_m-A_n| = |a_{n+1}-a_n|+\\cdots + |a_m-a_{m-1}|\\ge |a_{m}-a_{n}|\n$$\n故$a_n$为Cauchy序列，故$a_n$收敛。\n\n\n\n#### Stolz公式\n\n引理：设$b_k>0(1\\le k\\le n)$，且\n$$\nm\\le \\frac{a_k}{b_k}\\le M,\\forall 1\\le k\\le n\n$$\n则有\n$$\nm\\le \\frac{a_1+a_2+\\cdots+a_n}{b_1+b_2+\\cdots+b_n}\\le M\n$$\n\n\n定理(Stolz公式之一)：设$\\{x_n\\},\\{y_n\\}$为数列，且$\\{y_n\\}$严格单调地趋于$+\\infty$，如果\n$$\n\\lim_{n\\rightarrow\\infty}\\frac{x_n-x_{n-1}}{y_n-y_{n-1}}=A\n$$\n则\n$$\n\\lim_{n\\rightarrow\\infty}\\frac{x_n}{y_n}=A\n$$\n\n> 证明见课本60页\n\n定理(Stolz公式之二)：设数列$\\{y_n\\}$严格单调递减趋于$0$，数列$\\{x_n\\}$也收敛到$0$，如果：\n$$\n\\lim_{n\\rightarrow\\infty}\\frac{x_n-x_{n-1}}{y_n-y_{n-1}}=A\n$$\n则\n$$\n\\lim_{n\\rightarrow\\infty}\\frac{x_n}{y_n}=A\n$$\n","tags":["数学分析"],"categories":["数学"]},{"title":"线性模型","url":"/2021/12/18/线性模型/","content":"\n## 线性模型\n\n### 基本形式\n\n$$\nf(x) = w^Tx+b\n$$\n\n### 线性回归\n\n对于一元变量：\n线性回归试图学得\n$$\nf\\left(x_{i}\\right)=w x_{i}+b, \\text { 使得 } f\\left(x_{i}\\right) \\simeq y_{i}\n$$\n对于多元变量：\n我们令$\\hat{w}=(w;b)$，相应地，把数据集$D$表示为一个$m\\times(d+1)$大小的矩阵$\\mathrm{X}$，即\n$$\n\\mathbf{X}=\\left(\\begin{array}{ccccc}\n\nx_{11} & x_{12} & \\ldots & x_{1 d} & 1 \\\\\n\nx_{21} & x_{22} & \\ldots & x_{2 d} & 1 \\\\\n\n\\vdots & \\vdots & \\ddots & \\vdots & \\vdots \\\\\n\nx_{m 1} & x_{m 2} & \\ldots & x_{m d} & 1\n\n\\end{array}\\right)=\\left(\\begin{array}{cc}\n\nx_{1}^{T} & 1 \\\\\n\nx_{2}^{T} & 1 \\\\\n\n\\vdots & \\vdots \\\\\n\nx_{m}^{T} & 1\n\n\\end{array}\\right)\n$$\n我们有\n$$\n\\hat{w}^\\star = \\arg_{\\hat{w}}\\min(y-\\mathrm{X}\\hat{w})^T(y-\\mathrm{X}\\hat{w})\n$$\n令$E_{\\hat{w}}=(y-\\mathrm{X}\\hat{w})^T(y-\\mathrm{X}\\hat{w})$，对$\\hat{w}$求导得到：\n$$\n\\frac{\\partial E_{\\hat{w}}}{\\partial\\hat{w}} = 2\\mathrm{X}^T(\\mathrm{X}\\hat{w}-y)\n$$\n\n当$\\mathrm{X}^T\\mathrm{X}$为满秩矩阵或正定矩阵时，我们有\n$$\n\\hat{w}^\\star = (\\mathrm{X}^T\\mathrm{X})^{-1}\\mathrm{X}^Ty\n$$\n更一般地，考虑单调可微函数$g(\\cdot)$，令\n$$\ny = g^{-1}(w^Tx+b)\n$$\n这样的模型称为\"广义线性模型\"。\n\n### 对数几率回归\n\n$$\ny = \\frac{1}{1+e^{-(w^Tx+b)}}\n$$\n\n变化得\n$$\n\\ln\\frac{y}{1-y} = w^Tx+b\n$$\n若将$y$视为样本$x$作为正例的可能性，则$1-y$是其反例的可能性，两者的比值：\n$$\n\\frac{y}{1-y}\n$$\n称为几率，反映了$x$作为正例的相对可能性。\n\n我们令\n$$\n\\begin{aligned}\np(y=1|x) &= \\frac{e^{w^Tx+b}}{1+e^{w^Tx+b}}\\\\\np(y=0|x)&=\\frac{1}{1+e^{w^Tx+b}}\n\\end{aligned}\n$$\n我们采用最大似然法来估计$w$和$b$：\n$$\n\\ell(w,b) = \\sum_{i=1}^m\\ln p(y_i|x_i;w,b)\n$$\n我们令$\\beta=(w;b),\\hat{x} = (x;1)$，即$w^Tx+b=\\beta^T\\hat{x}$，令$p_1(\\hat{x};\\beta)=p(y=1|\\hat{x};\\beta),p_0(\\hat{x};\\beta)=p(y=0|\\hat{x};\\beta)=1-p_1(\\hat{x};\\beta)$\n我们将似然项写为\n$$\np(y_i|x_i;w,b) = p_1(\\hat{x}_i;\\beta)^{y_i}p_0(\\hat{x}_i;\\beta)^{1-y_i}\n$$\n代入似然函数，我们得\n$$\n\\ell(\\boldsymbol{\\beta})=\\sum_{i=1}^{m}\\left(-y_{i} \\boldsymbol{\\beta}^{\\mathrm{T}} \\hat{\\boldsymbol{x}}_{i}+\\ln \\left(1+e^{\\boldsymbol{\\beta}^{\\mathrm{T}} \\hat{\\boldsymbol{\\alpha}}_{i}}\\right)\\right)\n$$\n最小化似然函数可以用**梯度下降法**和**牛顿法**。\n\n牛顿法\n根据二阶泰勒展开我们有：\n$$\nf(x+\\Delta x) = f(x) - \\Delta x^T\\nabla f + \\Delta x^T\\nabla^2f\\Delta x\n$$\n对其进行求导，\n$$\n-\\nabla f + 2\\Delta x\\nabla^2 f=0\n$$\n\n\n使导数为零得\n$$\n\\Delta x = \\nabla^2f^{-1}\\nabla f\n$$\n所以以牛顿法为例，其第$t+1$轮迭代解的更新公式为：\n$$\n\\beta^{t+1}=\\beta^{t}-\\left(\\frac{\\partial^{2} \\ell(\\beta)}{\\partial \\beta \\partial \\beta^{\\mathrm{T}}}\\right)^{-1} \\frac{\\partial \\ell(\\boldsymbol{\\beta})}{\\partial \\boldsymbol{\\beta}}\n$$\n\n### LDA判别分析\n\nLDA的思想非常朴素：给定训练例集，设法将样例投影到一条直线，使得同类样例的投影点尽可能接近、异类样例的投影点尽可能远离。\n\n![](https://static01.imgkr.com/temp/4b179e7140244df5ad861570daec95ae.png)\n\n给定数据集$D=\\{(x_i,y_i)\\}_{i=1}^m,y_i\\in \\{0,1\\}$。令$X_i,\\mu_i,\\Sigma_i$分别表示第$i\\in \\{0,1\\}$类示例的集合、均值向量、协方差矩阵。若将数据投影到直线$w$上，则两类样本的中心再直线上的投影分别为$w^T\\mu_0$和$w^T\\mu_1$，若将所有的点都投影到直线上，则两类样本的协方差分别为$w^T\\Sigma_0w$和$w^T\\Sigma_1 w$。\n\n欲使同类样例的投影点尽可能接近，可以让$w^T\\Sigma_0w+w^T\\Sigma_1w$尽可能小；而欲使异类样例的投影点尽可能原理，可以使$||w^T\\mu_0-w^T\\mu_1||_2^2$尽可能大，同时考虑两者得：\n$$\n\\begin{aligned}\nJ &= \\frac{||w^T\\mu_0-w^T\\mu_1||_2^2}{w^T\\Sigma_0w+w^T\\Sigma_1w}\\\\\n&= \\frac{w^T(\\mu_0-\\mu_1)(\\mu_0-\\mu_1)^Tw}{w^T(\\Sigma_0+\\Sigma_1)w}\n\\end{aligned}\n$$\n定义\"类内散度矩阵\"\n$$\n\\begin{aligned}\nS_w &= \\Sigma_0+\\Sigma_1\\\\\n&= \\sum_{x\\in X_0}(x-\\mu_0)(x-\\mu_0)^T+\\sum_{x\\in X_1}(x-\\mu_1)(x-\\mu_1)^T\n\\end{aligned}\n$$\n以及类间散度矩阵：\n$$\nS_b = (\\mu_0-\\mu_1)(\\mu_0-\\mu_1)^T\n$$\n则可以重写为：\n$$\nJ = \\frac{w^TS_bw}{w^TS_ww}\n$$\n这就是LDA欲最大化得目标，即$S_b$与$S_w$的广义瑞利熵。\n\n\n\n如何求解$w$呢？我们发现，如果将$w$乘以常数$\\alpha$，则分子分母上的$\\alpha$约去，所以最优解与$\\alpha$的长度无关而与其方向有关。所以我们令$w^TS_ww=1$($S_w$为常数)。则上式等价于：\n$$\n\\begin{aligned}\n\\min_w&\\quad -w^TS_bw\\\\\n\\operatorname{s.t.}&\\quad w^TS_ww=1\n\\end{aligned}\n$$\n根据拉格朗日乘子法，我们有：\n$$\nL(w,\\lambda) = -w^TS_bw+\\lambda(w^TS_ww-1)\n$$\n对其求导得：\n$$\n-2S_bw+2\\lambda S_ww=0\n$$\n即\n$$\nS_bw = \\lambda S_ww\n$$\n由于我们想要求解的只有$w$，而$\\lambda$这个拉格朗日乘子具体取多少值都无所谓，于是我们任意设定$\\lambda$来配合我们求解$w$。我们注意到：\n$$\nS_bw = (\\mu_0-\\mu_1)(\\mu_0-\\mu_1)^Tw\n$$\n如果我们令$\\lambda$恒等于$(\\mu_0-\\mu_1)^Tw$，那么上式即可改写为：\n$$\nS_bw = \\lambda(\\mu_0-\\mu_1)\n$$\n代入得：\n$$\nw = S_w^{-1}(\\mu_0-\\mu_1)\n$$\n考虑到数值稳定性，在实践中通常是对$S_w$进行奇异值分解，即$S_w^{-1}=U\\Sigma V^T$，然后得$S_w^{-1}=V\\Sigma^{-1}U^T$。\n\n\n\n可以将LDA推广到多分类任务中。假定存在$N$个类，且第$i$类示例数为$m_i$，总样本数为$m$。我们先定义\"全局散度矩阵\"：\n$$\n\\begin{aligned}\nS_t &= S_b+S_w\\\\\n&= \\sum_{i=1}^m(x_i-\\mu)(x_i-\\mu)^T\n\\end{aligned}\n$$\n其中$\\mu$是所有示例的均值向量。将类内散度矩阵$S_w$定义为每个类别的散度矩阵之和，即\n$$\nS_w=\\sum_{i=1}^NS_{w_i}\n$$\n其中\n$$\nS_{w_i} = \\sum_{x\\in X_i}(x-\\mu_i)(x-\\mu_i)^T\n$$\n我们可以推得：\n$$\n\\begin{aligned}\n\\mathbf{S}_{b} &=\\mathbf{S}_{t}-\\mathbf{S}_{w} \\\\\n&=\\sum_{i=1}^{m}\\left(\\boldsymbol{x}_{i}-\\boldsymbol{\\mu}\\right)\\left(\\boldsymbol{x}_{i}-\\boldsymbol{\\mu}\\right)^{\\mathrm{T}}-\\sum_{i=1}^{N} \\sum_{\\boldsymbol{x} \\in X_{i}}\\left(\\boldsymbol{x}-\\boldsymbol{\\mu}_{i}\\right)\\left(\\boldsymbol{x}-\\boldsymbol{\\mu}_{i}\\right)^{\\mathrm{T}} \\\\\n&=\\sum_{i=1}^{N}\\left(\\sum_{\\boldsymbol{x} \\in X_{i}}\\left((\\boldsymbol{x}-\\boldsymbol{\\mu})(\\boldsymbol{x}-\\boldsymbol{\\mu})^{\\mathrm{T}}-\\left(\\boldsymbol{x}-\\boldsymbol{\\mu}_{i}\\right)\\left(\\boldsymbol{x}-\\boldsymbol{\\mu}_{i}\\right)^{\\mathrm{T}}\\right)\\right) \\\\\n&=\\sum_{i=1}^{N}\\left(\\sum_{\\boldsymbol{x} \\in X_{i}}\\left((\\boldsymbol{x}-\\boldsymbol{\\mu})\\left(\\boldsymbol{x}^{\\mathrm{T}}-\\boldsymbol{\\mu}^{\\mathrm{T}}\\right)-\\left(\\boldsymbol{x}-\\boldsymbol{\\mu}_{i}\\right)\\left(\\boldsymbol{x}^{\\mathrm{T}}-\\boldsymbol{\\mu}_{i}^{\\mathrm{T}}\\right)\\right)\\right) \\\\\n&=\\sum_{i=1}^{N}\\left(\\sum_{\\boldsymbol{x} \\in X_{i}}\\left(\\boldsymbol{x} \\boldsymbol{x}^{\\mathrm{T}}-\\boldsymbol{x} \\boldsymbol{\\mu}^{\\mathrm{T}}-\\boldsymbol{\\mu} \\boldsymbol{x}^{\\mathrm{T}}+\\boldsymbol{\\mu} \\boldsymbol{\\mu}^{\\mathrm{T}}-\\boldsymbol{x} \\boldsymbol{x}^{\\mathrm{T}}+\\boldsymbol{x} \\boldsymbol{\\mu}_{i}^{\\mathrm{T}}+\\boldsymbol{\\mu}_{i} \\boldsymbol{x}^{\\mathrm{T}}-\\boldsymbol{\\mu}_{i} \\boldsymbol{\\mu}_{i}^{\\mathrm{T}}\\right)\\right) \\\\\n&=\\sum_{i=1}^{N}\\left(\\sum_{\\boldsymbol{x} \\in X_{i}}\\left(-\\boldsymbol{x} \\boldsymbol{\\mu}^{\\mathrm{T}}-\\boldsymbol{\\mu} \\boldsymbol{x}^{\\mathrm{T}}+\\boldsymbol{\\mu} \\boldsymbol{\\mu}^{\\mathrm{T}}+\\boldsymbol{x} \\boldsymbol{\\mu}_{i}^{\\mathrm{T}}+\\boldsymbol{\\mu}_{i} \\boldsymbol{x}^{\\mathrm{T}}-\\boldsymbol{\\mu}_{i} \\boldsymbol{\\mu}_{i}^{\\mathrm{T}}\\right)\\right) \\\\\n&=\\sum_{i=1}^{N}\\left(-\\sum_{\\boldsymbol{x} \\in X_{i}} \\boldsymbol{x} \\boldsymbol{\\mu}^{\\mathrm{T}}-\\sum_{\\boldsymbol{x} \\in X_{i}} \\boldsymbol{\\mu} \\boldsymbol{x}^{\\mathrm{T}}+\\sum_{\\boldsymbol{x} \\in X_{i}} \\boldsymbol{\\mu} \\boldsymbol{\\mu}^{\\mathrm{T}}+\\sum_{\\boldsymbol{x} \\in X_{i}} \\boldsymbol{x} \\boldsymbol{\\mu}_{i}^{\\mathrm{T}}+\\sum_{\\boldsymbol{x} \\in X_{i}} \\boldsymbol{\\mu}_{i} \\boldsymbol{x}^{\\mathrm{T}}-\\sum_{\\boldsymbol{x} \\in X_{i}} \\boldsymbol{\\mu}_{i} \\boldsymbol{\\mu}_{i}^{\\mathrm{T}}\\right) \\\\\n&=\\sum_{i=1}^{N}\\left(-m_{i} \\boldsymbol{\\mu}_{i} \\boldsymbol{\\mu}^{\\mathrm{T}}-m_{i} \\boldsymbol{\\mu} \\boldsymbol{\\mu}_{i}^{\\mathrm{T}}+m_{i} \\boldsymbol{\\mu} \\boldsymbol{\\mu}^{\\mathrm{T}}+m_{i} \\boldsymbol{\\mu}_{i} \\boldsymbol{\\mu}_{i}^{\\mathrm{T}}+m_{i} \\boldsymbol{\\mu}_{i} \\boldsymbol{\\mu}_{i}^{\\mathrm{T}}-m_{i} \\boldsymbol{\\mu}_{i} \\boldsymbol{\\mu}_{i}^{\\mathrm{T}}\\right) \\\\\n&=\\sum_{i=1}^{N}\\left(-m_{i} \\boldsymbol{\\mu}_{i} \\boldsymbol{\\mu}^{\\mathrm{T}}-m_{i} \\boldsymbol{\\mu} \\boldsymbol{\\mu}_{i}^{\\mathrm{T}}+m_{i} \\boldsymbol{\\mu} \\boldsymbol{\\mu}^{\\mathrm{T}}+m_{i} \\boldsymbol{\\mu}_{i} \\boldsymbol{\\mu}_{i}^{\\mathrm{T}}\\right) \\\\\n&=\\sum_{i=1}^{N} m_{i}\\left(-\\boldsymbol{\\mu}_{i} \\boldsymbol{\\mu}^{\\mathrm{T}}-\\boldsymbol{\\mu} \\boldsymbol{\\mu}_{i}^{\\mathrm{T}}+\\boldsymbol{\\mu} \\boldsymbol{\\mu}^{\\mathrm{T}}+\\boldsymbol{\\mu}_{i} \\boldsymbol{\\mu}_{i}^{\\mathrm{T}}\\right) \\\\\n&=\\sum_{i=1}^{N} m_{i}\\left(\\boldsymbol{\\mu}_{i}-\\boldsymbol{\\mu}\\right)\\left(\\boldsymbol{\\mu}_{i}-\\boldsymbol{\\mu}\\right)^{\\mathrm{T}}\n\\end{aligned}\n$$\n即\n$$\nS_b=\\sum_{i=1}^{N} m_{i}\\left(\\boldsymbol{\\mu}_{i}-\\boldsymbol{\\mu}\\right)\\left(\\boldsymbol{\\mu}_{i}-\\boldsymbol{\\mu}\\right)^{\\mathrm{T}}\n$$\n\n\n我们常用的是实现目标：\n$$\n\\max_W\\frac{\\operatorname{tr}(W^TS_bW)}{\\operatorname{tr}(W^TS_wW)}\n$$\n此公式是二维情况下的推广形式，证明如下：设$W = (w_1,w_2,\\cdots,w_i,\\cdots,w_{N-1})\\in \\mathbb{R}^{d\\times(N-1)}$，其中$w_i\\in \\mathbb{R}^{d\\times 1}$为$d$行$1$列的列向量，则\n$$\n\\begin{cases}\n\\operatorname{tr}(W^TS_bW) = \\sum_{i=1}^{N-1}w_i^TS_bw_i\\\\\n\\operatorname{tr}(W^TS_wW)=\\sum_{i=1}^{N-1}w_i^TS_wW_i\n\\end{cases}\n$$\n所以上述实现目标可以变形为：\n$$\n\\max_W\\frac{\\sum_{i=1}^{N-1}w_i^TS_bw_i}{\\sum_{i=1}^{N-1}w_i^TS_wW_i}\n$$\n可以看出是对二分类结果的推广。\n\n上式可以通过如下方式求解：\n$$\nS_bW = \\lambda S_wW\n$$\n这个问题与上面二维的时候大致一样，我们也固定分母为$1$，那么优化问题就等价于：\n$$\n\\begin{aligned}\n\\min_W\\quad&-\\operatorname{tr}(W^TS_bW)\\\\\n\\operatorname{s.t.}\\quad&\\operatorname{tr}(W^TS_wW)=1\n\\end{aligned}\n$$\n应用拉格朗日乘子法，上述优化问题的拉格朗日函数为：\n$$\nL(W,\\lambda) =-\\operatorname{tr}(W^TS_bW)+\\lambda(\\operatorname{tr}(W^TS_wW)-1)\n$$\n根据矩阵求导公式：\n$$\n\\frac{\\partial}{\\partial X}\\operatorname{tr}(X^TBX) = (B+B^T)X\n$$\n对上式求导令导数等于$0$，得：\n$$\nS_b W = \\lambda S_wW\n$$\n那么$W$为$S_w^{-1}S_b$的$N-1$个最大广义特征值所对应的特征向量组成的矩阵。\n\n如果将$W$视为一个投影矩阵，则多分类LDA将样本投影到$N-1$维空间，$N-1$通常远小于数据原有的属性数。并且在投影的过程中使用了类别的信息，故LDA也常用来降维。\n\n\n\n","tags":["机器学习"],"categories":["西瓜书"]},{"title":"实数与极限","url":"/2021/12/18/数学分析/","content":"\n## 实数与极限\n\n我们先复习一下函数(映射)的概念，假设有两个集合$X$和$Y$，我们定义一个对应法则$f$，记为$X\\xrightarrow{f}Y$，使得对于$\\forall x\\in X,x\\xrightarrow{f}y\\in Y$。则$X$为定义域，$Y$为到达域。\n下面我们引入一个记号$f(X):=\\{y\\in Y|\\exists x(\\left(x\\in X)\\wedge(y=f(x))\\right)\\}$，称为值域。\n我们微积分研究的对象：$X\\xrightarrow{f}Y\\subset \\mathbb{R}^n$，其中$f$为可微函数。\n\n### 实数的基本公理\n\n加法公理：\n\n+ $\\exists$零元$0$，使得$0+x=x+0=x$\n+ $\\exists$负元$-x$，使得$x+(-x)=(-x)+x=0$\n+ 结合律：$\\forall x,y,z\\in \\mathbb{R}$，使得$(x+y)+z=x+(y+z)$\n\n\n\n集合$G$上存在加法运算，且满足上述三条加法公理，则说明$G$为一个群。\n\n\n\n+ 交换律：对于$\\forall x,y\\in R,x+y=y+x$\n\n如果运算还满足交换律，则群成为交换群(阿贝尔群)。\n\n乘法公理：\n\n+ 单位元(中性元)：$1\\in \\mathbb{R}-0$，$\\forall x\\in \\mathbb{R},x\\cdot1=1\\cdot x=x$\n+ $\\forall x\\in\\mathbb{R}-0,\\exists x^{-1}\\in\\mathbb{R},x^{-1}\\cdot x=x\\cdot x^{-1}=1$\n+ $\\forall x,y,z\\in\\mathbb{R},(x\\cdot y)\\cdot z = x\\cdot(y\\cdot z)$\n+ $\\forall x,y\\in\\mathbb{R},x\\cdot y = y\\cdot x$\n\n则$\\mathbb{R}-0$关于乘法构成一个群。\n\n有了加法和乘法，我们就可以定义加法和乘法的附加公理(分配律)：\n$$\n\\forall x,y,z\\in \\mathbb{R},z\\cdot(x+y) = (x+y)\\cdot z = x\\cdot z + y\\cdot z\n$$\n\n\n\n域\n如果集合上定义了满足上述公理的$+$和$\\cdot$运算以及结合律，则该集合为域(代数域)。\n\n\n\n序公理：我们在集合上定义一个不等关系$\\le$，使得$\\forall x,y\\in\\mathbb{R},(x\\le y)\\vee(y\\le x)$\n\n+ $\\forall x\\in \\mathbb{R},x\\le x$\n+ $(x\\le y)\\wedge(y\\le x)\\Rightarrow(x=y)$\n+ $(x\\le y)\\wedge(y\\le z)\\Rightarrow (x\\le z)$\n+ $(x\\le y)\\vee (y\\le x)$\n\n加法和序也存在附加公理：\n$$\n(x\\le y)\\Rightarrow (x+z)\\le (y+z)\n$$\n乘法与序也存在附加公理：\n$$\n(0\\le x)\\wedge(0\\le y)\\Rightarrow (0\\le xy)\n$$\n\n\n如果集合$X$上存在不等关系$\\le$，满足上述公理的前三条，我们称$X$为一个偏序集，如果还满足第四条，我们称他为线性的序集。\n\n\n\n完备性公理(连续性公理)：给定集合$X,Y\\subset \\mathbb{R}$，使得$\\forall x\\in X,\\forall y \\in Y,x\\le y$，则$\\exists c\\in \\mathbb{R}$满足$x\\le c\\le y,\\forall x\\in X,\\forall y\\in Y$。\n\n给定一个集合，满足加法公理、乘法公理，定义了一个满足序公理的序关系，满足完备性公理，则成为是实数集的一个具体实现。\n\n如十进制的小数是$\\mathbb{R}$的一个实现，还有数轴。\n\n### 实数运算的代数性质\n\n我们首先研究加法。\n首先我们证明$\\mathbb{R}$上存在唯一的加法零元。\n证明：假设存在两个加法零元$0_1,0_2$，则\n$$\n0_1 = 0_1+0_2 = 0_2+0_1=0_2\n$$\n所以$0_1=0_2$，所以存在唯一的加法零元。\n\n下面证明$\\forall x\\in \\mathbb{R},\\exists$唯一的负元。\n证明：假设存在两个负元$x_1,x_2$，则\n$$\nx_1 = x_1 + 0 = x_1 + (x+x_2) = (x_1+x)+x_2 = 0+x_2 = x_2\n$$\n所以存在唯一的负元。\n\n紧接着证明方程$a+x=b$有唯一解$x = b + (-a) = b-a$\n证明：因为$a\\in \\mathbb{R}$，所以$\\exists!$唯一负元$-a$。\n$$\n(a+x=b)\\Rightarrow((-a)+a+x=(-a)+b) \\Rightarrow x = b + (-a) = b-a\n$$\n\n下面我们研究乘法。\n首先我们需要证明$\\mathbb{R}$上存在唯一的$1$\n证明：假设存在两个单位元$1_1,1_2$，则\n$$\n1_1  = 1_1\\cdot 1_2 = 1_2\\cdot 1_1 = 1_2\n$$\n\n下面我们证明$\\forall x\\in\\mathbb{R}-0,\\exists!$逆元$x^{-1}$\n证明：假设存在两个逆元$x_1,x_2$，则\n$$\nx_1 = x_1\\cdot 1 = x_1\\cdot(x\\cdot x_2) = (x_1\\cdot x)\\cdot x_2 = x_2\n$$\n所以$x_1=x_2$\n\n紧接着我们证明$\\forall a\\in \\mathbb{R}-0,a\\cdot x=b$存在唯一的解$x = b\\cdot a^{-1} = a^{-1}\\cdot b$。\n证明：因为$a\\in \\mathbb{R}-0$，所以存在唯一的逆元$a^{-1}$，所以\n$$\n(a\\cdot x = b)\\Rightarrow (a^{-1}\\cdot(a\\cdot x) = a^{-1}\\cdot b)\\Rightarrow (x = a^{-1}\\cdot b)\n$$\n\n下面我们证明加法与乘法相联系的公理的推论\n$\\forall x\\in \\mathbb{R},x\\cdot 0 = 0$\n证明：\n$x\\cdot 0 =x\\cdot(0+0) = x\\cdot 0+x\\cdot 0$，两边同时加上逆元$-x\\cdot 0$，得$0 = x\\cdot 0$。\n\n$x\\in\\mathbb{R}-0\\Rightarrow x^{-1}\\neq0$\n证明：若$x^{-1}=0\\Rightarrow x\\cdot x^{-1} = x\\cdot 0 = 0$，得$1=0$，矛盾。\n\n$x\\cdot y=0\\Rightarrow(x=0)\\vee(y=0)$\n证明：设$y\\neq 0$，所以$y^{-1}\\neq 0$，所以\n$$\n(x\\cdot y = 0)\\Rightarrow(x\\cdot y\\cdot y^{-1} = 0\\cdot y^{-1}=0)\\Rightarrow x = 0\n$$\n同理可证当$x\\neq0$时$y=0$。\n\n$-x = -1\\cdot x$\n证明：\n$x + (-1\\cdot x) = 1\\cdot x + -1\\cdot x = (1+(-1))\\cdot x = 0\\cdot x = 0$\n所以$-1\\cdot x$为$x$的逆元，所以$-x = -1\\cdot x$。\n\n$(-1)\\cdot (-x)=x$\n证明：我们只需证明$(-1)\\cdot(-x)$为$-x$的逆元即可。\n$$\n(-1)\\cdot(-x) + (-x) = ((-1)+1)\\cdot(-x) = 0\\cdot (-x) = 0\n$$\n得证。\n\n$(-x)\\cdot(-x)=x\\cdot x$\n证明：\n$$\n(-x)\\cdot (-x) = ((-1)\\cdot x)\\cdot(-x) = (x\\cdot (-1))\\cdot(-x) = x\\cdot((-1)\\cdot(-x)) = x\\cdot x\n$$\n\n下面我们研究序公理的推论\n\n$x\\le y$若$x\\neq y$，则记作$x<y$，称为严格不等式。\n那么对于$x,y\\in\\mathbb{R}$，$x<y,x=y,y<x$只有一个成立。\n\n\n下面我们证明：\n\n+ $(x<y)\\wedge(y\\le z)\\Rightarrow x<z$\n+ $(x\\le y)\\wedge(y<z)\\Rightarrow x<z$\n\n我们证明第一个，第二个与其类似：\n$$\n(x<y)\\wedge(y\\le z)\\Rightarrow((x\\neq y)\\wedge(x\\le y)\\wedge (y\\le z))\\Rightarrow((x\\neq y)\\wedge(x\\le z))\n$$\n若$x=z$，我们有$(x<y)\\wedge(y\\le x)\\Rightarrow(x\\neq y)\\wedge(x\\le y)\\wedge(y\\le x)\\Rightarrow(x\\neq y)\\wedge(x=y)$，矛盾。所以$x\\neq z$，所以$x<z$。\n\n### 序公理与加法公理以及乘法公理的推论\n\n#### 序公理和加法公理\n\n我们需要证明以下定理：\n\n1. $(x>y)\\Rightarrow (x+z>y+z)$\n2. $(x>0)\\Rightarrow (-x<0)$\n3. $(x>y)\\wedge(z\\ge w)\\Rightarrow (x+z)>(y+w)$\n4. $(x\\ge y)\\wedge (z>w)\\Rightarrow (x+z)>(y+w)$\n\n下面我们开始证明：\n\n第一个：\n$$\n(x>y)\\Rightarrow (x\\ge y)\\Rightarrow (x+z\\ge y+z)\n$$\n所以我们只需要证明$(x+z\\neq y+z)$，即证明$x\\neq y$，所以得证。\n\n第二个：\n$$\n(x>0)\\Rightarrow (x+(-x)>0+(-x))\\Rightarrow (0>-x)\n$$\n\n\n得证。\n\n第三个和第四个证明一个即可：\n$$\n((x>y)\\wedge(z\\ge w))\\Rightarrow ((x+z>y+z)\\wedge(z+y\\ge w+y))\\Rightarrow ((x>y)>(y+w)) \n$$\n得证。\n\n#### 序公理与乘法公理\n\n1. $(x>0)\\wedge(y>0)\\Rightarrow (xy>0)$\n2. $(x<0)\\wedge(y<0)\\Rightarrow (xy>0)$\n3. $(x>0)\\wedge(y<0)\\Rightarrow (xy<0)$\n4. $(x>y)\\wedge(z> 0)\\Rightarrow(xz>yz)$\n5. $(x>y)\\wedge(z< 0)\\Rightarrow(xz<yz)$\n\n\n\n我们先证明第一条：\n$$\n(x>0)\\wedge(y>0)\\Rightarrow (x\\ge 0)\\wedge(y\\ge0)\\Rightarrow xy\\ge 0\n$$\n所以我们至于要证明$xy\\neq0$，根据之前的定理我们知道$(xy=0)\\Rightarrow(x=0)\\vee(y=0)$，而我们条件中$x,y$都不等于$0$，所以得证。\n\n\n\n第二条：\n$$\n((x<0)\\wedge(y<0))\\Rightarrow((-x>0)\\wedge(-y>0))\\Rightarrow(-x)\\cdot(-y)>0\\Rightarrow(xy)>0\n$$\n第三条与第二条思路相同，不在赘述。\n\n下面证明第四条：\n$$\n((x>y)\\wedge(z>0))\\Rightarrow((x-y>0)\\wedge(z>0))\\Rightarrow((x-y)z>0)\\Rightarrow(xz-yz)>0\\Rightarrow xz>yz\n$$\n第五条类似。\n\n\n\n下面我们证明$1>0$：\n\n我们知道$1>0,1=0,1<0$之中只能有一个成立，我们已经知道$1\\neq 0$。所以我们假设$1<0$，那我们有：\n$$\n(1<0)\\wedge(1<0)\\Rightarrow(1\\cdot1>0)\\Rightarrow 1>0\n$$\n互相矛盾，所以$1>0$。\n\n\n\n下面我们再证明：\n\n1. $(0<x)\\Rightarrow(x^{-1}>0)$\n2. $(0<x<y)\\Rightarrow(0<y^{-1}<x^{-1})$\n\n\n\n我们先证明第一条：\n\n我们假设$x^{-1}<0$，则\n$$\n(0<x)\\wedge(x^{-1}<0)\\Rightarrow (x\\cdot x^{-1})<0\\Rightarrow 1<0\n$$\n矛盾，得证。\n\n第二条：\n$$\n(0<x<y)\\Rightarrow(y^{-1}>0)\\wedge(x>0)\\Rightarrow(x\\cdot y^{-1}>0)\\Rightarrow(y^{-1}>x^{-1})\n$$\n得证。\n\n\n\n下面我们讨论正数和负数：\n\n所有大于零的数为正数，如果$x$为正数，则$x^{-1}$也为正数。\n\n\n\n负数：小于零的数为负数。\n\n### 完备性公理与数集上下确界的存在性\n\n定义：$X\\subset \\mathbb{R}$，若$\\exists c\\in \\mathbb{R},\\forall x\\in X,x\\le c$，则称$X$上有界集合，称$c$为$X$的上界；若$\\forall x\\in X,x\\ge c$，则称$X$下有界集合，称$c$为$X$的下界。\n\n\n\n定义(有界集)：既上有界也下有界。即$\\exists c_1,c_2\\in \\mathbb{R}$使得$\\forall x\\in X,c_1<x<c_2$。\n\n\n\n定义(最大元、极大元)：集合$X\\subset\\mathbb{R}$，若$\\exists a\\in X$使得$\\forall x\\in X,x\\le a$。称$a$为$X$的最大元。即：\n$$\n\\max X := (a\\in X)\\wedge(\\forall x\\in X,x\\le a)\n$$\n定义(最小元、极小元)：\n$$\n\\min X:= ((a\\in X)\\wedge (\\forall x\\in X,x\\ge a))\n$$\n\n定义(上确界)：上界当中的最小元，记为$\\sup X$\n$$\n(\\sup X=s):= (\\forall x\\in X,x\\le s)\\wedge (\\forall s^{\\prime}<s,\\exists x^{\\prime}\\in X,x^{\\prime}>s^{\\prime})\n$$\n\n\n定义(下确界)：下界当中的最大元，记为$\\inf X$。\n\n\n$$\n(\\inf X=s):= (\\forall x\\in X,x\\ge s)\\wedge (\\forall s^{\\prime}>s,\\exists x^{\\prime}\\in X,x^{\\prime}<s^{\\prime})\n$$\n例：$[0,1)$的的上确界为$1$。\n\n因为$\\forall x\\in [0,1),x \\le1$，并且$\\forall a<1$，我们有$\\frac{1+a}{2}\\in [0,1)$，但是$\\frac{1+a}{2}>a$。\n\n\n\n则如果一个集合存在一个最大元，那么它必为上确界；存在最小元，那么它必为下确界。\n\n\n\n那么什么时候存在上确界和下确界呢？\n\n上确界引理：若$X$为有上确界非空集合，则$X$有唯一的上确界。\n\n证明：\n\n我们关于上确界还有一个定义：\n$$\n\\sup X := \\min \\{c|\\forall x\\in X,x\\le c\\}\n$$\n由此可以看出上确界就是上界集合中的最小元。我们首先证明唯一性，假设有两个上确界$c_1,c_2$，因为上确界是上界集合中的最小元，所以我们有$c_1\\le c_2,c_2\\le c_1$，所以$c_1=c_2$。所以如果存在上确界的话，上确界就是唯一的。\n\n下面我们证明存在上确界。\n\n我们设$Y=\\{y:\\forall x\\in X,x\\le y\\}$，由此可以看出$y$为$X$的上界集合，因为$X$有上界，所以$Y$非空。所以我们就有$\\forall x\\in X,\\forall y\\in Y,x\\le y$。根据完备性公理，$\\exists x\\in \\mathbb{R}$，使得$\\forall x\\in X,\\forall y\\in Y,x\\le c\\le y$，由此我们可以推出$c\\in Y$，并且$c$为$Y$的最小元，所以上确界存在。\n\n得证。\n\n\n\n下确界的证明类似。\n\n### 完备性公理相关的基本引理\n\n定义(序列)：如果函数$f:\\mathbb{N}\\rightarrow X$，则称$f(n)$为序列。记作：\n$$\nx_n:= f(n)\n$$\n定义(集列套)：对于$X_i\\subset \\mathbb{R}$，如果$\\forall n\\in \\mathbb{N},X_{i}\\supset X_{i+1}$，则称其为集列套。\n\n\n\n闭区间套引理：若闭区间套$I_1\\supset I_1\\supset I_3\\supset \\cdots$，存在$c\\in \\mathbb{R}$使得$c\\in I_i,\\forall i\\in \\mathbb{N}$。如果对于$\\forall \\epsilon >0, \\exists I_n$使得$|I_n|<\\epsilon$，则$c$唯一。\n\n证明：\n\n记$I_n=[a_i,b_i]$，$\\forall I_n=[a_n,b_n],I_m=[a_m,b_m]$，我们都有$a_n\\le b_m$。我们令$X=\\{a_n\\},Y=\\{b_n\\}$。所以对于$\\forall a_n\\in X,\\forall b_m\\in Y$，我们都有$a_n\\le b_m$，根据完备性定理：$\\exists c\\in \\mathbb{R}$，使得$\\forall a_n\\in X,\\forall b_m\\in Y$，都有$a_n\\le c\\le b_m$，我们取$m=n$，则$a_n\\le c\\le b_n$，即$c\\in I_n$。则存在性就证明完了。\n\n若$c_1<c_2\\in I_n$，则$a_n\\le c_1<c_2\\le b_n$，则$c_2-c_1< b_n-a_n=|I_n|$，我们取$\\epsilon = \\frac{c_2-c_1}{2}$，则不存在这样的$I_n$。得证。\n\n\n\n定义：如果$S=\\{X\\}$，即$S$为集合的集合，令$Y\\subset \\bigcup_{X\\in S}X$，则称$S$为$Y$的一个覆盖。这就说明：$\\forall y\\in Y,\\exists X\\in S$使得$y\\in X$。\n\n有限覆盖引理：如果$I=[a,b],I\\subset \\bigcup U_n$，其中$U_n=[\\alpha_n,\\beta_n)$，则$\\exists U_1\\cdots U_k$使得$I\\subset \\bigcup_{i=1}^k U_i$。\n\n证明感觉没怎么看懂，等之后再补上。主要是运用反证法。\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n","tags":["数学分析"],"categories":["数学"]},{"title":"贝叶斯滤波与卡尔曼滤波","url":"/2021/12/16/贝叶斯滤波与卡尔曼滤波/","content":"\n## 随机过程与卡尔曼滤波\n\n随机过程：$x_1,x_2,\\cdots,x_n$是随机变量，但是不相互独立（无法做随机试验）。\n\n随机实验：\n\n1. 在相同条件下，试验可以重复进行（独立性）\n2. 一次试验，结果不确定，所有可能的结果已知\n3. 试验之前，试验结果预先未知\n\n随机过程中独立性不存在，我们无法对概率进行赋值。\n\n\n\n补充：大数定律\n\n假设我们进行抛硬币试验：$P(正) = \\frac{1}{2}, P(反) = \\frac{1}{2}$，抛硬币，试验可重复进行。\n\n由大数定律，设$n$为试验次数，$\\mu$为正面朝上的次数。\n\n大数定律：在$n$次独立的试验中，对于任意正数$\\epsilon$，有\n$$\n\\lim_{n\\rightarrow \\infty}P(|\\frac{\\mu}{n}-P_1|<\\epsilon)=1\n$$\n当$n\\rightarrow \\infty$时，$\\frac{\\mu}{n}$依概率收敛于$P_1$。\n\n\n\n但是对于一个随机过程来说，$x_1,\\cdots,x_n$不独立。\n\n例如：股票、分子的扩散、温度的变化都属于随机过程。\n\n\n\n下面我们继续研究随机过程：假设随机过程$x_1,x_2,\\cdots,x_n$不相互独立，但是我们能找到它们之间的关系，如：\n$$\nx_k = f(x_{k-1})\\\\\np(x_k) = f(p(x_{k-1}))\n$$\n但是这样也是不够的，因为我们只知道它们之间的关系，还是不知道它们的概率，因此我们需要一个初值条件：\n\n$p(x_1) = ?$   初值的选取\n\n\n\n有的随机过程的初值可以做随机试验，故可以确定初值，如随机游走：\n$$\n\\begin{aligned}\nx_k &= x_{k-1} + D\\\\\nD &\\sim \\begin{cases}\nP(\\text{往前走1米}) = \\frac{1}{2}\\\\\nP(\\text{往后走1米}) = \\frac{1}{2}\n\\end{cases}\n\\end{aligned}\n$$\n在这个随机过程中，我们可以人为规定初值$P(x_0=0)=1$。\n\n\n\n有的初值不可以做随机试验，只能使用主观概率。\n\n\n\n随机过程：$x_1,x_2,\\cdots,x_n$\n\n我们已经找到它们之间的关系：$x_k = f(x_{k-1})$，\n\n我们选取不同的初值$p(x_1)$，不同的初值（主观概率）可能会导致不同的结果。\n\n这是我们不想要的结果，我们想尽可能削弱主观概率的差距。\n\n我们通过引用外部的观测（证据、信息）来对主观概率进行修正：\n$$\n\\text{主观概率}\\xrightarrow{\\text{外部观测}}\\text{相对客观的概率}\n$$\n主观概率可称为先验概率（先于实验的概率）\n\n相对客观的概率又称为后验概率（实验之后的概率）\n\n## 贝叶斯滤波的三大概率\n\n先验概率\n\n后验概率\n\n\n\n我们用$X,Y$表示随机变量，$x,y$表示随机变量的取值，代表随机试验一个可能的结果。\n\n离散：$P(X=x) = P_x$\n\n连续：$P(X<x)=\\int_{-\\infty}^x\\frac{1}{\\sqrt{2\\pi}}e^{-\\frac{t^2}{2}}dt$\n\n\n\n条件概率：\n\n离散：$P(X= x|Y=y) = \\frac{P(X=x,Y=y)}{P(Y=y)}$\n\n连续：$P(X<x|Y=y) = \\int_{-\\infty}^x\\frac{f(x,y)}{f(y)}dx$\n\n\n\n我们现在讲一下第三个概率，我们用一个例子来讲解一下\n\n\n\n温度：今天多少度？\n\n首先我们要给一个先验概率分布，比如：\n$$\n\\begin{cases}\nP(T=10) = 0.8\\\\\nP(T=11) = 0.2\n\\end{cases}\n$$\n其次，用温度计测一下温度：$T_m = 10.3$，但是温度计也有误差，所以温度计的测量值也存在一个概率分布\n\n最后我们根据贝叶斯公式计算后验概率分布：\n$$\n\\begin{aligned}\nP(T=10|T_m=10.3) = \\frac{P(T_m=10.3|T=10)P(T=10)}{P(T_m=10.3)}\\\\\nP(T=11|T_m=10.3) = \\frac{P(T_m=10.3|T=11)P(T=11)}{P(T_m=10.3)}\\\\\n\\end{aligned}\n$$\n而概率$P(T_m=10.3|T=10)$和$P(T_m=10.3|T=11)$被称为似然概率：代表观测的准确度。\n\n观察上面两个公式，我们发现还有一个概率$P(T_m = 10.3)$，在很多教程上对这个概率大都一笔带过：$P(T_m=10.3)$与$T$无关，所以$P(T=10|T_m=10.3)=\\eta P(T_m=10.3|T=10)P(T=10)$\n\n但是$T_m$和$T$真的是无关的吗？其实不是，这就涉及到独立、无关和无影响几个概念了。\n\n根据全概率公式，我们有：\n$$\nP(T_m=10.3) = P(T_m = 10.3|T=10)P(T=10)+ P(T_m=10.3|T=11)P(T=11)\n$$\n$P(T_m=10.3)$与$T$的取值无关，但是与$T$的分布律有关。\n\n因为$T=10,T=11$代表随机试验的一个结果，结果不会影响到分布律，\n\n所以$P(T_m=10.3)$与$T$的取值无关。\n\n但为什么$P(T_m=10.3)$是一个常数呢？这是因为$T$的分布律及我们的先验概率，他是我们事先给定的，而似然概率表示传感器的精度，是传感器固有的性质，也是给定好的。所以根据全概率公式，$P(T_m=10.3)$为常数。\n\n所以：\n$$\nP(T=10|T_m=10.3)=\\eta P(T_m=10.3|T=10)P(T=10)\\\\\nP(T=11|T_m=10.3)=\\eta P(T_m=10.3|T=11)P(T=11)\n$$\n那我么如何算$\\eta$呢？\n$$\n\\begin{aligned}\n\\sum(\\text{后验概率}) &= \\eta\\sum\\text{似然概率}\\cdot\\text{先验概率}\\\\\n\\sum\\text{后验概率}&=1\\\\\n\\eta &= \\frac{1}{\\sum\\text{似然概率}\\cdot\\text{先验概率}}\n\\end{aligned}\n$$\n\n\n对似然概率的一些解释：\n\n似然：likelihood 表示可能性，相似、像，源于最大似然估计\n\n表示：哪个原因最可能（最像）导致了结果\n\n例：A班  99男1女    B班  99女1男\n\n先随机抽取一个班，再从此班级中抽出一个人进行观测，结果是女生，此女生最像是从B班中抽出。\n\n\n$$\nP(\\text{状态}|\\text{观测}) = \\eta P(\\text{观测}|\\text{状态})P(\\text{状态})\n$$\n我们通常把观测作为果，将状态作为因，后验概率即为由结果推原因，即观测最有可能导致什么的状态，而似然概率为由原因推理结果，即我这样的观测最有可能是什么样的状态导致的。\n\n后验分布：\n$$\n\\begin{aligned}\nP(\\text{状态1}|\\text{观测})\\\\\nP(\\text{状态2}|\\text{观测})\n\\end{aligned}\n$$\n似然概率\n$$\n\\begin{aligned}\nP(\\text{观测}|\\text{状态1})\\\\\nP(\\text{观测}|\\text{状态2})\n\\end{aligned}\n$$\n\n\n关于独立和函数关系的一些说明：独立未必没有函数关系：$Y = f(X)$，$Y$与$X$可能独立，也可能不独立。\n\n例：\n\n必然事件：$Y = X+1$，$P(X=1)=1,P(Y=2)=1,P(X=1,Y=2) = P(X=1)*P(Y=2)=1$，所以$X$和$Y$相互独立。\n\n\n\n随机事件：设有一个正态概率分布$N(\\mu,\\sigma^2),(\\mu, \\sigma)$未知，从此分布中，抽取$n$个独立的样本，$X_1,\\cdots,X_n$，则$X_1,\\cdots,X_n$独立同分布，则随机变量\n$$\n\\begin{aligned}\n\\bar{X} &= \\frac{X_1+X_2+\\cdots+X_n}{n}\\\\\nS^2 &= \\frac{1}{n-1}\\sum_{i=1}^n(X_i-\\bar{X})^2\n\\end{aligned}\n$$\n相互独立\n\n## 连续变量的贝叶斯公式\n\n离散：\n$$\nP(X=x|Y=y) = \\frac{P(Y=y|X=x)}{P(Y=y)}\n$$\n如果将此公式直接推广到连续变量上\n$$\nP(X<x|Y=y) = \\frac{P(Y=y|X<x)P(X<x)}{P(Y=y)}\n$$\n显然这样是没有意义的。\n\n所以贝叶斯公式无法直接运用于连续随机变量。\n\n\n\n化积分为求和：\n$$\n\\begin{aligned}\nP(X<x) &= \\sum_{u = -\\infty}^xP(X=u)\\\\\nP(X<x|Y=y) &= \\sum_{u = -\\infty}^xP(X=u|Y=y)\\\\\n&=\\sum_{u=-\\infty}^x\\frac{P(Y=y|X=u)P(X=u)}{P(Y=y)}\\\\\n&=\\lim_{\\epsilon\\rightarrow 0}\\sum_{u = -\\infty}^x\\frac{P(y<Y<y+\\epsilon|X=u)P(u<X<u+\\epsilon)}{P(y<Y<y+\\epsilon)}\\\\\n&=\\lim_{\\epsilon\\rightarrow \\infty}\\sum_{u = -\\infty}^x \\frac{(f_{Y|X}(\\zeta_1|u)\\epsilon)(f_X(\\zeta_2)\\epsilon)}{f_Y(\\zeta_3)\\epsilon}\\\\\n&= \\lim_{\\epsilon\\rightarrow 0}\\sum_{u=-\\infty}^x \\frac{f_{Y|X}(y|u)f_X(u)}{f_Y(y)}\\epsilon\\\\\n&=\\int_{-\\infty}^x\\frac{f_{Y|X}(y|u)f_X(u)}{f_Y(y)}du\\\\\n&=\\int_{-\\infty}^x\\frac{f_{Y|X}(y|x)f_X(x)}{f_Y(y)}dx\\\\\n&\\zeta_1 \\in (y,y+\\epsilon),\\zeta_2\\in(u,u+\\epsilon),\\zeta_3\\in(y,y+\\epsilon)\n\\end{aligned}\n$$\n这就是连续随机变量的贝叶斯公式\n$$\nP(X<x|Y=y) = \\int_{-\\infty}^x\\frac{f_{Y|X}(y|x)f_X(x)}{f_Y(y)}dx\n$$\n我们将$P(X<x|Y=y)$的概率密度函数写为：\n$$\n\\begin{aligned}\nP(X<x|Y=y) &= \\int_{-\\infty}^xf_{X|Y}(x|y)dx\\\\\n&\\Rightarrow f_{X|Y}(x|y) = \\frac{f_{Y|X}(y|x)f_X(x)}{f_Y(y)}\n\\end{aligned}\n$$\n又因为\n$$\n\\begin{aligned}\nf_Y(y) &= \\int_{-\\infty}^{+\\infty}f(y,x)dx\\\\\n&=\\int_{-\\infty}^{+\\infty}f_{Y|X}(y|x)f(x)dx \\equiv C\n\\end{aligned}\n$$\n所以，令\n$$\n\\eta = \\frac{1}{\\int_{-\\infty}^{+\\infty}f_{Y|X}(y|x)f_X(x)dx}\n$$\n所以\n$$\nf_{X|Y}(x|y) = \\eta f_{Y|X}(y|x)f_X(x)\n$$\n\n## 似然概率与狄拉克函数\n\n$X$：状态   $Y$：观测\n\n例：测温度\n\n我们的先验概率分布为：$f_X(x) = \\frac{1}{\\sqrt{2\\pi}}e^{-\\frac{(x-10)^2}{2}}$，倾向于认为$X = 10$。\n\n观测：$y = 9$\n\n\n\n假设$\\epsilon$为一个足够小的数，则\n$$\n\\begin{aligned}\nf_{Y|X}(y|x)\\cdot \\epsilon &= P(y<Y<y+\\epsilon|X=x)\\\\\nf_{Y|X}(y|x) &= \\lim_{\\epsilon \\rightarrow\\infty}\\frac{P(y<Y<y+\\epsilon|X=x)}{\\epsilon}\n\\end{aligned}\n$$\n例：温度计精度为$\\pm0.2$，当真实值$=x$，$y = x\\pm 0.2$\n\n$P(x-0.2<Y<x+0.2|X=x)$较大，以及$P(Y<x-0.2\\text{或}Y>x+0.2|X=x)$较小。\n\n\n\n例\n$$\nP(x-0.2<Y<x+0.2|X=x) = 1 \\Rightarrow \\int_{y = x-0.2}^{y = x+0.2}f_{Y|X}(y|x)dy = 1\n$$\n但是似然概率在每一点的概率我们并不知道，因为传感器只会给我们一些精度的指标，所以可能需要我们自己假设似然模型。\n\n似然模型：\n\n\n\n等可能型：$f_Y(y|x)=C$，即符合均匀分布。\n\n![](https://raw.githubusercontent.com/HFC666/image/master/img/kal1.jpg)\n$$\nf_{Y|X}(y|x) = \\begin{cases}\n2.5&\\quad& |y-x|\\le0.2\\\\\n0&\\quad&|y-x|>0.2\n\\end{cases}\n$$\n阶梯型\n\n![](https://raw.githubusercontent.com/HFC666/image/master/img/kal2.jpg)\n\n阶梯型的推广：直方图型\n\n![](https://raw.githubusercontent.com/HFC666/image/master/img/kal3.jpg)\n\n正态分布\n\n![](https://raw.githubusercontent.com/HFC666/image/master/img/4.jpg)\n\n\n\n再继续讨论上文提到的温度的例子：\n\n测温度，先验：\n$$\nf_X(x) = \\frac{1}{\\sqrt{2\\pi}}e^{-\\frac{(x-10)^2}{2}}\n$$\n观测$y=9$，似然\n$$\nf_{Y|X}(y|x) = \\frac{1}{\\sqrt{2\\pi}\\cdot0.2}e^{-\\frac{(9-x)^2}{2\\cdot0.2^2}}\n$$\n后验：\n$$\n\\begin{aligned}\nf_{X|Y}(x|9) &= \\eta\\frac{1}{2\\pi\\cdot 0.2}e^{-\\frac{1}{2}[(x-10)^2+\\frac{(9-x)^2}{0.2^2}]}\\\\\n\\eta &= (\\int_{-\\infty}^{+\\infty}(\\frac{1}{2\\pi\\cdot0.2}e^{-\\frac{1}{2}[(x-10)^2+\\frac{(9-x)^2}{0.2^2}]})dx)^{-1}\n\\end{aligned}\n$$\n经计算的\n$$\nf_{X|Y}(x|9) = \\frac{1}{\\sqrt{2\\pi}0.038}e^{-\\frac{(x-9.0385)^2}{2\\cdot(0.038)^2}}\\sim N(9.0385,0.038^2)\n$$\n由计算可得：\n\n先验：$N(10,1)$\t似然：$N(9,0.2^2)$\t后验：$N(9.0385, 0.038^2)$\n\n由结果可得，方差显著降低，不确定性减小，所以称为滤波\n\n\n\n重要定理：\n\n若\n$$\n\\begin{aligned}\nf_X(x)&\\sim N(\\mu_1, \\sigma_1^2)\\\\\nf_{Y|X}(y|x)&\\sim N(\\mu_2,\\sigma_2^2)\n\\end{aligned}\n$$\n则：\n$$\nf_{X|Y}(x|y)\\sim N(\\frac{\\sigma_1^2}{\\sigma_1^2+\\sigma_2^2}\\mu_2 + \\frac{\\sigma_2^2}{\\sigma_1^2+\\sigma_2^2}\\mu_1, \\frac{\\sigma_1^2\\sigma_2^2}{\\sigma_1^2+\\sigma_2^2})\n$$\n![](https://raw.githubusercontent.com/HFC666/image/master/img/kal5.jpg)\n\n\n\n下面我们来看一下狄拉克函数：$\\delta(x)$\n$$\nf_{Y|X}(y|x) = \\frac{1}{\\sqrt{2\\pi}\\sigma}e^{-\\frac{(y-x)^2}{2\\sigma^2}}\n$$\n当$\\sigma\\rightarrow 0$时，$f_{Y|X}(y-x) = \\delta(y-x)$\n$$\n\\delta(x) = \\begin{cases}\n0&\\quad& x\\neq 0\\\\\n\\infty&\\quad&x=0\n\\end{cases}\n$$\n\n$$\n\\begin{aligned}\n\\int_{-\\infty}^{+\\infty}\\delta(x)dx &= 1\\\\\n\\int_{-\\infty}^{+\\infty}f(x)\\delta(x)dx &= f(0)\n\\end{aligned}\n$$\n\n$\\delta(x)$实质上为必然事件的概率密度。\n\n设其分布函数为$H(x)$\n\n则\n$$\nH(x) = \\begin{cases}\n1&\\quad& x\\ge 0\\\\\n0&\\quad& x<0\n\\end{cases}\n$$\n则：$\\delta(x) = \\frac{d}{dx}H(x)$\n\n\n\n推论：\n\n1. $\\int_a^b\\delta(x)dx=1, a<0<b$\n2. $\\int_a^bf(x)\\delta(x)dx = f(0), a<0<b$\n3. $\\int_c^df(x)\\delta(x-a)dx = f(a),c<a<d$\n\n\n\n例：\n\n先验：$N(\\mu,\\sigma^2)$\n\n观测：$y=0$，似然：$\\delta(10-x)$\n\n后验：\n$$\n\\begin{aligned}\nf_{X|Y}(x|y) &= \\eta\\cdot \\delta(10-x)\\frac{1}{\\sqrt{2\\pi}\\sigma}e^{-\\frac{(x-\\mu)^2}{2\\sigma^2}}\\\\\n\\eta &= \\frac{1}{\\int_{-\\infty}^{\\infty}\\delta(10-x)\\frac{1}{\\sqrt{2\\pi}\\sigma}e^{-\\frac{(x-\\mu)^2}{2\\sigma^2}}dx}\\\\\n&=\\frac{1}{\\frac{1}{\\sqrt{2\\pi}\\sigma}e^{-\\frac{(10-\\mu)^2}{2\\sigma^2}}}\n\\end{aligned}\n$$\n\n\n![](https://raw.githubusercontent.com/HFC666/image/master/img/kal6.jpg)\n\n## 随机过程的贝叶斯滤波\n\n\n\n随机过程![](https://raw.githubusercontent.com/HFC666/image/master/img/kal7.jpg)\n\n有一个初值$X_0$，有$k$个观测值$y_1,y_2,\\cdots,y_k$\n\n这种问题怎么处理？\n\n1. 所有$X_0\\sim X_k$的先验概率都依靠猜\n   1. 缺点：过于依赖观测，放弃了预测信息，虽然说如果观测很准的话最后得到的结果也没问题，但是会丢失掉一部分信息。\n   2. 比如：$X_k = 2X_{k-1}+Q_k$和$X_k = X_{k-1}^2+Q_k$这两个过程，如果先验概率都依赖于猜，那么它的结果就消失了，这两个随机过程就相当于一个随机过程了。\n2. 只有$X_0$的概率是猜的，$X_1,\\cdots,X_k$的先验概率是递推的。\n\n\n\n怎么做：通过状态方程，观测方程。（建模）\n\n状态方程：$X_k$与$X_{k-1}$是什么关系\n\n假设：$X_k = \\frac{1}{2}gt^2+Q$\n\n对$X_k$进行泰勒展开，得到：\n$$\n\\begin{aligned}\nX_k &= \\frac{1}{2}gt^2 + Q\\\\\n&=X_{k-1} + \\dot{X_{k-1}}(t_k-t_{k-1}) + Q\\\\\n&=X_{k-1} + gt(t_k-t_{k-1}) + Q\n\\end{aligned}\n$$\n但是很多的随机过程不等写出这样精确的状态方程，比如：\n\n![](https://raw.githubusercontent.com/HFC666/image/master/img/kal8.jpg)\n\n这时候我们可以将状态方程写作：\n$$\nX_k = X_{k-1} + Q\\quad Q\\sim N(0,1000)\n$$\n将方差设的大一点，得到一个比较粗糙的状态方程。\n\n状态方程，反映了$X_k$与$X_{k-1}$之间的关系，$X_k = f(X_{k-1})+Q_k$，$Q_k$为预测噪声。\n\n\n\n观测方程：反映了状态是如何引起传感器的读数\n\n\n\n如测温度：\n\n状态：温度， 观测：温度，$Y_k = X_{k} + R_k$\n\n或是测位移：\n\n 状态：位移，观测：角度，$Y_k = \\arcsin{X_k} + R_k$\n\n\n\n观测方程：$Y_k = h(X_k)+R_k$，$R_k$：观测噪声。\n\n\n$$\n\\begin{cases}\nX_k &=& f(X_{k-1}) + Q_k\\Rightarrow\\text{随机过程}\\\\\nY_k &=& h(X_k) + R_k\\Rightarrow\\text{观测}\n\\end{cases}\n$$\n那么问题来了，我们怎么递推？\n\n设$X_k = 2X_{k-1}$，无$Q_k$，无观测\n\n首先$X_0\\sim N(0,1)$(猜的)\n\n$X_1 = 2X_0\\sim N(0, 2^2)$\n\n$X_2 = 2X_1 \\sim N(0,2^4)$方差越来越大，不是我们想要的结果，故这种递推方式是不对的。\n\n\n\n真正的递推：\n\n$X_0 \\sim N(0,1) \\xrightarrow{\\text{预测步}}X_1^-\\sim N(0, 2^2) \\xrightarrow{\\text{更新步}\\text{(运用观测}y_1=0)}X_1^+\\sim N(0,0.8)\\xrightarrow{\\text{再预测}}X_2^-$\n\n更新步也成为后验步，运用观测值进行后验估计。\n\n之后再以此类推\n\n![](https://raw.githubusercontent.com/HFC666/image/master/img/kal.jpg)\n\n\n\n总的来说就是分两步：\n\n1. 预测步：$\\text{上一时刻的后验}\\xrightarrow{\\text{状态方程}}\\text{这一时刻的先验}$\n2. 更新步：$\\text{这一时刻的先验}\\xrightarrow{观测方程}\\text{这一时刻的后验/下一时刻的先验}$\n\n\n\n那么我们具体应该怎么做？\n\n贝叶斯滤波算法的推导：\n\n我们现在已经有的东西（原料）：\n$$\n\\begin{cases}\nX_k &=& f(X_{k-1})+Q_k\\\\\nY_k &=& h(X_k) + R_k\n\\end{cases}\n$$\n其中：$X_k,X_{k-1},Y_k,Q_k,R_k$都是随机变量\n\n**假设：$X_0,Q_1,\\cdots,Q_k,R_1,\\cdots,R_k$相互独立**\n\n有观测值：$y_1,y_2,\\cdots,y_k$\n\n设初值$X_0$的`pdf`：$f_0(x)$，$Q_k$的`pdf`$f_{Q_k}(x)$，$R_k$的`pdf`：$f_{R_k}(x)$\n\n**重要定理：条件概率里的条件可以做逻辑推导**\n\n例：\n$$\n\\begin{aligned}\nP(X=1|Y=2,Z=3) &= P(X+Y=3|Y=2,Z=3) = P(X+Y=3|Y=Z+1,Z-Y=1)\\\\\n&\\neq P(X+Y=3|Z=3)\\\\\n&\\neq P(X=1|X+Y=3,Z=3)\n\\end{aligned}\n$$\n即条件概率里的条件可以作为已知量。\n\n\n\n预测步：\n$$\n\\begin{aligned}\nP(X_1 < x) &= \\sum_{u = -\\infty}^xP(X_1=u)\\\\\nP(X_1 = u) &= \\sum_{v = -\\infty}^{+\\infty}P(X_1 = u|X_0=v)P(X_0 = v)\\\\\n&=\\sum_{v = -\\infty}^{+\\infty} P(X_1 - f(X_0) = u-f(v)|X_0=v)P(X_0=v)\\\\\n&=\\sum_{v=-\\infty}^{+\\infty}P(Q_1 = u-f(v)|X_0=v)P(X_0=v)\\\\\n&\\text{之前我们已经假设过}Q_1\\text{和}X_0\\text{相互独立，所以}P(Q_1=u-f(v)|X_0=v) = P(Q_1 = u-f(v))\\\\\\text{，但是这只对}Q_1\\text{和}X_0\\text{成立，要想递推我们需证明}Q_k\\text{与}X_{k-1}\\text{相互独立}\\\\\n&\\sum_{v = -\\infty}^{+\\infty}P(Q_1 = u-f(v))P(X_0=v)\\\\\n&=\\lim_{\\epsilon\\rightarrow0}\\sum_{v = -\\infty}^{+\\infty}f_{Q_1}(u-f(v))\\epsilon f_0(v)\\epsilon\\\\\n&=\\lim_{\\epsilon\\rightarrow0}\\int_{-\\infty}^{+\\infty}f_{Q_1}(u-f(v))f_0(v)dv\\cdot \\epsilon\n\\end{aligned}\n$$\n\n\n所以\n$$\n\\begin{aligned}\nP(X_1<x) &= \\sum_{u = -\\infty}^{x}P(X_1=u)\\\\\n&= \\sum_{u = -\\infty}^x\\lim_{\\epsilon\\rightarrow 0}\\int_{-\\infty}^{+\\infty}f_{Q_1}(u-f(v))f_0(v)dv\\cdot\\epsilon\\\\\n&=\\int_{-\\infty}^x\\int_{-\\infty}^{+\\infty}f_{Q_1}(u-f(v))f_0(v)dvdu\n\\end{aligned}\n$$\n那么$x_1$的先验概率分布为\n$$\nf_1^-(x) = \\frac{d}{dx}(P(X_1<x)) = \\int_{-\\infty}^{+\\infty}f_{Q_1}(x-f(v))f_0(v)dv\n$$\n\n\n下面我们看更新步：\n\n观测：$Y_1 = y_1$\n\n似然概率：\n$$\n\\begin{aligned}\nf_{Y_1|X_1}(y_1|x) &= \\lim_{\\epsilon\\rightarrow 0}\\frac{P(y_1<Y_1<y_1+\\epsilon|X_1=x)}{\\epsilon}\\\\\n&=\\lim_{\\epsilon\\rightarrow 0} \\frac{P(y_1-h(x)<Y_1-h(x)<y_1-h(x)+\\epsilon|X_1=x)}{\\epsilon}\\\\\n&=\\lim_{\\epsilon\\rightarrow 0} \\frac{P(y_1-h(x)<Y_1-h(x)<y_1-h(x)+\\epsilon|X_1=x)}{\\epsilon}\\\\\n&=\\lim_{\\epsilon\\rightarrow 0} \\frac{P(y_1-h(x)<R_1<y_1-h(x)+\\epsilon)}{\\epsilon}\\\\\n&\\text{需要证明}R_1\\text{与}X_1相互独立\\\\\n&=f_{R_1}[y_1-h(x)]\n\\end{aligned}\n$$\n所以\n$$\n\\begin{aligned}\nf_1^+(x) &= \\eta f_{R_1}[y_1-h(x)]f_1^-(x)\\\\\n\\eta &= (\\int_{-\\infty}^{+\\infty}f_{R_1}[y_1-h(x)]f_1^-(x)dx)^{-1}\n\\end{aligned}\n$$\n\n\n总结一下：\n$$\n\\begin{aligned}\nf_0(x) \\rightarrow f_1^-(x) &= \\int_{-\\infty}^{+\\infty}f_{Q_1}[x-f(v)]f_0(v)dv \\rightarrow f_1^+(x) = \\eta f_{R_1}[y_1-h(x)]f_1^-(x)\\rightarrow \\\\\nf_2^-(x) &= \\int_{-\\infty}^{+\\infty}f_{Q_2}[x-f(v)]f_1^+(v)dv\\rightarrow f_2^+(x) = \\eta f_{R_2}[y_2-h(x)]f_2^-(x)\n\\end{aligned}\n$$\n\n\n最后还有两个小尾巴：$Q_k$与$X_{k-1}$独立， $X_k$与$R_k$独立\n\n证明：\n\n![](https://raw.githubusercontent.com/HFC666/image/master/img/kal10.jpg)\n\n\n\n完整算法\n\n设初值：$X_0$的`pdf` $f_0(x)$\n\n预测步：$f_k^-(x) = \\int_{-\\infty}^{+\\infty}f_{Q_k}[x-f(v)]f^+_{k-1}(v)dv$\n\n更新步：$f_1^+(x) = \\eta f_{R_1}[y_1-h(x)]f_1^-(x)\\\\\n\\eta = (\\int_{-\\infty}^{+\\infty}f_{R_1}[y_1-h(x)]f_1^-(x)dx)^{-1}$\n\n但是到这里算法还没有完结，因为到现在我们得到的才是概率密度函数，而不是我们想要的状态。\n\n我们对概率密度函数求期望：\n$$\n\\hat{x_k^+} = \\int_{-\\infty}^{+\\infty}xf_k^+(x)dx\n$$\n\n\n贝叶斯滤波的缺点：\n\n大都情况下都需要算积分，大多数情况下无解析解。\n\n![](https://raw.githubusercontent.com/HFC666/image/master/img/kal11.jpg)\n\n直方图滤波指的是把复杂的函数分为一个一个小的区间，类似于直方图。\n\n![](https://raw.githubusercontent.com/HFC666/image/master/img/kal12.jpg)\n\n## 卡尔曼滤波\n\n卡尔曼滤波的假设：\n\n$f(X_{k}) = F\\cdot X_{k-1}, h(X_k) = H\\cdot X_k$，$F,H$均为常数，即状态方程和观测方程都为线性方程。\n\n$Q\\sim N(0, Q), R\\sim N(0,R)$，即$f_Q(x) = (2\\pi Q)^{-1/2}e^{-\\frac{x^2}{2Q}}, f_R(x) = (2\\pi R)^{-1/2}e^{-\\frac{x^2}{2R}}$\n\n即观测误差和预测误差呈方差为$0$的正态分布。\n\n\n\n设$X_{k-1}^+\\sim N(\\mu_{k-1}^+, \\sigma_{k-1}^+)$\n\n预测步：\n$$\n\\begin{aligned}\nf_k^-(x) &= \\int_{-\\infty}^{+\\infty}f_Q[x-f(v)]f_{k-1}^+(v)dv\\\\\n&=\\int_{-\\infty}^{+\\infty}(2\\pi Q)^{-1/2}e^{-\\frac{(x-F\\cdot v)^2}{2Q}}\\cdot(2\\pi \\sigma_{k-1}^+)^{1/2}e^{-\\frac{(v-\\mu_{k-1}^+)^2}{2\\sigma_{k-1}^+}}dv\\\\\n&\\sim N(F\\cdot \\mu_{k-1}^+, F^2\\cdot\\sigma_{k-1}^++Q)\n\\end{aligned}\n$$\n这一步的证明可以用 \n\n1. Mathematica 来证明。\n2. 复变函数  留数定理\n3. 傅里叶变换  卷积\n\n下面我们利用傅里叶变换和卷积的性质来证明：\n$$\nX_k = FX_{k-1} + Q_k\\quad X_{k-1}\\text{与}Q_k\\text{独立}\n$$\n\n$$\nX_{k-1} \\sim N(\\mu_{k-1}^+, \\sigma_{k-1}^+)\\quad FX_{k-1}\\sim N(F\\mu_{k-1}^+, F^2\\mu_{k-1}^+)\\quad Q_k\\sim N(0, Q)\n$$\n\n因为$X_{k-1}\\text{与}Q_k\\text{独立}$，而$X_k$为这两个随机变量相加，因此$X_k$的概率密度函数实际上是$FX_{k-1}$与$Q_k$的卷积。\n\n由傅里叶变换的性质，我们有：\n$$\nh = f*g\\quad G(h) = G(f)\\cdot G(g)\n$$\n进行傅里叶变换\n$$\n\\begin{aligned}\nFX_{k-1} &\\xrightarrow{F.T}g_1(t) = e^{iF\\mu_{k-1}^+t-\\frac{F^2\\sigma_{k-1}^+}{2}t^2}\\\\\nQ_k&\\xrightarrow{F.T}g_2(t) = e^{-\\frac{Q}{2}t^2}\\\\\ng_1(t)g_2(t) &= e^{iF\\mu_{k-1}^+t-\\frac{F^2\\sigma_{k-1}^++Q}{2}t^2}\\xrightarrow{I.F.T}N(F\\mu_{k-1}^+, F^2\\sigma_{k-1}^++Q)\n\\end{aligned}\n$$\n正态分布的傅里叶变换为：\n$$\nN(\\mu,\\sigma^2)\\xrightarrow{F.T} e^{i\\mu t-\\frac{\\sigma^2}{2}t^2}\n$$\n设$f_k^-(x)\\sim N(\\mu_k^-, \\sigma_k^-)$\n\n我们有\n\n1. $\\mu_k^- = F\\mu_{k-1}^+$\n2. $\\sigma_k^- = F^2\\sigma_{k-1}^++Q$\n\n预测步完成\n\n\n\n更新步\n$$\n\\begin{aligned}\nf_k^-(x)&\\sim N(\\mu_k^-, \\sigma_k^-)\\\\\nf_k^+ (x) &= \\eta f_R(y_k-H\\cdot x)\\cdot f_k^-(x)\\\\\n&=\\eta (2\\pi R)^{-\\frac{1}{2}}e^{-\\frac{(y_k-H\\cdot x)^2}{2R}}\\cdot(2\\pi \\sigma_k^-)^{-\\frac{1}{2}}e^{-\\frac{(x-\\mu_k^-)^2}{2\\sigma_k^-}}\\\\\n\\eta &= [\\int_{-\\infty}^{+\\infty}(2\\pi R)^{-\\frac{1}{2}}e^{-\\frac{(y_k-H\\cdot x)^2}{2R}}\\cdot(2\\pi \\sigma_k^-)^{-\\frac{1}{2}}e^{-\\frac{(x-\\mu_k^-)^2}{2\\sigma_k^-}}dx]^{-1}\n\\end{aligned}\n$$\n由数学软件计算可得：\n$$\nX_k^+\\sim N(\\frac{H\\sigma_k^-y_k+R\\mu_k^-}{H^2\\sigma_k^-+R}, \\frac{R\\sigma_k^-}{H^2\\sigma_k^-+R})\n$$\n$X_k^+\\sim N(\\mu_k^+, \\sigma_k^+)$，则\n\n3. $$\n   \\mu_k^+ = \\frac{H\\sigma_k^-}{H^2\\sigma_k^-+R}(y_k-H\\mu_k^-)+\\mu_k^-\n   $$\n\n   \n\n4. $$\n   \\sigma_k^- = (1-\\frac{H^2\\sigma_k^-}{H^2\\sigma_k^-+R})\\sigma_k^-\n   $$\n\n   \n\n5. 我们观察上面两个公式都有一个共同的因子，我们称之为卡尔曼增益$K$\n   $$\n   K = \\frac{H\\sigma_k^-}{H^2\\sigma_k^-+R}\n   $$\n\n这就是卡尔曼滤波的$5$个公式。\n\n\n\n我们现在研究一下卡尔曼增益的性质\n$$\nK = \\frac{H}{H^2+R/\\sigma_k^-}\n$$\n当$R>>\\sigma_k^-, k\\rightarrow 0, \\mu_k^+ = \\mu_k^- + k(y_k-H\\cdot\\mu_k^-) = \\mu_k^-$，相信预测\n\n当$R<<\\sigma_k^-, k\\rightarrow\\frac{1}{H}, \\mu_k^+=\\mu_k^-+\\frac{y_k}{H}-\\mu_k^- = \\frac{y_k}{H}$，相信观测\n\n\n\n矩阵形式的卡尔曼滤波\n\n$\\mu_k\\rightarrow \\vec{\\mu_k},\\sigma_k\\rightarrow \\Sigma_k$，$F,H$皆为矩阵。\n\n类推：\n$$\n\\begin{aligned}\n\\vec{\\mu_k^-} &= F\\cdot\\vec{\\mu_{k-1}^+}\\\\\n\\Sigma_k^- &= F\\Sigma_{k-1}^+F^T+Q\\\\\nK &= \\Sigma_k^-H^T(H\\Sigma_k^-H^T+R)^{-1}\\\\\n\\vec{\\mu_k^+} &= \\vec{\\mu_k^-}+K(\\vec{y_k}-H\\vec{\\mu_k^-})\\\\\n\\Sigma_k^+ &= (I-KH)\\Sigma_k^-\n\\end{aligned}\n$$\n矩阵形式的推导可以阅读《概率机器人》。\n\n\n\n![](https://raw.githubusercontent.com/HFC666/image/master/img/kal13.jpg)\n\n\n\n其实马尔可夫假设和观测独立假设可以由我们的已知条件推出，没有必要给出。\n\n证明若\n$$\n\\begin{cases}\nX_k = f(X_{k-1})+Q_k\\\\\nY_k = h(X_k) + R_k\n\\end{cases},X_0,Q_1,\\cdots,Q_k,R_1,\\cdots,R_k\\text{独立}\n$$\n，则\n$$\n\\begin{aligned}\nP(X_k = x_k|X_{k-1} = x_{k-1},X_{k-2} &= x_{k-2},\\cdots,X_0=x_0) = P(X_k=x_k|X_{k-1}=x_{k-1})\\quad\\text{马尔可夫假设}\\\\\nP(Y_k = y_k|X_k = x_k,X_{k-1}=x_{k-1},\\cdots,X_0=x_0) &= P(Y_k=y_k|X_k=x_k)\\quad\\text{观测独立}\n\\end{aligned}\n$$\n\n\n证明：\n\n![](https://raw.githubusercontent.com/HFC666/image/master/img/kal14.jpg)\n\n![](D:/joplin/joplin/贝叶斯滤波与卡尔曼滤波/image/15.jpg)\n\n观测独立性假设同理。\n\n\n\n## 从零开始码出卡尔曼滤波代码\n\n~~~matlab\n%%%%kalman filter\n%X(K) = F*X(K-1)+Q\n%Y(K) = H*X(K)+R\n%%% 第一个问题，生成一段随机信号，并滤波\n\n%生成一段时间t\nt = 0.1:0.01:1;\nL = length(t);\n%生成真实信号x，以及观测y\n%生成信号，设x=t^2\nx = t.^2;\ny = x + normrnd(0,0.1,1, L);\n% 绘制信号和观测数据\n% plot(t, x, t, y)\n\n%%%%%%%滤波算法\n%%%预测方程观测方程怎么写\n%观测方程好写Y(K) = X(K)+R R~N(0,1)\n%预测方程不好写，在这里，可以猜一猜是线性增长，信号是杂乱无章的，怎么办？\n%模型一，最粗糙的建模\n%X(K) = X(K-1)+Q\n%Y(K) = X(K) + R\n%猜Q~N(0,1)\n\nF1 = 1;\nH1 = 1;\nQ1 = 1;\nR1 = 1;\n%初始化x(k)+\nXplus1 = zeros(1, L);\n\n%设置一个初值，假设Xplus1(1)~N(0.01, 0.01^2)\nXplus1(1) = 0.01;\nPplus1 = 0.01^2;\n\n%%%卡尔曼滤波算法\n%X(K)- = F*X(K-1)+\n%P(K)- = F*P(K-1)+*F'+Q\n%K = P(K)-*H'*inv(H*P(K)-*H'+R)\n%X(K)+=X(K)-+K*(y(k)-H*X(k)-)\n%P(K)+=(1-K*H)*P(K)-\n\nfor i=2:L\n    Xminus1 = F1*Xplus1(i-1);\n    Pminus1 = F1*Pplus1*F1+Q1;\n    K1 = (Pminus1*H1)/(H1*Pminus1*H1+R1);\n    Xplus1(i) = Xminus1+K1*(y(i)-H1*Xminus1);\n    Pplus1 = (1-K1*H1)*Pminus1;\nend\n% plot(t, y, t, Xplus1);\n\n%%%模型二\n%X(K)=X(K-1)+X'(K-1)*dt + X\"(K-1)*dt^2*(1/2!)+Q2\n%Y(K)=X(K)+R R~N(0,1)\n%此时状态变量X=[X(K) X'(K) X\"(K)]T(列向量)\n%Y(K)=H*X+R  H = [1 0 0](行向量)\n\n%预测方程\n%X(K) = X(K-1) + X'(K-1)*dt + X\"(K-1)*dt^2*(1/2!)+Q2\n%X(K)' = 0*X(K-1)+X'(K-1)+X\"(K-1)*dt+Q3\n%X(K)\" = 0*X(K-1) + 0*X'(K-1) + X\"(K-1) + Q4\n% F = [1 dt 0.5*dt^2\n%      0  1    dt\n%      0  0    1\n% H = [1 0 0]\n% Q = [Q2 0 0 \n%      0 Q3 0\n%      0 0 Q4]\n\ndt = t(2)-t(1);\nF2 = [1 dt 0.5*dt^2;0 1 dt;0 0 1];\nH2 = [1 0 0];\nQ2 = [1 0 0; 0 0.01 0; 0 0 0.0001];\nR2 = 20;\n\n%%%设置初值\nXplus2 = zeros(3, L);\nXplus2(1, 1) = 0.1^2;\nXplus2(2, 1) = 0;\nXplus2(3, 1) = 0;\nPplus2 = [0.01, 0, 0; 0, 0.01, 0; 0, 0, 0.0001];\nfor i = 2:L\n    Xminus2 = F2*Xplus2(:,i-1);\n    Pminus2 = F2*Pplus2*F2'+Q2;\n    \n    K2 = (Pminus2*H2')*inv(H2*Pminus2*H2'+R2);\n    Xplus2(:,i) = Xminus2 + K2*(y(i)-H2*Xminus2);\n    Pplus2 = (eye(3)-K2*H2)*Pminus2;\nend\nplot(t, y, t, Xplus2(1,:))\n~~~\n\n## 粒子滤波\n\n应用最广泛，原理很复杂，术语最多。\n\n从贝叶斯滤波开始：\n\n$X_k = f(X_{k-1})+Q_k$\n\n$Y_k = h(X_k)+R_k$\n\n$X_0,Q_1,Q_2,\\cdots,Q_k,R_1,R_2,\\cdots,R_k$互相独立\n\n$Q_1,Q_2,\\cdots,Q_k,R_1,R_2,\\cdots,R_k$满足正态分布\n\n\n\n粒子滤波适用于静态环境、动态可预测环境，如电池电量估算，视频跟踪，封闭环境导航。\n\n\n\n下面再复习一下贝叶斯滤波的几个公式：\n\n初值：$X_0\\xrightarrow{pdf}f_0^+$\n\n预测：$f_k^-(x) = \\int_{-\\infty}^{+\\infty}f_Q[x-f(v)]f_{k-1}^+(v)dv$\n\n更新：$f_k^+(x) = \\eta f_R[y_k-h(x)]f_k^-(x)\\quad \\eta = (\\int_{-\\infty}^{+\\infty}f_R[y_k-h(x)]f_k^-(x)dx)^{-1}$\n\n估计：$\\hat{x_k^+} = \\int_{-\\infty}^{+\\infty}xf_k^+(x)dx$\n\n缺点：无穷积分，一般无解析解。\n\n\n\n由大数定律引发的遐想\n\n大数定律：设$X$为随机变量，$E(X)$存在，对$X$做$n$次随机试验，结果记为$x_1,x_2,x_3,\\cdots,x_n$，则有\n$$\n\\lim_{n\\rightarrow \\infty}P(|\\frac{1}{n}\\sum_{i}x_i-E(X)|<\\epsilon)=1\n$$\n暗示了什么？当$n$足够大时，$\\frac{1}{n}\\sum_{i}x_i\\approx E(X)$\n$$\nE(x) = \\int_{-\\infty}^{+\\infty}xf(x)dx\\Rightarrow \\lim_{n\\rightarrow \\infty}\\frac{1}{n}\\sum_ix_i = \\int_{-\\infty}^{+\\infty}xf(x)dx\n$$\n我们用到$\\delta$函数：\n$$\n\\delta(x) \\Rightarrow \\int_c^df(x)\\delta(x-a)dx = f(a)\\quad a\\in(c,d)\n$$\n可得\n$$\nx_1 = \\int_{-\\infty}^{+\\infty}x\\delta(x-x_1)dx,x_2 = \\int_{-\\infty}^{+\\infty}x\\delta(x-x_2)dx,\\cdots,x_n = \\int_{-\\infty}^{+\\infty}x\\delta(x-x_n)dx\n$$\n所以\n$$\n\\frac{1}{n}\\sum_ix_i = \\frac{1}{n}\\int_{-\\infty}^{+\\infty}x\\sum_i\\delta(x-x_i)dx = \\int_{-\\infty}^{+\\infty}xf(x)dx\n$$\n$f(x)$为$X$的`pdf`\n\n由此可以看出，当$n\\rightarrow \\infty$时，$f(x)\\approx \\frac{1}{n}\\sum_{i}\\delta(x-x_i)$，好积分。\n\n大数定律暗示了可以用一堆粒子来近似概率密度，这就是粒子滤波。\n\n![](https://raw.githubusercontent.com/HFC666/image/master/img/kal16.jpg)\n\n由此可以看出，虽然其概率密度函数不是很想，但是其**分布函数**的图像很相像，标准正态分布在原点处的概率最大，分布函数导数值最大，对应于采样的函数其采的点数量越多，也就越陡峭。\n\n\n\n缺点：需要大量粒子，如何用少量粒子表示`pdf`\n\n![](https://raw.githubusercontent.com/HFC666/image/master/img/kal17.jpg)\n\n当我们用上文提到的方法时，当遇到较大的导数值时，由于我们的粒子每次只走$1/n$，所以需要大量的粒子来将我们的函数值抬上去，但是如果我们给粒子赋予权重，在导数较大的位置的粒子赋予较高的权重，那么就能用较少的粒子来近似。\n\n$f(x)\\approx \\frac{1}{n}\\sum_i\\delta(x-x_i) = \\sum_i\\frac{1}{n}\\delta(x-x_i)$，每个粒子的权重都是$1/n$\n\n我们改进后的\n$$\n\\begin{aligned}\nf(x) &= \\sum_iw_i\\delta(x-x_i)\\\\\n\\sum_iw_i&=1\n\\end{aligned}\n$$\n粒子的位置和权重完全决定了`cdf`，也就决定了`pdf`\n\n![](https://raw.githubusercontent.com/HFC666/image/master/img/kal18.jpg)\n\n$x_i$是从$f(x_i)$采样出来的，$x_i$的位置天然满足概率分布的规律：<img src=\"https://raw.githubusercontent.com/HFC666/image/master/img/kal19.jpg\" style=\"zoom:50%;\" />\n\n$w_i$如何分配？\n\n![](https://raw.githubusercontent.com/HFC666/image/master/img/kal20.jpg)原则，`pdf`大的$w_i$高\n\n如\n$$\nw_i = \\frac{f(x_i)}{f(x_1)+f(x_2)+f(x_3)}\n$$\n按比例分配，满足归一化$\\sum w_i=1$\n\n\n\n也可以$w_i = \\frac{1}{n}$，但是$n$要足够大($50$个以上)\n\n也可以综合：$n$大，$w_i = \\frac{f(x_i)}{\\sum_if(x_i)}$\n\n\n\n贝叶斯滤波：$X_0$的`pdf`为$f_0(x)$，在$f_0(x)$中采了$n$个样本（怎么采样？）\n\n采样很难，我们先假设$X_0$是一个正态分布，利用Matlab可以进行采样\n\n设采集的样本为$x_0^{(1)},x_0^{(2)},\\cdots,x_0^{(n)}$\n\n设$f_0(x) = \\sum_iw_0^{(i)}\\delta(x-x_0^{(i)}),w_0^{(i)}$可以为$1/n$，也可以按照比例分配。\n\n则\n$$\n\\begin{aligned}\nf_1^-(x) &= \\int_{-\\infty}^{+\\infty}f_Q[x-f(v)]dv = \\sum_iw_if_Q[x-f(x_0^{(i)})]\\\\\nf_1^+(x) &= \\eta f_R[y-h(x)]f_1^-(x)\n\\end{aligned}\n$$\n那么我们的粒子哪里去了呢？\n\n我们得到的$f_1^+(x)$和$f_1^-(x)$都没有了$\\delta(x)$函数，无法进行积分。\n\n因为我们由$f_1^+(x)$到$f_2^-(x)$还需要计算无穷积分，即还需要对概率密度函数进行采样。\n\n但是对概率密度函数进行采样是一件很难的事情，我们可以再对$f_1^+(x)$进行采样，但是采样的过程非常耗时。\n\n\n\n通过$f_1^-(x)$生成一堆粒子，理论上$f_1^-(x) = \\sum_i w_if_Q[x-f(x_0^{(i)})]$也可以采样，也可以计算出新的$w$，但是速度太慢。\n\n怎么办？$\\Rightarrow f_Q[x-f(x_0)^{(i)}]$，假设$Q$为正态分布。\n\n$f_Q[x-f(x_0^{(i)})] = (2\\pi Q)^{-\\frac{1}{2}}e^{-\\frac{[x-f(x_0)^{(i)}]^2}{2Q}}\\sim N(f(x_0^{(i)}),Q)$\n\n\n$$\nN(f(x_0^{(i)}, Q)\\xrightarrow{F.T} e^{if(x_0^{(i)})t-\\frac{Q}{2}t^2}\n$$\n我们对一个相对较为复杂的概率密度函数进行傅里叶变换，将其分解为一系列较为简单的概率密度函数。\n\n例如，正态分布函数\n$$\nN(f(x_0^{(i)}, Q)\\xrightarrow{F.T} e^{if(x_0^{(i)})t-\\frac{Q}{2}t^2} = e^{if(x_0^{(i)})t}\\cdot e^{-\\frac{Q}{2}t^2}\n$$\n\n\n我们对其两个因子进行傅里叶逆变换\n$$\ne^{if(x_0^{(i)})t}\\xrightarrow{i.F.T} \\delta(x-f(x_0^{(i)}))\\quad \\int_{-\\infty}^{+\\infty}\\delta(x-f(x_0^{(i)}))e^{ixt} = e^{if(x_0^{(i)})t}\\\\\n$$\n\n$$\ne^{-\\frac{Q}{2}t^2}\\xrightarrow{i.F.T} N(0,Q)\n$$\n\n\n\n$\\delta(x-f(x_0^{(i)}))$是必然事件$X_0 = f(x_0^{(i)})$的`pdf`\n\n$N(0,Q)$为$Q$的`pdf`\n\n\n\n定理：若$X$的`pdf`为$f$，$Y$的`pdf`为$g$，$X,Y$独立，则$Z = X+Y$的$pdf$为$f*g$\n\n设$Z$的`pdf`为$h$，则$h = f*g$。\n\n卷积性质：设$G$为傅里叶变换，$G^{-1}$为傅里叶逆变换。\n\n则$G(h) = G(f)\\cdot G(g)$\n\n设$A$的`pdf`$f_A=f_Q[x-f(x_0^{(i)})], G(f_A)=e^{if(x_0^{(i)})t}\\cdot e^{-\\frac{Q}{2}t^2}$\n\n而$G^{-1}(e^{if(x_0^{(i)})t}) = \\delta(x-f(x_0^{(i)}))$\n\n$G^{-1}(e^{-\\frac{Q}{2}t^2}) = (2\\pi Q)^{-1/2}e^{-\\frac{x^2}{2Q}}$\n\n设$X$的`pdf`为$f_X = \\delta(x-f(x_0^{(i)}))$，$Y$的`pdf`为$f_Y = (2\\pi Q)^{-1/2}e^{-\\frac{x^2}{2Q}}$\n\n$G(f_A) = G(f_x)\\cdot G(f_Y)\\Rightarrow A+X+Y$\n\n$X$为必然事件，$Y\\sim N(0, Q)$，$X,Y$独立\n\n如何生成粒子：$f_1^-(x)=\\sum_i w_0^{(i)}f_Q[x-f(x_0^{(i)})]$\n\n对于每一个$f_Q[x-f(x_0^{(i)})]$可以看作是一个必然事件$X=f(x_0^{(i)})$与一个随机数$Y\\sim N(0,Q)$叠加\n\n$f_1^-(x)$粒子$x_1^{-(1)},x_1^{-(2)},\\cdots,x_1^{-(n)}$\n\n$x_1^{-(i)} = f(x_0^{(i)})+v$，$v\\sim N(0,Q)$\n\n\n\n例：$X_1 = 2X_0+Q,Q\\sim N(0,1)$\n\n设$X_0\\sim N(0,1)$\n\n样本$x_0^{(1)}=0, x_0^{(2)}=0.1, x_0^{(3)}=0.1$\n\n$x_1^{-(i)} = f(x_0^{(i)})+v$\n\n$x_1^{-(0)}=0\\cdot2+0.12 = 0.12, x_1^{-(1)} = 2\\cdot0.1+0.08=0.28, x_1^{-(2)} = 2\\cdot -0.1+0.3 = 0.1$\n\n\n\n$f_1^-(x) = \\sum_i^n w_0^{(i)}f_Q[x-f(x_0^{(i)})]$，对于每一个$f_Q[x-f(x_0^{(i)})]$，生成一个粒子即可。\n\n此时，$x_1^{-(i)} = f(x_0^{(i)})+Q$，本质是改变了粒子的位置，并未改变粒子的权重。\n\n![](https://raw.githubusercontent.com/HFC666/image/master/img/kal21.jpg)\n\n下面我们讲一下粒子滤波算法\n\n1. 设初值$X_0\\sim N(\\mu, \\sigma^2)$\n\n2. 生成$X_0$的样本$x_0^{(1)},\\cdots,x_0^{(n)}$\n\n3. 生成$X_0$样本对应的权重$w_0^{(i)}$，可以都为$1/n$，也可以为$\\frac{f(x_0^{(i)})}{\\sum_if(x_0)^{(i)}}$，$f(x)$为$X_0$的`pdf`\n\n4. 生成$X_1^-$的样本，$x_1^{-(i)} = f(x_0^{(i)})+Q$，$Q\\sim N(0, Q)$\n\n5. $f_1^-(x) = \\sum_i w_0^{(i)}\\delta(x-x_1^{-(i)})$，此时改变了粒子的位置，但是没有改变权重\n\n6. 预测步结束\n\n7. 观测到了一个数据$y_1$\n\n8. $$\n   \\begin{aligned}\n   f_1^+(x) &= \\eta f_R[y_1-h(x)]f_1^-(x) = \\sum_{i=1}^n\\eta f_R[y_1-h(x)]w_0^{(i)}\\delta(x-x_1^{-(i)})\\\\\n   &=\\sum_{i=1}^n\\eta f_R[y_1-h(x_1^{-(i)})]w_0^{(i)}\\delta(x-x_1^{-(i)})\n   \\end{aligned}\n   $$\n\n   设$w_1^{(i)} = f_R[y_1-h(x_1^{-(i)})]w_0^{(i)}$，所以$f_1^+(x) = \\sum_{i=1}^n w_1^{(i)}\\delta(x-x_1^{-(i)})$，更新步并未改变粒子的位置，但是改变了粒子的权重。\n\n9. $$\n   \\eta = (\\sum_iw_1^{(i)})^{-1},\\text{归一化}\n   $$\n\n   \n\n因为在更新步里并没有改变粒子，所以我们统一把粒子都命名为$x_1^{(i)}$\n\n\n\n下面我们给出一个完整的粒子滤波算法\n\n1. 给初值$X_0\\sim N(\\mu, \\sigma^2)$\n2. 生成$x_0^{(i)},w_0^{(i)} = 1/n$\n3. 预测步，生成$x_1^{(i)} = f(x_0^{(i)})+v, v\\sim N(0,Q)$\n4. 更新步，设观测值为$y_1$，生成$w_1^{(i)} = f_R[y-h(x_1^{(i)})]w_0^{(i)}$\n5. 将$w_1^{(i)}$归一化，$w_1^{(i)} = \\frac{w_1^{(i)}}{\\sum w_1^{(i)}}$\n6. 此时，得新的权重$w_1^{(i)}$\n7. 再由预测步生成$x_2^{(i)} = f(x_1^{(i)})+v$\n8. 再由更新步产生$w_2^{(i)} = f_R[y_2-h(x_2^{(i)})]w_1^{i}$\n9. 将$w_2^{(i)}$归一化，$w_2^{(i)} = \\frac{w_2^{(i)}}{\\sum w_2^{(i)}}$\n10. 如此递推\n\n\n\n粒子滤波如何求期望和方差呢？\n\n$f(x) = \\sum_{i=1}^n w_i\\delta(x-x_i)$\n$$\n\\begin{aligned}\n\\hat{x_k^+}  &=\\int_{-\\infty}^{+\\infty}\\sum_{i=1}^nxw_i\\delta(x-x_i)dx=\\sum_{i=1}^n w_ix_i\\\\\nD(X) &= E(X^2) - [E(X)]^2 = \\int_{-\\infty}^{+\\infty}x^2f(x)dx-(\\int_{-\\infty}^{+\\infty}xf(x))^2 \\\\\n&=\\sum_{i=1}^n(w_ix_i^2) - (\\hat{x_k^+})^2\n\\end{aligned}\n$$\n\n\n## 重采样\n\n重采样是为了解决粒子退化问题：只有少数粒子具有较高的权重，大量粒子权重极低。\n\n那么导致粒子退化的原因是什么？\n\n1. 粒子的数量不能太多\n2. $w_k^{(i)} = f_R[y_k-h(x_k^{(i)})]w_{k-1}^{(i)},f_R[y_k-h(x_k^{(i)})]=(2\\pi R)^{1/2}e^{-\\frac{[y_k-h(x_k^{(i)})]^2}{2R}}$为$e^{-\\alpha x^2}$型函数，导致权重下降地非常快。\n\n如\n\n![](https://raw.githubusercontent.com/HFC666/image/master/img/kal22.jpg)\n\n如果有多个粒子的权重较大，这是比较好的情况：\n\n![](https://raw.githubusercontent.com/HFC666/image/master/img/kal23.jpg)\n\n但是若只有一个粒子的权重很大，这种情况就很差了\n\n![](https://raw.githubusercontent.com/HFC666/image/master/img/kal24.jpg)\n\n还有一种更坏的情况\n\n![](https://raw.githubusercontent.com/HFC666/image/master/img/kal25.jpg)\n\n![](D:/joplin/joplin/贝叶斯滤波与卡尔曼滤波/image/26.jpg)\n\n所以为了解决粒子退化的问题，重采样应运而生。\n\n\n\n粒子退步$\\rightarrow$更新失败(是这一步还是下一步？)，是下一步\n\n![](https://raw.githubusercontent.com/HFC666/image/master/img/kal27.jpg)\n\n当前步的更新发挥作用$\\rightarrow$粒子退化$\\rightarrow$下一步更新失效\n\n![](https://raw.githubusercontent.com/HFC666/image/master/img/kal28.jpg)\n\n\n\n重采样算法步骤\n\n假设有$4$个粒子，其中$x_1,w_1=0.1、x_2,w_2 = 0.1、x_3, w_3 = 0.7、x_4,w_4 = 0.1$\n\n我们按照权重在坐标轴上划分范围：\n\n![](https://raw.githubusercontent.com/HFC666/image/master/img/kal29.jpg)\n\n每个区间为$(0,w_1),(w_1,w_1+w_2),\\cdots,(\\sum_{i-1}w_{i-1},\\sum_iw_i)$\n\n\n\n生成一个随机数$a,a\\sim U(0,1)$，看$a$落在哪一个区间上就把对应的粒子复制。\n\n\n\n之后将所有的粒子权重都设为$1/n$\n\n\n\n重采样有一定减弱粒子退化的能力\n\n重采样必然会导致粒子多样性丧失，$N = \\frac{1}{\\sum w_i^2}$，$N$越小，退化越严重\n\n重采样必然减慢粒子滤波的速度。\n\n## 粒子滤波代码\n\n~~~matlab\n% x(i) = sin(x(i-1))+5*x(i-1)/(x(i-1)^2+1)+Q\n% y(i) = x(i)^2 + R\n\n% 状态值与观测值\nt = 0.01:0.01:1;\nx = zeros(1, 100);\ny = zeros(1, 100);\n% 给初值\nx(1) = 0.1;\ny(1) = 0.01^2;\n\n% 生成真实数据与观测数据\nfor i = 2:100\n    x(i) = sin(x(i-1)) + 5 * x(i-1) / (x(i-1)^2+1);\n    y(i) = x(i)^3 + normrnd(0, 1);\nend\n% plot(t, x, t, y)\n\n% 设粒子集合\nn = 100;\nxold = zeros(1, n);\nxnew = zeros(1, n);\nxplus = zeros(1, 100); % xplus用于存放滤波值，就是每一次后验概率的期望\nw = zeros(1, n);\n% 设置x0(i)，可以直接在正态分布中采样，如果对初值有自信，也可以让所有粒子都相同\nfor i = 1:n\n    xold(i) = 0.1;\n    w(i) = 1/n;\nend\nfor i = 2:100\n    % 预测步，由x0推出x1\n    for j = 1:n\n        xold(j) = sin(xold(j)) + 5 * xold(j)/(xold(j)^2+1) + normrnd(0,0.1);\n    end\n    % 预测步完毕\n    % 更新步\n    for j = 1:n\n        w(j) = exp(-((y(i)-xold(j)^3)^2/(2*1)));\n    end\n    % 归一化\n    w = w/sum(w);\n    \n    % 重采样\n    c = cumsum(w);\n    \n    for j = 1:n\n        a = unifrnd(0, 1);\n        for k = 1:n\n            if (a<c(k))\n                xnew(j) = xold(k);\n                break;\n            end\n        end\n    end\n    \n    xold = xnew;\n    \n    for j=1:n\n        w(j) = 1/n;\n    end\n    xplus(i) =sum(xnew)/n;\n    \nend\n\nplot(t, x, t, xplus);\n~~~\n\n## 粒子滤波拾遗：采样方法与预测方程\n\n采样方法：如何在复杂`pdf`上采样\n\n预测方程：$X = f(t)$，怎么由$X=f(t)\\Rightarrow X_k=F(X_{k-1})$(高精度)\n\n\n\n正态分布和均匀分布的概率密度函数很好采样：\n\n![](https://raw.githubusercontent.com/HFC666/image/master/img/kal30.jpg)\n\n![](D:/joplin/joplin/贝叶斯滤波与卡尔曼滤波/image/31.jpg)\n\n采样粒子的特点：`pdf`大的粒子多，`pdf`小的粒子少\n\n![](https://raw.githubusercontent.com/HFC666/image/master/img/kal32.jpg)\n\n也可以通过对正态分布去掉一些粒子来实现\n\n![](https://raw.githubusercontent.com/HFC666/image/master/img/kal33.jpg)\n\n怎么去掉\n\n![](https://raw.githubusercontent.com/HFC666/image/master/img/kal34.jpg)高`pdf`的地方有更大的概率保留，低`pdf`的地方有更大的概率去掉，类似于重采样。\n\n\n\n对于一个复杂的`pdf`：![](https://raw.githubusercontent.com/HFC666/image/master/img/kal35.jpg)\n\n1. 均匀分布生成粒子\n2. 取一个直线$M$，使得$M\\ge f(x)$\n3. ![](https://raw.githubusercontent.com/HFC666/image/master/img/kal36.jpg)，如图，我们对于生成的每一个粒子，做\"审判\"，生成一个随机数$a\\sim U(0,M)$，看$a$落在哪个区间，若$a\\in (0, f(x_i))$，则保留，反之舍弃。\n\n也可以从正态分布开始生成粒子：\n\n![](https://raw.githubusercontent.com/HFC666/image/master/img/kal37.jpg)\n\n\n\n但是前两种算法都无法控制粒子的数量，我们改进算法为接受-拒绝采样法\n\n\n\n待采样$f(x)$，容易采样的$g(x)$，$g(x)$又被称为建议分布。\n\n![](https://raw.githubusercontent.com/HFC666/image/master/img/kal38.jpg)\n\n1. 找到$M$，使得$Mg(x)\\ge f(x)$\n2. 在$g(x)$上采样一个粒子$x_1$\n3. 生成一个$a\\sim U(0, Mg(x_1))$，若$a\\in (0, f(x_1))$保留，反之则拒绝\n4. 重复\n\n那么什么样的提议分布是好的呢？\n\n\n\n首先$Mg(x)$拒绝率越低，效率越高，提议分布越好\n\n![](https://raw.githubusercontent.com/HFC666/image/master/img/kal39.jpg)\n\n提议分布也应尽可能要与$f(x)$形状逼近，越相似，拒绝率越低\n\n![](https://raw.githubusercontent.com/HFC666/image/master/img/kal40.jpg)\n\n位置也要尽可能相似\n\n![](https://raw.githubusercontent.com/HFC666/image/master/img/kal41.jpg)\n\n\n\n预测方程的写法可以参考数值分析。","tags":["卡尔曼滤波"],"categories":["算法"]},{"title":"数论","url":"/2021/12/11/数论/","content":"\n**开个新坑**\n\n{% pdf https://hfcouc.work/pdfs/Number_theory.pdf %}\n\n","tags":["数论"],"categories":["数论"]},{"title":"随机过程第一章：基本概念","url":"/2021/12/10/随机过程/","content":"\n{% pdf https://hfcouc.work/pdfs/Random_Processes.pdf %}","tags":["随机过程"],"categories":["随机过程"]},{"title":"贝叶斯统计分析","url":"/2021/12/05/贝叶斯统计/","content":"\n最近一次更新：根据梅老师的课更新了一下无信息先验。\n\n{% pdf https://hfcouc.work/pdfs/Bayesian.pdf %}\n\n","tags":["贝叶斯数据分析"],"categories":["贝叶斯机器学习"]},{"title":"马尔科夫链蒙特卡洛方法","url":"/2021/12/04/MCMC/","content":"\n## 蒙特卡洛方法\n\n### MC实质：随机抽样\n\n为什么要抽样？\n\n假设我们有关于$x$的一个正态分布的概率密度：\n$$\nf(x) = \\frac{1}{\\sqrt{2\\pi}\\sigma}\\exp\\left(-\\frac{(x-\\mu)^2}{2\\sigma^2}\\right)\n$$\n我们很容易得到其期望。但是如果我们要求$g(x)$的期望，我们要用到：\n$$\nE(g(X)) = \\int_{-\\infty}^{+\\infty}f(x)g(x)dx\n$$\n如果$g(x)=x^2$，那么我们将很难求得其期望值，那么我们应该怎么办呢？我们可以抽取样本$X$，然后计算$g(X)$，进而计算其平均：\n$$\nE(g(X)) = \\frac{1}{N}\\sum_{i=1}^Ng(x_i)\n$$\n但是给定我们一个分布，我们怎么求得符合这个分布的样本值呢？\n\n我们可以对概率密度函数进行积分，得到累积分布函数：\n$$\nF(x) = \\int_{-\\infty}^xf(t)dt\n$$\n累计分布函数为递增函数，其值域为$[0,1]$，因此我们可以在$[0,1]$上均匀采样，假设采样的值为$y\\in[0,1]$，则求$x = F^{-1}(y)$即为我们抽样的点。\n\n但是$F(x)$真的可求吗？对于复杂的$f(x)$，$F(x)$可能不好求，于是就有下面的取舍采样法：\n\n对于复杂的概率密度函数$f(x)$，我们可以找到一个简单的可求累积分布函数的概率密度函数$q(x)$，对于常数$m$，都有$mq(x)\\ge f(x)$。这样我们可以求得$q(x)$的累计分布函数$Q(x)$，在$Q(x)$上进行采样。假设采的样本为$x_i$，则：\n\n+ 以概率$P = \\frac{f(x)}{mq(x)}$接受\n+ 以概率$1-P$拒绝\n\n但是合适的$q(x)$好找吗？其实在高维情况下并不好找。\n\n于是我们便有了马尔科夫链蒙特卡洛采样\n\n### MCMC\n\n假设状态序列为：$x_{t-2},x_{t-1},x_t,x_{t+1},x_{t+2}$，则\n$$\nP(x_{t+1}|\\cdots,x_{t-2},x_{t-1},x_t) = P(x_{t+1}|x_t)\n$$\n我们有一个状态转移矩阵$P$，表示各个状态之间的转移概率。\n\n马尔科夫链有一个好的性质，就是其初始概率分布乘以状态转移矩阵$P$多次后会收敛达到==稳定的概率分布==。\n\n所以假设我们的概率分布为$\\pi^0,\\pi^1,\\cdots,\\pi^m$，假设经过$m$步后收敛，那么在$m$步之后我们都有：$\\pi P = \\pi$。\n\n那我们如何找到这个$P$呢？\n\n我们采用一个更强的条件来找$P$，即细致平衡条件：\n$$\n\\pi(i)P(i,j) = \\pi(j)P(j,i)\n$$\n有细致平衡条件可以推出$\\pi P = \\pi$，但是反过来不一定成立。\n$$\n\\sum_{i=1}^\\infty \\pi(i)P(i,j) = \\sum_{i=1}^\\infty\\pi(j)P(j,i) = \\pi(j)\\sum_{i=1}^\\infty P(j,i) = \\pi(j)\n$$\n由此可以推出$\\pi P = \\pi$。\n\n但是是不是所有的$Q$都满足这个条件呢？显然不是，对任意$Q$，有\n$$\n\\pi(i)Q(i,j)\\neq \\pi(j)Q(j,i)\n$$\n既然随便一个矩阵$Q$不行，那么我们引入$\\alpha$，使得\n$$\n\\pi(i)Q(i,j)\\alpha(i,j) = \\pi(j)Q(j,i)\\alpha(j,i)\n$$\n\n很容易得到，使得这个等式成立的$\\alpha$为：\n$$\n\\begin{aligned}\n\\alpha(i,j) &= \\pi(j)Q(j,i)\\\\\n\\alpha(j,i) &= \\pi(i)Q(i,j)\n\\end{aligned}\n$$\n由此可以将$\\alpha(i,j)$看作是一个概率，$\\alpha\\in [0,1]$。\n\n则\n$$\nP(i,j) = Q(i,j)\\alpha(i,j)\n$$\n\n> 在这里我有个疑问，如果$\\alpha(i,j)\\in [0,1]$，那么这个式子是不太可能成立的，因为$P(i,j)\\le Q(i,j)$，且两者都为概率函数，所以上式应该为：$P(i,j) = mQ(i,j)\\alpha(i,j),m>1$。这就与接受-拒绝采样差不多了。\n\n### Metropolis-Hastings采样\n\n因为$\\alpha$的值通常较小，我们用Metropolis-Hastings采样算法来解决这个问题：\n\n核心的公式仍然没变：\n$$\n\\pi(i)Q(i,j)\\alpha(i,j) = \\pi(j)Q(j,i)\\alpha(j,i)\n$$\n只是我让两边的$\\alpha$值同时扩大相同的倍数，等式仍然成立，直到其中一侧的$\\alpha$值扩大为了$1$。\n\n假设右边的大一点，原来是：\n$$\n\\pi_iQ(i,j)\\times 0.01 = \\pi_jQ(j,i)\\times 0.05\n$$\n\n\n现在是：\n$$\n\\pi_iQ(i,j)\\times 0.2 = \\pi_jQ(j,i)\\times 1\n$$\n\n\n这样我们的接受率实际是做了如下改进，即：\n$$\n\\alpha(i,j) = \\min\\{\\frac{\\pi(j)Q(j,i)}{\\pi(i)Q(i,j)},1\\}\n$$\n很多时候，我们选择的马尔科夫链状态转移矩阵如果是对称的，即满足$Q(i,j)=Q(j,i)$，这时我们的接受率可以进一步化简：\n$$\n\\alpha(i,j) = \\min\\{\\frac{\\pi(j)}{\\pi(i)}\\}\n$$\n\n#### 例\n\n假设目标平稳分布是一个均值为$10$，标准差为$5$的正态分布，而选择的马尔科夫链状态转移矩阵$Q(i,j)$的条件转移概率是以$i$为均值，方差为$1$的正态分布在位置$j$的值。\n\n~~~python\nimport random\nimport math\nfrom scipy.stats import norm\nimport matplotlib.pyplot as plt\n\ndef norm_dist_prob(theta):\n    y = norm.pdf(theta, loc=10, scale=5)\n    return y\n\nT = 5000\npi = [0 for i in range(T)]\nsigma = 1\nt = 0\nwhile t < T-1:\n    t = t + 1\n    pi_star = norm.rvs(loc=pi[t-1], scale=sigma, size=1, random_state=None)\n    alpha = min(1, norm_dist_prob(pi_star[0]) / norm_dist_prob(pi[t-1]))\n    \n    u = random.uniform(0, 1)\n    if u < alpha:\n        pi[t] = pi_star[0]\n    else:\n        pi[t] = pi[t-1]\n\nplt.scatter(pi, norm.pdf(pi, loc=10, scale=5),label='Target Distribution', c= 'red')\nnum_bins = 50\nplt.hist(pi, num_bins, density=1, facecolor='green', alpha=0.7,label='Samples Distribution')\nplt.legend()\nplt.show()\n~~~\n\n再假设目标平稳是一个$a = 2.37, b =0.627$的$\\beta$分布，而选择的马尔可夫转移矩阵$Q(i,j)$的条件概率是以$i$为均值，方差为$1$的正态分布在位置$i$的值。\n\n~~~python\nfrom scipy.stats import beta\na, b = 2.31, 0.627\n\ndef beta_dist_prob(theta):\n    y = beta(2.31, 0.627).pdf(theta)\n    return y\n\nT = 5000\npi = [0 for i in range(T)]\nsigma = 1\nt = 0\nwhile t < T-1:\n    t = t + 1\n    pi_star = norm.rvs(loc=pi[t-1], scale=sigma, size=1, random_state=None)\n    alpha = min(1, beta_dist_prob(pi_star[0]) / beta_dist_prob(pi[t-1]))\n    \n    u = random.uniform(0, 1)\n    if u < alpha:\n        pi[t] = pi_star[0]\n    else:\n        pi[t] = pi[t - 1]\n\nplt.scatter(pi, beta(2.31, 0.627).pdf(pi),label='Target Distribution', c= 'red')\nnum_bins = 50\nplt.hist(pi, num_bins, density=1, facecolor='green', alpha=0.7,label='Samples Distribution')\nplt.legend()\nplt.show()\n~~~\n\n### 吉布斯采样\n\nM-H采样在高维时计算时间较长，算法效率较低。而且，很多时候我们甚至很难求出目标的各特征维度联合分布，但是可以方便求出各个特征之间的条件概率分布。所以我们希望对条件概率分布进行抽样，得到样本的序列。\n\n对于二维概率密度函数：\n\n![](https://static01.imgkr.com/temp/7b4a4951c849420390f5190505229c78.png)\n\n假设我们取两个点$A$和$B$，我们有\n$$\n\\pi(A) = \\pi(x_1,y_1) = \\pi(x_1)\\pi(y_1|x_1),\n\\pi(B) = \\pi(x_1,y_2) = \\pi(x_1)\\pi(y_2|x_1)\n$$\n变化一下，得：\n$$\n\\pi(A)\\pi(y_2|x_1) = \\pi(x_1)\\pi(y_1|x_1)\\pi(y_2|x_1),\n\\pi(B)\\pi(y_1|x_1) = \\pi(x_1)\\pi(y_2|x_1)\\pi(y_1|x_1)\n$$\n所以\n$$\n\\pi(A)\\pi(y_2|x_1) = \\pi(B)\\pi(y_1|x_1)\n$$\n这与细致平衡条件非常相像：\n\n我们令$\\pi(y_2|x_1)$为状态转移概率$P(A\\rightarrow B)$。\n\n则\n$$\n\\pi(A)P(A\\rightarrow B) = \\pi(B)P(B\\rightarrow A)\n$$\n假设我们有第三个点$C$：\n\n则同理\n$$\n\\pi(A)\\pi(y_1|x_2) = \\pi(C)\\pi(y_1|x_1)\n$$\n即\n$$\n\\pi(A)P(A\\rightarrow C) = \\pi(C)P(C\\rightarrow A)\n$$\n那么对于所有$A^{\\prime}$都有：\n$$\n\\pi(A)P(A\\rightarrow A^{\\prime}) = \\pi(A)P(A^{\\prime}\\rightarrow A)\n$$\n![](https://static01.imgkr.com/temp/c09972e12900423281c5fcad2216d535.png)\n\n因为我们的状态转移矩阵$P(A\\rightarrow B)=\\pi(y_2|x_1)$已知，因此我们不存在拒绝采样的问题。\n\n但是我们前面的推理都是基于有一个坐标相等，如果每个坐标都不想等怎么办？\n\n![](https://static01.imgkr.com/temp/2b8c1e58ea604756b4836abb3331502d.png)\n\n如上图的$D$点，我们规定：\n$$\nP(A\\rightarrow D)=0\n$$\n即我们只允许在**平行坐标轴**上采样。\n\n\n\n吉布斯采样步骤：\n\n+ 给定平稳分布$\\pi(x_1,x_2)$\n+ $t=0$随机产生一个初始状态$(x_1^{(0)},x_2^{(0)})$\n+ 从条件概率分布$P(x_2|x_1^{(0)})$中采样$(x_1^{(0)},x_2^{(1)})$\n+ 从条件概率分布$P(x_1|x_2^{(1)})$中采样$(x_1^{(1)},x_2^{(1)})$\n+ 不停轮换坐标轴，采取指定数量样本为止\n\n#### 例\n\n假设我们要采样的是一个二维正态分布$N(\\mu,\\Sigma)$，其中：$\\mu=(\\mu_1,\\mu_2) = (5,-1),\\Sigma=\\left(\\begin{array}{cc} \\sigma_{1}^{2} & \\rho \\sigma_{1} \\sigma_{2} \\\\ \\rho \\sigma_{1} \\sigma_{2} & \\sigma_{2}^{2} \\end{array}\\right)=\\left(\\begin{array}{ll} 1 & 1 \\\\ 1 & 4\\end{array}\\right)$。\n\n首先要求得：采样过程中需要的状态转移条件分布：\n$$\nP(x_1|x_2) = N(\\mu_1+\\rho\\sigma_1/\\sigma_2(x_2-\\mu_2),(1-\\rho^2)\\sigma_1^2),\nP(x_2|x_1) = N(\\mu_2+\\rho\\sigma_2/\\sigma_1(x_1-\\mu_1),(1-\\rho^2)\\sigma_2^2)\n$$\n\n~~~python\nimport random\nimport math\nfrom scipy.stats import beta\nfrom scipy.stats import norm\nimport matplotlib.pyplot as plt\n\nfrom mpl_toolkits.mplot3d import Axes3D\nfrom scipy.stats import multivariate_normal\n\nsamplesource = multivariate_normal(mean=[5,-1], cov=[[1,1],[1,4]])\n\ndef p_ygivenx(x, m1, m2, s1, s2):\n    return (random.normalvariate(m2+0.5*s2/s1*(x-m1), math.sqrt(1-0.5**2)*s2))\n\ndef p_xgiveny(y, m1, m2, s1, s2):\n    return (random.normalvariate(m1+0.5*s1/s2*(y-m2), math.sqrt(1-0.5**2)*s1))\n\nN = 5000\nK = 50\nx_res = []\ny_res = []\nz_res = []\nm1 = 5\nm2 = -1\ns1 = 1\ns2 = 2\ny = m2\n\nfor i in range(N):\n    for j in range(K):\n        x = p_xgiveny(y, m1, m2, s1, s2)   #y给定得到x的采样\n        y = p_ygivenx(x, m1, m2, s1, s2)   #x给定得到y的采样\n        z = samplesource.pdf([x,y])\n        x_res.append(x)\n        y_res.append(y)\n        z_res.append(z)\n\nnum_bins = 50\nplt.scatter(x_res, norm.pdf(x_res, loc=5, scale=1),label='Target Distribution x', c= 'green')\nplt.scatter(y_res, norm.pdf(y_res, loc=-1, scale=2),label='Target Distribution y', c= 'orange')\nplt.hist(x_res, num_bins, density=1, facecolor='Cyan', alpha=0.5,label='x')\nplt.hist(y_res, num_bins, density=1, facecolor='magenta', alpha=0.5,label='y')\nplt.title('Histogram')\nplt.legend()\nplt.show()\n~~~\n\n","tags":["机器学习"],"categories":["算法"]},{"title":"SVM算法","url":"/2021/11/26/SVM算法/","content":"\n## 支持向量机(SVM)\n### 线性模型\n#### 线性可分训练集\n一个训练数据集线性可分是指：$\\{(x_i,y_i)\\}_{i=1\\sim N},\\exists(w,b)$，使对$\\forall i=1\\sim N$，有\n+ 若$y_i=+1$，则$w^Tx_i+b\\ge0$\n+ 若$y_i=-1$，则$w^Tx_i+b<0$\n\n即$y_i[w^Tx_i+b]\\ge0$(公式1)\n\n对于线性可分的数据集，我们需要划一条线来分割两类不同的样本。但是分割的线有无数条，我们怎么判断哪一条线更好呢？\n![](https://static01.imgkr.com/temp/bf661c3ba75549abacf5b4e9dde0e254.png)\n很多人认为第二条线是最好的。但是因为根据免费午餐定理，这三条曲线是一样好的。那么我们为什么会认为第二条曲线是最好的呢？这是因为我们在研究问题之前，对此问题存在先验假设。有很多先验假设认为第二条直线比其余两条要好。我们只考虑其中一种假设，即假设*训练样本的位置在特征空间有测量误差*。如下图所示：\n![](https://static01.imgkr.com/temp/51e3dc9f3d9847298c80f36fc1d22328.png)\n假设红色的叉和圆圈的真实位置为红色虚线圆圈，则线3和1都会分类错误，而2不会，这说明2号线更能抵御训练样本位置的误差。\n\n那么2号线是怎么画出来的呢？\n支持向量机的创造者Vapnik是这样回答的，它首先将直线向一侧平行移动，直到它叉到一个或几个样本为止；之后再向另一侧移动，直到叉到一个或多个样本未知。\n![](https://static01.imgkr.com/temp/a184f5d78b3b4cd78df273fbd4f0adfd.png)\n我们要找的2号线找的是使得间隔最大的且位于间隔中间的线。\n**在多维的情况下，直线变为超平面**。\n\n之后我们将支持向量机转化为一个优化问题，优化问题为：\n$$\n\\begin{aligned}\n\t&\\min \\frac{1}{2}||w||^2\\\\\n\t&\\operatorname{s.t.} \\quad y_i[w^T_i+b]\\ge1\n\\end{aligned}\n$$\n\n那么这是怎么得到的呢？那面我们详细讨论一下：\n\n事实一：$w^Tb+b=0$与$aw^Tx+ab=0$是同一个平面，$a\\in R^+$。即若$(w,b)$满足公式1，则$(aw,ab)$也满足公式一。\n> 公式1：$y_i[w^Tx_i+b]\\ge0$\n\n事实二：点到平面的距离公式。\n向量$x_0$到超平面$w^Tx+b=0$的距离：\n$$\nd = \\frac{|w^Tx_0+b|}{||w||}\n$$\n当$x_0$为支持向量时，我们要做的就是最大化$d$。\n根据事实1，我们可以用$a$去缩放：\n$$\n(w,b)\\rightarrow (aw,ab)\n$$\n最终使在支持向量上$x_0$上，有：\n$$\n\t|w^Tx_0+b|=1\n$$\n此时支持向量与平面距离：\n$$\n d = \\frac{1}{||w||}\n$$\n因为最大化$\\frac{1}{||w||}$相当于最小化$||w||^2$，所以得到上述的目标函数。\n\n下面看约束条件是如何得到的。\n因为在上面的描述中我们有，对于所有的支持向量，我们有\n$$\n|w^Tx_0+b|=1\n$$\n所以对于非支持向量，我们有：\n$$\n|w^Tx_0+b|>1\n$$\n又因为：$y_i[w^Tx_i+b]\\ge0$，所以综上我们有\n$$\ny_i[w^Tx_i+b] = |w^Tx_0+b|\\ge 1\n$$\n这样我们就得到了上面提到的优化问题。\n\n这个优化问题为凸优化问题中的*二次优化问题*。\n二次规划问题：\n1. 目标函数是二次项\n2. 限制条件是一次项\n\n这样就会导致要么无解，要么只有一个极值。\n### 非线性可分\n我们改写目标函数和约束条件，使其变为：\n$$\n\\begin{aligned}\n&\\min&\\quad\\frac{1}{2}||w||^2+C\\sum_{i=1}^N\\xi_i\\\\\n&\\operatorname{s.t.}\\quad &y_i[w^Tx_i+b]\\ge 1-\\xi_i\\\\\n&\\quad &\\xi_i\\ge0\n\\end{aligned}\n$$\n其中$\\xi_i$称为松弛变量，$C\\sum_{i=1}^N\\xi_i$称为正则项。\n### 非线性问题\n对于下图所示的问题，我们不能找到一个很好的直线将两类分开：\n![](https://static01.imgkr.com/temp/491f1d06bdce47cf8cff0ed74a575eb3.png)\n但是我们可以将其映射到高维空间中的点，然后在高维空间中寻找直线。\n我们定义一个从低维到高维的映射$\\phi(x)$：\n$$\nx\\xrightarrow{\\phi}\\phi(x)\n$$\n其中$x$为低维向量，而$\\phi(x)$为一个高维映射。\n\n下面我们举一个例子：如下图所示的异或问题\n![](https://static01.imgkr.com/temp/c0bcf66b2ef143ae9b7e93f699654ddb.png)\n这个问题我们在二维空间里无法找到一条直线将其分开。\n在上图中我们令四个点分别为：\n$$\nx_1 = \\begin{bmatrix}0\\\\0\\end{bmatrix},x_2 = \\begin{bmatrix}1\\\\1\\end{bmatrix}\\in C_1\n$$\n$$\nx_3 = \\begin{bmatrix}1\\\\0\\end{bmatrix},x_4 = \\begin{bmatrix}0\\\\1\\end{bmatrix}\\in C_2\n$$\n我们令\n$$\n\\phi(x): x = \\begin{bmatrix}a\\\\b\\end{bmatrix}\\xrightarrow{\\phi}\\phi(x) = \\begin{bmatrix}a^2\\\\b^2\\\\a\\\\b\\\\ab\\end{bmatrix}\n$$\n则经过映射得到：\n$$\n\\phi(x_1)= \\begin{bmatrix}0\\\\0\\\\0\\\\0\\\\0\\end{bmatrix},\\phi(x_2)= \\begin{bmatrix}1\\\\1\\\\1\\\\1\\\\1\\end{bmatrix}\n$$\n$$\n\\phi(x_3)= \\begin{bmatrix}1\\\\0\\\\1\\\\0\\\\0\\end{bmatrix},\\phi(x_4)= \\begin{bmatrix}0\\\\1\\\\0\\\\1\\\\0\\end{bmatrix}\n$$\n我们可以令\n$$\nw= \\begin{bmatrix}-1\\\\-1\\\\-1\\\\-1\\\\6\\end{bmatrix},b=1\n$$\n来达到区分的目的。\n\n有证明显示：在越高维度情况下，找打一个线性超平面来将样本分开的概率越大。我们如何选取$\\phi$，我们将$\\phi(x)$选择为无限维。但是$\\phi(x)$为无限维，$w$将为无限维，优化问题将不可做。\n\n我们可以不知道无限维映射$\\phi(x)$的显式表达，我们只要知道一个核函数：\n$$\nK(x_1,x_2) = \\phi(x_1)^T\\phi(x_2)\n$$\n下面的优化问题：\n$$\n\\begin{aligned}\n&\\min&\\quad\\frac{1}{2}||w||^2+C\\sum_{i=1}^N\\xi_i\\\\\n&\\operatorname{s.t.}\\quad &y_i[w^T\\phi(x_i)+b]\\ge 1-\\xi_i\\\\\n&\\quad &\\xi_i\\ge0\n\\end{aligned}\n$$\n仍然可解。\n\n在SVM中常用的核函数：\n$$\nK(x_1,x_2) = e^{-\\frac{||x_1-x_2||^2}{2\\sigma^2}} = \\phi(x_1)^T\\phi(x_2)\n$$\n为高斯核函数\n$$\nK(x_1,x_2) = (x_1^Tx_2+1)^d = \\phi(x_1)^T\\phi(x_2)\n$$\n为多项式核函数，$d$为阶数。\n\n核函数$K$必须满足某些条件才能被拆成内积的形式：\n$K(x_1,x_2)$能被写成$\\phi(x_1)^T\\phi(x_2)$的充要条件为：\n1. $K(x_1,x_2)=K(x_2,x_1)$\n2. $\\forall c_i\\in R,x_i(i=1\\sim N)$，有$$\\sum_{i=1}^N\\sum_{i=1}^Nc_ic_jK(x_i,x_j)\\ge 0$$\n\n### 原问题和对偶问题\n#### 原问题\n最小化：$f(w)$\n限制条件：\n+ $g_i(w)\\le0(i=1\\sim K)$\n+ $h_i(w)=0(i=1\\sim M)$\n\n#### 对偶问题\n定义：\n$$\n\\begin{aligned}\nL(w,\\alpha,\\beta) &= f(w) + \\sum_{i=1}^K\\alpha_ig_i(w)+\\sum_{i=1}^M\\beta_ih_i(w)\\\\\n&= f(w) + \\alpha^Tg(w)+\\beta^Th(w)\n\\end{aligned}\n$$\n\n\n对偶问题的定义\n最大化：$\\theta(\\alpha,\\beta)=\\inf_w(w,\\alpha,\\beta)$，$\\inf$表示下界\n限制条件：$\\alpha_i\\ge 0,\\beta_i\\ge0$\n\n#### 原问题和对偶问题解的关系\n定理：如果$w^\\star$是原问题的解，而$\\alpha^\\star,\\beta^\\star$是对偶问题的解，则有：\n$$\nf(w^\\star)\\ge \\theta(\\alpha^\\star,\\beta^\\star)\n$$\n证明：\n$$\n\\begin{aligned}\n\\theta(\\alpha^\\star,\\beta^\\star) &= \\inf_w L(w,\\alpha^\\star,\\beta^\\star)\\\\\n&\\le L(w^\\star,\\alpha^\\star,\\beta^\\star) = f(w^\\star) + \\sum_{i=1}^K\\alpha^\\star_ig_i(w^\\star)+\\sum_{i=1}^M\\beta^\\star_ih_i(w^\\star)\\\\\n&\\le f(w^\\star)\n\\end{aligned}\n$$\n\n定义：\n$$\nG = f(w^\\star) - \\theta(\\alpha^\\star,\\beta^\\star)\\ge0\n$$\n$G$叫做原问题与对偶问题的间距。对于某些特定优化问题，可以证明：$G=0$。\n\n强对偶定理：若$f(w)$为凸函数，且$g(w)=Aw+b,h(w)=Cw+d$，则此优化问题的原问题与对偶问题的间距为$0$。即\n$$\nf(w^\\star) = \\theta(\\alpha^\\star,\\beta^\\star)\n$$\n此时我们易得对$\\forall i=1\\sim K$：\n+ 或者$\\alpha^\\star_i=0$\n+ 或者$g^{\\star}_i(w^\\star)=0$\n\n这被称为**KKT条件**。\n### 利用对偶问题求解SVM\n我们先复习一下原问题：\n$$\n\\begin{aligned}\n&\\min&\\quad\\frac{1}{2}||w||^2+C\\sum_{i=1}^N\\xi_i\\\\\n&\\operatorname{s.t.}\\quad &y_i[w^T\\phi(x_i)+b]\\ge 1-\\xi_i\\\\\n&\\quad &\\xi_i\\ge0\n\\end{aligned}\n$$\n\n根据原问题的定义形式，我们将上述问题改写：\n$$\n\\begin{aligned}\n&\\min&\\quad\\frac{1}{2}||w||^2-C\\sum_{i=1}^N\\xi_i\\\\\n&\\operatorname{s.t.}&\\quad 1+\\xi_i-y_i w^T\\phi(x_i)-y_ib\\le 0\\\\\n&\\quad &\\xi_i\\le0\n\\end{aligned}\n$$\n\n\n\n凸函数定义：\n$$\nf(\\lambda x_1+(1-\\lambda)x_2)\\le \\lambda f(x_1)+(1-\\lambda)f(x_2)\n$$\n\n我们SVM的对偶问题为：\n最大化：\n$$\n\\theta(\\alpha,\\beta) = \\inf_{(w,\\xi_i,b)}\\{\\frac{1}{2}||w||^2-C\\sum_{i=1}^N\\xi_i+\\sum_{i=1}^N\\beta_i\\xi_i+\\sum_{i=1}^N\\alpha_i[1+\\xi_i-y_i w^T\\phi(x_i)-y_ib]\\}\n$$\n限制条件：\n+ $\\alpha_i\\ge 0$\n+ $\\beta_i\\ge 0$\n\n我们要想求得$\\theta(\\alpha,\\beta)$，首先要最优化$w,\\xi_i,b$，对$L$函数求偏导：\n$$\n\\begin{aligned}\n\\frac{\\partial L}{\\partial w} = 0&\\Rightarrow w = \\sum_{i=1}^N\\alpha_iy_i\\phi(x_i)\\\\\n\\frac{\\partial L}{\\partial \\xi_i} = 0&\\Rightarrow \\beta_i+\\alpha_i=C\\\\\n\\frac{\\partial L}{\\partial b}=0&\\Rightarrow \\sum_{i=1}^N\\alpha_iy_i=0\n\\end{aligned}\n$$\n将其代入，得到：\n$$\n\\theta(\\alpha,\\beta) = \\sum_{i=1}^N\\alpha_i-\\frac{1}{2}\\sum_{i=1}^N\\sum_{j=1}^N\\alpha_i\\alpha_jy_iy_jK(x_i,x_j) \n$$\n其中$K(x_i,x_j) = \\phi(x_i)^T\\phi(x_j)$。\n\n所以对偶优化问题变为：\n最大化：\n$$\n\\theta(\\alpha) = \\sum_{i=1}^N\\alpha_i-\\frac{1}{2}\\sum_{i=1}^N\\sum_{j=1}^N\\alpha_i\\alpha_jy_iy_jK(x_i,x_j) \n$$\n限制条件：\n1. $0\\le\\alpha_i\\le C$\n2. $\\sum_{i=1}^N\\alpha_iy_i=0$\n\n这也是一个凸优化问题，解这个问题有一个标准的算法(SMO)算法。\n\n这样我们就可以求出$\\alpha_i,i=1\\sim N$，但是我们要求的是$w$和$b$，那么我们应该如何求出$w,b$呢，我们可以用之前得到的$w = \\sum_{i=1}^N\\alpha_iy_i\\phi(x_i)$，但问题是我们并不知道$\\phi(x_i)$。\n\n但是在判断样本属于哪一类的时候我们并不需要知道$w$，假设有测试样本$x$，我们知道：\n+ 若$w^T\\phi(x)+b\\ge0$，则$y=+1$\n+ 若$w^T\\phi(x)+b<0$，则$y=-1$\n\n而\n$$\n\\begin{aligned}\nw^T\\phi(x) &= \\sum_{i=1}^N\\alpha_iy_i\\phi(x_i)^T\\phi(x)\\\\\n&= \\sum_{i=1}^N\\alpha_iy_iK(x_i,x)\n\\end{aligned}\n$$\n\n但是$b$应该怎么算呢？\n应用KKT条件，我们有\n+ 要么$\\beta_i=0$，要么$\\xi_i=0$\n+ 要么$\\alpha_i=0$，要么$1+\\xi_i-y_iw^T\\phi(x_i)-y_ib=0$\n\n我们取一个$0<\\alpha_i<C\\Rightarrow \\beta_i=C-\\alpha_i>0$，\n\n此时$\\beta_i\\neq0\\Rightarrow\\xi_i=0$，因为$\\alpha_i\\neq0\\Rightarrow b = y_i - \\sum_{j=1}^N\\alpha_jy_jK(x_i,x_j)$。也可以找到所有不等于$0$的$\\alpha_i$，求得$b$取平均。\n\n### SMO算法\n最大化：\n$$\n\\theta(\\alpha) = \\sum_{i=1}^N\\alpha_i-\\frac{1}{2}\\sum_{i=1}^N\\sum_{j=1}^N\\alpha_i\\alpha_jy_iy_jK(x_i,x_j) \n$$\n限制条件：\n1. $0\\le\\alpha_i\\le C$\n2. $\\sum_{i=1}^N\\alpha_iy_i=0$\n\n因为我们要优化的变量($\\alpha_i$)很多，所以我们每次迭代只选择几个变量进行更新；又因为我们的是有约束的优化问题，所以每次更新可能会破坏我们的约束条件，为了不破坏我们的约束条件，我们每次至少选择两个变量进行优化。因此我们每次选择两个变量来进行优化。\n\n#### 两个变量二次规划的求解过程\n+ 选择两个变量，其他变量固定\n+ SMO将对偶问题转化成一系列子问题\n\n$$\n\\begin{aligned}\n\n\\min _{\\alpha_{1}, \\alpha_{2}} & W\\left(\\alpha_{1}, \\alpha_{2}\\right)=\\frac{1}{2} K_{11} \\alpha_{1}^{2}+\\frac{1}{2} K_{22} \\alpha_{2}^{2}+y_{1} y_{2} K_{12} \\alpha_{1} \\alpha_{2} \\\\\n\n& -\\left(\\alpha_{1}+\\alpha_{2}\\right)+y_{1} \\alpha_{1} \\sum_{i=3}^{N} y_{i} \\alpha_{i} K_{i 1}+y_{2} \\alpha_{2} \\sum_{i=3}^{N} y_{i} \\alpha_{i} K_{i 2} \\\\\n\n\\text { s.t. } & \\alpha_{1} y_{1}+\\alpha_{2} y_{2}=-\\sum_{i=3}^{N} y_{i} \\alpha_{i}=\\zeta \\\\\n\n& 0 \\leq \\alpha_{i} \\leq C, i=1,2\n\\end{aligned}\n$$\n+ 根据约束条件，$\\alpha_2$可以表示为$\\alpha_1$的函数\n+ 优化问题有解析解\n+ 基于初始可行解$\\alpha_1^{old},\\alpha_2^{old}$，可以得到$\\alpha_1^{new},\\alpha_2^{new}$\n\n两个变量，约束条件用二维空间中的图形表示：\n![](https://static01.imgkr.com/temp/fb18fa3a67f14d0eaecf9214ecf952ec.png)\n下面首先考虑第一种情况，根据不等式条件$\\alpha_2^{new}$的取值范围：\n$$\n\tL\\le \\alpha_2^{new} \\le H\n$$\n其中\n$$\nL = \\max(0,\\alpha_2^{old}-\\alpha_1^{old})\\quad H = \\min(C,C+\\alpha_2^{old}-\\alpha_1^{old})\n$$\n\n同理对于第二种情况，根据不等式条件$\\alpha_2^{new}$的取值范围：\n$$\n\tL\\le \\alpha_2^{new} \\le H\n$$\n其中\n$$\nL = \\max(0,\\alpha_2^{old}+\\alpha_1^{old}-C)\\quad H = \\min(C,\\alpha_2^{old}+\\alpha_1^{old})\n$$\n\n下面开始求解，求得的过程为：\n+ 先求沿着约束方向未经剪辑时的$\\alpha_2^{new,unc}$\n+ 再求剪辑后的$\\alpha_2^{new}$\n\n我们记\n$g(x)=\\sum_{i=1}^N\\alpha_iy_iK(x_i,x)+b$，即我们的判别表达式，令：\n$$\nE_i = g(x_i)-y_i = \\left(\\sum_{j=1}^N\\alpha_jy_jK(x_j,x_i)+b\\right)-y_i\n$$\n为输入$x$的预测值和真实输出$y$的差。\n为了简便，引进记号：\n$$\nv_i = \\sum_{j=3}^N\\alpha_jy_jK(x_i,x_j) = g(x_i) - \\sum_{j=1}^2\\alpha_jy_jK(x_i,x_j)-b\n$$\n目标函数写成：\n$$\n\\begin{aligned}\n\nW\\left(\\alpha_{1}, \\alpha_{2}\\right)=& \\frac{1}{2} K_{11} \\alpha_{1}^{2}+\\frac{1}{2} K_{22} \\alpha_{2}^{2}+y_{1} y_{2} K_{12} \\alpha_{1} \\alpha_{2} \\\\\n\n&-\\left(\\alpha_{1}+\\alpha_{2}\\right)+y_{1} v_{1} \\alpha_{1}+y_{2} v_{2} \\alpha_{2}\n\n\\end{aligned}\n$$\n由$\\alpha_1y_1 = \\zeta-\\alpha_2y_2$及$y_i^2=1$，我们得$\\alpha_1 = (\\zeta-y_2\\alpha_2)y_1$，代入上式得到只是$\\alpha_2$的函数的目标函数：\n$$\n\\begin{aligned}\nW(\\alpha_2) &= \\frac{1}{2}K_{11}(\\zeta-\\alpha_2y_2)^2 + \\frac{1}{2}K_{22}\\alpha_2^2+y_2K_{12}(\\zeta-\\alpha_2y_2)\\alpha_2\\\\\n&-(\\zeta-\\alpha_2y_2)y_1-\\alpha_2+v_1(\\zeta-\\alpha_2y_2)+y_2v_2\\alpha_2\n\\end{aligned}\n$$\n对$\\alpha_2$求导并令其等于$0$，得：\n$$\n\\begin{aligned}\n\n&\\left(K_{11}+K_{22}-2 K_{12}\\right) \\alpha_{2}=y_{2}\\left(y_{2}-y_{1}+\\zeta K_{11}-\\zeta K_{12}+v_{1}-v_{2}\\right) \\\\\n\n&=y_{2}\\left[y_{2}-y_{1}+\\zeta K_{11}-\\zeta K_{12}+\\left(g\\left(x_{1}\\right)-\\sum_{j=1}^{2} y_{j} \\alpha_{j} K_{1 j}-b\\right)-\\left(g\\left(x_{2}\\right)-\\sum_{j=1}^{2} y_{j} \\alpha_{j} K_{2 j}-b\\right)\\right]\n\n\\end{aligned}\n$$\n将$\\zeta=\\alpha_1^{old}y_1+\\alpha_2^{old}y_2$代入：\n$$\n\\begin{aligned}\n\n\\left(K_{11}+K_{22}-2 K 12\\right) \\alpha_{2}^{n e w, u n c} &\\left.=y_{2}\\left(\\left(K_{11}+K_{22}-2 K_{12}\\right) \\alpha_{2}^{\\text {old }} y_{2}+y_{2}-y_{1}+g\\left(x_{1}\\right)-g\\left(x_{2}\\right)\\right)\\right) \\\\\n\n&=\\left(K_{11}+K_{22}-2 K_{12}\\right) \\alpha_{2}^{\\text {old }}+y_{2}\\left(E_{1}-E_{2}\\right)\n\n\\end{aligned}\n$$\n将$\\eta = K_{11}+K_{22}-2K_{12}$代入：\n$$\n\t\\alpha_2^{new,unc} = \\alpha_2^{old}+\\frac{y_2(E_1-E_2)}{\\eta}\n$$\n我们之后对解进行剪辑：\n$$\n\\begin{cases}\nH,\\quad &\\alpha_2^{new,unc}>H\\\\\n\\alpha_2^{new,unc},\\quad&L\\le \\alpha_2^{new,unc}\\le H\\\\\nL,\\quad& \\alpha_2^{new,unc}<L\n\\end{cases}\n$$\n得到$\\alpha_1$的解：\n$$\n\\alpha_1^{new} = \\alpha_1^{old}+y_1y_2(\\alpha_2^{old}-\\alpha_2^{new})\n$$\n关于KKT条件，我们有：\n$$\n\\begin{array}{r}\n\n\\alpha_{i}=0 \\Leftrightarrow y_{i} g\\left(x_{i}\\right) \\geqslant 1 \\\\\n\n0<\\alpha_{i}<C \\Leftrightarrow y_{i} g\\left(x_{i}\\right)=1 \\\\\n\n\\alpha_{i}=C \\Leftrightarrow y_{i} g\\left(x_{i}\\right) \\leqslant 1\n\n\\end{array}\n$$\n下面我们计算阈值$b$和$E_i$\n由KKT条件，如果$0<\\alpha_1^{new}<C$，则\n$$\n\\sum_{i=1}^N\\alpha_iy_iK_{i1}+b=y_1\n$$\n$$\nb_1^{new} = y_1-\\sum_{i=3}^N\\alpha_iy_iK_{i1}-\\alpha_1^{new}y_1K_{11}-\\alpha_2^{new}y_2K_{21}\n$$\n$$\nE_i = g(x_i)-y_i = \\left(\\sum_{j=1}^N\\alpha_jy_jK(x_j,x_i)+b\\right)-y_i\n$$\n$$\nE_1 = \\sum_{i=3}^N\\alpha_iy_iK_{i1}+\\alpha_1^{old}y_1K_{11}+\\alpha_2^{old}y_2K_{21}+b^{old}-y_1\n$$\n$E_1$的表达式与$b_1^{new}$相结合，得：\n$$\nb_1^{new} = -E_1-y_1K_{11}(\\alpha_1^{new}-\\alpha_1^{old})-y_2K_{21}(\\alpha_2^{new}-\\alpha_2^{old})+b^{old}\n$$\n同理，如果$0<\\alpha_2^{new}<C$，则\n$$\n  \n\n\\begin{aligned}\n\n&0<\\alpha_{2}^{\\text {new }}<C \\\\\n\n&b_{2}^{\\text {new }}=-E_{2}-y_{1} K_{12}\\left(\\alpha_{1}^{\\text {new }}-\\alpha_{1}^{\\text {old }}\\right)-y_{2} K_{22}\\left(\\alpha_{2}^{\\text {new }}-\\alpha_{2}^{\\text {old }}\\right)+b^{\\text {old }} \\\\\n\n&E_{i}^{\\text {new }}=\\sum_{S} y_{j} \\alpha_{j} K\\left(x_{i}, x_{j}\\right)+b^{\\text {new }}-y_{i}\n\n\\end{aligned}\n$$\n如果$\\alpha_1^{new},\\alpha_2^{new}$同时满足条件$0<\\alpha_i^{new}<C$，那么$b_1^{new}=b_2^{new}$。如果$\\alpha_1^{new},\\alpha_2^{new}$是$0$或者$C$，那么$b_1^{new},b_2^{new}$以及它们之间的数都是符合KKT条件的阈值，这时选择它们的中点作为$b^{new}$。\n在每次完成两个变量的优化之后，还必须更新对应的$E_i$值，并将它们保存在列表中。$E_i$值的更新要用到$b^{new}$值，以及所有支持向量对应的$\\alpha_j$：\n$$\nE_i^{new} = \\sum_{S}y_ja_jK(x_i,x_j)+b^{new}-y_i\n$$\n其中，$S$是所有支持向量的集合。\n[关于此方面的解释](https://zhuanlan.zhihu.com/p/62367247)\n#### 变量的启发式选择\nSMO算法在每个子问题中选择两个变量优化，其中至少一个变量是违反KKT条件的。\n1. 第一个变量的选择：外循环\n\t1. 违反KKT条件最严重的样本点\n\t2. 检验样本点是否满足KKT条件：\n\t3. $$\\begin{array}{r}\n\t\\alpha_{i}=0 \\Leftrightarrow y_{i} g\\left(x_{i}\\right) \\geqslant 1 \\\\ 0<\\alpha_{i}<C \\Leftrightarrow y_{i} g\\left(x_{i}\\right)=1 \\\\ \\alpha_{i}=C \\Leftrightarrow y_{i} g\\left(x_{i}\\right) \\leqslant 1\n\t\\end{array}$$\n\n该检验是在$\\epsilon$范围内进行的。在检验过程中，外层循环首先遍历满足条件$0<\\alpha_i<C$的样本点，即在间隔边界上的支持向量点，检验它们是否满足KKT条件，如果这些样本点都满足KKT条件，那么遍历整个训练集，检验他们是否满足KKT条件。\n1. 第二个变量的检查：内循环\n\t1. 选择的标准是希望能使目标函数有足够大的变化\n\t\t1. 即对应$|E_1-E_2|$最大\n\t2. 如果内循环通过上述方法找到的点不能使目标函数有足够大的下降，则：遍历间隔边界上的样本点，测试目标函数下降\n\t\t1. 如果下降不大，则遍历所有样本点\n\t\t2. 如果依然下降不大，则丢弃外循环点，重新选择\n\n[算法实现](https://zhuanlan.zhihu.com/p/138556326)\n~~~python\ndef kernal(a, b):\n    # 高斯核, s为方差\n    return a.dot(b)\nclass SVM:\n    def __init__(self,data,label):\n        self.data = data\n        self.label = label\n        self.len = data.shape[0]\n        self.E = np.zeros((self.len,1))\n        self.alpha = np.random.random((self.len,1))\n        self.b = 0\n    def f(self,i):\n        # 传入索引\n        s = 0\n        for k in range(self.len):\n            s += self.alpha[k]*self.label[k]*kernal(self.data[i,:],self.data[k,:])\n        return s + self.b\n    def error(self):\n        # 计算E\n        for j in range(self.len):\n            E = 0\n            for i in range(self.len):\n                E += self.alpha[i]*self.label[i]*kernal(self.data[i,:],self.data[j,:])\n            self.E[j] = E + self.b - self.label[j]\n        \n    def bound(self, i,j,alpha_i, alpha_j, C):\n        # 传入两个索引，为需要优化的alpha的索引\n        # 求解alpha_j的范围\n        # C为正则化项系数\n        if self.label[i] != self.label[j]:\n            L, H = np.max([0, alpha_j-alpha_i]), np.min([C, C+alpha_j-alpha_i])\n        else:\n            L, H = np.max([0, alpha_j+alpha_i-C]), np.min([C, alpha_j+alpha_i])\n        return (L,H)\n    \n    def update(self, i,j, C):\n        # 更新alpha和b\n        # 传入索引\n        eta = kernal(self.data[i,:],self.data[i,:]) + kernal(self.data[j,:],self.data[j,:]) - 2*kernal(self.data[i,:],self.data[j,:])\n        \n        # 下面需要补充逻辑关系\n        alpha_old_i = self.alpha[i]\n        alpha_old_j = self.alpha[j]\n        self.alpha[j] = self.alpha[j] + self.label[j]*(self.E[i]-self.E[j])/eta\n        L, H = self.bound(i,j,alpha_old_i,alpha_old_j,C)\n        if self.alpha[j] >= H:\n            self.alpha[j] = H\n        elif self.alpha[j] <= L:\n            self.alpha[j] = L\n        else:\n            self.alpha[j] = self.alpha[j]\n        self.alpha[i] = self.alpha[i] + self.label[i]*self.label[j]*(alpha_old_j-self.alpha[j])\n        \n        b1 = self.b - self.E[i] - self.label[i]*(self.alpha[i]-alpha_old_i)*kernal(self.data[i,:],self.data[i,:])-self.label[j]*(self.alpha[j]-alpha_old_j)*kernal(self.data[j,:],self.data[i,:])\n        b2 = self.b - self.E[j] - self.label[i]*(self.alpha[i]-alpha_old_i)*kernal(self.data[i,:],self.data[j,:])-self.label[j]*(self.alpha[j]-alpha_old_j)*kernal(self.data[j,:],self.data[j,:])\n        \n        self.b = (b1+b2)/2\n        self.error()\n    def smo(self, epsilon, max_iter, C):\n        for k in range(max_iter):\n            I = np.intersect1d(np.argwhere(self.alpha>0),np.argwhere(self.alpha<C))\n            d = []\n            for i in I:\n                d.append(np.abs(self.label[i]*self.f(i)-1))\n            i = np.argmax(np.array(d))\n            j = np.argmax(self.E-self.E[i])\n            self.update(i,j,C)\n        \n            ge = np.array([i  for i in range(self.len) if self.alpha[i]>=1])\n            eq = np.array([i  for i in range(self.len) if self.alpha[i]==1])\n            le = np.array([i  for i in range(self.len) if self.alpha[i]<=1])\n            Ge = []\n            Eq = []\n            Le = []\n            for ig in ge:\n                Ge.appned(self.label[ig]*self.f(ig))\n            for ie in eq:\n                Eq.append(self.label[ie]*self.f(ie))\n            for il in le:\n                Le.append(self.label[il]*self.f(il))\n            \n            Ge = np.array(Ge)\n            Eq = np.array(Eq)\n            Le = np.array(Le)\n            \n            ne = np.sum(np.abs(Eq-1)>epsilon)\n            ng1 = np.sum(np.abs(Ge-1)<0)\n            ng2 = np.sum(np.abs(Ge-1)>epsilon)\n            nl1 = np.sum(np.abs(Le-1)>0)\n            nl2 = np.sum(np.abs(Le-1)>epsilon)\n            if (ne+ng1+ng2+nl1+nl2)==0:\n                break\n    def predict(self, x):\n        s = 0\n        for k in range(self.len):\n            s += self.alpha[k]*self.label[k]*kernal(self.data[k,:],x)\n        s = s + self.b\n        if s >=0:\n            return 1\n        else:\n            return -1\n~~~\n","tags":["机器学习"],"categories":["算法"]},{"title":"计算机视觉中的线性代数第一章","url":"/2021/10/31/Linear-algebra-in-cv/","content":"\n# 向量空间、基、线性映射\n## 线性组合、线性独立和秩\n在线性优化问题中，我们经常会遇到线性方程组。例如，考虑求解下列具有三个变量$x_1,x_2,x_2\\mathbb{R}$的三个线性方程组：\n$$\n\\begin{aligned}\nx_1 + 2x_2 - x_3 &= 1\\\\\n2x_1 + x_2 + x_3 &= 2\\\\\nx_1 - 2x_2 - 2x_3 &= 3\n\\end{aligned}\n$$\n解决这个问题的一个方法是引入向量$u,v,w$和$b$，为\n$$u=\\left(\\begin{array}{l}1 \\\\ 2 \\\\ 1\\end{array}\\right) \\quad v=\\left(\\begin{array}{c}2 \\\\ 1 \\\\ -2\\end{array}\\right) \\quad w=\\left(\\begin{array}{c}-1 \\\\ 1 \\\\ -2\\end{array}\\right) \\quad b=\\left(\\begin{array}{l}1 \\\\ 2 \\\\ 3\\end{array}\\right)$$\n所以我们的线性系统可以写为：\n$$\nx_1u + x_2v + x_3w = b\n$$\n> 我们通常将列向量写为$\\mathbb{R}^{3\\times1}$表示$3$行$1$列，而行向量写作$\\mathbb{R}^3$。\n\n下面的公式\n$$\nx_1u + x_2v + x_3w\n$$\n其中$u,v,w$为向量并且$x_i\\in \\mathbb{R}$被称为线性映射。使用这种符号，我们线性系统的解\n$$\nx_1u + x_2v + x_3w\n$$\n等价于确定$b$是否可以被写成$u,v,w$的线性组合的形式。\n如果$u,v,w$是线性独立的，着意味着不存在三元组$(x_1,x_2,x_3)\\neq(0,0,0)$使得\n$$\nx_1u + x_2v + x_3w = 0\n$$\n可以证明所有属于$\\mathbb{R}^{3\\times 1}$的向量可以写成$u,v,w$的线性组合。\n事实上，任何向量$z\\in \\mathbb{R}^{3\\times 1}$可以被唯一地写成下列线性组合的形式：\n$$\nz = x_1u+x_2v+x_3w\n$$\n这是因为：\n\n> 假设\n> $$\n> z = x_1u + x_2v + x_3w = y_1u+y_2v+y_2w\n> $$\n> 移项，得：\n> $$\n> (y_1-x_1)u + (y_2-x_2)v + (y_3-x_3)w = 0\n> $$\n> 通过线性独立，我们得到：\n> $$\n> y_1-x_1 = y_2-x_2 = y_3-x_3=0\n> $$\n> 这意味着$z$只有一种线性组合的表示方法。\n\n但是我们如何确定一些向量是否是线性独立的呢？\n一个办法是计算数值$\\det(u,v,w)$，称作$(u,v,w)$的秩，并检查其是否为零，不为零说明线性独立。\n我们也可以将我们的线性系统写为矩阵的形式。我们的线性系统以矩阵形式表示为$Ax=b$的观点强调了这样一个事实，即映射$x\\mapsto Ax$是一个线性映射。这意味着：\n$$\nA(\\lambda x) = \\lambda(Ax)\n$$\n对于任何$x\\in \\mathbb{R}^{3\\times 1}$和$\\lambda \\in \\mathbb{R}$，并且\n$$\nA(u+v) = Au + Av\n$$\n对于任何$u,v\\in \\mathbb{R}^{3\\times 1}$。我们可以将矩阵$A$看作是表达从$\\mathbb{R}^{3\\times1}$到$\\mathbb{R}^{3\\times1}$的线性映射并且求解系统$Ax=b$相当于确定$b$是否为此线性映射的像(image)。\n考虑一个$3\\times3$的矩阵$A$，\n$$\nA =\\left(\\begin{array}{lll}a_{11} & a_{12} & a_{13} \\\\ a_{21} & a_{22} & a_{23} \\\\ a_{31} & a_{32} & a_{33}\\end{array}\\right)\n$$\n它的列是三个向量，表示为$A^1,A^2,A^3$，给定任意向量$x = (x_1,x_2,x_3)$，我们将乘积$Ax$定义为线性组合的形式\n$$\n  \n\nA x=x_{1} A^{1}+x_{2} A^{2}+x_{3} A^{3}=\\left(\\begin{array}{l}\n\na_{11} x_{1}+a_{12} x_{2}+a_{13} x_{3} \\\\\n\na_{21} x_{1}+a_{22} x_{2}+a_{23} x_{3} \\\\\n\na_{31} x_{1}+a_{32} x_{2}+a_{33} x_{3}\n\n\\end{array}\\right)\n$$\n。常见的模式是，$Ax$的第$i$个坐标由行向量($A$的第$i$行)乘以列向量$x$的某种称为内积的乘积给出：\n$$\n\\left(\\begin{array}{lll}a_{i 1} & a_{i 2} & a_{i 3}\\end{array}\\right) \\cdot\\left(\\begin{array}{l}x_{1} \\\\ x_{2} \\\\ x_{3}\\end{array}\\right)=a_{i 1} x_{1}+a_{i 2} x_{2}+a_{i 3} x_{3}\n$$\n内积非常重要。首先，我们量化\n$$\n ||x||_2= \\sqrt{x\\cdot x} = (x_1^2+\\cdots+x_n^2)^{1/2}\n$$\n为向量长度的推广，称为欧几里得范数，或者$\\ell^2$范数。另外，可以证明以下不等式\n$$\n|x\\cdot y| \\le ||x||\\cdot||y||\n$$\n如果$x,y\\neq0$，比值$(x\\cdot y)/(||x||\\cdot||y||)$可以被看作是角度的余弦值。\n\n保持内积不变的矩阵(正交矩阵)$Q$，即对所有的$x,y\\in \\mathbb{R}^n, <Qx, Qy>=<x,y>$也有着非常重要的作用。它们可以被认为是广义的旋转。 ^zkwecp\n\n返回到矩阵，如果$A$为一个包含$n$列$A^1,\\cdots,A^n\\in \\mathbb{R}^m$的$m\\times n$矩阵，并且$B$为一个包含$p$列$B^1,\\cdots,B^p\\in \\mathbb{R}^n$的$n\\times p$矩阵，我们可以生成$p$个向量：  ^614093\n$$\nAB^1,\\cdots,AB^p\n$$\n这$p$个向量构成$m\\times p$矩阵$AB$，其第$j$列为$AB^j$。但是我们知道$AB^j$的第$i$个坐标是$A$的第$i$行和$B$的第$j$列的内积：\n$$\n\\left(\\begin{array}{llll}a_{i1}&a_{i2}&\\cdots&a_{in}\\end{array}\\right)\\cdot\\left(\\begin{array}{c}b_{1j}\\\\b_{2j}\\\\\\vdots\\\\b_{nj}\\end{array}\\right) = \\sum_{k=1}^na_{ik}b_{kj}\n$$\n所以我们定义了矩阵的乘积\n$$\n(AB)_{ij} = \\sum_{k=1}^na_{ik}b_{kj}\n$$\n假设$A$是一个$n\\times n$矩阵并且我们想要求解线性系统\n$$\nAx = b\n$$\n其中$b\\in \\mathbb{R}^n$。假设我们可以找到一个$n\\times n$的矩阵$B$使得\n$$\nBA^i = e_i,\\quad i=1,\\cdots,n\n$$\n其中$e_i = (0,\\cdots,0,1,0,\\cdots,0)$只有第$i$个位置的数据为$1$。我们称$n\\times n$矩阵\n$$\n  \n\nI_{n}=\\left(\\begin{array}{cccccc}1 & 0 & 0 & \\cdots & 0 & 0 \\\\ 0 & 1 & 0 & \\cdots & 0 & 0 \\\\ 0 & 0 & 1 & \\cdots & 0 & 0 \\\\ \\vdots & \\vdots & \\vdots & \\ddots & \\vdots & \\vdots \\\\ 0 & 0 & 0 & \\cdots & 1 & 0 \\\\ 0 & 0 & 0 & \\cdots & 0 & 1\\end{array}\\right)\n$$\n为单位矩阵，它的第$i$列为$e_i$，则上面等价于\n$$\nBA = I_n\n$$\n如果$Ax = b$，在等式的两边左乘以$B$，我们有\n$$\nB(Ax) = Bb\n$$\n因为$B(Ax) = (BA)x = I_nx = x$，因此我们有\n$$\nx = Bb\n$$\n矩阵$B$为矩阵$A$的逆，常被表示为$A^{-1}$。很容易证明存在唯一的矩阵使得：\n$$\nAA^{-1} = A^{-1}A = I_n\n$$\n如果一个方阵有逆矩阵，则说明他是可逆或者非奇异的，反之为奇异的。\n总之，如果$A$是一个可逆方阵，则线性系统$Ax = b$有唯一解$x=A^{-1}b$。但是在实践中，这并不是求解线性系统的一个很好的方法，因为计算$A^{-1}$的代价太大了。一个求解线性系统的实用的方法是高斯消去。其他的求解线性系统$Ax = b$的实用的方法利用$A$的因式分解(QR分解，SVD分解)，用到下面定义的正交矩阵的概念。\n给定一个$m\\times n$的矩阵$A=(a_{kl})$，$n\\times m$矩阵$A^T = (a_{ij}^T)$的第$i$行是$A$的第$i$列，这意味着$a_{ij}^T=a_{ji},i=1,\\cdots,n,\\quad j=1,\\cdots,m$称为$A$的转置。一个$n\\times n$矩阵$Q$使得\n$$\nQQ^T = Q^TQ = I_n\n$$\n被称为正交矩阵。正交矩阵$Q$的逆$Q^{-1}$等于其转置$Q$。正交矩阵具有非常重要的作用。在几何上，它们对应于保持长度不变的线性变换。线性代数的一个重要的结果表明，每个$m\\times n$矩阵$A$都可以表示为：\n$$\nA = V\\Sigma U^T\n$$\n其中$V$是一个$m\\times m$的正交矩阵，$U$是一个$n\\times n$的正交矩阵，$\\Sigma$是一个$m\\times n$的矩阵，其非零项为非负对角线项$\\sigma_1\\ge \\sigma_2\\ge \\cdots\\ge\\sigma_p$，其中$p = \\min(m,n)$，称为$A$的奇异值。分解$A = V\\Sigma U^T$被称为$A$的奇异值分解，或SVD。\nSVD分解可以用来\"求解\"线性系统$Ax=b$其中$A$为一个$m\\times n$矩阵，甚至当此系统无解的时候。这个在公式的数量大于变量的数量的情况下发生($m>n$)，这种情况下系统被称为超定的。\n当然并不存在奇迹使一个无法解决的系统有解。但是我们可以寻找一个好的近似解，即最小化误差$Ax-b$的某个度量方法的向量$x$。我们可以使用误差的平方欧几里得范数$||Ax-b||_2^2$。这个度量是可导的，并且存在唯一的向量$x^+$最小化$||Ax-b||_2^2$。此外，$x^+$由表达式$x^+=A^+b$给出，其中$A^+$为$A$的伪逆，并且$A^+$可以由$A$的SVD分解$A = V\\Sigma U^T$计算。事实上，$A^+=U\\Sigma^+V^T$，其中$\\Sigma^+$是通过将$\\Sigma$的所有非零奇异值变为$\\sigma_i^{-1}$，零值不变并转置。\n除了寻找使欧几里得范数$||Ax-b||_2^2$最小的向量外，我们可以添加一个惩罚项$K||x||_2^2(K>0)$来最小化$||Ax-b||_2^2+K||x||_2^2$。这种方法被称为岭回归。事实证明，存在唯一的极小化$x^+=(A^TA+KI_n)^{-1}A^Tb$。\n另一种方法是用$K||x||_1$替换惩罚项$K||x||_2^2$，其中$||x||_1=|x_1|+\\cdots+|x_n|$($x$的$\\ell^1$范数)。值得注意的是，使$||Ax-b||_2^2+K||x||_1$最小的$x$为稀疏的，这意味着$x$的很多元素是零。这种方法被称为laoss。\n在现实世界中，线性代数被证明非常有效的另一个很好的例子是数据压缩问题，也就是说，使用小得多的存储量来表示非常大的数据集。\n一般数据集表示为一个$m\\times n$的矩阵$A$其中每一行表示一个$n$维数据点并且一般$m\\ge n$。在大多数的应用中，数据不是独立的因此$A$的秩比$\\min(m,n)$小的多，并且低秩分解的目标是将$A$分解为两个矩阵$B$和$C$，其中$B$为$m\\times k$矩阵，$C$为$k\\times n$矩阵，$k\\ll\\min(a,b)$：\n$$\n\\left(\\begin{array}{c}A \\\\ m \\times n\\end{array}\\right)=\\left(\\begin{array}{c}B \\\\ m \\times k\\end{array}\\right)\\left(\\begin{array}{c}C \\\\ k \\times n\\end{array}\\right)\n$$\n现在要找到如上所述的精确因式分解通常成本太高，所以我们寻找一个低秩矩阵$A^{\\prime}$，它是$A$的“好”近似值。为了使这一陈述精确，我们需要定义一种机制来确定两个矩阵的距离。这可以使用矩阵范数来完成。我们的目标是找到一个低秩矩阵$A^{\\prime}$来最小化范数\n$$\n||A-A^{\\prime}||^2\n$$\n在秩最大为$k$的矩阵$A^{\\prime}$上。\n## 向量空间\n### 群\n一个实向量空间是具有两种运算的集合$E$：$+:E\\times E\\rightarrow E$和$\\cdot:\\mathbb{R}\\times E\\rightarrow E$，称为加法和数乘法，并且满足一些简单的性质。首先，加法下的$E$必须是可交换的。\n```ad-note\n注意，向量空间不仅是代数对象；它们也是几何对象。\n```\n定义：\n群是具有二元运算$\\cdot:G\\times G\\rightarrow G$的集合$G$，它将元素$a\\cdot b\\in G$与每一对元素$a,b\\in G$相关联，并且具有如下性质：满足结合律、有一个单位元$e\\in G$，并且$G$中的每个元素都有逆元。即满足下列三条性质：\n\n1. $a\\cdot(b\\cdot c) = (a\\cdot b)\\cdot c$\n2. $a\\cdot e = e\\cdot a = a$\n3. 对于每一个$a\\in G$，存在$a^{-1}\\in G$使得$a\\cdot a^{-1}=a^{-1}\\cdot a = e$\n\n一个群是可交换的如果：\n$$\na\\cdot b = b\\cdot a\n$$\n具有运算$\\cdot:M\\times M\\rightarrow M$的集合$M$，并且元素$e$只满足条件1和条件2，被称为一个幺半群。例如，集合$\\mathbb{N} = \\{0,1,\\cdots,n,\\cdots\\}$是以$0$为单位元的加法幺半群，但不是群。\n\n群的例子\n\n> 这个群没看懂什么意思：\n>\n> 给定任意非空集$S$，双射集$f:S\\rightarrow S$，也称为$S$的置换，是函数合成下的群。单位元为单位函数$\\operatorname{id}_S$。只要$S$中有两个以上的元素，这个群就不是交换群。\n\n具有实或复数元素的$n\\times n$矩阵集合是矩阵加法下的交换群，单位元为零矩阵。符号表示为$\\mathrm{M}_n(\\mathbb{R})$或者$\\mathrm{M}_n(\\mathbb{C})$。\n\n具有实数或复数元素的$n\\times n$可逆矩阵集合是矩阵乘法下的群，单位元为单位矩阵$I_n$。这个群被称为一般线性群，通常用$\\mathbf{GL}(n,\\mathbb{R})$或者$\\mathbf{GL}(n,\\mathbb{C})$表示。\n\n具有实数或复数元素并且行列式为$+1$的$n\\times n$可逆矩阵的集合矩阵乘法下的群，单位元为单位矩阵$I_n$。这个群被称为特殊线性群并且用$\\mathbf{SL}(n,\\mathbb{R})$或$\\mathbf{SL}(n,\\mathbb{C})$表示。\n\n具有实数元素并且满足$RR^T = R^TR = I_n$的并且行列式为$+1$的可逆矩阵集合为矩阵乘法下的群，被称为特殊正交群，用符号$\\mathbf{SO}(n)$表示。它与$\\mathbb{R}$上的旋转相对应。\n\n给定开区间$(a,b)$和连续函数：$f:(a,b)\\rightarrow \\mathbb{R}$的集合$\\mathcal{C}(a,b)$是一个运算$f+g$下的交换群：\n$$\n(f+g)(x) = f(x) + g(x)\n$$\n\n通常用$+$表示交换群$G$的运算，在这种情况下，元素$a^{-1}$的逆用$-a$表示。\n\n群的单位元是唯一的。事实上，我们可以证明一个更一般的命题：\n\n命题：如果一个二元运算：$\\cdot:M\\times M\\rightarrow M$为结合的并且$e^{\\prime}$为左单位元，$e^{\\prime\\prime}$为右单位元，这意味着：\n$$\ne^{\\prime}\\cdot a = a\\quad \\forall a\\in M\n$$\n并且\n$$\na\\cdot e^{\\prime\\prime} = a\\quad \\forall a\\in M\n$$\n则\n$$\ne^{\\prime} = e^{\\prime\\prime}\n$$\n> 如果我们令$a = e^{\\prime\\prime}$，则\n> $$\n> e^{\\prime}\\cdot e^{\\prime\\prime} = e^{\\prime\\prime}\n> $$\n> 并且令$a = e^{\\prime}$，我们有\n> $$\n> e^{\\prime}\\cdot e^{\\prime\\prime}\n> $$\n> 因此\n> $$\n> e^{\\prime} = e^{\\prime}\\cdot e^{\\prime\\prime} = e^{\\prime\\prime}\n> $$\n\n命题：在一个单位元为$e$的幺半群$M$中，如果元素$a\\in M$有左逆$a^{\\prime}\\in M$和右逆$a^{\\prime\\prime}\\in M$，则$a^{\\prime} = a^{\\prime\\prime}$。\n\n> $$\n> (a^{\\prime}\\cdot a)\\cdot a^{\\prime\\prime} = e\\cdot a^{\\prime\\prime} = a^{\\prime\\prime}\n> $$\n>\n> $$\n> a^{\\prime}\\cdot (a\\cdot a^{\\prime\\prime}) = a^{\\prime\\prime}\\cdot e = a^{\\prime}\n> $$\n> 所以\n> $$\n> a^{\\prime} = a^{\\prime\\prime}\n> $$\n\n### 环\n向量空间是具有标量乘法$\\cdot:K\\times E\\rightarrow E$的交换群，它允许$K$中的元素对$E$的向量进行重新缩放。集合$K$本身是一个称为域的代数结构。域是环的一种特殊的结构。下面我们先介绍环。\n\n环的定义：环是具有两种运算$+:A\\times A\\rightarrow A$(加法)和$*:A\\times A\\rightarrow A$(乘法)的集合$A$，并满足下列性质：\n1. $A$是加法上的交换群\n2. $*$是可结合的并且有单位元$1\\in A$\n3. $*$对于$+$是满足分配律的\n\n加法的单位元用$0$表示，$a$的加法逆元用$-a$表示。具体地，环具有如下性质：\n1. $a+(b+c) = (a+b)+c$：加法结合律\n2. $a+b = b+a$：加法交换律\n3. $a + 0 = 0 + a$：零\n4. $a + (-a) = (-a) + a = 0$：加法逆元\n5. $a*(b*c) = (a*b)*c$：乘法结合律\n6. $a*1 = 1*a=a$：乘法单位元\n7. $(a+b)*c = a*b + b*c$：乘法分配律\n8. $a*(b+c) = a*b+a*c$：乘法分配律\n\n环$A$是交换的如果\n$$\na*b = b*a\\quad \\forall a,b\\in A\n$$\n我们还能推出它的两条性质：$0\\cdot \\alpha = \\alpha\\cdot 0=0,(-\\alpha)\\cdot v = \\alpha\\cdot(-v)=-(\\alpha v)$\n\n> $$\n> \\begin{aligned}\n> a*0 &= 0*a = 0\\\\\n> a*(-b)&= (-a)*b = -(a*b)\n> \\end{aligned}\n> $$\n> $$\n> a*0 = a*(0+0) = a*0 + a*0\\rightarrow a*0=0\n> $$\n> 对于第二个性质假设$\\forall a,b\\in A$\n> $$\n> a*(-b) + a*b = a*(b+(-b)) = a*0 = 0\n> $$\n> $$\n> (-a)*b + a*b = ((-a)+a)*b = 0*b = 0\n> $$\n> 所以$(-a)*b$和$a*(-b)$都为$ab$的加法逆元即为$-ab$\n\n上式意味着如果$1=0$，则$a=0,\\forall a\\in A$，那么$A = \\{0\\}$。环$A=\\{0\\}$被称为**平凡环**。$1\\neq0$的环被称为非平凡环。\n### 域\n域是一个交换环$K$，满足$K-\\{0\\}$是乘法下的群。\n\n定义：集合$K$是一个域如果它是满足下列性质的环：\n1. $0\\neq1$\n2. $K^*=K-\\{0\\}$是乘法下的群\n3. $*$是交换的\n\n如果只满足1和2但是不满足3我们称其为**斜域**或非交换域。\n### 向量空间\n\n定义\n一个实向量空间是向量集合$E$，集合$E$上有两个运算：$+:E\\times E\\rightarrow E$(称为向量加法)和$\\cdot:\\mathbb{R}\\times E\\rightarrow E$(数乘法)并满足下列条件，对于$\\alpha,\\beta\\in \\mathbb{R}$和$u,v\\in E$：\n\n1. $E$为加法上的交换群，单位元为$0$\n2. $\\alpha\\cdot(u+v) = (\\alpha\\cdot u)+(\\alpha\\cdot v)$\n3. $(\\alpha+\\beta)\\cdot u = (\\alpha\\cdot u)+(\\beta\\cdot u)$\n4. $(\\alpha*\\beta)\\cdot u = \\alpha\\cdot(\\beta\\cdot u)$\n5. $1\\cdot u = u$\n\n从上面的性质我们可以推出：$\\alpha$\n\n其中$*$表示$\\mathbb{R}$上的乘法。\n\n> 对于任何$u\\in E$和任何$\\lambda\\in E$，如果$\\lambda \\neq 0$并且$\\lambda\\cdot u=0$，则$u=0$。\n>\n> 事实上，因为$\\lambda\\neq0$，它有乘法逆元$\\lambda^{-1}$，因此从$\\lambda\\cdot u=0$，我们有\n> $$\n> \\lambda^{-1}\\cdot(\\lambda\\cdot u) = \\lambda^{-1}\\cdot 0 \n> $$\n> 但是，我们可以得到$\\lambda^{-1}\\cdot0=0$，所以\n> $$\n> \\lambda^{-1}\\cdot(\\lambda\\cdot u) = (\\lambda^{-1}\\lambda)\\cdot u = 1\\cdot u = u\n> $$\n> 所以$u=0$。\n\n向量空间的一个非常重要的例子是两个向量空间之间的线性映射集。令$X$为一个非空集合并且$E$为一个向量空间。所有函数$f:X\\rightarrow E$的集合可以构成如下向量空间：给定两个任意函数$f:X\\rightarrow E$和$g:X\\rightarrow E$，令$(f+g):X\\rightarrow E$定义为\n$$\n(f+g)(x) = f(x)+g(x)\n$$\n并且对于任何$\\lambda\\in \\mathbb{R}$，令$\\lambda f:X\\rightarrow E$定义为\n$$\n(\\lambda f)(x) = \\lambda f(x)\n$$\n。\n## 索引族；求和符号$\\sum_{i\\in I}a_i$\n给定一个集合$A$，回想一下序列是一个有序的$n$元组$(a_1,\\cdots,a_n)\\in A^n$，其元素来自于$A$，对于某些自然数$n$。序列的元素不需要不同并且顺序是很重要的。例如，$(a_1,a_2,a_1)$和$(a_2,a_1,a_1)$是$A^3$上两个不同的序列。它们的基本集合是$\\{a_1,a_2\\}$。\n我们刚才定义的是**有限**序列，这可以被看作是从$\\{1,2,\\cdots,n\\}$到集合$A$的函数，序列$(a_1,\\cdots,a_n)$的第$i$个元素是$i$在此函数下的象。这一观点是非常非常有用的，因为它允许我们将有限序列定义为函数：$s:\\mathbb{N}\\rightarrow A$。但是，我们为什么要把自己限制在像$\\{1,2,\\cdots\\}$或$\\mathbb{N}$这样的索引集内呢？\n索引集的主要作用是对每个元素进行唯一标记，尽管方便，但标记的顺序并不重要。\n定义：给定集合$A$，$A$中元素的一个$I-$索引集为一个函数$a:I\\rightarrow A$，其中$I$是可以被看作索引集的任何集合。因为函数$a$由它的图所确定：\n$$\n\\{(i,a(i))|i\\in I\\}\n$$\n族$a$可以被看作对$a=\\{(i,a(i))|i\\in I\\}$的集合。为了符号上的简单我们用$a_i$表示$a(i)$，用$(a_i)_{i\\in I}$表示族$a=\\{(i,a(i))|i\\in I\\}$。\n\n如果索引集合$I$为完全有序的，族$(a_i)_{i\\in I}$常被称为$I-$序列。有趣的是，集合$A$可以被看作$A-$索引族$\\{(a,a)|a\\in I\\}$对应于单位函数。\n\n\n我们也需要注意一个问题，那就是定义形式$\\sum_{i\\in I}a_i$的和，其中$I$是任何有限的索引集并且$(a_i)_{i\\in I}$为具有二元运算$+:A\\times A\\rightarrow A$的一些集合$A$中的一族元素，并且$+$是结合和交换的。\n\n问题是$+$运算只告诉我们如何去计算两个元素之间的$a_1+a_2$，没有告诉我们如何进行两个元素以上的运算。我们要做的是通过序列来定义$a_1+a_2+a_3$，每一步包含两个元素。并且如果$+$是结合和交换的，很冥想$\\sum_{i\\in I}a_i$的和并不依赖于运算的顺序。\n\n首先我们定义和$\\sum_{i\\in I}a_i$，其中$I$为有限不同的自然数的序列，写作$I=(i_1,\\cdots,i_m)$。如果$I=(i_1,\\cdots,i_m)$其中$m\\ge2$，我们用$I-\\{i_1\\}$表示序列$(i_2,\\cdots,i_m)$。我们对$I$的大小$m$进行归纳。令\n$$\n\\begin{aligned}\n\\sum_{i\\in I}a_i&=a_{i1},\\quad \\text{if }m=1\\\\\n\\sum_{i\\in I}a_i &= a_{i1}+\\left(\\sum_{i\\in I-\\{i_1\\}}a_i\\right),\\quad \\text{if }m>1\n\\end{aligned}\n$$\n例如，如果$I=(1,2,3,4)$，我们有\n$$\n\\sum_{i\\in I}a_i = a_1 + (a_2+(a_3+a_4))\n$$\n如果$+$不满足结合律的话，那么不同组的划分将得到不同的答案。\n\n但是，如果$+$是结合的，只要元素的顺序保持不变，则$\\sum_{i\\in I}a_i$不依赖于组的划分。例如，如果$I=(1,2,3,4,5)$，$J_1=(1,2)$和$J_2=(3,4,5)$，我们希望\n$$\n\\sum_{i\\in I}a_i = \\left(\\sum_{j\\in J_1}a_j\\right) + \\left(\\sum_{j\\in J_2}a_j\\right)\n$$\n这个是成立的，因为我们有以下性质。\n\n性质：给定任何具有结合二元运算$+:A\\times A\\rightarrow A$的非空集合$A$，对于任何具有不同自然数的非空有限序列$I$和对于将$I$分为$p$个非空序列$I_{k_1},\\cdots,I_{k_p}$的任何分割，对于具有不同自然数的非空序列$K=(k_1,\\cdots,k_p)$，使得$k_i<k_j$暗示$\\alpha<\\beta$对于所有$\\alpha\\in I_{k_i}$和所有$\\beta\\in I_{k_j}$，对于$A$中元素$(a_i)_{i\\in I}$的每一个序列，我们有：\n$$\n\\sum_{\\alpha\\in I}a_{\\alpha} = \\sum_{k\\in K}(\\sum_{\\alpha\\in I_k}a_{\\alpha})\n$$\n\n证明：我们对大小为$n$的$I$进行归纳。\n如果$n=1$，则我们一定有$p=1$并且$I_{k1}=I$，因此性质一定成立。\n之后，假设$n>1$。如果$p=1$则$I_{k1}=I$并且公式是容易解决的，因此假设$p\\ge2$，并且令$J=(k_2,\\cdots,k_p)$。这样有两种情况：\n情况1：序列$I_{k_1}$有单个元素，为$\\beta$，为$I$的第一个元素。在这种情况下，$C$表示$I$去除掉第一个元素$\\beta$后的序列。通过定义：\n$$\n\\sum_{\\alpha\\in I}a_{\\alpha} = a_{\\beta} + \\left(\\sum_{\\alpha\\in C}a_{\\alpha}\\right)\n$$\n和\n$$\n\\sum_{k\\in K}\\left(\\sum_{\\alpha\\in I_k}a_{\\alpha}\\right) = a_{\\beta}+\\left(\\sum_{j\\in J}\\left(\\sum_{\\alpha\\in I_j}a_{\\alpha}\\right)\\right)\n$$\n因为$|C|=n-1$，通过归纳法的假设，我们有：\n$$\n\\left(\\sum_{\\alpha\\in C}a_{\\alpha}\\right)= \\sum_{j\\in J}\\left(\\sum_{\\alpha\\in I_j}a_{\\alpha}\\right)\n$$\n这就证明了上述情况。\n\n情况2：序列$I_{k_1}$至少有两个元素。在这种情况下，令$\\beta$为$I$的第一个元素(也是$I_{k_1}$的)，令$I^{\\prime}$为去除它的第一个元素$\\beta$后的序列；$I^{\\prime}_{k_1}$为$I_{k_1}$去除$\\beta$后的序列，并且令$I^{\\prime}_{k_i}=I_{k_i},i=2,\\cdots,p$。序列$I^{\\prime}$有$n-1$个元素，因此将归纳假设应用到$I^{\\prime}$和$I^{\\prime}_{k_i}$，我们得到：\n$$\n\\sum_{\\alpha\\in I^{\\prime}}a_{\\alpha} = \\sum_{k\\in K}\\left(\\sum_{\\alpha\\in I^{\\prime}_k}a_{\\alpha}\\right) = \\left(\\sum_{\\alpha\\in I^{\\prime}_{k_1}}a_{\\alpha}\\right)+\\sum_{j\\in J}\\left(\\sum_{\\alpha\\in I_j}a_{\\alpha}\\right)\n$$\n如果我们把左边加到$\\alpha_{\\beta}$上，通过定义我们得到：\n$$\n\\sum_{\\alpha\\in I}a_{\\alpha}\n$$\n如果我们把右边加到$a_{\\beta}$上，使用结合性和索引和的定义：\n$$\n\\begin{aligned}\na_{\\beta}+\\left(\\left(\\sum_{\\alpha \\in I_{k_{1}}^{\\prime}} a_{\\alpha}\\right)+\\left(\\sum_{j \\in J}\\left(\\sum_{\\alpha \\in I_{j}} a_{\\alpha}\\right)\\right)\\right) \\\\\n&=\\left(a_{\\beta}+\\left(\\sum_{\\alpha \\in I_{k_{1}}^{\\prime}} a_{\\alpha}\\right)\\right)+\\left(\\sum_{j \\in J}\\left(\\sum_{\\alpha \\in I_{j}} a_{\\alpha}\\right)\\right) \\\\\n&=\\left(\\sum_{\\alpha \\in I_{k_{1}}} a_{\\alpha}\\right)+\\left(\\sum_{j \\in J}\\left(\\sum_{\\alpha \\in I_{j}} a_{\\alpha}\\right)\\right)=\\sum_{k \\in K}\\left(\\sum_{\\alpha \\in I_{k}} a_{\\alpha}\\right)\n\\end{aligned}\n$$\n得证。\n如果$I=(1,\\cdots,n)$，我们用$\\sum_{i=1}^n$代替$\\sum_{i\\in I}a_i$。因为$+$是结合的，上述性质证明和$\\sum_{i\\in I}a_i$与元素的分组无关，这说明了符号$a_1+\\cdots+a_n$的正确性(没有括号)。\n如果我们假设我们$A$上的结合的二元运算符是交换的，那么我们可以证明和$\\sum_{i\\in I}a_i$不依赖于索引集$I$的顺序。\n","tags":["线性代数"],"categories":["数学"]},{"title":"算法学习笔记第一节","url":"/2021/10/31/算法学习笔记/","content":"\n{% raw %}\n\n<div style=\"position: relative; width: 100%; height: 0; padding-bottom: 75%;\"><iframe src=\"//player.bilibili.com/player.html?aid=933642480&bvid=BV1YT4y1o727&cid=428541775&page=2\" scrolling=\"no\" border=\"0\" frameborder=\"no\" framespacing=\"0\" allowfullscreen=\"true\" style=\"position: absolute; width: 100%; \nheight: 100%; left: 0; top: 0;\"> </iframe></div>\n\n{% endraw %}\n\n## 认识时间复杂度\n\n常数时间的操作：一个操作如果跟数据量没有关系，每次都是固定时间内完成的操作，叫做常数操作。\n时间复杂度为一个算法流程中，常数操作数量的质量，常用$O$来表示。具体来说，只要高阶项，不要低阶项，也不要高阶项的系数。剩下的部分记为$f(N)$，那么时间复杂度为$O(f(N))$。\n\n## 对数器\n有一个你想要测的方法$a$\n实现一个绝对正确但是复杂度不好的方法$b$\n实现一个随机样本产生器\n实现比对的方法\n把方法$a$和方法$b$比对很多次来验证方法$a$是否正确\n如果有一个样本使得比对出错，打印样本分析是哪个方法出错\n当样本数量很多时比对测试仍然正确，可以确定方法$a$已经正确\n\n## 冒泡排序\n先比较1和2位置的数，如果1大于2则位置交换，再比较2和3位置的数，以此类推，每进行一次都将最大的数放在最后一位；第二次循环的时候就不用管最后一位了，以此类推。\n\n~~~java\npublic class Bubble {\n    public static void bubbleSort(int[] arr) {\n        if (arr == null || arr.length < 2) {\n            return;\n        }\n        for (int end = arr.length-1; end > 0; end--) {\n            for (int i = 0; i < end; i++) {\n                if (arr[i] > arr[i+1]) {\n                    swap(arr, i, i+1);\n                }\n            }\n        }\n    }\n\n    public static void swap(int[] arr, int i, int j) {\n        int temp = arr[i];\n        arr[i] = arr[j];\n        arr[j] = temp;\n    }\n}\n~~~\n\n## 选择排序\n我们首先从0开始到$N-1$，找最小的数的下标并与$0$位置的数交换，只有再从$1$开始，找到从$1$到$N-1$的最小数的下标与$1$交换，以此类推。\n\n~~~java\npublic class Selection {\n    public static void selectionSort(int[] arr) {\n\n        if (arr == null || arr.length < 2) {\n            return;\n        }\n\n        for (int i = 0; i < arr.length-1; i++) {\n            int minindex = i;\n            for (int j=i; j < arr.length; j++) {\n                minindex = arr[j] < arr[minindex] ? j : minindex;\n            }\n            swap(arr, i, minindex);\n\n        }\n    }\n    public static void swap(int[] arr, int i, int j) {\n        int temp = arr[i];\n        arr[i] = arr[j];\n        arr[j] = temp;\n    }\n    \n}\n~~~\n\n## 插入排序\n我们首先认为第$0$位置的数是自己排好的，然后再看$0\\sim1$位置的数，如果$0$位置的数大于$1$位置的数则交换，现在$0\\sim1$位置的数是排好的，之后看$0\\sim2$位置的数，用$2$位置与$1$位置的数比较，$1$位置数大于$2$位置数交换，否则不交换；以此类推。\n\n~~~java\npublic class InsertionSort {\n    public static void insertionSort(int[] arr) {\n        if (arr == null || arr.length < 2) {\n            return;\n        }\n\n        for (int i = 1; i < arr.length; i++) {\n            for (int j = i-1; j>=0 && arr[j] > arr[j + 1]; j--) {\n                swap(arr, j, j+1);\n            }\n        }\n    }\n    public static void swap(int[] arr, int i, int j) {\n        int temp = arr[i];\n        arr[i] = arr[j];\n        arr[j] = temp;\n    }\n    \n}\n~~~\n\n## 递归\n现在假设我们用递归函数来返回数组的最大值。\n\n~~~java\npublic class GetMaxRe {\n    public static int getMax(int[] arr, int L, int R) {\n        if (L==R) {\n            return arr[L];\n        }\n        int mid = (L+R)/2;\n        int maxLeft = getMax(arr, L, mid);\n        int maxRight = getMax(arr, mid+1, R);\n        return Math.max(maxLeft, maxRight);\n    }\n    public static void main(String[] args) {\n        int[] arr = {1,2,3,4};\n        System.out.println(getMax(arr, 0, arr.length-1));\n    }\n    \n}\n~~~\n\n## 归并排序\n时间复杂度为$O(N\\log N)$，额外空间复杂度为$O(N)$。\n步骤：先将样本分为左右两个部分，分别排好序再后外排。\n所以其时间复杂度公式为：\n$$\nT(N) = 2T(N/2) + O(N)\n$$\n因为两边排完序后还要进行外排，外排需要遍历所有$N$个数据，所以其复杂度为$O(N)$。\n~~~java\npublic class MergeSort {\n\n    public static void sortProcess(int[] arr, int L, int R) {\n        if (L==R) {\n            return;\n        }\n\n        int mid = (L+R)/2;\n        sortProcess(arr, L, mid);\n        sortProcess(arr, mid+1, R);\n        merge(arr, L, mid, R);\n    }\n\n    public static void merge(int[] arr, int L, int mid, int R) {\n        int[] help = new int[R - L + 1];\n        int i = 0;\n        int p1 = L;\n        int p2 = mid + 1;\n        while(p1 <= mid && p2 <= R) {\n            help[i++] = arr[p1] < arr[p2] ? arr[p1++] : arr[p2++];\n        }\n\n        while (p1<=mid) {\n            help[i++] = arr[p1++];\n        }\n\n        while (p2<=R) {\n            help[i++] = arr[p2++];\n        }\n\n        for (i = 0; i < help.length; i++) {\n            arr[L+i] = help[i];\n        }\n    }\n    \n}\n\n~~~\n## 小和问题\n再一个数组中，每一个数左边必当前数小的数累加起来，叫做这个数的小和。求一个数组的小和。\n\n例子：\n[1,3,4,2,5]\n1的左边比1小的数，没有；\n3的左边比1小的数，1；\n4的左边比4小的数，1、3；\n2的左边比2小的数，1；\n5的左边比5小的数，1、3、4、2；\n所以小和为$1+1+3+1+1+3+4+2=16$\n\n~~~java\npublic class SmallSum {\n\n    public static int smallSum(int[] arr) {\n        if (arr == null || arr.length < 2) {\n            return 0;\n        }\n        return mergeSort(arr, 0, arr.length-1);\n    }\n\n    public static int mergeSort(int arr[], int l, int r) {\n        if (l==r) {\n            return 0;\n        }\n\n        int mid = (r+l)/2;\n        return mergeSort(arr, l, mid) + mergeSort(arr, mid+1, r) + merge(arr, l, mid, r);\n    }\n\n    public static int merge(int[] arr, int l, int m, int r) {\n        int[] help = new int[r - l + 1];\n        int i = 0;\n        int p1 = l;\n        int p2 = m+1;\n        int res = 0;\n        while (p1 <= m && p2 <= r) {\n            res += arr[p1] < arr[p2] ? (r-p2+1)*arr[p1] : 0;\n            help[i++] = arr[p1] < arr[p2] ? arr[p1++] : arr[p2++];\n        }\n        while (p1<=m) {\n            help[i++] = arr[p1++];\n        }\n\n        while (p2<=r) {\n            help[i++] = arr[p2++];\n        }\n\n        for (i = 0; i < help.length; i++) {\n            arr[l+i] = help[i];\n        }\n        return res;\n    }\n    \n}\n~~~\n\n> 在我们的代码中我们写`m = (l+r)/2`，这样的话可能会溢出，所以我们可以将其写为`m = l + (r-l)/2`，这样就不会溢出了，另外除以$2$的操作我们还可以写为位运算的形式，位运算要比算数运算快，`a/2 = a>>1`，相当于右移一位。所以`l+(r-l)/2 = l+(r-l)>>1`。\n\n## 逆序对问题\n\n在一个数组中，左边的数如果比右边的数大，则这两个数构成一个逆序对，请打印所有的逆序对。\n相当于是求右边有多少数比他小。\n\n~~~java\npublic class InversePair {\n\n    public static void inversePair(int[] arr) {\n        if (arr == null || arr.length < 2) {\n            return;\n        }\n        sortProcess(arr, 0, arr.length-1);\n    }\n\n    public static void sortProcess(int[] arr, int L, int R) {\n        if (L==R) {\n            return;\n        }\n\n        int mid = (L+R)/2;\n        sortProcess(arr, L, mid);\n        sortProcess(arr, mid+1, R);\n        merge(arr, L, mid, R);\n    }\n\n    public static void merge(int[] arr, int L, int mid, int R) {\n        int[] help = new int[R - L + 1];\n        int i = 0;\n        int p1 = L;\n        int p2 = mid + 1;\n        int res = 0;\n        while(p1 <= mid && p2 <= R) {\n            res = arr[p1] > arr[p2] ? (R-p2+1) : 0;\n            for (int k=0;k<res;k++){\n                System.out.println(\"[\"+arr[p1]+\" \"+arr[p2+k]+\"]\");\n            }\n            help[i++] = arr[p1] > arr[p2] ? arr[p1++] : arr[p2++];\n\n            \n        }\n\n        while (p1<=mid) {\n            help[i++] = arr[p1++];\n        }\n\n        while (p2<=R) {\n            help[i++] = arr[p2++];\n        }\n\n        for (i = 0; i < help.length; i++) {\n            arr[L+i] = help[i];\n        }\n    }\n\n    public static void main(String[] args) {\n        int[] arr = {5,3,1,4,2,8,9,7};\n        inversePair(arr);\n    }\n    \n}\n~~~\n","tags":["算法"],"categories":["算法"]},{"title":"线性代数应该这样学：线性空间","url":"/2021/09/12/线性空间/","content":"\n## 向量空间\n\n线性代数是研究有限维向量空间上的线性映射的学科。在线性代数中，如果研究复数和实数，会出现更好的定理和更多的洞察力。因此我们将从介绍复数和它们的基本概念开始。\n\n我们将平面和平凡空间(ordinary space)的例子推广到$\\mathbb{R}^n$和$\\mathbb{C}^n$，然后我们将它们推广到向量空间的概念。\n\n然后我们的下一个主题是子空间，它对于向量空间来说所扮演的角色等同于子集对于集合所扮演的角色。最后我们看一下子空间的和(等同于子集的并集)和子空间的直和(相当于不相交子集的并集)。\n\n### $\\mathbb{R}^n$ and $\\mathbb{C}^n$\n\n#### 复数\n\n定义\n\n一个复数就是一个有序数对$(a,b)$，其中$a,b\\in \\mathbb{R}$，不过我们将其写作$a+bi$。所有复数的集合表示为$\\mathbb{C} = \\{a+bi:a,b\\in \\mathbb{R}\\}$。\n\n#### 复数运算的性质\n\n交换律\n$$\n\\alpha + \\beta = \\beta + \\alpha,\\alpha\\beta = \\beta\\alpha,\\forall \\alpha,\\beta\\in \\mathbb{C}\n$$\n结合律\n$$\n(\\alpha+\\beta)+\\lambda = \\alpha+(\\beta+\\lambda),(\\alpha\\beta)\\lambda=\\alpha(\\beta\\lambda),\\forall\\alpha,\\beta\\in \\mathbb{C}\n$$\n单位元\n$$\n\\lambda+0=\\lambda,\\lambda1=\\lambda,\\forall \\lambda\\in \\mathbb{C}\n$$\n加法逆元\n\n对于任何$\\alpha\\in \\mathbb{C}$，存在唯一一个$\\beta\\in \\mathbb{C}$，使得$\\alpha+\\beta=0$。\n\n乘法逆元\n\n对于任何$\\alpha\\in \\mathbb{C}$，存在唯一的$\\beta\\in \\mathbb{C}$使得$\\alpha\\beta=1$。\n\n分配律\n$$\n\\lambda(\\alpha+\\beta) = \\lambda\\alpha+\\lambda\\beta,\\forall \\lambda,\\alpha,\\beta\\in \\mathbb{C}\n$$\n\n***\n\n#### 定义： $-\\alpha$，减法，$1/\\alpha$，除法\n\n$\\alpha,\\beta\\in \\mathbb{C}$\n\n+ 令$-\\alpha$表示$\\alpha$的加法逆元。因此$-\\alpha$是满足$\\alpha+(-\\alpha)=0$的唯一复数。\n+ 减法：$\\mathbb{C}$上的减法定义为：$\\beta-\\alpha=\\beta+(-\\alpha)$\n+ 对于$\\alpha\\neq 0$，令$1/\\alpha$表示$\\alpha$的乘法逆元。因此$1/\\alpha$为满足$\\alpha(1/\\alpha)=1$的唯一复数。\n+ $\\mathbb{C}$上的除法定义为：$\\beta/\\alpha=\\beta(1/\\alpha)$。\n\n***\n\n用$\\mathbb{F}$表示$\\mathbb{R}$或者$\\mathbb{C}$。对于$\\alpha\\in \\mathbb{F}$并且$m$为正数，我们定义$\\alpha^m$来表示$\\alpha$连乘$m$次：\n$$\n\\alpha^{m}=\\underbrace{\\alpha \\cdots \\alpha}_{m \\text { times }}\n$$\n很显然$(\\alpha^m)^n=\\alpha^{mn}$并且$(\\alpha\\beta)^m=\\alpha^m\\beta^m, \\forall \\alpha,\\beta\\in \\mathbb{F}$。\n\n***\n\n#### 定义：列表(list)，长度(length)\n\n假设$n$是一个非负整数。长度为$n$的列表是一个被括号包围用逗号分隔的$n$元有序数对。长度为$n$的列表如下：\n$$\n（x_1,\\cdots,x_n)\n$$\n两个列表是相等的当且仅当它们长度相等并且在相同的位置有相同的元素。\n\n> 长度无限的不能称为列表\n\n长度为零的列表像这样：$()$。我们将其当作列表以免不必要的例外情况。\n\n#### 定义：$\\mathbb{F}^n$\n\n$\\mathbb{F}^n$是所有元素来自$\\mathbb{F}$的$n$元有序数对的集合：\n$$\n\\mathbb{F}=\\{(x_1,\\cdots,x_n):x_j\\in \\mathbb{F},\\forall j=1,\\cdots,n\\}\n$$\n\n#### $\\mathbb{F}^n$上的加法\n\n$\\mathbb{F}^n$上的加法定义为相对应的元素相加：\n$$\n(x_1,\\cdots,x_n) + (y_1,\\cdots,y_n) = (x_1+y_1,\\cdots,x_n+y_n)\n$$\n\n#### $\\mathbb{F}^n$上加法的交换律\n\n如果$x,y\\in \\mathbb{F}^n$，则$x+y = y+x$\n\n证明：假设$x=(x_1,\\cdots,x_n)$和$y=(y_1,\\cdots,y_n)$。\n$$\n\\begin{aligned}\nx + y &= (x_1,\\cdots,x_n)+(y_1,\\cdots,y_n)\\\\\n&= (x_1+y_1,\\cdots,x_n+y_n)\\\\\n&= (y_1+x_1,\\cdots,y_n+x_n)\\\\\n&= (y_1,\\cdots,y_n) + (x_1,\\cdots,x_n)\\\\\n&= y+x\n\\end{aligned}\n$$\n\n#### 定义 $0$\n\n令$0$表示长度为$n$并且元素全部为$0$的列表：\n$$\n0 = (0,\\cdots,0)\n$$\n\n#### $\\mathbb{F}$上的加法逆元\n\n对于$x\\in \\mathbb{F}^n$，$x$的加法逆元，表示为$-x$，为向量$-x \\in \\mathbb{F}^n$使得\n$$\nx + (-x) = 0\n$$\n换句话说，如果$x = (x_1,\\cdots,x_n)$，则$-x = (-x_1,\\cdots,-x_n)$。\n\n#### $\\mathbb{F}$上的数乘\n\n数字$\\lambda$和$\\mathbb{F}^n$中向量的乘法通过用$\\lambda$乘以$\\mathbb{F}^n$中的每一个元素来完成。\n$$\n\\lambda(x_1,\\cdots,x_n) = (\\lambda x_1,\\cdots,\\lambda x_n)\n$$\n在这里$\\lambda\\in \\mathbb{F}$并且$(x_1,\\cdots,x_n)\\in \\mathbb{F}^n$.\n\n### 向量空间的定义\n\n定义向量空间的动机来自于$\\mathbb{F}^n$中的加法和标量乘法的性质：加法是可交换的、结合的并且有单位元。每个元素也都有加法逆元。标量乘法具有结合律。加法和数乘通过分配律相联系。\n\n我们把向量空间定义为在$\\mathbb{V}$上具有加法和数乘的集合$\\mathbb{V}$，该集合具有上面段落提到的性质。\n\n#### 加法、数乘\n\n集合$\\mathbb{V}$上的数乘是将元素$u+v\\in \\mathbb{V}$分配给每对元素$u,v\\in \\mathbb{V}$的函数。\n\n集合$\\mathbb{V}$上的数乘是将元素$\\lambda v\\in \\mathbb{V}$分配给每一个$\\lambda \\in \\mathbb{F}$和每一个$v\\in \\mathbb{V}$的函数。\n\n现在我们准备好给向量空间一个正式的定义。\n\n#### 定义：向量空间\n\n向量空间是具有$\\mathbb{V}$上加法和$\\mathbb{V}$上数乘的集合$\\mathbb{V}$，使得其满足以下性质：\n\n+ 交换律：$u+v = v+u,\\forall u,v\\in \\mathbb{V}$\n+ 结合律：$(u+v)+w = u+(v+w)$和$(ab)v = a(bv),\\forall u,v,w\\in \\mathbb{V}\\text{ and }a,b\\in \\mathbb{F}$\n+ 加法单位元：存在一个元素$0\\in \\mathbb{V}$使得$v+0=v,\\forall v\\in \\mathbb{V}$\n+ 加法逆元：对于任意的$v\\in \\mathbb{V}$，存在$w\\in \\mathbb{V}$，使得$v+w=0$\n+ 乘法单位元：$1v=v,\\forall v\\in \\mathbb{V}$\n+ 分配律：$a(u+v)=au+av$和$(a+b)v=av+bv,\\forall a,b\\in \\mathbb{F}\\text{ and }u,v\\in\\mathbb{V}$\n\n#### 向量、点\n\n向量空间的元素被称为向量或点。\n\n向量空间的数乘依赖于$\\mathbb{F}$。因当我们想要精确时，我们将会说$\\mathbb{V}$是$\\mathbb{F}$上的向量空间而不是说$\\mathbb{V}$是向量空间。\n\n#### 实向量空间、虚向量空间\n\n$\\mathbb{R}$上的向量空间被称为实向量空间。\n\n$\\mathbb{C}$上的向量空间被称为复向量空间。\n\n#### $\\mathbb{F}^S$\n\n如果$S$为一个集合，则$\\mathbb{F}^S$表示从$S$到$\\mathbb{F}$的函数的集合。\n\n对于$f,g\\in \\mathbb{F}^S$，和$f+g\\in \\mathbb{F}^S$是定义为\n$$\n(f+g)(x) = f(x) + g(x)\n$$\n的函数，对于任何$x\\in S$。\n\n对于$\\lambda \\in \\mathbb{F}$和$f\\in \\mathbb{F}^S$，乘积$\\lambda f\\in \\mathbb{F}^S$是定义为\n$$\n(\\lambda f)(x) = \\lambda f(x)\n$$\n对于所有$x\\in S$。\n\n作为上面定义的一个例子，如果$S$是区间$[0,1]$并且$\\mathbb{F} = \\mathbb{R}$，则$\\mathbb{R}^{[0,1]}$是区间$[0,1]$上的实值函数。\n\n\n\n$\\mathbb{F}^S$是向量空间：\n\n+ 如果$S$是一个非空集合，则$\\mathbb{F}^S$(有上面定义的加法和数乘运算)是$\\mathbb{F}$上的向量空间。\n+ $\\mathbb{F}^S$的加法单位元为函数：$0:S\\rightarrow \\mathbb{F}$定义为：$0(x)=0,\\forall x\\in S$\n+ 对于$f \\in \\mathbb{F}^S$，$f$的加法逆元为函数$-f$：$S\\rightarrow \\mathbb{F}$定义为：$(-f)(x)=-f(x),\\forall x \\in S$\n\n> 我们之前定义的$\\mathbb{F}^n$也可以看作是向量空间$\\mathbb{F}^S$的一个特例，我们可以认为$\\mathbb{F}^n$是从$\\{1,2,\\cdots,n\\}$到$\\mathbb{F}$的函数，即将$\\mathbb{F}^n$看作$\\mathbb{F}^{\\{1,2,\\cdots,n\\}}$。我的理解是例如$n=2$，则$\\mathbb{F}^2$可以看作从$\\{1,2\\}$到$\\mathbb{F}$的函数，当取$1$时从$\\mathbb{F}$中取一个数，等于$2$时再从$\\mathbb{F}$中取一个数。\n\n#### 加法单位元的唯一性\n\n一个向量空间有唯一的加法逆元。\n\n证明：假设$0$和$0^{\\prime}$都是一些向量空间$\\mathbb{V}$的加法单位元。则：\n$$\n0^{\\prime} = 0^{\\prime} + 0 = 0+ 0^{\\prime} = 0\n$$\n因此$0^{\\prime} = 0$，证明$\\mathbb{V}$只有一个加法单位元。\n\n#### 加法逆元的唯一性\n\n在向量空间的每一个元素都有唯一的加法逆元。\n\n证明：假设$\\mathbb{V}$是一个向量空间。令$v\\in \\mathbb{V}$。假设$w$和$w^{\\prime}$都是$v$的逆。则：\n$$\nw = w +0 = w+(v+w^{\\prime}) = (w+v)+w^{\\prime} = 0+w^{\\prime}=w^{\\prime}\n$$\n因此$w = w^{\\prime}$，即只有一个加法逆元。\n\n#### $-v,w-v$\n\n令$v,w\\in \\mathbb{V}$。则\n\n+ $-v$表示$v$的加法逆元\n+ $w-v$定义为$w+(-v)$\n\n#### 数字$0$乘以一个向量\n\n$0v=0,\\forall v\\in \\mathbb{V}$\n\n证明：\n$$\n0v = (0+0)v = 0v + 0v\n$$\n两边同时加上$0v$的逆元，得到$0=0v$。\n\n#### 数字乘以向量$0$\n\n$a0=0$对于任何$a\\in \\mathbb{F}$。\n\n证明：$a\\in \\mathbb{F}$，我们有\n$$\na0 = a(0+0)=a0+a0\n$$\n两边同时加上$a0$的加法逆元，即可得到$0=a0$。\n\n#### 数字$-1$乘以一个向量\n\n$(-1)v=-v$对于任意$v\\in \\mathbb{V}$。\n\n证明：对于$v\\in \\mathbb{V}$，我们有：\n$$\nv + (-1)v = 1v+(-1)v = (1+(-1))v = 0v = 0\n$$\n因此$(-1)v$是$v$的加法逆元。\n\n### 子空间\n\n$V$的子集$U$被称为是子空间如果$U$也是向量空间。\n\n> 一些数学家使用属于线性子空间来描述子空间。\n\n#### 子空间条件\n\n子集$U$是$V$的子空间当且仅当$U$满足下列三个条件：\n\n+ 加法单位元：$0\\in U$\n+ 加法封闭：$u,w\\in U\\rightarrow u+w\\in U$\n+ 数乘封闭：$a\\in \\mathbb{F},u\\in U\\rightarrow au\\in U$\n\n#### 例子\n\n如果$b\\in \\mathbb{F}$，则：\n$$\n\\{(x_1,x_2,x_3,x_4)\\}\\in \\mathbb{F}^4:x_3=5x_4+b\n$$\n为$\\mathbb{F}^4$的子空间，当且仅当$b=0$。\n\n***\n\n区间$[0,1]$上的连续实值函数的几何是$\\mathbb{R}^{[0,1]}$的子空间。\n\n***\n\n$\\mathbb{R}$上的可导实值函数的集合是$\\mathbb{R}^{\\mathbb{R}}$的子空间。\n\n***\n\n使$f^{\\prime}(2)=b$成立的在区间$(0,3)$上的实值可导函数$f$的集合是$\\mathbb{R}^{(0,3)}$的子空间，当且仅当$b=0$。\n\n***\n\n所有极限为0的复数序列的集合是$\\mathbb{C}^{\\infty}$的子空间。\n\n***\n\n> 很容易发现$\\{0\\}$是$\\mathbb{V}$的最小的子空间，$\\mathbb{V}$是$\\mathbb{V}$最大的子空间。\n\n#### 子集的和\n\n假设$U_1,\\cdots,U_m$是$V$的子集。$U_1,\\cdots,U_m$的和，表示为$U_1+\\cdots+U_m$，是$U_1,\\cdots,U_m$的所有可能的元素和的集合。更具体地：\n$$\nU_1+\\cdots+U_m = \\{u_1+\\cdots+u_m:u_1\\in U_1,\\cdots,u_m\\in U_m\\}\n$$\n\n#### Sum of subspaces is the smallest containing subspace\n\n假设$U_1,\\cdots,U_m$是$V$的子空间。则$U_1+\\cdots+U_m$是$V$包含$U_1,\\cdots,U_m$的最小子空间。\n\n证明：\n\n很容易发现$0\\in U_1+\\cdots+U_m$并且$U_1+\\cdots+U_m$对加法和数乘法封闭。因此它为子空间。\n\n显然，$U_1,\\cdots,U_m$都包含在$U_1+\\cdots+U_m$中。相反地，$V$的每个包含$U_1,\\cdots,U_m$的子空间都包含$U_1+\\cdots+U_m$，因此$U_1+\\cdots+U_m$是包含$U_1,\\cdots,U_m$的$V$的最小的子空间。\n\n#### 直和\n\n假设$U_1,\\cdots,U_m$是$V$的子空间。$U_1+\\cdots+U_m$的每一个元素都可以被写为以下形式：\n$$\nu_1+\\cdots+u_m\n$$\n其中$u_j$在$U_j$里。我们可能对$U_1+\\cdots+U_m$中每一个元素都可以用唯一一种上述形式表示的例子感兴趣。这种情况非常重要以至于我们给它一个特殊的名字：直和。\n\n***\n\n定义：直和\n\n假设$U_1,\\cdots,U_m$是$V$的子空间\n\n+ 和$U_1+\\cdots+U_m$被称为直和如果$U_1+\\cdots+U_m$的每一个元素可以唯一地表示为$u_1+\\cdots+u_m$，其中每一个$u_j$在$U_j$内。\n+ 如果$U_1+\\cdots+U_m$是直和，则$U_1\\oplus \\cdots \\oplus U_m$表示$U_1+\\cdots+U_m$，用符号$\\oplus$来表示这是直和。\n\n#### 直和的条件\n\n假设$U_1,\\cdots,U_m$是$V$的子空间。则$U_1+\\cdots+U_m$是直和当且仅当将$0$写为一个和$u_1+\\cdots+u_m$的唯一形式是让每一个$u_j$等于$0$，其中$u_j$位于$U_j$中。\n\n证明：\n\n首先假设$U_1+\\cdots+U_m$是一个直和。则直和的定义意味着将$0$写为$u_1+\\cdots+u_m$的和的唯一形式是通过使每个$u_j$等于零。\n\n现在假设将零写为$u_1+\\cdots+u_m$的唯一形式是令每一个$u_j$等于零。为了证明$U_1+\\cdots+U_m$是一个直和，令$v\\in U_1+\\cdots+U_m$。我们可以写为\n$$\nv = u_1+\\cdots+u_m\n$$\n为了证明这种表示是唯一的，假设我们有\n$$\nv = v_1+\\cdots+v_m\n$$\n将两个方程相减，我们有\n$$\n0 = (u_1-v_1)+\\cdots+(u_m-v_m)\n$$\n因此$u_1=v_1,\\cdots,u_m=v_m$。\n\n证毕。\n\n#### 两个子空间的直和\n\n假设$U$和$W$是$V$的子空间。则$U+W$是直和当且仅当$U\\cap W = \\{0\\}$。\n\n证明\n\n首先假设$U+W$是直和。如果$v\\in U\\cap W$，则$0 = v+(-v)$，其中$v\\in U, -v\\in W$。\n\n> 注：这一步是因为$v\\in U\\cap W$，则$v\\in U\\text{且} v\\in W$，所以$-v \\in W$。\n\n通过$0$的唯一的作为$U$中向量和$V$中向量的和表示方式，我们有$v=0$。因此$U\\cap W = \\{0\\}$，这完成了一个方向的证明。\n\n为了证明另一个方向，现在假设$U\\cap W = \\{0\\}$。为了证明$U+W$是直和，假设$u\\in U, w\\in W$，并且：\n$$\n0 = u+w\n$$\n为了完成证明，我们只需要证明$u=w=0$。上面的方程意味着$u = -w\\in W$。因此$u \\in U\\cap W$。所以$u=0$，这也意味着$w=0$，得证。\n\n证毕。\n\n","tags":["线性代数"],"categories":["数学"]},{"title":"高斯混合模型和EM算法","url":"/2021/08/31/高斯混合模型和EM算法/","content":"## Gaussian mixture models and the EM algorithm\n\n我们使用简写符号$X_1^n$来表示$X_1,X_2,\\cdots,X_n$，相似地，$x_1^n$表示$x_1,x_2,\\cdots,x_n$。\n\n### The model\n\n假设我们有有编号的人$i=1,\\cdots,n$。我们观察表示每个人的身高的随机变量$Y_i\\in \\mathbb{R}$，同时假设有一个观测不到的标签$C_i\\in \\{\\operatorname{M,F}\\}$表示人的性别。在这了，小写字母$c$代表\"class\"。我们也假设两组具有相同的已知方差$\\sigma^2$，但不同的未知均值$\\mu_M$和$\\mu_F$。类标签符合伯努利分布：\n$$\np_{C_i}(c_i) = q^{\\mathbb{1}(c_i=M)}(1-q)^{\\mathbb{1}(c_i=F)}\n$$\n我们也假设$q$是已知的。为了简化符号，我们令$\\pi_M=q$和$\\pi_F=1-q$，因此我们可以写作：\n$$\np_{C_i}(c_i) = \\prod_{c\\in \\{M,F\\}}\\pi_c^{\\mathbb{1}(c_i=c)}\n$$\n每一类的条件分布都为高斯分布：\n$$\np_{Y_i|C_i}(y_i|c_i) = \\prod_c\\mathcal{N}(y_i;\\mu_c,\\sigma^2)^{\\mathbb{1}(c_i=c)}\n$$\n\n### Parameter estimation: a first attempt\n\n假设我们观测到独立同分布的身高$Y_1=y_1,\\cdots,Y_n=y_n$，并且我们想要去找到参数$\\mu_M,\\mu_F$的极大似然估计。这是一个非监督问题：我们不知道我们数据的性别标签，但是我们想根据这些标签学习参数。\n\n根据上文提到的模型设计，计算所有数据点$P_{Y_1,\\cdots,Y_n}$的联合密度，以$\\mu_M,\\mu_F,\\sigma,q$表示。取$\\log$后计算$\\log$似然，然后对$\\mu_M$进行求导。为什么优化这么困难？\n\n我们先对单个数据点$Y_i=y_i$求密度：\n$$\n\\begin{aligned}\nP_{Y_i}(y_i) &= \\sum_{c_i}p_{C_i}(c_i)p_{Y_i|C_i}(y_i|c_i)\\\\\n&= \\sum_{c_i}(\\pi_c\\mathcal{N}(y_i;\\mu_C,\\sigma^2))^{\\mathbb{1}(c_i=c)}\\\\\n&= q\\mathcal{N}(y_i;\\mu_M,\\sigma^2) + (1-q)\\mathcal{N}(y_i;\\mu_F,\\sigma^2)\n\\end{aligned}\n$$\n现在，所有观测的联合分布为\n$$\nP_{Y_1^n}(y_1^n) = \\prod_{i=1}^n(q\\mathcal{N}(y_i;\\mu_M,\\sigma^2) + (1-q)\\mathcal{N}(y_i;\\mu_F,\\sigma^2))\n$$\n则$\\log$似然函数为：\n$$\n\\ln p_{Y_1^n}(y_1^n) = \\sum_{i=1}^n\\ln(\\pi_M\\mathcal{N}(y_i;\\mu_M,\\sigma^2) + \\pi_F\\mathcal{N}(y_i;\\mu_F,\\sigma^2))\n$$\n我们已经遇到了一个问题，和的形式阻止我们将$\\log$应用于内部的正态分布密度函数。通过对称性，我们只需要看其中一个均值，另一个将遵循相同的过程。在我们深入区分之前，我们注意到：\n$$\n\\begin{aligned}\n\\frac{d}{d\\mu}\\mathcal{N}(x;\\mu,\\sigma^2) &= \\frac{d}{d\\mu}[\\frac{1}{\\sigma\\sqrt{2\\pi}}e^{-\\frac{(x-\\mu)^2}{2\\sigma^2}}]\\\\\n&= \\frac{1}{\\sigma\\sqrt{2\\pi}}e^{-\\frac{-(x-\\mu)^2}{2\\sigma^2}}\\cdot\\frac{2(x-\\mu)}{2\\sigma^2}\\\\\n&= \\mathcal{N}(x;\\mu,\\sigma^2)\\cdot\\frac{(x-\\mu)}{\\sigma^2}\n\\end{aligned}\n$$\n对数似然函数对$\\mu_M$进行求导，得\n\n[^1]: $\\sum_{i=1}^n\\frac{1}{\\pi_M\\mathcal{N}(y_i;\\mu_M,\\sigma^2)+\\pi_F\\mathcal{N}(y_i;\\mu_F,\\sigma^2)}\\pi_M\\mathcal{N}(y_i;\\mu_M,\\sigma^2)\\frac{y_i-\\mu_M}{\\sigma^2}=0$\n\n$$\n\\sum_{i=1}^n\\frac{1}{\\pi_M\\mathcal{N}(y_i;\\mu_M,\\sigma^2)+\\pi_F\\mathcal{N}(y_i;\\mu_F,\\sigma^2)}\\pi_M\\mathcal{N}(y_i;\\mu_M,\\sigma^2)\\frac{y_i-\\mu_M}{\\sigma^2}=0\n$$\n\n对于这个表达式我们无法求解。\n\n### Using hidden variables and the EM Algorithm\n\n退一步，什么会使这个计算容易。如果我们知道隐变量$C_i$的值，则对参数们做最大似然估计就会很容易：我们将会取所有$C_i=M$的点用来估计$\\mu_M$，然后对所有$C_i=F$的点重复此过程来估计$\\mu_F$。受此启发，我们尝试计算给定观测下$C_i$的分布。我们将从贝叶斯规则开始：\n$$\n\\begin{aligned}\np_{C_i|Y_i}(c_i|y_i) &= \\frac{p_{Y_i|C_i}(y_i|c_i)p_{C_i}(c_i)}{p_{Y_i}(y_i)}\\\\\n&= \\frac{\\prod_{c\\in \\{M,F\\}}(\\pi_c\\mathcal{N}(y_i;\\mu_c,\\sigma^2))^{\\mathbb{1}(c=c_i)}}{\\pi_M\\mathcal{N}(y_i;\\mu_M,\\sigma^2)+\\pi_F\\mathcal{N}(y_i;\\mu_F,\\sigma^2)} = q_{C_i}(c_i)\n\\end{aligned}\n$$\n我们看一下$C_i=M$的后验概率：\n$$\np_{C_i|Y_i}(M|y_i) = \\frac{\\pi_M\\mathcal{N}(y_i;\\mu_M,\\sigma^2)}{\\pi_M\\mathcal{N}(y_i;\\mu_M,\\sigma^2)+\\pi_F\\mathcal{N}(y_i;\\mu_F,\\sigma^2)} = q_{C_i}(M)\n$$\n这个看起来很熟悉，这是式[^1]中的一部分，我们可以将式[^1]用$q_{C_i}$重写，并且假定其跟$\\mu_M$无关\n$$\n\\sum_{i=1}^nq_{C_i}(M)\\frac{y_i-\\mu_M}{\\sigma^2}=0\\\\\n\\mu_M = \\frac{\\sum_{i=1}^nq_{C_i}(M)y_i}{\\sum_{i=1}^nq_{C_i}(M)}\n$$\n这样就看起来好多了：$\\mu_M$是身高的加权平均值，其中每个身高都根据该人是男性的可能性进行加权。\n\n因此现在我们形成了一个循环，如果我们知道参数我们将会很容易计算出$C_1^n$的后验概率，并且如果我们知道后验概率我们将很容易地估计出参数，这就陷入了死循环。这就暗示了以下策略，我们可以固定一个来求解另一个。这种方法通常被称为`EM`算法。它的工作原理大致如下：\n\n+ 首先，我们固定参数(在这种情况下为高斯分布的均值$\\mu_M$和$\\mu_F$)并且求解隐变量的后验分布(在这种情况下记为$q_{C_i}$)。\n+ 之后，我们固定隐变量的后验分布，利用隐变量的期望值来最优化参数。\n+ 重复两个步骤直到收敛。\n\n### The EM Algorithm: a more formal look\n\n正如我们将在几个短步骤中展示的那样，EM算法实际上是最大化对数似然的下限(换句话说，每一步都保证改进我们的答案直到收敛)。\n\n假设我们观测到了随机变量$Y$。现在假设我们也有一些隐变量$C$并且$Y$依赖于$C$。我们可以说$C$和$Y$的分布中有一些我们不知道的参数$\\theta$，并且我们有兴趣找到它们。\n\n在我们上一个例子中，我们观测到有隐变量(性别)$C=\\{C_1,\\cdots,C_n\\}$的身高变量$Y=\\{Y_1,\\cdots,Y_n\\}$，并且$Y$和$C$是独立同分布的，我们的参数是$\\mu_M$和$\\mu_F$。\n\n在我们真正推导算法之前，我们需要一个关键结论：`Jensen's inequation`(琴声不等式)。在这个例子中我们需要的琴声不等式的特例：\n$$\n\\log(\\mathbb{E}[X])\\ge \\mathbb{E}[\\log(X)]\n$$\n下图是关于琴声不等式的几何直观：\n\n![](https://raw.githubusercontent.com/HFC666/image/master/img/EM.png)\n\n> 琴声不等式的特例说明：对于任何随机变量$X$，$\\mathbb{E}[\\log X]\\le\\log\\mathbb{E}[X]$。令$X$的PDF为红色的曲线。令$Z=\\log X$。中间和右边的图展示了怎么构建$Z$的PDF(蓝色曲线)：因为$\\log$，与$X$的PDF相比，它倾向于更小的值。$\\log\\mathbb{E}(X)$是由中间黑色曲线或$\\mathbb{E}(Z)$给出的点。但是，$\\mathbb{E}[\\log X]$或者$\\mathbb{E}[Z]$，将始终较小(或至少永远不会较大)因为对数挤压了分布较大的一端(其中$Z$比较大)并拉伸了较小的一端(其中$Z$较小)。\n\n> 关于琴声不等式\n>\n> 对于一个实函数$\\phi(x)$，在区间$I$内它是凸的($\\frac{d^2\\phi(x)}{d^2x}>0, \\forall x\\in I$)，那么它满足下面关系：\n> $$\n> \\phi(\\sum_{i=1}^Np_ix_i)\\le \\sum_{i=1}^Np_i\\phi(x_i)\n> $$\n> 其中$p_i\\ge0,\\sum_{i=1}^Np_i=1$，且$x_i\\in I,(i=1,\\cdots,N)$。\n>\n> **证明**：令$A=\\sum_{i=1}^Np_ix_i$，显然$A\\in I$。取\n> $$\n> \\begin{aligned}\n> S &= \\sum_{i=1}^N\\phi(x_i) - \\phi(A)\\\\\n> &= \\sum_{i=1}^Np_i[\\phi(x_i)-\\phi(A)]\\\\\n> &= \\sum_{i=1}^N p_i\\int_A^{x_i}\\phi^{\\prime}(x)dx\n> \\end{aligned}\n> $$\n> 若$A\\le x_i$，因为$\\phi^{\\prime}(x)$在区间$I$上是递增的，所以\n> $$\n> \\int_A^{x_i}\\phi^{\\prime}(x)dx\\ge \\phi^{\\prime}(A)(x_i-A)\n> $$\n> 若$A_i>x_i$，则\n> $$\n> \\int_A^{x_i}\\phi^{\\prime}(x)dx = -\\int_{x_i}^A\\phi^{\\prime}(x)dx\\ge-(A-x_i)\\phi(A) = (x_i-A)\\phi^{\\prime}(A)\n> $$\n> 所以：\n> $$\n> \\begin{aligned}\n> S &\\ge \\sum_{i=1}^Np_i\\phi^{\\prime}(A)(x_i-A)\\\\\n> &= \\phi^{\\prime}(A)[\\sum_{i=1}^Np_i(x_i-A)]\\\\\n> &= \\phi^{\\prime}(A)(A-A)\\\\\n> &=0\n> \\end{aligned}\n> $$\n> 证毕。\n\n在本文中，求期望相当于加权平均，而$\\ln(x)$的二阶导小于$0$，故与上文提到的不等式符号相反。\n\n#### The EM Algorithm\n\n我们想要去最大化似然函数。我们通过边缘化$C$来计算对数似然函数：\n$$\n\\log p_Y(y;\\theta) = \\log\\left(\\sum_cp_{Y,C}(y,c)\\right)\n$$\n我们现在也遭受了困境，我们无法对和取对数。如果我们交换它们的顺序不是更好吗？对，期望是一种特殊的求和，并且琴声不等式可以让我们互换它们如果我们有期望的话。因此，我们将要引进一个隐变量$C$的新的分布$q_C$：\n$$\n\\begin{aligned} \\log p_{Y}(y ; \\theta) & \\\\ \\left.\\text { (Marginalizing over } C \\text { and introducing } q_{C}(c) / q_{C}(c)\\right) &=\\log \\left(\\sum_{c} q_{C}(c) \\frac{p_{Y, C}(y, c ; \\theta)}{q_{C}(c)}\\right) \\\\ \\text { (Rewriting as an expectation) } &=\\log \\left(\\mathbb{E}_{q_{C}}\\left[\\frac{p_{Y, C}(y, C ; \\theta)}{q_{C}(C)}\\right]\\right) \\\\ \\text { (Using Jensen's inequality) } & \\geq \\mathbb{E}_{q_{C}}\\left[\\log \\frac{p_{Y, C}(y, C ; \\theta)}{q_{C}(C)}\\right](2) \\\\ \\text { Using definition of conditional probability } &=\\mathbb{E}_{q_{C}}\\left[\\log \\frac{p_{Y}(y ; \\theta) p_{C \\mid Y}(C \\mid y ; \\theta)}{q_{C}(C)}\\right](3) \\end{aligned}\n$$\n\n\n现在我们有可以很容易优化的$\\log p_Y(y;\\theta)$的下界了。因为我们已经引入了$q_C$，我们想要在$\\theta$和$q_C$上实施最大化。\n\n我们将使用(2)和(3)分别进行优化。首先先用(2)寻找最好的参数：\n$$\n\\mathbb{E}_{q_{C}}\\left[\\log \\frac{p_{Y, C}(y, C ; \\theta)}{q_{C}(C)}\\right] = \\mathbb{E}_{q_C}[\\log p_{Y, C}(y, C ; \\theta)] - \\mathbb{E}_{q_C}[\\log q_C(C)]\n$$\n因为$q_C$不依赖于$\\theta$，因此我们可以只优化第一项：\n$$\n\\widehat{\\theta} \\leftarrow \\underset{\\theta}{\\operatorname{argmax}} \\mathbb{E}_{q_{C}}\\left[\\log p_{Y, C}(y, C ; \\theta)\\right]\n$$\n这被称作`M-step`：`M`代表最大化，因为我们正在最大化参数。现在，我们用(3)来找到最好的$q_C$：\n$$\n\\mathbb{E}_{q_{C}}\\left[\\log \\frac{p_{Y}(y ; \\theta) p_{C \\mid Y}(C \\mid y ; \\theta)}{q_{C}(C)}\\right]=\\mathbb{E}_{q_{C}}\\left[\\log p_{Y}(y ; \\theta)\\right]+\\mathbb{E}_{q_{C}}\\left[\\log \\frac{p_{C \\mid Y}(C \\mid y ; \\theta)}{q_{C}(C)}\\right]\n$$\n第一项不依赖于$c$，并且第二项看起来像KL散度：\n$$\n\\begin{aligned}\n&= \\log p_Y(y;\\theta) - \\mathbb{E}_{q_C}[\\log\\frac{q_C(C)}{p_{C|Y}(C|y;\\theta)}]\\\\\n&= \\log p_Y(y;\\theta) - D(q_C(\\cdot)||p_{C|Y}(\\cdot|y;\\theta))\n\\end{aligned}\n$$\n因此，当最大化上述值时，我们想要最小化KL散度。KL散度总是大于等于零，当两个分布完全一样时取零。因此，最优化的$q_C$即为$p_{C|Y}(c|y;\\theta)$：\n$$\n\\hat{q}_C(c) \\leftarrow p_{C|Y}(c|y;\\theta)\n$$\n这被称为`E-step`：`E`代表期望，因为我们正在计算$q_C$以便我们可以将其用于期望值。\n\n#### The algorithm\n\n![](https://raw.githubusercontent.com/HFC666/image/master/img/EM_a.png)\n\n> 信息熵可以表达数据的信息量大小，是信息处理的一个非常重要的概念。对于离散型随机变量，信息熵公式如下：\n> $$\n> H(p) = H(X) = \\mathrm{E}_{x\\sim p(x)}[-\\log p(x)] = -\\sum_{i=1}^n p(x)\\log p(x)\n> $$\n> 对于连续性随机变量，信息熵公式如下：\n> $$\n> H(p) = H(X) = E_{x\\sim p(x)}[-\\log p(x)] = -\\int p(x)\\log p(x)dx\n> $$\n> 接下来我们来说一下相对熵，又被称为KL散度或信息散度，是两个概率分布差异的非对称度量。在信息论中，相对熵等价于两个概率分布的信息熵的插值，若其中一个概率分布为真实分布，另一个为理论分布，则此时相对熵等于交叉熵与真实分布的信息熵之差，表示使用理论分布拟合真实分布时产生的信息损耗。公式为：\n> $$\n> D_{\\operatorname{KL}}(p||q) = \\sum_{i=1}^N[p(x_i)\\log p(x_i) - p(x_i)\\log q(x_i)]\n> $$\n> 上面的$p(x_i)$为真实事件的概率分布，$q(x_i)$为理论拟合出来的该事件的概率分布。当拟合出来的事件概率分布跟真实的一模一样时，相对熵等于零，而拟合出来不一样时，相对熵大于零。\n>\n> 最后我们来证明一下相对熵公式只有在$p(x_i)$等于$q(x_i)$的时候等于$0$，其他时候大于$0$。\n>\n> 要证：\n> $$\n> D_{\\operatorname{KL}}(p||q) = \\sum_{i=1}^N[p(x_i)\\log p(x_i)-p(x_i)\\log q(x_i)]\\ge 0\n> $$\n> 即证：\n> $$\n> \\sum_{i=1}^Np(x_i)\\log\\frac{q(x_i)}{p(x_i)}\\le 0\n> $$\n> 又$\\ln(x)\\le x-1$，当且仅当$x=1$时等号成立\n>\n> 故\n> $$\n> \\sum_{i=1}^Np(x_i)\\log\\frac{q(x_i)}{p(x_i)}\\le \\sum_{i=1}^Np(x_i)(\\frac{q(x_i)}{p(x_i)}-1) = \\sum_{i=1}^N[p(x_i)-q(x_i)]=0\n> $$\n> 上面式子中$=$只在$p(x_i)=q(x_i)$时成立。\n\n### Example: Applying the general algorithm to GMMS\n\n现在，让我们重现看一下关于身高的GMM模型，看看我们如何应用这两个步骤。我们有观测变量$Y=\\{Y_1,\\cdots,Y_n\\}$和隐变量$C=\\{C_1,\\cdots,C_n\\}$。对于`E-Step`，我们计算后验概率$p_{C|Y}(c|y)$，之前已经计算过了。对于`M-Step`，我们得去计算联合概率：\n$$\n\\begin{aligned}\n\\mathbb{E}_{q_C}[\\ln p_{Y,C}(y,C)] &= \\mathbb{E}[\\ln p_{Y|C}(y|C)p_C(C)]\\\\\n&= \\mathbb{E}_{q_C}\\left[\\ln \\prod_{i=1}^n\\prod_{c\\in \\{M,F\\}}(\\pi_c\\mathcal{N}(y_i;\\mu_c,\\sigma^2))^{\\mathbb{1}(C_i=c)}\\right]\\\\\n&= \\mathbb{E}_{q_C}\\left[\\sum_{i=1}^n\\sum_{c\\in \\{M,F\\}}\\mathbb{1}(C_i=c)(\\ln\\pi_c+\\ln\\mathcal{N}(y_i;\\mu_c,\\sigma^2))\\right]\\\\\n&= \\sum_{i=1}^n\\sum_{c\\in \\{M,F\\}}\\mathbb{E}_{q_C}[\\mathbb{1}(C_i=c)]\\left(\\ln\\pi_c + \\ln\\frac{1}{\\sigma\\sqrt{2\\pi}}-\\frac{(y_i-\\mu_c)^2}{2\\sigma^2}\\right)\n \\end{aligned}\n$$\n$\\mathbb{E}_{q_C}[\\mathbb{1}(C_i=c)]$是$C_i$为$c$的概率。现在我们对$\\mu_M$求导：\n$$\n\\frac{d}{d \\mu_{M}} \\mathbb{E}_{q_{C}}\\left[\\ln p_{Y \\mid C}(y \\mid C) p_{C}(C)\\right]=\\sum_{i=1}^{n} q_{C_{i}}(M)\\left(\\frac{y_{i}-\\mu_{M}}{\\sigma^{2}}\\right)=0\n$$\n得：\n$$\n\\mu_{M}=\\frac{\\sum_{i=1}^{n} q_{C_{i}}(M) y_{i}}{\\sum_{i=1}^{n} q_{C_{i}}(M)}\n$$\n\n同理可得：\n$$\n\\mu_{F}=\\frac{\\sum_{i=1}^{n} q_{C_{i}}(F) y_{i}}{\\sum_{i=1}^{n} q_{C_{i}}(F)}\n$$\n","tags":["机器学习"],"categories":["算法"]},{"title":"贝叶斯数据分析","url":"/2021/08/11/贝叶斯数据分析/","content":"\n当初为什么要用英文写笔记？？\n{% pdf https://hfcouc.work/pdfs/Bayesian_Data_Analysis.pdf %}","tags":["贝叶斯数据分析"],"categories":["贝叶斯机器学习"]},{"title":"微分方程","url":"/2021/08/09/微分方程/","content":"\n微分方程学习笔记\n\n{% pdf https://hfcouc.work/pdfs/Differential_equation.pdf %}\n","tags":["微分方程"],"categories":["数学"]},{"title":"数值分析笔记","url":"/2021/07/30/数值分析/","content":"\n学习数值分析的笔记\n\n{% pdf https://hfcouc.work/pdfs/Numerical_analysis.pdf %}\n\n","tags":["数值分析"],"categories":["数学"]},{"title":"统计学习基础","url":"/2021/07/28/统计学习精要笔记/","content":"\n自己学习统计学习基础的笔记。\n\n{% pdf https://hfcouc.work/pdfs/ESLII.pdf %}\n\n","tags":["机器学习"],"categories":["机器学习"]},{"title":"只争朝夕，不负韶华","url":"/2021/07/13/index/","content":"\n{% meting \"346089\" \"netease\" \"song\" \"theme:#555\" \"mutex:true\" \"listmaxheight:340px\" \"preload:auto\" %}\n\n\n\n<img src=\"https://cdn.jsdelivr.net/gh/HFC666/image@master/20210713/3.178msyh9rg68.png\" style=\"zoom:80%;\" />\n\n\n\n> 现状是没那么容易改变的\n> 即便是足够努力\n> 也很难在短时间内看出效果\n> 所以有时你认为的无法改变\n> 也可能只是暂时没看出效果而己\n> 而不是不够努力\n\n<img src=\"https://cdn.jsdelivr.net/gh/HFC666/image@master/20210713/4.36iwffkaky00.png\" style=\"zoom:80%;\" />\n\n\n\n> 不能再浑浑噩噩\n> 如果不把眼皮用力抬起看个真切\n> 或许就会错过人生中按下快门的良机\n\n\n\n**即使是一个人，也需要好好吃饭，这是治愈自己的一种方式**\n\n\n\n“不被时间和社会所束缚，幸福的填饱肚子，短时间内变得随心所欲，变得自由，不被打扰；毫不费神的吃东西的这种孤高行为，是现代人，都平等的拥有的最高治愈。”\n\n{% raw %}\n\n<div style=\"position: relative; width: 100%; height: 0; padding-bottom: 75%;\"><iframe \nsrc=\"//player.bilibili.com/player.html?\taid=73923411&bvid=BV1AE411a7Nw&cid=126453908&page=1\" scrolling=\"no\" border=\"0\" \nframeborder=\"no\" framespacing=\"0\" allowfullscreen=\"true\" style=\"position: absolute; width: 100%; \nheight: 100%; left: 0; top: 0;\"> </iframe></div>\n\n\n{% endraw %}\n\n\n\n<div style=\"position: relative; width: 100%; height: 0; padding-bottom: 75%;\"><iframe \nsrc=\"//player.bilibili.com/player.html?aid=52761403&bvid=BV1r4411776v&cid=92328539&page=1\" scrolling=\"no\" border=\"0\" \nframeborder=\"no\" framespacing=\"0\" allowfullscreen=\"true\" style=\"position: absolute; width: 100%; \nheight: 100%; left: 0; top: 0;\"> </iframe></div>\n\n\n\n\n"}]