<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>独自赏晴雨</title>
  
  
  <link href="https://www.hfcouc.work/atom.xml" rel="self"/>
  
  <link href="https://www.hfcouc.work/"/>
  <updated>2021-08-31T15:27:21.801Z</updated>
  <id>https://www.hfcouc.work/</id>
  
  <author>
    <name>HFC</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>高斯混合模型和EM算法</title>
    <link href="https://www.hfcouc.work/2021/08/31/%E9%AB%98%E6%96%AF%E6%B7%B7%E5%90%88%E6%A8%A1%E5%9E%8B%E5%92%8CEM%E7%AE%97%E6%B3%95/"/>
    <id>https://www.hfcouc.work/2021/08/31/%E9%AB%98%E6%96%AF%E6%B7%B7%E5%90%88%E6%A8%A1%E5%9E%8B%E5%92%8CEM%E7%AE%97%E6%B3%95/</id>
    <published>2021-08-31T15:24:15.000Z</published>
    <updated>2021-08-31T15:27:21.801Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="\assets\js\Meting.min.js"></script><h2 id="Gaussian-mixture-models-and-the-EM-algorithm"><a href="#Gaussian-mixture-models-and-the-EM-algorithm" class="headerlink" title="Gaussian mixture models and the EM algorithm"></a>Gaussian mixture models and the EM algorithm</h2><p>我们使用简写符号$X_1^n$来表示$X_1,X_2,\cdots,X_n$，相似地，$x_1^n$表示$x_1,x_2,\cdots,x_n$。</p><h3 id="The-model"><a href="#The-model" class="headerlink" title="The model"></a>The model</h3><p>假设我们有有编号的人$i=1,\cdots,n$。我们观察表示每个人的身高的随机变量$Y_i\in \mathbb{R}$，同时假设有一个观测不到的标签$C_i\in {\operatorname{M,F}}$表示人的性别。在这了，小写字母$c$代表”class”。我们也假设两组具有相同的已知方差$\sigma^2$，但不同的未知均值$\mu_M$和$\mu_F$。类标签符合伯努利分布：<br>$$<br>p_{C_i}(c_i) = q^{\mathbb{1}(c_i=M)}(1-q)^{\mathbb{1}(c_i=F)}<br>$$<br>我们也假设$q$是已知的。为了简化符号，我们令$\pi_M=q$和$\pi_F=1-q$，因此我们可以写作：<br>$$<br>p_{C_i}(c_i) = \prod_{c\in {M,F}}\pi_c^{\mathbb{1}(c_i=c)}<br>$$<br>每一类的条件分布都为高斯分布：<br>$$<br>p_{Y_i|C_i}(y_i|c_i) = \prod_c\mathcal{N}(y_i;\mu_c,\sigma^2)^{\mathbb{1}(c_i=c)}<br>$$</p><h3 id="Parameter-estimation-a-first-attempt"><a href="#Parameter-estimation-a-first-attempt" class="headerlink" title="Parameter estimation: a first attempt"></a>Parameter estimation: a first attempt</h3><p>假设我们观测到独立同分布的身高$Y_1=y_1,\cdots,Y_n=y_n$，并且我们想要去找到参数$\mu_M,\mu_F$的极大似然估计。这是一个非监督问题：我们不知道我们数据的性别标签，但是我们想根据这些标签学习参数。</p><p>根据上文提到的模型设计，计算所有数据点$P_{Y_1,\cdots,Y_n}$的联合密度，以$\mu_M,\mu_F,\sigma,q$表示。取$\log$后计算$\log$似然，然后对$\mu_M$进行求导。为什么优化这么困难？</p><p>我们先对单个数据点$Y_i=y_i$求密度：<br>$$<br>\begin{aligned}<br>P_{Y_i}(y_i) &amp;= \sum_{c_i}p_{C_i}(c_i)p_{Y_i|C_i}(y_i|c_i)\<br>&amp;= \sum_{c_i}(\pi_c\mathcal{N}(y_i;\mu_C,\sigma^2))^{\mathbb{1}(c_i=c)}\<br>&amp;= q\mathcal{N}(y_i;\mu_M,\sigma^2) + (1-q)\mathcal{N}(y_i;\mu_F,\sigma^2)<br>\end{aligned}<br>$$<br>现在，所有观测的联合分布为<br>$$<br>P_{Y_1^n}(y_1^n) = \prod_{i=1}^n(q\mathcal{N}(y_i;\mu_M,\sigma^2) + (1-q)\mathcal{N}(y_i;\mu_F,\sigma^2))<br>$$<br>则$\log$似然函数为：<br>$$<br>\ln p_{Y_1^n}(y_1^n) = \sum_{i=1}^n\ln(\pi_M\mathcal{N}(y_i;\mu_M,\sigma^2) + \pi_F\mathcal{N}(y_i;\mu_F,\sigma^2))<br>$$<br>我们已经遇到了一个问题，和的形式阻止我们将$\log$应用于内部的正态分布密度函数。通过对称性，我们只需要看其中一个均值，另一个将遵循相同的过程。在我们深入区分之前，我们注意到：<br>$$<br>\begin{aligned}<br>\frac{d}{d\mu}\mathcal{N}(x;\mu,\sigma^2) &amp;= \frac{d}{d\mu}[\frac{1}{\sigma\sqrt{2\pi}}e^{-\frac{(x-\mu)^2}{2\sigma^2}}]\<br>&amp;= \frac{1}{\sigma\sqrt{2\pi}}e^{-\frac{-(x-\mu)^2}{2\sigma^2}}\cdot\frac{2(x-\mu)}{2\sigma^2}\<br>&amp;= \mathcal{N}(x;\mu,\sigma^2)\cdot\frac{(x-\mu)}{\sigma^2}<br>\end{aligned}<br>$$<br>对数似然函数对$\mu_M$进行求导，得</p><p>$$<br>\sum_{i=1}^n\frac{1}{\pi_M\mathcal{N}(y_i;\mu_M,\sigma^2)+\pi_F\mathcal{N}(y_i;\mu_F,\sigma^2)}\pi_M\mathcal{N}(y_i;\mu_M,\sigma^2)\frac{y_i-\mu_M}{\sigma^2}=0<br>$$</p><p>对于这个表达式我们无法求解。</p><h3 id="Using-hidden-variables-and-the-EM-Algorithm"><a href="#Using-hidden-variables-and-the-EM-Algorithm" class="headerlink" title="Using hidden variables and the EM Algorithm"></a>Using hidden variables and the EM Algorithm</h3><p>退一步，什么会使这个计算容易。如果我们知道隐变量$C_i$的值，则对参数们做最大似然估计就会很容易：我们将会取所有$C_i=M$的点用来估计$\mu_M$，然后对所有$C_i=F$的点重复此过程来估计$\mu_F$。受此启发，我们尝试计算给定观测下$C_i$的分布。我们将从贝叶斯规则开始：<br>$$<br>\begin{aligned}<br>p_{C_i|Y_i}(c_i|y_i) &amp;= \frac{p_{Y_i|C_i}(y_i|c_i)p_{C_i}(c_i)}{p_{Y_i}(y_i)}\<br>&amp;= \frac{\prod_{c\in {M,F}}(\pi_c\mathcal{N}(y_i;\mu_c,\sigma^2))^{\mathbb{1}(c=c_i)}}{\pi_M\mathcal{N}(y_i;\mu_M,\sigma^2)+\pi_F\mathcal{N}(y_i;\mu_F,\sigma^2)} = q_{C_i}(c_i)<br>\end{aligned}<br>$$<br>我们看一下$C_i=M$的后验概率：<br>$$<br>p_{C_i|Y_i}(M|y_i) = \frac{\pi_M\mathcal{N}(y_i;\mu_M,\sigma^2)}{\pi_M\mathcal{N}(y_i;\mu_M,\sigma^2)+\pi_F\mathcal{N}(y_i;\mu_F,\sigma^2)} = q_{C_i}(M)<br>$$<br>这个看起来很熟悉，这是式<a href="$%5Csum_%7Bi=1%7D%5En%5Cfrac%7B1%7D%7B%5Cpi_M%5Cmathcal%7BN%7D(y_i;%5Cmu_M,%5Csigma%5E2)+%5Cpi_F%5Cmathcal%7BN%7D(y_i;%5Cmu_F,%5Csigma%5E2)%7D%5Cpi_M%5Cmathcal%7BN%7D(y_i;%5Cmu_M,%5Csigma%5E2)%5Cfrac%7By_i-%5Cmu_M%7D%7B%5Csigma%5E2%7D=0$">^1</a>中的一部分，我们可以将式<a href="$%5Csum_%7Bi=1%7D%5En%5Cfrac%7B1%7D%7B%5Cpi_M%5Cmathcal%7BN%7D(y_i;%5Cmu_M,%5Csigma%5E2)+%5Cpi_F%5Cmathcal%7BN%7D(y_i;%5Cmu_F,%5Csigma%5E2)%7D%5Cpi_M%5Cmathcal%7BN%7D(y_i;%5Cmu_M,%5Csigma%5E2)%5Cfrac%7By_i-%5Cmu_M%7D%7B%5Csigma%5E2%7D=0$">^1</a>用$q_{C_i}$重写，并且假定其跟$\mu_M$无关<br>$$<br>\sum_{i=1}^nq_{C_i}(M)\frac{y_i-\mu_M}{\sigma^2}=0\<br>\mu_M = \frac{\sum_{i=1}^nq_{C_i}(M)y_i}{\sum_{i=1}^nq_{C_i}(M)}<br>$$<br>这样就看起来好多了：$\mu_M$是身高的加权平均值，其中每个身高都根据该人是男性的可能性进行加权。</p><p>因此现在我们形成了一个循环，如果我们知道参数我们将会很容易计算出$C_1^n$的后验概率，并且如果我们知道后验概率我们将很容易地估计出参数，这就陷入了死循环。这就暗示了以下策略，我们可以固定一个来求解另一个。这种方法通常被称为<code>EM</code>算法。它的工作原理大致如下：</p><ul><li>首先，我们固定参数(在这种情况下为高斯分布的均值$\mu_M$和$\mu_F$)并且求解隐变量的后验分布(在这种情况下记为$q_{C_i}$)。</li><li>之后，我们固定隐变量的后验分布，利用隐变量的期望值来最优化参数。</li><li>重复两个步骤直到收敛。</li></ul><h3 id="The-EM-Algorithm-a-more-formal-look"><a href="#The-EM-Algorithm-a-more-formal-look" class="headerlink" title="The EM Algorithm: a more formal look"></a>The EM Algorithm: a more formal look</h3><p>正如我们将在几个短步骤中展示的那样，EM算法实际上是最大化对数似然的下限(换句话说，每一步都保证改进我们的答案直到收敛)。</p><p>假设我们观测到了随机变量$Y$。现在假设我们也有一些隐变量$C$并且$Y$依赖于$C$。我们可以说$C$和$Y$的分布中有一些我们不知道的参数$\theta$，并且我们有兴趣找到它们。</p><p>在我们上一个例子中，我们观测到有隐变量(性别)$C={C_1,\cdots,C_n}$的身高变量$Y={Y_1,\cdots,Y_n}$，并且$Y$和$C$是独立同分布的，我们的参数是$\mu_M$和$\mu_F$。</p><p>在我们真正推导算法之前，我们需要一个关键结论：<code>Jensen&#39;s inequation</code>(琴声不等式)。在这个例子中我们需要的琴声不等式的特例：<br>$$<br>\log(\mathbb{E}[X])\ge \mathbb{E}[\log(X)]<br>$$<br>下图是关于琴声不等式的几何直观：</p><p><img src="https://raw.githubusercontent.com/HFC666/image/master/img/EM.png"></p><blockquote><p>琴声不等式的特例说明：对于任何随机变量$X$，$\mathbb{E}[\log X]\le\log\mathbb{E}[X]$。令$X$的PDF为红色的曲线。令$Z=\log X$。中间和右边的图展示了怎么构建$Z$的PDF(蓝色曲线)：因为$\log$，与$X$的PDF相比，它倾向于更小的值。$\log\mathbb{E}(X)$是由中间黑色曲线或$\mathbb{E}(Z)$给出的点。但是，$\mathbb{E}[\log X]$或者$\mathbb{E}[Z]$，将始终较小(或至少永远不会较大)因为对数挤压了分布较大的一端(其中$Z$比较大)并拉伸了较小的一端(其中$Z$较小)。</p></blockquote><blockquote><p>关于琴声不等式</p><p>对于一个实函数$\phi(x)$，在区间$I$内它是凸的($\frac{d^2\phi(x)}{d^2x}&gt;0, \forall x\in I$)，那么它满足下面关系：<br>$$<br>\phi(\sum_{i=1}^Np_ix_i)\le \sum_{i=1}^Np_i\phi(x_i)<br>$$<br>其中$p_i\ge0,\sum_{i=1}^Np_i=1$，且$x_i\in I,(i=1,\cdots,N)$。</p><p><strong>证明</strong>：令$A=\sum_{i=1}^Np_ix_i$，显然$A\in I$。取<br>$$<br>\begin{aligned}<br>S &amp;= \sum_{i=1}^N\phi(x_i) - \phi(A)\<br>&amp;= \sum_{i=1}^Np_i[\phi(x_i)-\phi(A)]\<br>&amp;= \sum_{i=1}^N p_i\int_A^{x_i}\phi^{\prime}(x)dx<br>\end{aligned}<br>$$<br>若$A\le x_i$，因为$\phi^{\prime}(x)$在区间$I$上是递增的，所以<br>$$<br>\int_A^{x_i}\phi^{\prime}(x)dx\ge \phi^{\prime}(A)(x_i-A)<br>$$<br>若$A_i&gt;x_i$，则<br>$$<br>\int_A^{x_i}\phi^{\prime}(x)dx = -\int_{x_i}^A\phi^{\prime}(x)dx\ge-(A-x_i)\phi(A) = (x_i-A)\phi^{\prime}(A)<br>$$<br>所以：<br>$$<br>\begin{aligned}<br>S &amp;\ge \sum_{i=1}^Np_i\phi^{\prime}(A)(x_i-A)\<br>&amp;= \phi^{\prime}(A)[\sum_{i=1}^Np_i(x_i-A)]\<br>&amp;= \phi^{\prime}(A)(A-A)\<br>&amp;=0<br>\end{aligned}<br>$$<br>证毕。</p></blockquote><p>在本文中，求期望相当于加权平均，而$\ln(x)$的二阶导小于$0$，故与上文提到的不等式符号相反。</p><h4 id="The-EM-Algorithm"><a href="#The-EM-Algorithm" class="headerlink" title="The EM Algorithm"></a>The EM Algorithm</h4><p>我们想要去最大化似然函数。我们通过边缘化$C$来计算对数似然函数：<br>$$<br>\log p_Y(y;\theta) = \log\left(\sum_cp_{Y,C}(y,c)\right)<br>$$<br>我们现在也遭受了困境，我们无法对和取对数。如果我们交换它们的顺序不是更好吗？对，期望是一种特殊的求和，并且琴声不等式可以让我们互换它们如果我们有期望的话。因此，我们将要引进一个隐变量$C$的新的分布$q_C$：<br>$$<br>\begin{aligned} \log p_{Y}(y ; \theta) &amp; \ \left.\text { (Marginalizing over } C \text { and introducing } q_{C}(c) / q_{C}(c)\right) &amp;=\log \left(\sum_{c} q_{C}(c) \frac{p_{Y, C}(y, c ; \theta)}{q_{C}(c)}\right) \ \text { (Rewriting as an expectation) } &amp;=\log \left(\mathbb{E}<em>{q</em>{C}}\left[\frac{p_{Y, C}(y, C ; \theta)}{q_{C}(C)}\right]\right) \ \text { (Using Jensen’s inequality) } &amp; \geq \mathbb{E}<em>{q</em>{C}}\left<a href="2">\log \frac{p_{Y, C}(y, C ; \theta)}{q_{C}(C)}\right</a> \ \text { Using definition of conditional probability } &amp;=\mathbb{E}<em>{q</em>{C}}\left<a href="3">\log \frac{p_{Y}(y ; \theta) p_{C \mid Y}(C \mid y ; \theta)}{q_{C}(C)}\right</a> \end{aligned}<br>$$</p><p>现在我们有可以很容易优化的$\log p_Y(y;\theta)$的下界了。因为我们已经引入了$q_C$，我们想要在$\theta$和$q_C$上实施最大化。</p><p>我们将使用(2)和(3)分别进行优化。首先先用(2)寻找最好的参数：<br>$$<br>\mathbb{E}<em>{q</em>{C}}\left[\log \frac{p_{Y, C}(y, C ; \theta)}{q_{C}(C)}\right] = \mathbb{E}<em>{q_C}[\log p</em>{Y, C}(y, C ; \theta)] - \mathbb{E}<em>{q_C}[\log q_C(C)]<br>$$<br>因为$q_C$不依赖于$\theta$，因此我们可以只优化第一项：<br>$$<br>\widehat{\theta} \leftarrow \underset{\theta}{\operatorname{argmax}} \mathbb{E}</em>{q_{C}}\left[\log p_{Y, C}(y, C ; \theta)\right]<br>$$<br>这被称作<code>M-step</code>：<code>M</code>代表最大化，因为我们正在最大化参数。现在，我们用(3)来找到最好的$q_C$：<br>$$<br>\mathbb{E}<em>{q</em>{C}}\left[\log \frac{p_{Y}(y ; \theta) p_{C \mid Y}(C \mid y ; \theta)}{q_{C}(C)}\right]=\mathbb{E}<em>{q</em>{C}}\left[\log p_{Y}(y ; \theta)\right]+\mathbb{E}<em>{q</em>{C}}\left[\log \frac{p_{C \mid Y}(C \mid y ; \theta)}{q_{C}(C)}\right]<br>$$<br>第一项不依赖于$c$，并且第二项看起来像KL散度：<br>$$<br>\begin{aligned}<br>&amp;= \log p_Y(y;\theta) - \mathbb{E}<em>{q_C}[\log\frac{q_C(C)}{p</em>{C|Y}(C|y;\theta)}]\<br>&amp;= \log p_Y(y;\theta) - D(q_C(\cdot)||p_{C|Y}(\cdot|y;\theta))<br>\end{aligned}<br>$$<br>因此，当最大化上述值时，我们想要最小化KL散度。KL散度总是大于等于零，当两个分布完全一样时取零。因此，最优化的$q_C$即为$p_{C|Y}(c|y;\theta)$：<br>$$<br>\hat{q}<em>C(c) \leftarrow p</em>{C|Y}(c|y;\theta)<br>$$<br>这被称为<code>E-step</code>：<code>E</code>代表期望，因为我们正在计算$q_C$以便我们可以将其用于期望值。</p><h4 id="The-algorithm"><a href="#The-algorithm" class="headerlink" title="The algorithm"></a>The algorithm</h4><p><img src="https://raw.githubusercontent.com/HFC666/image/master/img/EM_a.png"></p><blockquote><p>信息熵可以表达数据的信息量大小，是信息处理的一个非常重要的概念。对于离散型随机变量，信息熵公式如下：<br>$$<br>H(p) = H(X) = \mathrm{E}<em>{x\sim p(x)}[-\log p(x)] = -\sum</em>{i=1}^n p(x)\log p(x)<br>$$<br>对于连续性随机变量，信息熵公式如下：<br>$$<br>H(p) = H(X) = E_{x\sim p(x)}[-\log p(x)] = -\int p(x)\log p(x)dx<br>$$<br>接下来我们来说一下相对熵，又被称为KL散度或信息散度，是两个概率分布差异的非对称度量。在信息论中，相对熵等价于两个概率分布的信息熵的插值，若其中一个概率分布为真实分布，另一个为理论分布，则此时相对熵等于交叉熵与真实分布的信息熵之差，表示使用理论分布拟合真实分布时产生的信息损耗。公式为：<br>$$<br>D_{\operatorname{KL}}(p||q) = \sum_{i=1}^N[p(x_i)\log p(x_i) - p(x_i)\log q(x_i)]<br>$$<br>上面的$p(x_i)$为真实事件的概率分布，$q(x_i)$为理论拟合出来的该事件的概率分布。当拟合出来的事件概率分布跟真实的一模一样时，相对熵等于零，而拟合出来不一样时，相对熵大于零。</p><p>最后我们来证明一下相对熵公式只有在$p(x_i)$等于$q(x_i)$的时候等于$0$，其他时候大于$0$。</p><p>要证：<br>$$<br>D_{\operatorname{KL}}(p||q) = \sum_{i=1}^N[p(x_i)\log p(x_i)-p(x_i)\log q(x_i)]\ge 0<br>$$<br>即证：<br>$$<br>\sum_{i=1}^Np(x_i)\log\frac{q(x_i)}{p(x_i)}\le 0<br>$$<br>又$\ln(x)\le x-1$，当且仅当$x=1$时等号成立</p><p>故<br>$$<br>\sum_{i=1}^Np(x_i)\log\frac{q(x_i)}{p(x_i)}\le \sum_{i=1}^Np(x_i)(\frac{q(x_i)}{p(x_i)}-1) = \sum_{i=1}^N[p(x_i)-q(x_i)]=0<br>$$<br>上面式子中$=$只在$p(x_i)=q(x_i)$时成立。</p></blockquote><h3 id="Example-Applying-the-general-algorithm-to-GMMS"><a href="#Example-Applying-the-general-algorithm-to-GMMS" class="headerlink" title="Example: Applying the general algorithm to GMMS"></a>Example: Applying the general algorithm to GMMS</h3><p>现在，让我们重现看一下关于身高的GMM模型，看看我们如何应用这两个步骤。我们有观测变量$Y={Y_1,\cdots,Y_n}$和隐变量$C={C_1,\cdots,C_n}$。对于<code>E-Step</code>，我们计算后验概率$p_{C|Y}(c|y)$，之前已经计算过了。对于<code>M-Step</code>，我们得去计算联合概率：<br>$$<br>\begin{aligned}<br>\mathbb{E}<em>{q_C}[\ln p</em>{Y,C}(y,C)] &amp;= \mathbb{E}[\ln p_{Y|C}(y|C)p_C(C)]\<br>&amp;= \mathbb{E}<em>{q_C}\left[\ln \prod</em>{i=1}^n\prod_{c\in {M,F}}(\pi_c\mathcal{N}(y_i;\mu_c,\sigma^2))^{\mathbb{1}(C_i=c)}\right]\<br>&amp;= \mathbb{E}<em>{q_C}\left[\sum</em>{i=1}^n\sum_{c\in {M,F}}\mathbb{1}(C_i=c)(\ln\pi_c+\ln\mathcal{N}(y_i;\mu_c,\sigma^2))\right]\<br>&amp;= \sum_{i=1}^n\sum_{c\in {M,F}}\mathbb{E}<em>{q_C}[\mathbb{1}(C_i=c)]\left(\ln\pi_c + \ln\frac{1}{\sigma\sqrt{2\pi}}-\frac{(y_i-\mu_c)^2}{2\sigma^2}\right)<br> \end{aligned}<br>$$<br>$\mathbb{E}</em>{q_C}[\mathbb{1}(C_i=c)]$是$C_i$为$c$的概率。现在我们对$\mu_M$求导：<br>$$<br>\frac{d}{d \mu_{M}} \mathbb{E}<em>{q</em>{C}}\left[\ln p_{Y \mid C}(y \mid C) p_{C}(C)\right]=\sum_{i=1}^{n} q_{C_{i}}(M)\left(\frac{y_{i}-\mu_{M}}{\sigma^{2}}\right)=0<br>$$<br>得：<br>$$<br>\mu_{M}=\frac{\sum_{i=1}^{n} q_{C_{i}}(M) y_{i}}{\sum_{i=1}^{n} q_{C_{i}}(M)}<br>$$</p><p>同理可得：<br>$$<br>\mu_{F}=\frac{\sum_{i=1}^{n} q_{C_{i}}(F) y_{i}}{\sum_{i=1}^{n} q_{C_{i}}(F)}<br>$$</p>]]></content>
    
    
      
      
    <summary type="html">&lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;\assets\css\APlayer.min.css&quot;&gt;&lt;script src=&quot;\assets\js\APlayer.min.js&quot; cla</summary>
      
    
    
    
    
    <category term="机器学习" scheme="https://www.hfcouc.work/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>贝叶斯数据分析</title>
    <link href="https://www.hfcouc.work/2021/08/11/%E8%B4%9D%E5%8F%B6%E6%96%AF%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90/"/>
    <id>https://www.hfcouc.work/2021/08/11/%E8%B4%9D%E5%8F%B6%E6%96%AF%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90/</id>
    <published>2021-08-10T23:43:01.000Z</published>
    <updated>2021-08-10T23:48:44.401Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="\assets\js\Meting.min.js"></script><p>当初为什么要用英文写笔记？？</p><div class="row">    <embed src="https://hfcouc.work/pdfs/Bayesian_Data_Analysis.pdf" width="100%" height="550" type="application/pdf"></div>]]></content>
    
    
      
      
    <summary type="html">&lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;\assets\css\APlayer.min.css&quot;&gt;&lt;script src=&quot;\assets\js\APlayer.min.js&quot; cla</summary>
      
    
    
    
    
    <category term="贝叶斯数据分析" scheme="https://www.hfcouc.work/tags/%E8%B4%9D%E5%8F%B6%E6%96%AF%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90/"/>
    
  </entry>
  
  <entry>
    <title>微分方程</title>
    <link href="https://www.hfcouc.work/2021/08/09/%E5%BE%AE%E5%88%86%E6%96%B9%E7%A8%8B/"/>
    <id>https://www.hfcouc.work/2021/08/09/%E5%BE%AE%E5%88%86%E6%96%B9%E7%A8%8B/</id>
    <published>2021-08-09T07:14:29.000Z</published>
    <updated>2021-08-09T07:18:47.286Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="\assets\js\Meting.min.js"></script><p>微分方程学习笔记</p><div class="row">    <embed src="https://hfcouc.work/pdfs/Differential_equation.pdf" width="100%" height="550" type="application/pdf"></div>]]></content>
    
    
      
      
    <summary type="html">&lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;\assets\css\APlayer.min.css&quot;&gt;&lt;script src=&quot;\assets\js\APlayer.min.js&quot; cla</summary>
      
    
    
    
    
    <category term="微分方程" scheme="https://www.hfcouc.work/tags/%E5%BE%AE%E5%88%86%E6%96%B9%E7%A8%8B/"/>
    
  </entry>
  
  <entry>
    <title>数值分析笔记</title>
    <link href="https://www.hfcouc.work/2021/07/30/%E6%95%B0%E5%80%BC%E5%88%86%E6%9E%90/"/>
    <id>https://www.hfcouc.work/2021/07/30/%E6%95%B0%E5%80%BC%E5%88%86%E6%9E%90/</id>
    <published>2021-07-30T13:35:47.000Z</published>
    <updated>2021-08-10T23:49:00.412Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="\assets\js\Meting.min.js"></script><p>学习数值分析的笔记</p><div class="row">    <embed src="https://hfcouc.work/pdfs/Numerical_analysis.pdf" width="100%" height="550" type="application/pdf"></div>]]></content>
    
    
      
      
    <summary type="html">&lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;\assets\css\APlayer.min.css&quot;&gt;&lt;script src=&quot;\assets\js\APlayer.min.js&quot; cla</summary>
      
    
    
    
    
    <category term="数值分析" scheme="https://www.hfcouc.work/tags/%E6%95%B0%E5%80%BC%E5%88%86%E6%9E%90/"/>
    
  </entry>
  
  <entry>
    <title>统计学习基础</title>
    <link href="https://www.hfcouc.work/2021/07/28/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E7%B2%BE%E8%A6%81%E7%AC%94%E8%AE%B0/"/>
    <id>https://www.hfcouc.work/2021/07/28/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E7%B2%BE%E8%A6%81%E7%AC%94%E8%AE%B0/</id>
    <published>2021-07-28T10:00:54.000Z</published>
    <updated>2021-07-30T10:47:51.104Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="\assets\js\Meting.min.js"></script><p>自己学习统计学习基础的笔记。</p><div class="row">    <embed src="https://hfcouc.work/pdfs/ESLII.pdf" width="100%" height="550" type="application/pdf"></div>]]></content>
    
    
      
      
    <summary type="html">&lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;\assets\css\APlayer.min.css&quot;&gt;&lt;script src=&quot;\assets\js\APlayer.min.js&quot; cla</summary>
      
    
    
    
    
    <category term="机器学习" scheme="https://www.hfcouc.work/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>只争朝夕，不负韶华</title>
    <link href="https://www.hfcouc.work/2021/07/13/index/"/>
    <id>https://www.hfcouc.work/2021/07/13/index/</id>
    <published>2021-07-13T12:13:25.471Z</published>
    <updated>2021-07-30T10:48:08.269Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="\assets\js\Meting.min.js"></script>    <div id="aplayer-iTUtayyD" class="aplayer aplayer-tag-marker meting-tag-marker"         data-id="33378114" data-server="netease" data-type="song" data-mode="circulation" data-autoplay="false" data-mutex="true" data-listmaxheight="340px" data-preload="auto" data-theme="#555"    ></div><img src="https://cdn.jsdelivr.net/gh/HFC666/image@master/20210713/3.178msyh9rg68.png" style="zoom:80%;" /><blockquote><p>现状是没那么容易改变的<br>即便是足够努力<br>也很难在短时间内看出效果<br>所以有时你认为的无法改变<br>也可能只是暂时没看出效果而己<br>而不是不够努力</p></blockquote><img src="https://cdn.jsdelivr.net/gh/HFC666/image@master/20210713/4.36iwffkaky00.png" style="zoom:80%;" /><blockquote><p>不能再浑浑噩噩<br>如果不把眼皮用力抬起看个真切<br>或许就会错过人生中按下快门的良机</p></blockquote><p><strong>即使是一个人，也需要好好吃饭，这是治愈自己的一种方式</strong></p><p>“不被时间和社会所束缚，幸福的填饱肚子，短时间内变得随心所欲，变得自由，不被打扰；毫不费神的吃东西的这种孤高行为，是现代人，都平等的拥有的最高治愈。”</p><div style="position: relative; width: 100%; height: 0; padding-bottom: 75%;"><iframe src="//player.bilibili.com/player.html?aid=73923411&bvid=BV1AE411a7Nw&cid=126453908&page=1" scrolling="no" border="0" frameborder="no" framespacing="0" allowfullscreen="true" style="position: absolute; width: 100%; height: 100%; left: 0; top: 0;"> </iframe></div><div style="position: relative; width: 100%; height: 0; padding-bottom: 75%;"><iframe src="//player.bilibili.com/player.html?aid=52761403&bvid=BV1r4411776v&cid=92328539&page=1" scrolling="no" border="0" frameborder="no" framespacing="0" allowfullscreen="true" style="position: absolute; width: 100%; height: 100%; left: 0; top: 0;"> </iframe></div>]]></content>
    
    
      
      
    <summary type="html">&lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;\assets\css\APlayer.min.css&quot;&gt;&lt;script src=&quot;\assets\js\APlayer.min.js&quot; cla</summary>
      
    
    
    
    
  </entry>
  
</feed>
