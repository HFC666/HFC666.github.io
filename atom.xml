<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>独自赏晴雨</title>
  
  
  <link href="https://www.hfcouc.work/atom.xml" rel="self"/>
  
  <link href="https://www.hfcouc.work/"/>
  <updated>2021-09-12T12:01:17.423Z</updated>
  <id>https://www.hfcouc.work/</id>
  
  <author>
    <name>HFC</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>线性代数应该这样学：数域</title>
    <link href="https://www.hfcouc.work/2021/09/12/%E6%95%B0%E5%9F%9F/"/>
    <id>https://www.hfcouc.work/2021/09/12/%E6%95%B0%E5%9F%9F/</id>
    <published>2021-09-12T11:58:31.000Z</published>
    <updated>2021-09-12T12:01:17.423Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="\assets\js\Meting.min.js"></script><h2 id="向量空间"><a href="#向量空间" class="headerlink" title="向量空间"></a>向量空间</h2><p>线性代数是研究有限维向量空间上的线性映射的学科。在线性代数中，如果研究复数和实数，会出现更好的定理和更多的洞察力。因此我们将从介绍复数和它们的基本概念开始。</p><p>我们将平面和平凡空间(ordinary space)的例子推广到$\R^n$和$\C^n$，然后我们将它们推广到向量空间的概念。</p><p>然后我们的下一个主题是子空间，它对于向量空间来说所扮演的角色等同于子集对于集合所扮演的角色。最后我们看一下子空间的和(等同于子集的并集)和子空间的直和(相当于不相交子集的并集)。</p><h3 id="R-n-and-C-n"><a href="#R-n-and-C-n" class="headerlink" title="$\R^n$ and $\C^n$"></a>$\R^n$ and $\C^n$</h3><h4 id="复数"><a href="#复数" class="headerlink" title="复数"></a>复数</h4><p>定义</p><p>一个复数就是一个有序数对$(a,b)$，其中$a,b\in \R$，不过我们将其写作$a+bi$。所有复数的集合表示为$\C = \{a+bi:a,b\in \R\}$。</p><h4 id="复数运算的性质"><a href="#复数运算的性质" class="headerlink" title="复数运算的性质"></a>复数运算的性质</h4><p>交换律</p><script type="math/tex; mode=display">\alpha + \beta = \beta + \alpha,\alpha\beta = \beta\alpha,\forall \alpha,\beta\in \C</script><p>结合律</p><script type="math/tex; mode=display">(\alpha+\beta)+\lambda = \alpha+(\beta+\lambda),(\alpha\beta)\lambda=\alpha(\beta\lambda),\forall\alpha,\beta\in \C</script><p>单位元</p><script type="math/tex; mode=display">\lambda+0=\lambda,\lambda1=\lambda,\forall \lambda\in \C</script><p>加法逆元</p><p>对于任何$\alpha\in \C$，存在唯一一个$\beta\in \C$，使得$\alpha+\beta=0$。</p><p>乘法逆元</p><p>对于任何$\alpha\in \C$，存在唯一的$\beta\in \C$使得$\alpha\beta=1$。</p><p>分配律</p><script type="math/tex; mode=display">\lambda(\alpha+\beta) = \lambda\alpha+\lambda\beta,\forall \lambda,\alpha,\beta\in \C</script><hr><h4 id="定义：-alpha-，减法，-1-alpha-，除法"><a href="#定义：-alpha-，减法，-1-alpha-，除法" class="headerlink" title="定义： $-\alpha$，减法，$1/\alpha$，除法"></a>定义： $-\alpha$，减法，$1/\alpha$，除法</h4><p>$\alpha,\beta\in \C$</p><ul><li>令$-\alpha$表示$\alpha$的加法逆元。因此$-\alpha$是满足$\alpha+(-\alpha)=0$的唯一复数。</li><li>减法：$\C$上的减法定义为：$\beta-\alpha=\beta+(-\alpha)$</li><li>对于$\alpha\neq 0$，令$1/\alpha$表示$\alpha$的乘法逆元。因此$1/\alpha$为满足$\alpha(1/\alpha)=1$的唯一复数。</li><li>$\C$上的除法定义为：$\beta/\alpha=\beta(1/\alpha)$。</li></ul><hr><p>用$\mathbb{F}$表示$\R$或者$\C$。对于$\alpha\in \mathbb{F}$并且$m$为正数，我们定义$\alpha^m$来表示$\alpha$连乘$m$次：</p><script type="math/tex; mode=display">\alpha^{m}=\underbrace{\alpha \cdots \alpha}_{m \text { times }}</script><p>很显然$(\alpha^m)^n=\alpha^{mn}$并且$(\alpha\beta)^m=\alpha^m\beta^m, \forall \alpha,\beta\in \mathbb{F}$。</p><hr><h4 id="定义：列表-list-，长度-length"><a href="#定义：列表-list-，长度-length" class="headerlink" title="定义：列表(list)，长度(length)"></a>定义：列表(list)，长度(length)</h4><p>假设$n$是一个非负整数。长度为$n$的列表是一个被括号包围用逗号分隔的$n$元有序数对。长度为$n$的列表如下：</p><script type="math/tex; mode=display">（x_1,\cdots,x_n)</script><p>两个列表是相等的当且仅当它们长度相等并且在相同的位置有相同的元素。</p><blockquote><p>长度无限的不能称为列表</p></blockquote><p>长度为零的列表像这样：$()$。我们将其当作列表以免不必要的例外情况。</p><h4 id="定义：-mathbb-F-n"><a href="#定义：-mathbb-F-n" class="headerlink" title="定义：$\mathbb{F}^n$"></a>定义：$\mathbb{F}^n$</h4><p>$\mathbb{F}^n$是所有元素来自$\mathbb{F}$的$n$元有序数对的集合：</p><script type="math/tex; mode=display">\mathbb{F}=\{(x_1,\cdots,x_n):x_j\in \mathbb{F},\forall j=1,\cdots,n\}</script><h4 id="mathbb-F-n-上的加法"><a href="#mathbb-F-n-上的加法" class="headerlink" title="$\mathbb{F}^n$上的加法"></a>$\mathbb{F}^n$上的加法</h4><p>$\mathbb{F}^n$上的加法定义为相对应的元素相加：</p><script type="math/tex; mode=display">(x_1,\cdots,x_n) + (y_1,\cdots,y_n) = (x_1+y_1,\cdots,x_n+y_n)</script><h4 id="mathbb-F-n-上加法的交换律"><a href="#mathbb-F-n-上加法的交换律" class="headerlink" title="$\mathbb{F}^n$上加法的交换律"></a>$\mathbb{F}^n$上加法的交换律</h4><p>如果$x,y\in \mathbb{F}^n$，则$x+y = y+x$</p><p>证明：假设$x=(x_1,\cdots,x_n)$和$y=(y_1,\cdots,y_n)$。</p><script type="math/tex; mode=display">\begin{aligned}x + y &= (x_1,\cdots,x_n)+(y_1,\cdots,y_n)\\&= (x_1+y_1,\cdots,x_n+y_n)\\&= (y_1+x_1,\cdots,y_n+x_n)\\&= (y_1,\cdots,y_n) + (x_1,\cdots,x_n)\\&= y+x\end{aligned}</script><h4 id="定义-0"><a href="#定义-0" class="headerlink" title="定义 $0$"></a>定义 $0$</h4><p>令$0$表示长度为$n$并且元素全部为$0$的列表：</p><script type="math/tex; mode=display">0 = (0,\cdots,0)</script><h4 id="mathbb-F-上的加法逆元"><a href="#mathbb-F-上的加法逆元" class="headerlink" title="$\mathbb{F}$上的加法逆元"></a>$\mathbb{F}$上的加法逆元</h4><p>对于$x\in \mathbb{F}^n$，$x$的加法逆元，表示为$-x$，为向量$-x \in \mathbb{F}^n$使得</p><script type="math/tex; mode=display">x + (-x) = 0</script><p>换句话说，如果$x = (x_1,\cdots,x_n)$，则$-x = (-x_1,\cdots,-x_n)$。</p><h4 id="mathbb-F-上的数乘"><a href="#mathbb-F-上的数乘" class="headerlink" title="$\mathbb{F}$上的数乘"></a>$\mathbb{F}$上的数乘</h4><p>数字$\lambda$和$\mathbb{F}^n$中向量的乘法通过用$\lambda$乘以$\mathbb{F}^n$中的每一个元素来完成。</p><script type="math/tex; mode=display">\lambda(x_1,\cdots,x_n) = (\lambda x_1,\cdots,\lambda x_n)</script><p>在这里$\lambda\in \mathbb{F}$并且$(x_1,\cdots,x_n)\in \mathbb{F}^n$.</p>]]></content>
    
    
      
      
    <summary type="html">&lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;\assets\css\APlayer.min.css&quot;&gt;&lt;script src=&quot;\assets\js\APlayer.min.js&quot; cla</summary>
      
    
    
    
    <category term="数学" scheme="https://www.hfcouc.work/categories/%E6%95%B0%E5%AD%A6/"/>
    
    
    <category term="线性代数" scheme="https://www.hfcouc.work/tags/%E7%BA%BF%E6%80%A7%E4%BB%A3%E6%95%B0/"/>
    
  </entry>
  
  <entry>
    <title>高斯混合模型和EM算法</title>
    <link href="https://www.hfcouc.work/2021/08/31/%E9%AB%98%E6%96%AF%E6%B7%B7%E5%90%88%E6%A8%A1%E5%9E%8B%E5%92%8CEM%E7%AE%97%E6%B3%95/"/>
    <id>https://www.hfcouc.work/2021/08/31/%E9%AB%98%E6%96%AF%E6%B7%B7%E5%90%88%E6%A8%A1%E5%9E%8B%E5%92%8CEM%E7%AE%97%E6%B3%95/</id>
    <published>2021-08-31T15:24:15.000Z</published>
    <updated>2021-09-12T11:57:39.916Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="\assets\js\Meting.min.js"></script><h2 id="Gaussian-mixture-models-and-the-EM-algorithm"><a href="#Gaussian-mixture-models-and-the-EM-algorithm" class="headerlink" title="Gaussian mixture models and the EM algorithm"></a>Gaussian mixture models and the EM algorithm</h2><p>我们使用简写符号$X_1^n$来表示$X_1,X_2,\cdots,X_n$，相似地，$x_1^n$表示$x_1,x_2,\cdots,x_n$。</p><h3 id="The-model"><a href="#The-model" class="headerlink" title="The model"></a>The model</h3><p>假设我们有有编号的人$i=1,\cdots,n$。我们观察表示每个人的身高的随机变量$Y_i\in \mathbb{R}$，同时假设有一个观测不到的标签$C_i\in \{\operatorname{M,F}\}$表示人的性别。在这了，小写字母$c$代表”class”。我们也假设两组具有相同的已知方差$\sigma^2$，但不同的未知均值$\mu_M$和$\mu_F$。类标签符合伯努利分布：</p><script type="math/tex; mode=display">p_{C_i}(c_i) = q^{\mathbb{1}(c_i=M)}(1-q)^{\mathbb{1}(c_i=F)}</script><p>我们也假设$q$是已知的。为了简化符号，我们令$\pi_M=q$和$\pi_F=1-q$，因此我们可以写作：</p><script type="math/tex; mode=display">p_{C_i}(c_i) = \prod_{c\in \{M,F\}}\pi_c^{\mathbb{1}(c_i=c)}</script><p>每一类的条件分布都为高斯分布：</p><script type="math/tex; mode=display">p_{Y_i|C_i}(y_i|c_i) = \prod_c\mathcal{N}(y_i;\mu_c,\sigma^2)^{\mathbb{1}(c_i=c)}</script><h3 id="Parameter-estimation-a-first-attempt"><a href="#Parameter-estimation-a-first-attempt" class="headerlink" title="Parameter estimation: a first attempt"></a>Parameter estimation: a first attempt</h3><p>假设我们观测到独立同分布的身高$Y_1=y_1,\cdots,Y_n=y_n$，并且我们想要去找到参数$\mu_M,\mu_F$的极大似然估计。这是一个非监督问题：我们不知道我们数据的性别标签，但是我们想根据这些标签学习参数。</p><p>根据上文提到的模型设计，计算所有数据点$P_{Y_1,\cdots,Y_n}$的联合密度，以$\mu_M,\mu_F,\sigma,q$表示。取$\log$后计算$\log$似然，然后对$\mu_M$进行求导。为什么优化这么困难？</p><p>我们先对单个数据点$Y_i=y_i$求密度：</p><script type="math/tex; mode=display">\begin{aligned}P_{Y_i}(y_i) &= \sum_{c_i}p_{C_i}(c_i)p_{Y_i|C_i}(y_i|c_i)\\&= \sum_{c_i}(\pi_c\mathcal{N}(y_i;\mu_C,\sigma^2))^{\mathbb{1}(c_i=c)}\\&= q\mathcal{N}(y_i;\mu_M,\sigma^2) + (1-q)\mathcal{N}(y_i;\mu_F,\sigma^2)\end{aligned}</script><p>现在，所有观测的联合分布为</p><script type="math/tex; mode=display">P_{Y_1^n}(y_1^n) = \prod_{i=1}^n(q\mathcal{N}(y_i;\mu_M,\sigma^2) + (1-q)\mathcal{N}(y_i;\mu_F,\sigma^2))</script><p>则$\log$似然函数为：</p><script type="math/tex; mode=display">\ln p_{Y_1^n}(y_1^n) = \sum_{i=1}^n\ln(\pi_M\mathcal{N}(y_i;\mu_M,\sigma^2) + \pi_F\mathcal{N}(y_i;\mu_F,\sigma^2))</script><p>我们已经遇到了一个问题，和的形式阻止我们将$\log$应用于内部的正态分布密度函数。通过对称性，我们只需要看其中一个均值，另一个将遵循相同的过程。在我们深入区分之前，我们注意到：</p><script type="math/tex; mode=display">\begin{aligned}\frac{d}{d\mu}\mathcal{N}(x;\mu,\sigma^2) &= \frac{d}{d\mu}[\frac{1}{\sigma\sqrt{2\pi}}e^{-\frac{(x-\mu)^2}{2\sigma^2}}]\\&= \frac{1}{\sigma\sqrt{2\pi}}e^{-\frac{-(x-\mu)^2}{2\sigma^2}}\cdot\frac{2(x-\mu)}{2\sigma^2}\\&= \mathcal{N}(x;\mu,\sigma^2)\cdot\frac{(x-\mu)}{\sigma^2}\end{aligned}</script><p>对数似然函数对$\mu_M$进行求导，得</p><blockquote id="fn_1"><sup>1</sup>. $\sum_{i=1}^n\frac{1}{\pi_M\mathcal{N}(y_i;\mu_M,\sigma^2)+\pi_F\mathcal{N}(y_i;\mu_F,\sigma^2)}\pi_M\mathcal{N}(y_i;\mu_M,\sigma^2)\frac{y_i-\mu_M}{\sigma^2}=0$<a href="#reffn_1" title="Jump back to footnote [1] in the text."> &#8617;</a></blockquote><script type="math/tex; mode=display">\sum_{i=1}^n\frac{1}{\pi_M\mathcal{N}(y_i;\mu_M,\sigma^2)+\pi_F\mathcal{N}(y_i;\mu_F,\sigma^2)}\pi_M\mathcal{N}(y_i;\mu_M,\sigma^2)\frac{y_i-\mu_M}{\sigma^2}=0</script><p>对于这个表达式我们无法求解。</p><h3 id="Using-hidden-variables-and-the-EM-Algorithm"><a href="#Using-hidden-variables-and-the-EM-Algorithm" class="headerlink" title="Using hidden variables and the EM Algorithm"></a>Using hidden variables and the EM Algorithm</h3><p>退一步，什么会使这个计算容易。如果我们知道隐变量$C_i$的值，则对参数们做最大似然估计就会很容易：我们将会取所有$C_i=M$的点用来估计$\mu_M$，然后对所有$C_i=F$的点重复此过程来估计$\mu_F$。受此启发，我们尝试计算给定观测下$C_i$的分布。我们将从贝叶斯规则开始：</p><script type="math/tex; mode=display">\begin{aligned}p_{C_i|Y_i}(c_i|y_i) &= \frac{p_{Y_i|C_i}(y_i|c_i)p_{C_i}(c_i)}{p_{Y_i}(y_i)}\\&= \frac{\prod_{c\in \{M,F\}}(\pi_c\mathcal{N}(y_i;\mu_c,\sigma^2))^{\mathbb{1}(c=c_i)}}{\pi_M\mathcal{N}(y_i;\mu_M,\sigma^2)+\pi_F\mathcal{N}(y_i;\mu_F,\sigma^2)} = q_{C_i}(c_i)\end{aligned}</script><p>我们看一下$C_i=M$的后验概率：</p><script type="math/tex; mode=display">p_{C_i|Y_i}(M|y_i) = \frac{\pi_M\mathcal{N}(y_i;\mu_M,\sigma^2)}{\pi_M\mathcal{N}(y_i;\mu_M,\sigma^2)+\pi_F\mathcal{N}(y_i;\mu_F,\sigma^2)} = q_{C_i}(M)</script><p>这个看起来很熟悉，这是式<sup><a href="#fn_1" id="reffn_1">1</a></sup>中的一部分，我们可以将式<sup><a href="#fn_1" id="reffn_1">1</a></sup>用$q_{C_i}$重写，并且假定其跟$\mu_M$无关</p><script type="math/tex; mode=display">\sum_{i=1}^nq_{C_i}(M)\frac{y_i-\mu_M}{\sigma^2}=0\\\mu_M = \frac{\sum_{i=1}^nq_{C_i}(M)y_i}{\sum_{i=1}^nq_{C_i}(M)}</script><p>这样就看起来好多了：$\mu_M$是身高的加权平均值，其中每个身高都根据该人是男性的可能性进行加权。</p><p>因此现在我们形成了一个循环，如果我们知道参数我们将会很容易计算出$C_1^n$的后验概率，并且如果我们知道后验概率我们将很容易地估计出参数，这就陷入了死循环。这就暗示了以下策略，我们可以固定一个来求解另一个。这种方法通常被称为<code>EM</code>算法。它的工作原理大致如下：</p><ul><li>首先，我们固定参数(在这种情况下为高斯分布的均值$\mu_M$和$\mu_F$)并且求解隐变量的后验分布(在这种情况下记为$q_{C_i}$)。</li><li>之后，我们固定隐变量的后验分布，利用隐变量的期望值来最优化参数。</li><li>重复两个步骤直到收敛。</li></ul><h3 id="The-EM-Algorithm-a-more-formal-look"><a href="#The-EM-Algorithm-a-more-formal-look" class="headerlink" title="The EM Algorithm: a more formal look"></a>The EM Algorithm: a more formal look</h3><p>正如我们将在几个短步骤中展示的那样，EM算法实际上是最大化对数似然的下限(换句话说，每一步都保证改进我们的答案直到收敛)。</p><p>假设我们观测到了随机变量$Y$。现在假设我们也有一些隐变量$C$并且$Y$依赖于$C$。我们可以说$C$和$Y$的分布中有一些我们不知道的参数$\theta$，并且我们有兴趣找到它们。</p><p>在我们上一个例子中，我们观测到有隐变量(性别)$C=\{C_1,\cdots,C_n\}$的身高变量$Y=\{Y_1,\cdots,Y_n\}$，并且$Y$和$C$是独立同分布的，我们的参数是$\mu_M$和$\mu_F$。</p><p>在我们真正推导算法之前，我们需要一个关键结论：<code>Jensen&#39;s inequation</code>(琴声不等式)。在这个例子中我们需要的琴声不等式的特例：</p><script type="math/tex; mode=display">\log(\mathbb{E}[X])\ge \mathbb{E}[\log(X)]</script><p>下图是关于琴声不等式的几何直观：</p><p><img src="https://raw.githubusercontent.com/HFC666/image/master/img/EM.png" alt=""></p><blockquote><p>琴声不等式的特例说明：对于任何随机变量$X$，$\mathbb{E}[\log X]\le\log\mathbb{E}[X]$。令$X$的PDF为红色的曲线。令$Z=\log X$。中间和右边的图展示了怎么构建$Z$的PDF(蓝色曲线)：因为$\log$，与$X$的PDF相比，它倾向于更小的值。$\log\mathbb{E}(X)$是由中间黑色曲线或$\mathbb{E}(Z)$给出的点。但是，$\mathbb{E}[\log X]$或者$\mathbb{E}[Z]$，将始终较小(或至少永远不会较大)因为对数挤压了分布较大的一端(其中$Z$比较大)并拉伸了较小的一端(其中$Z$较小)。</p><p>关于琴声不等式</p><p>对于一个实函数$\phi(x)$，在区间$I$内它是凸的($\frac{d^2\phi(x)}{d^2x}&gt;0, \forall x\in I$)，那么它满足下面关系：</p><script type="math/tex; mode=display">\phi(\sum_{i=1}^Np_ix_i)\le \sum_{i=1}^Np_i\phi(x_i)</script><p>其中$p_i\ge0,\sum_{i=1}^Np_i=1$，且$x_i\in I,(i=1,\cdots,N)$。</p><p><strong>证明</strong>：令$A=\sum_{i=1}^Np_ix_i$，显然$A\in I$。取</p><script type="math/tex; mode=display">\begin{aligned}S &= \sum_{i=1}^N\phi(x_i) - \phi(A)\\&= \sum_{i=1}^Np_i[\phi(x_i)-\phi(A)]\\&= \sum_{i=1}^N p_i\int_A^{x_i}\phi^{\prime}(x)dx\end{aligned}</script><p>若$A\le x_i$，因为$\phi^{\prime}(x)$在区间$I$上是递增的，所以</p><script type="math/tex; mode=display">\int_A^{x_i}\phi^{\prime}(x)dx\ge \phi^{\prime}(A)(x_i-A)</script><p>若$A_i&gt;x_i$，则</p><script type="math/tex; mode=display">\int_A^{x_i}\phi^{\prime}(x)dx = -\int_{x_i}^A\phi^{\prime}(x)dx\ge-(A-x_i)\phi(A) = (x_i-A)\phi^{\prime}(A)</script><p>所以：</p><script type="math/tex; mode=display">\begin{aligned}S &\ge \sum_{i=1}^Np_i\phi^{\prime}(A)(x_i-A)\\&= \phi^{\prime}(A)[\sum_{i=1}^Np_i(x_i-A)]\\&= \phi^{\prime}(A)(A-A)\\&=0\end{aligned}</script><p>证毕。</p></blockquote><p>在本文中，求期望相当于加权平均，而$\ln(x)$的二阶导小于$0$，故与上文提到的不等式符号相反。</p><h4 id="The-EM-Algorithm"><a href="#The-EM-Algorithm" class="headerlink" title="The EM Algorithm"></a>The EM Algorithm</h4><p>我们想要去最大化似然函数。我们通过边缘化$C$来计算对数似然函数：</p><script type="math/tex; mode=display">\log p_Y(y;\theta) = \log\left(\sum_cp_{Y,C}(y,c)\right)</script><p>我们现在也遭受了困境，我们无法对和取对数。如果我们交换它们的顺序不是更好吗？对，期望是一种特殊的求和，并且琴声不等式可以让我们互换它们如果我们有期望的话。因此，我们将要引进一个隐变量$C$的新的分布$q_C$：</p><script type="math/tex; mode=display">\begin{aligned} \log p_{Y}(y ; \theta) & \\ \left.\text { (Marginalizing over } C \text { and introducing } q_{C}(c) / q_{C}(c)\right) &=\log \left(\sum_{c} q_{C}(c) \frac{p_{Y, C}(y, c ; \theta)}{q_{C}(c)}\right) \\ \text { (Rewriting as an expectation) } &=\log \left(\mathbb{E}_{q_{C}}\left[\frac{p_{Y, C}(y, C ; \theta)}{q_{C}(C)}\right]\right) \\ \text { (Using Jensen's inequality) } & \geq \mathbb{E}_{q_{C}}\left[\log \frac{p_{Y, C}(y, C ; \theta)}{q_{C}(C)}\right](2) \\ \text { Using definition of conditional probability } &=\mathbb{E}_{q_{C}}\left[\log \frac{p_{Y}(y ; \theta) p_{C \mid Y}(C \mid y ; \theta)}{q_{C}(C)}\right](3) \end{aligned}</script><p>现在我们有可以很容易优化的$\log p_Y(y;\theta)$的下界了。因为我们已经引入了$q_C$，我们想要在$\theta$和$q_C$上实施最大化。</p><p>我们将使用(2)和(3)分别进行优化。首先先用(2)寻找最好的参数：</p><script type="math/tex; mode=display">\mathbb{E}_{q_{C}}\left[\log \frac{p_{Y, C}(y, C ; \theta)}{q_{C}(C)}\right] = \mathbb{E}_{q_C}[\log p_{Y, C}(y, C ; \theta)] - \mathbb{E}_{q_C}[\log q_C(C)]</script><p>因为$q_C$不依赖于$\theta$，因此我们可以只优化第一项：</p><script type="math/tex; mode=display">\widehat{\theta} \leftarrow \underset{\theta}{\operatorname{argmax}} \mathbb{E}_{q_{C}}\left[\log p_{Y, C}(y, C ; \theta)\right]</script><p>这被称作<code>M-step</code>：<code>M</code>代表最大化，因为我们正在最大化参数。现在，我们用(3)来找到最好的$q_C$：</p><script type="math/tex; mode=display">\mathbb{E}_{q_{C}}\left[\log \frac{p_{Y}(y ; \theta) p_{C \mid Y}(C \mid y ; \theta)}{q_{C}(C)}\right]=\mathbb{E}_{q_{C}}\left[\log p_{Y}(y ; \theta)\right]+\mathbb{E}_{q_{C}}\left[\log \frac{p_{C \mid Y}(C \mid y ; \theta)}{q_{C}(C)}\right]</script><p>第一项不依赖于$c$，并且第二项看起来像KL散度：</p><script type="math/tex; mode=display">\begin{aligned}&= \log p_Y(y;\theta) - \mathbb{E}_{q_C}[\log\frac{q_C(C)}{p_{C|Y}(C|y;\theta)}]\\&= \log p_Y(y;\theta) - D(q_C(\cdot)||p_{C|Y}(\cdot|y;\theta))\end{aligned}</script><p>因此，当最大化上述值时，我们想要最小化KL散度。KL散度总是大于等于零，当两个分布完全一样时取零。因此，最优化的$q_C$即为$p_{C|Y}(c|y;\theta)$：</p><script type="math/tex; mode=display">\hat{q}_C(c) \leftarrow p_{C|Y}(c|y;\theta)</script><p>这被称为<code>E-step</code>：<code>E</code>代表期望，因为我们正在计算$q_C$以便我们可以将其用于期望值。</p><h4 id="The-algorithm"><a href="#The-algorithm" class="headerlink" title="The algorithm"></a>The algorithm</h4><p><img src="https://raw.githubusercontent.com/HFC666/image/master/img/EM_a.png" alt=""></p><blockquote><p>信息熵可以表达数据的信息量大小，是信息处理的一个非常重要的概念。对于离散型随机变量，信息熵公式如下：</p><script type="math/tex; mode=display">H(p) = H(X) = \mathrm{E}_{x\sim p(x)}[-\log p(x)] = -\sum_{i=1}^n p(x)\log p(x)</script><p>对于连续性随机变量，信息熵公式如下：</p><script type="math/tex; mode=display">H(p) = H(X) = E_{x\sim p(x)}[-\log p(x)] = -\int p(x)\log p(x)dx</script><p>接下来我们来说一下相对熵，又被称为KL散度或信息散度，是两个概率分布差异的非对称度量。在信息论中，相对熵等价于两个概率分布的信息熵的插值，若其中一个概率分布为真实分布，另一个为理论分布，则此时相对熵等于交叉熵与真实分布的信息熵之差，表示使用理论分布拟合真实分布时产生的信息损耗。公式为：</p><script type="math/tex; mode=display">D_{\operatorname{KL}}(p||q) = \sum_{i=1}^N[p(x_i)\log p(x_i) - p(x_i)\log q(x_i)]</script><p>上面的$p(x_i)$为真实事件的概率分布，$q(x_i)$为理论拟合出来的该事件的概率分布。当拟合出来的事件概率分布跟真实的一模一样时，相对熵等于零，而拟合出来不一样时，相对熵大于零。</p><p>最后我们来证明一下相对熵公式只有在$p(x_i)$等于$q(x_i)$的时候等于$0$，其他时候大于$0$。</p><p>要证：</p><script type="math/tex; mode=display">D_{\operatorname{KL}}(p||q) = \sum_{i=1}^N[p(x_i)\log p(x_i)-p(x_i)\log q(x_i)]\ge 0</script><p>即证：</p><script type="math/tex; mode=display">\sum_{i=1}^Np(x_i)\log\frac{q(x_i)}{p(x_i)}\le 0</script><p>又$\ln(x)\le x-1$，当且仅当$x=1$时等号成立</p><p>故</p><script type="math/tex; mode=display">\sum_{i=1}^Np(x_i)\log\frac{q(x_i)}{p(x_i)}\le \sum_{i=1}^Np(x_i)(\frac{q(x_i)}{p(x_i)}-1) = \sum_{i=1}^N[p(x_i)-q(x_i)]=0</script><p>上面式子中$=$只在$p(x_i)=q(x_i)$时成立。</p></blockquote><h3 id="Example-Applying-the-general-algorithm-to-GMMS"><a href="#Example-Applying-the-general-algorithm-to-GMMS" class="headerlink" title="Example: Applying the general algorithm to GMMS"></a>Example: Applying the general algorithm to GMMS</h3><p>现在，让我们重现看一下关于身高的GMM模型，看看我们如何应用这两个步骤。我们有观测变量$Y=\{Y_1,\cdots,Y_n\}$和隐变量$C=\{C_1,\cdots,C_n\}$。对于<code>E-Step</code>，我们计算后验概率$p_{C|Y}(c|y)$，之前已经计算过了。对于<code>M-Step</code>，我们得去计算联合概率：</p><script type="math/tex; mode=display">\begin{aligned}\mathbb{E}_{q_C}[\ln p_{Y,C}(y,C)] &= \mathbb{E}[\ln p_{Y|C}(y|C)p_C(C)]\\&= \mathbb{E}_{q_C}\left[\ln \prod_{i=1}^n\prod_{c\in \{M,F\}}(\pi_c\mathcal{N}(y_i;\mu_c,\sigma^2))^{\mathbb{1}(C_i=c)}\right]\\&= \mathbb{E}_{q_C}\left[\sum_{i=1}^n\sum_{c\in \{M,F\}}\mathbb{1}(C_i=c)(\ln\pi_c+\ln\mathcal{N}(y_i;\mu_c,\sigma^2))\right]\\&= \sum_{i=1}^n\sum_{c\in \{M,F\}}\mathbb{E}_{q_C}[\mathbb{1}(C_i=c)]\left(\ln\pi_c + \ln\frac{1}{\sigma\sqrt{2\pi}}-\frac{(y_i-\mu_c)^2}{2\sigma^2}\right) \end{aligned}</script><p>$\mathbb{E}_{q_C}[\mathbb{1}(C_i=c)]$是$C_i$为$c$的概率。现在我们对$\mu_M$求导：</p><script type="math/tex; mode=display">\frac{d}{d \mu_{M}} \mathbb{E}_{q_{C}}\left[\ln p_{Y \mid C}(y \mid C) p_{C}(C)\right]=\sum_{i=1}^{n} q_{C_{i}}(M)\left(\frac{y_{i}-\mu_{M}}{\sigma^{2}}\right)=0</script><p>得：</p><script type="math/tex; mode=display">\mu_{M}=\frac{\sum_{i=1}^{n} q_{C_{i}}(M) y_{i}}{\sum_{i=1}^{n} q_{C_{i}}(M)}</script><p>同理可得：</p><script type="math/tex; mode=display">\mu_{F}=\frac{\sum_{i=1}^{n} q_{C_{i}}(F) y_{i}}{\sum_{i=1}^{n} q_{C_{i}}(F)}</script>]]></content>
    
    
      
      
    <summary type="html">&lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;\assets\css\APlayer.min.css&quot;&gt;&lt;script src=&quot;\assets\js\APlayer.min.js&quot; cla</summary>
      
    
    
    
    <category term="算法" scheme="https://www.hfcouc.work/categories/%E7%AE%97%E6%B3%95/"/>
    
    
    <category term="机器学习" scheme="https://www.hfcouc.work/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>贝叶斯数据分析</title>
    <link href="https://www.hfcouc.work/2021/08/11/%E8%B4%9D%E5%8F%B6%E6%96%AF%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90/"/>
    <id>https://www.hfcouc.work/2021/08/11/%E8%B4%9D%E5%8F%B6%E6%96%AF%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90/</id>
    <published>2021-08-10T23:43:01.000Z</published>
    <updated>2021-09-12T09:21:27.040Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="\assets\js\Meting.min.js"></script><p>当初为什么要用英文写笔记？？<br><div class="row">    <embed src="https://hfcouc.work/pdfs/Bayesian_Data_Analysis.pdf" width="100%" height="550" type="application/pdf"></div></p>]]></content>
    
    
      
      
    <summary type="html">&lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;\assets\css\APlayer.min.css&quot;&gt;&lt;script src=&quot;\assets\js\APlayer.min.js&quot; cla</summary>
      
    
    
    
    <category term="贝叶斯机器学习" scheme="https://www.hfcouc.work/categories/%E8%B4%9D%E5%8F%B6%E6%96%AF%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
    <category term="贝叶斯数据分析" scheme="https://www.hfcouc.work/tags/%E8%B4%9D%E5%8F%B6%E6%96%AF%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90/"/>
    
  </entry>
  
  <entry>
    <title>微分方程</title>
    <link href="https://www.hfcouc.work/2021/08/09/%E5%BE%AE%E5%88%86%E6%96%B9%E7%A8%8B/"/>
    <id>https://www.hfcouc.work/2021/08/09/%E5%BE%AE%E5%88%86%E6%96%B9%E7%A8%8B/</id>
    <published>2021-08-09T07:14:29.000Z</published>
    <updated>2021-09-12T09:21:37.003Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="\assets\js\Meting.min.js"></script><p>微分方程学习笔记</p><div class="row">    <embed src="https://hfcouc.work/pdfs/Differential_equation.pdf" width="100%" height="550" type="application/pdf"></div>]]></content>
    
    
      
      
    <summary type="html">&lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;\assets\css\APlayer.min.css&quot;&gt;&lt;script src=&quot;\assets\js\APlayer.min.js&quot; cla</summary>
      
    
    
    
    <category term="数学" scheme="https://www.hfcouc.work/categories/%E6%95%B0%E5%AD%A6/"/>
    
    
    <category term="微分方程" scheme="https://www.hfcouc.work/tags/%E5%BE%AE%E5%88%86%E6%96%B9%E7%A8%8B/"/>
    
  </entry>
  
  <entry>
    <title>数值分析笔记</title>
    <link href="https://www.hfcouc.work/2021/07/30/%E6%95%B0%E5%80%BC%E5%88%86%E6%9E%90/"/>
    <id>https://www.hfcouc.work/2021/07/30/%E6%95%B0%E5%80%BC%E5%88%86%E6%9E%90/</id>
    <published>2021-07-30T13:35:47.000Z</published>
    <updated>2021-09-12T09:21:05.125Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="\assets\js\Meting.min.js"></script><p>学习数值分析的笔记</p><div class="row">    <embed src="https://hfcouc.work/pdfs/Numerical_analysis.pdf" width="100%" height="550" type="application/pdf"></div>]]></content>
    
    
      
      
    <summary type="html">&lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;\assets\css\APlayer.min.css&quot;&gt;&lt;script src=&quot;\assets\js\APlayer.min.js&quot; cla</summary>
      
    
    
    
    <category term="数学" scheme="https://www.hfcouc.work/categories/%E6%95%B0%E5%AD%A6/"/>
    
    
    <category term="数值分析" scheme="https://www.hfcouc.work/tags/%E6%95%B0%E5%80%BC%E5%88%86%E6%9E%90/"/>
    
  </entry>
  
  <entry>
    <title>统计学习基础</title>
    <link href="https://www.hfcouc.work/2021/07/28/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E7%B2%BE%E8%A6%81%E7%AC%94%E8%AE%B0/"/>
    <id>https://www.hfcouc.work/2021/07/28/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E7%B2%BE%E8%A6%81%E7%AC%94%E8%AE%B0/</id>
    <published>2021-07-28T10:00:54.000Z</published>
    <updated>2021-09-12T09:20:36.616Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="\assets\js\Meting.min.js"></script><p>自己学习统计学习基础的笔记。</p><div class="row">    <embed src="https://hfcouc.work/pdfs/ESLII.pdf" width="100%" height="550" type="application/pdf"></div>]]></content>
    
    
      
      
    <summary type="html">&lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;\assets\css\APlayer.min.css&quot;&gt;&lt;script src=&quot;\assets\js\APlayer.min.js&quot; cla</summary>
      
    
    
    
    <category term="机器学习" scheme="https://www.hfcouc.work/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
    <category term="机器学习" scheme="https://www.hfcouc.work/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>只争朝夕，不负韶华</title>
    <link href="https://www.hfcouc.work/2021/07/13/index/"/>
    <id>https://www.hfcouc.work/2021/07/13/index/</id>
    <published>2021-07-13T12:13:25.471Z</published>
    <updated>2021-07-30T10:48:08.269Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="\assets\js\Meting.min.js"></script>    <div id="aplayer-npIGFPqm" class="aplayer aplayer-tag-marker meting-tag-marker"         data-id="33378114" data-server="netease" data-type="song" data-mode="circulation" data-autoplay="false" data-mutex="true" data-listmaxheight="340px" data-preload="auto" data-theme="#555"    ></div><p><img src="https://cdn.jsdelivr.net/gh/HFC666/image@master/20210713/3.178msyh9rg68.png" style="zoom:80%;" /></p><blockquote><p>现状是没那么容易改变的<br>即便是足够努力<br>也很难在短时间内看出效果<br>所以有时你认为的无法改变<br>也可能只是暂时没看出效果而己<br>而不是不够努力</p></blockquote><p><img src="https://cdn.jsdelivr.net/gh/HFC666/image@master/20210713/4.36iwffkaky00.png" style="zoom:80%;" /></p><blockquote><p>不能再浑浑噩噩<br>如果不把眼皮用力抬起看个真切<br>或许就会错过人生中按下快门的良机</p></blockquote><p><strong>即使是一个人，也需要好好吃饭，这是治愈自己的一种方式</strong></p><p>“不被时间和社会所束缚，幸福的填饱肚子，短时间内变得随心所欲，变得自由，不被打扰；毫不费神的吃东西的这种孤高行为，是现代人，都平等的拥有的最高治愈。”</p><div style="position: relative; width: 100%; height: 0; padding-bottom: 75%;"><iframe src="//player.bilibili.com/player.html?aid=73923411&bvid=BV1AE411a7Nw&cid=126453908&page=1" scrolling="no" border="0" frameborder="no" framespacing="0" allowfullscreen="true" style="position: absolute; width: 100%; height: 100%; left: 0; top: 0;"> </iframe></div><div style="position: relative; width: 100%; height: 0; padding-bottom: 75%;"><iframe src="//player.bilibili.com/player.html?aid=52761403&bvid=BV1r4411776v&cid=92328539&page=1" scrolling="no" border="0" frameborder="no" framespacing="0" allowfullscreen="true" style="position: absolute; width: 100%; height: 100%; left: 0; top: 0;"> </iframe></div>]]></content>
    
    
      
      
    <summary type="html">&lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;\assets\css\APlayer.min.css&quot;&gt;&lt;script src=&quot;\assets\js\APlayer.min.js&quot; cla</summary>
      
    
    
    
    
  </entry>
  
</feed>
