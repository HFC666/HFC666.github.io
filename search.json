[{"title":"线性代数应该这样学：线性空间","url":"/2021/09/12/线性空间/","content":"\n## 向量空间\n\n线性代数是研究有限维向量空间上的线性映射的学科。在线性代数中，如果研究复数和实数，会出现更好的定理和更多的洞察力。因此我们将从介绍复数和它们的基本概念开始。\n\n我们将平面和平凡空间(ordinary space)的例子推广到$\\mathbb{R}^n$和$\\mathbb{C}^n$，然后我们将它们推广到向量空间的概念。\n\n然后我们的下一个主题是子空间，它对于向量空间来说所扮演的角色等同于子集对于集合所扮演的角色。最后我们看一下子空间的和(等同于子集的并集)和子空间的直和(相当于不相交子集的并集)。\n\n### $\\mathbb{R}^n$ and $\\mathbb{C}^n$\n\n#### 复数\n\n定义\n\n一个复数就是一个有序数对$(a,b)$，其中$a,b\\in \\mathbb{R}$，不过我们将其写作$a+bi$。所有复数的集合表示为$\\mathbb{C} = \\{a+bi:a,b\\in \\mathbb{R}\\}$。\n\n#### 复数运算的性质\n\n交换律\n$$\n\\alpha + \\beta = \\beta + \\alpha,\\alpha\\beta = \\beta\\alpha,\\forall \\alpha,\\beta\\in \\mathbb{C}\n$$\n结合律\n$$\n(\\alpha+\\beta)+\\lambda = \\alpha+(\\beta+\\lambda),(\\alpha\\beta)\\lambda=\\alpha(\\beta\\lambda),\\forall\\alpha,\\beta\\in \\mathbb{C}\n$$\n单位元\n$$\n\\lambda+0=\\lambda,\\lambda1=\\lambda,\\forall \\lambda\\in \\mathbb{C}\n$$\n加法逆元\n\n对于任何$\\alpha\\in \\mathbb{C}$，存在唯一一个$\\beta\\in \\mathbb{C}$，使得$\\alpha+\\beta=0$。\n\n乘法逆元\n\n对于任何$\\alpha\\in \\mathbb{C}$，存在唯一的$\\beta\\in \\mathbb{C}$使得$\\alpha\\beta=1$。\n\n分配律\n$$\n\\lambda(\\alpha+\\beta) = \\lambda\\alpha+\\lambda\\beta,\\forall \\lambda,\\alpha,\\beta\\in \\mathbb{C}\n$$\n\n***\n\n#### 定义： $-\\alpha$，减法，$1/\\alpha$，除法\n\n$\\alpha,\\beta\\in \\mathbb{C}$\n\n+ 令$-\\alpha$表示$\\alpha$的加法逆元。因此$-\\alpha$是满足$\\alpha+(-\\alpha)=0$的唯一复数。\n+ 减法：$\\mathbb{C}$上的减法定义为：$\\beta-\\alpha=\\beta+(-\\alpha)$\n+ 对于$\\alpha\\neq 0$，令$1/\\alpha$表示$\\alpha$的乘法逆元。因此$1/\\alpha$为满足$\\alpha(1/\\alpha)=1$的唯一复数。\n+ $\\mathbb{C}$上的除法定义为：$\\beta/\\alpha=\\beta(1/\\alpha)$。\n\n***\n\n用$\\mathbb{F}$表示$\\mathbb{R}$或者$\\mathbb{C}$。对于$\\alpha\\in \\mathbb{F}$并且$m$为正数，我们定义$\\alpha^m$来表示$\\alpha$连乘$m$次：\n$$\n\\alpha^{m}=\\underbrace{\\alpha \\cdots \\alpha}_{m \\text { times }}\n$$\n很显然$(\\alpha^m)^n=\\alpha^{mn}$并且$(\\alpha\\beta)^m=\\alpha^m\\beta^m, \\forall \\alpha,\\beta\\in \\mathbb{F}$。\n\n***\n\n#### 定义：列表(list)，长度(length)\n\n假设$n$是一个非负整数。长度为$n$的列表是一个被括号包围用逗号分隔的$n$元有序数对。长度为$n$的列表如下：\n$$\n（x_1,\\cdots,x_n)\n$$\n两个列表是相等的当且仅当它们长度相等并且在相同的位置有相同的元素。\n\n> 长度无限的不能称为列表\n\n长度为零的列表像这样：$()$。我们将其当作列表以免不必要的例外情况。\n\n#### 定义：$\\mathbb{F}^n$\n\n$\\mathbb{F}^n$是所有元素来自$\\mathbb{F}$的$n$元有序数对的集合：\n$$\n\\mathbb{F}=\\{(x_1,\\cdots,x_n):x_j\\in \\mathbb{F},\\forall j=1,\\cdots,n\\}\n$$\n\n#### $\\mathbb{F}^n$上的加法\n\n$\\mathbb{F}^n$上的加法定义为相对应的元素相加：\n$$\n(x_1,\\cdots,x_n) + (y_1,\\cdots,y_n) = (x_1+y_1,\\cdots,x_n+y_n)\n$$\n\n#### $\\mathbb{F}^n$上加法的交换律\n\n如果$x,y\\in \\mathbb{F}^n$，则$x+y = y+x$\n\n证明：假设$x=(x_1,\\cdots,x_n)$和$y=(y_1,\\cdots,y_n)$。\n$$\n\\begin{aligned}\nx + y &= (x_1,\\cdots,x_n)+(y_1,\\cdots,y_n)\\\\\n&= (x_1+y_1,\\cdots,x_n+y_n)\\\\\n&= (y_1+x_1,\\cdots,y_n+x_n)\\\\\n&= (y_1,\\cdots,y_n) + (x_1,\\cdots,x_n)\\\\\n&= y+x\n\\end{aligned}\n$$\n\n#### 定义 $0$\n\n令$0$表示长度为$n$并且元素全部为$0$的列表：\n$$\n0 = (0,\\cdots,0)\n$$\n\n#### $\\mathbb{F}$上的加法逆元\n\n对于$x\\in \\mathbb{F}^n$，$x$的加法逆元，表示为$-x$，为向量$-x \\in \\mathbb{F}^n$使得\n$$\nx + (-x) = 0\n$$\n换句话说，如果$x = (x_1,\\cdots,x_n)$，则$-x = (-x_1,\\cdots,-x_n)$。\n\n#### $\\mathbb{F}$上的数乘\n\n数字$\\lambda$和$\\mathbb{F}^n$中向量的乘法通过用$\\lambda$乘以$\\mathbb{F}^n$中的每一个元素来完成。\n$$\n\\lambda(x_1,\\cdots,x_n) = (\\lambda x_1,\\cdots,\\lambda x_n)\n$$\n在这里$\\lambda\\in \\mathbb{F}$并且$(x_1,\\cdots,x_n)\\in \\mathbb{F}^n$.\n\n### 向量空间的定义\n\n定义向量空间的动机来自于$\\mathbb{F}^n$中的加法和标量乘法的性质：加法是可交换的、结合的并且有单位元。每个元素也都有加法逆元。标量乘法具有结合律。加法和数乘通过分配律相联系。\n\n我们把向量空间定义为在$\\mathbb{V}$上具有加法和数乘的集合$\\mathbb{V}$，该集合具有上面段落提到的性质。\n\n#### 加法、数乘\n\n集合$\\mathbb{V}$上的数乘是将元素$u+v\\in \\mathbb{V}$分配给每对元素$u,v\\in \\mathbb{V}$的函数。\n\n集合$\\mathbb{V}$上的数乘是将元素$\\lambda v\\in \\mathbb{V}$分配给每一个$\\lambda \\in \\mathbb{F}$和每一个$v\\in \\mathbb{V}$的函数。\n\n现在我们准备好给向量空间一个正式的定义。\n\n#### 定义：向量空间\n\n向量空间是具有$\\mathbb{V}$上加法和$\\mathbb{V}$上数乘的集合$\\mathbb{V}$，使得其满足以下性质：\n\n+ 交换律：$u+v = v+u,\\forall u,v\\in \\mathbb{V}$\n+ 结合律：$(u+v)+w = u+(v+w)$和$(ab)v = a(bv),\\forall u,v,w\\in \\mathbb{V}\\text{ and }a,b\\in \\mathbb{F}$\n+ 加法单位元：存在一个元素$0\\in \\mathbb{V}$使得$v+0=v,\\forall v\\in \\mathbb{V}$\n+ 加法逆元：对于任意的$v\\in \\mathbb{V}$，存在$w\\in \\mathbb{V}$，使得$v+w=0$\n+ 乘法单位元：$1v=v,\\forall v\\in \\mathbb{V}$\n+ 分配律：$a(u+v)=au+av$和$(a+b)v=av+bv,\\forall a,b\\in \\mathbb{F}\\text{ and }u,v\\in\\mathbb{V}$\n\n#### 向量、点\n\n向量空间的元素被称为向量或点。\n\n向量空间的数乘依赖于$\\mathbb{F}$。因当我们想要精确时，我们将会说$\\mathbb{V}$是$\\mathbb{F}$上的向量空间而不是说$\\mathbb{V}$是向量空间。\n\n#### 实向量空间、虚向量空间\n\n$\\mathbb{R}$上的向量空间被称为实向量空间。\n\n$\\mathbb{C}$上的向量空间被称为复向量空间。\n\n#### $\\mathbb{F}^S$\n\n如果$S$为一个集合，则$\\mathbb{F}^S$表示从$S$到$\\mathbb{F}$的函数的集合。\n\n对于$f,g\\in \\mathbb{F}^S$，和$f+g\\in \\mathbb{F}^S$是定义为\n$$\n(f+g)(x) = f(x) + g(x)\n$$\n的函数，对于任何$x\\in S$。\n\n对于$\\lambda \\in \\mathbb{F}$和$f\\in \\mathbb{F}^S$，乘积$\\lambda f\\in \\mathbb{F}^S$是定义为\n$$\n(\\lambda f)(x) = \\lambda f(x)\n$$\n对于所有$x\\in S$。\n\n作为上面定义的一个例子，如果$S$是区间$[0,1]$并且$\\mathbb{F} = \\mathbb{R}$，则$\\mathbb{R}^{[0,1]}$是区间$[0,1]$上的实值函数。\n\n\n\n$\\mathbb{F}^S$是向量空间：\n\n+ 如果$S$是一个非空集合，则$\\mathbb{F}^S$(有上面定义的加法和数乘运算)是$\\mathbb{F}$上的向量空间。\n+ $\\mathbb{F}^S$的加法单位元为函数：$0:S\\rightarrow \\mathbb{F}$定义为：$0(x)=0,\\forall x\\in S$\n+ 对于$f \\in \\mathbb{F}^S$，$f$的加法逆元为函数$-f$：$S\\rightarrow \\mathbb{F}$定义为：$(-f)(x)=-f(x),\\forall x \\in S$\n\n> 我们之前定义的$\\mathbb{F}^n$也可以看作是向量空间$\\mathbb{F}^S$的一个特例，我们可以认为$\\mathbb{F}^n$是从$\\{1,2,\\cdots,n\\}$到$\\mathbb{F}$的函数，即将$\\mathbb{F}^n$看作$\\mathbb{F}^{\\{1,2,\\cdots,n\\}}$。我的理解是例如$n=2$，则$\\mathbb{F}^2$可以看作从$\\{1,2\\}$到$\\mathbb{F}$的函数，当取$1$时从$\\mathbb{F}$中取一个数，等于$2$时再从$\\mathbb{F}$中取一个数。\n\n#### 加法单位元的唯一性\n\n一个向量空间有唯一的加法逆元。\n\n证明：假设$0$和$0^{\\prime}$都是一些向量空间$\\mathbb{V}$的加法单位元。则：\n$$\n0^{\\prime} = 0^{\\prime} + 0 = 0+ 0^{\\prime} = 0\n$$\n因此$0^{\\prime} = 0$，证明$\\mathbb{V}$只有一个加法单位元。\n\n#### 加法逆元的唯一性\n\n在向量空间的每一个元素都有唯一的加法逆元。\n\n证明：假设$\\mathbb{V}$是一个向量空间。令$v\\in \\mathbb{V}$。假设$w$和$w^{\\prime}$都是$v$的逆。则：\n$$\nw = w +0 = w+(v+w^{\\prime}) = (w+v)+w^{\\prime} = 0+w^{\\prime}=w^{\\prime}\n$$\n因此$w = w^{\\prime}$，即只有一个加法逆元。\n\n#### $-v,w-v$\n\n令$v,w\\in \\mathbb{V}$。则\n\n+ $-v$表示$v$的加法逆元\n+ $w-v$定义为$w+(-v)$\n\n#### 数字$0$乘以一个向量\n\n$0v=0,\\forall v\\in \\mathbb{V}$\n\n证明：\n$$\n0v = (0+0)v = 0v + 0v\n$$\n两边同时加上$0v$的逆元，得到$0=0v$。\n\n#### 数字乘以向量$0$\n\n$a0=0$对于任何$a\\in \\mathbb{F}$。\n\n证明：$a\\in \\mathbb{F}$，我们有\n$$\na0 = a(0+0)=a0+a0\n$$\n两边同时加上$a0$的加法逆元，即可得到$0=a0$。\n\n#### 数字$-1$乘以一个向量\n\n$(-1)v=-v$对于任意$v\\in \\mathbb{V}$。\n\n证明：对于$v\\in \\mathbb{V}$，我们有：\n$$\nv + (-1)v = 1v+(-1)v = (1+(-1))v = 0v = 0\n$$\n因此$(-1)v$是$v$的加法逆元。\n\n### 子空间\n\n$V$的子集$U$被称为是子空间如果$U$也是向量空间。\n\n> 一些数学家使用属于线性子空间来描述子空间。\n\n#### 子空间条件\n\n子集$U$是$V$的子空间当且仅当$U$满足下列三个条件：\n\n+ 加法单位元：$0\\in U$\n+ 加法封闭：$u,w\\in U\\rightarrow u+w\\in U$\n+ 数乘封闭：$a\\in \\mathbb{F},u\\in U\\rightarrow au\\in U$\n\n#### 例子\n\n如果$b\\in \\mathbb{F}$，则：\n$$\n\\{(x_1,x_2,x_3,x_4)\\}\\in \\mathbb{F}^4:x_3=5x_4+b\n$$\n为$\\mathbb{F}^4$的子空间，当且仅当$b=0$。\n\n***\n\n区间$[0,1]$上的连续实值函数的几何是$\\mathbb{R}^{[0,1]}$的子空间。\n\n***\n\n$\\mathbb{R}$上的可导实值函数的集合是$\\mathbb{R}^{\\mathbb{R}}$的子空间。\n\n***\n\n使$f^{\\prime}(2)=b$成立的在区间$(0,3)$上的实值可导函数$f$的集合是$\\mathbb{R}^{(0,3)}$的子空间，当且仅当$b=0$。\n\n***\n\n所有极限为0的复数序列的集合是$\\mathbb{C}^{\\infty}$的子空间。\n\n***\n\n> 很容易发现$\\{0\\}$是$\\mathbb{V}$的最小的子空间，$\\mathbb{V}$是$\\mathbb{V}$最大的子空间。\n\n#### 子集的和\n\n假设$U_1,\\cdots,U_m$是$V$的子集。$U_1,\\cdots,U_m$的和，表示为$U_1+\\cdots+U_m$，是$U_1,\\cdots,U_m$的所有可能的元素和的集合。更具体地：\n$$\nU_1+\\cdots+U_m = \\{u_1+\\cdots+u_m:u_1\\in U_1,\\cdots,u_m\\in U_m\\}\n$$\n\n#### Sum of subspaces is the smallest containing subspace\n\n假设$U_1,\\cdots,U_m$是$V$的子空间。则$U_1+\\cdots+U_m$是$V$包含$U_1,\\cdots,U_m$的最小子空间。\n\n证明：\n\n很容易发现$0\\in U_1+\\cdots+U_m$并且$U_1+\\cdots+U_m$对加法和数乘法封闭。因此它为子空间。\n\n显然，$U_1,\\cdots,U_m$都包含在$U_1+\\cdots+U_m$中。相反地，$V$的每个包含$U_1,\\cdots,U_m$的子空间都包含$U_1+\\cdots+U_m$，因此$U_1+\\cdots+U_m$是包含$U_1,\\cdots,U_m$的$V$的最小的子空间。\n\n#### 直和\n\n假设$U_1,\\cdots,U_m$是$V$的子空间。$U_1+\\cdots+U_m$的每一个元素都可以被写为以下形式：\n$$\nu_1+\\cdots+u_m\n$$\n其中$u_j$在$U_j$里。我们可能对$U_1+\\cdots+U_m$中每一个元素都可以用唯一一种上述形式表示的例子感兴趣。这种情况非常重要以至于我们给它一个特殊的名字：直和。\n\n***\n\n定义：直和\n\n假设$U_1,\\cdots,U_m$是$V$的子空间\n\n+ 和$U_1+\\cdots+U_m$被称为直和如果$U_1+\\cdots+U_m$的每一个元素可以唯一地表示为$u_1+\\cdots+u_m$，其中每一个$u_j$在$U_j$内。\n+ 如果$U_1+\\cdots+U_m$是直和，则$U_1\\oplus \\cdots \\oplus U_m$表示$U_1+\\cdots+U_m$，用符号$\\oplus$来表示这是直和。\n\n#### 直和的条件\n\n假设$U_1,\\cdots,U_m$是$V$的子空间。则$U_1+\\cdots+U_m$是直和当且仅当将$0$写为一个和$u_1+\\cdots+u_m$的唯一形式是让每一个$u_j$等于$0$，其中$u_j$位于$U_j$中。\n\n证明：\n\n首先假设$U_1+\\cdots+U_m$是一个直和。则直和的定义意味着将$0$写为$u_1+\\cdots+u_m$的和的唯一形式是通过使每个$u_j$等于零。\n\n现在假设将零写为$u_1+\\cdots+u_m$的唯一形式是令每一个$u_j$等于零。为了证明$U_1+\\cdots+U_m$是一个直和，令$v\\in U_1+\\cdots+U_m$。我们可以写为\n$$\nv = u_1+\\cdots+u_m\n$$\n为了证明这种表示是唯一的，假设我们有\n$$\nv = v_1+\\cdots+v_m\n$$\n将两个方程相减，我们有\n$$\n0 = (u_1-v_1)+\\cdots+(u_m-v_m)\n$$\n因此$u_1=v_1,\\cdots,u_m=v_m$。\n\n证毕。\n\n#### 两个子空间的直和\n\n假设$U$和$W$是$V$的子空间。则$U+W$是直和当且仅当$U\\cap W = \\{0\\}$。\n\n证明\n\n首先假设$U+W$是直和。如果$v\\in U\\cap W$，则$0 = v+(-v)$，其中$v\\in U, -v\\in W$。\n\n> 注：这一步是因为$v\\in U\\cap W$，则$v\\in U\\text{且} v\\in W$，所以$-v \\in W$。\n\n通过$0$的唯一的作为$U$中向量和$V$中向量的和表示方式，我们有$v=0$。因此$U\\cap W = \\{0\\}$，这完成了一个方向的证明。\n\n为了证明另一个方向，现在假设$U\\cap W = \\{0\\}$。为了证明$U+W$是直和，假设$u\\in U, w\\in W$，并且：\n$$\n0 = u+w\n$$\n为了完成证明，我们只需要证明$u=w=0$。上面的方程意味着$u = -w\\in W$。因此$u \\in U\\cap W$。所以$u=0$，这也意味着$w=0$，得证。\n\n证毕。\n\n","tags":["线性代数"],"categories":["数学"]},{"title":"高斯混合模型和EM算法","url":"/2021/08/31/高斯混合模型和EM算法/","content":"## Gaussian mixture models and the EM algorithm\n\n我们使用简写符号$X_1^n$来表示$X_1,X_2,\\cdots,X_n$，相似地，$x_1^n$表示$x_1,x_2,\\cdots,x_n$。\n\n### The model\n\n假设我们有有编号的人$i=1,\\cdots,n$。我们观察表示每个人的身高的随机变量$Y_i\\in \\mathbb{R}$，同时假设有一个观测不到的标签$C_i\\in \\{\\operatorname{M,F}\\}$表示人的性别。在这了，小写字母$c$代表\"class\"。我们也假设两组具有相同的已知方差$\\sigma^2$，但不同的未知均值$\\mu_M$和$\\mu_F$。类标签符合伯努利分布：\n$$\np_{C_i}(c_i) = q^{\\mathbb{1}(c_i=M)}(1-q)^{\\mathbb{1}(c_i=F)}\n$$\n我们也假设$q$是已知的。为了简化符号，我们令$\\pi_M=q$和$\\pi_F=1-q$，因此我们可以写作：\n$$\np_{C_i}(c_i) = \\prod_{c\\in \\{M,F\\}}\\pi_c^{\\mathbb{1}(c_i=c)}\n$$\n每一类的条件分布都为高斯分布：\n$$\np_{Y_i|C_i}(y_i|c_i) = \\prod_c\\mathcal{N}(y_i;\\mu_c,\\sigma^2)^{\\mathbb{1}(c_i=c)}\n$$\n\n### Parameter estimation: a first attempt\n\n假设我们观测到独立同分布的身高$Y_1=y_1,\\cdots,Y_n=y_n$，并且我们想要去找到参数$\\mu_M,\\mu_F$的极大似然估计。这是一个非监督问题：我们不知道我们数据的性别标签，但是我们想根据这些标签学习参数。\n\n根据上文提到的模型设计，计算所有数据点$P_{Y_1,\\cdots,Y_n}$的联合密度，以$\\mu_M,\\mu_F,\\sigma,q$表示。取$\\log$后计算$\\log$似然，然后对$\\mu_M$进行求导。为什么优化这么困难？\n\n我们先对单个数据点$Y_i=y_i$求密度：\n$$\n\\begin{aligned}\nP_{Y_i}(y_i) &= \\sum_{c_i}p_{C_i}(c_i)p_{Y_i|C_i}(y_i|c_i)\\\\\n&= \\sum_{c_i}(\\pi_c\\mathcal{N}(y_i;\\mu_C,\\sigma^2))^{\\mathbb{1}(c_i=c)}\\\\\n&= q\\mathcal{N}(y_i;\\mu_M,\\sigma^2) + (1-q)\\mathcal{N}(y_i;\\mu_F,\\sigma^2)\n\\end{aligned}\n$$\n现在，所有观测的联合分布为\n$$\nP_{Y_1^n}(y_1^n) = \\prod_{i=1}^n(q\\mathcal{N}(y_i;\\mu_M,\\sigma^2) + (1-q)\\mathcal{N}(y_i;\\mu_F,\\sigma^2))\n$$\n则$\\log$似然函数为：\n$$\n\\ln p_{Y_1^n}(y_1^n) = \\sum_{i=1}^n\\ln(\\pi_M\\mathcal{N}(y_i;\\mu_M,\\sigma^2) + \\pi_F\\mathcal{N}(y_i;\\mu_F,\\sigma^2))\n$$\n我们已经遇到了一个问题，和的形式阻止我们将$\\log$应用于内部的正态分布密度函数。通过对称性，我们只需要看其中一个均值，另一个将遵循相同的过程。在我们深入区分之前，我们注意到：\n$$\n\\begin{aligned}\n\\frac{d}{d\\mu}\\mathcal{N}(x;\\mu,\\sigma^2) &= \\frac{d}{d\\mu}[\\frac{1}{\\sigma\\sqrt{2\\pi}}e^{-\\frac{(x-\\mu)^2}{2\\sigma^2}}]\\\\\n&= \\frac{1}{\\sigma\\sqrt{2\\pi}}e^{-\\frac{-(x-\\mu)^2}{2\\sigma^2}}\\cdot\\frac{2(x-\\mu)}{2\\sigma^2}\\\\\n&= \\mathcal{N}(x;\\mu,\\sigma^2)\\cdot\\frac{(x-\\mu)}{\\sigma^2}\n\\end{aligned}\n$$\n对数似然函数对$\\mu_M$进行求导，得\n\n[^1]: $\\sum_{i=1}^n\\frac{1}{\\pi_M\\mathcal{N}(y_i;\\mu_M,\\sigma^2)+\\pi_F\\mathcal{N}(y_i;\\mu_F,\\sigma^2)}\\pi_M\\mathcal{N}(y_i;\\mu_M,\\sigma^2)\\frac{y_i-\\mu_M}{\\sigma^2}=0$\n\n$$\n\\sum_{i=1}^n\\frac{1}{\\pi_M\\mathcal{N}(y_i;\\mu_M,\\sigma^2)+\\pi_F\\mathcal{N}(y_i;\\mu_F,\\sigma^2)}\\pi_M\\mathcal{N}(y_i;\\mu_M,\\sigma^2)\\frac{y_i-\\mu_M}{\\sigma^2}=0\n$$\n\n对于这个表达式我们无法求解。\n\n### Using hidden variables and the EM Algorithm\n\n退一步，什么会使这个计算容易。如果我们知道隐变量$C_i$的值，则对参数们做最大似然估计就会很容易：我们将会取所有$C_i=M$的点用来估计$\\mu_M$，然后对所有$C_i=F$的点重复此过程来估计$\\mu_F$。受此启发，我们尝试计算给定观测下$C_i$的分布。我们将从贝叶斯规则开始：\n$$\n\\begin{aligned}\np_{C_i|Y_i}(c_i|y_i) &= \\frac{p_{Y_i|C_i}(y_i|c_i)p_{C_i}(c_i)}{p_{Y_i}(y_i)}\\\\\n&= \\frac{\\prod_{c\\in \\{M,F\\}}(\\pi_c\\mathcal{N}(y_i;\\mu_c,\\sigma^2))^{\\mathbb{1}(c=c_i)}}{\\pi_M\\mathcal{N}(y_i;\\mu_M,\\sigma^2)+\\pi_F\\mathcal{N}(y_i;\\mu_F,\\sigma^2)} = q_{C_i}(c_i)\n\\end{aligned}\n$$\n我们看一下$C_i=M$的后验概率：\n$$\np_{C_i|Y_i}(M|y_i) = \\frac{\\pi_M\\mathcal{N}(y_i;\\mu_M,\\sigma^2)}{\\pi_M\\mathcal{N}(y_i;\\mu_M,\\sigma^2)+\\pi_F\\mathcal{N}(y_i;\\mu_F,\\sigma^2)} = q_{C_i}(M)\n$$\n这个看起来很熟悉，这是式[^1]中的一部分，我们可以将式[^1]用$q_{C_i}$重写，并且假定其跟$\\mu_M$无关\n$$\n\\sum_{i=1}^nq_{C_i}(M)\\frac{y_i-\\mu_M}{\\sigma^2}=0\\\\\n\\mu_M = \\frac{\\sum_{i=1}^nq_{C_i}(M)y_i}{\\sum_{i=1}^nq_{C_i}(M)}\n$$\n这样就看起来好多了：$\\mu_M$是身高的加权平均值，其中每个身高都根据该人是男性的可能性进行加权。\n\n因此现在我们形成了一个循环，如果我们知道参数我们将会很容易计算出$C_1^n$的后验概率，并且如果我们知道后验概率我们将很容易地估计出参数，这就陷入了死循环。这就暗示了以下策略，我们可以固定一个来求解另一个。这种方法通常被称为`EM`算法。它的工作原理大致如下：\n\n+ 首先，我们固定参数(在这种情况下为高斯分布的均值$\\mu_M$和$\\mu_F$)并且求解隐变量的后验分布(在这种情况下记为$q_{C_i}$)。\n+ 之后，我们固定隐变量的后验分布，利用隐变量的期望值来最优化参数。\n+ 重复两个步骤直到收敛。\n\n### The EM Algorithm: a more formal look\n\n正如我们将在几个短步骤中展示的那样，EM算法实际上是最大化对数似然的下限(换句话说，每一步都保证改进我们的答案直到收敛)。\n\n假设我们观测到了随机变量$Y$。现在假设我们也有一些隐变量$C$并且$Y$依赖于$C$。我们可以说$C$和$Y$的分布中有一些我们不知道的参数$\\theta$，并且我们有兴趣找到它们。\n\n在我们上一个例子中，我们观测到有隐变量(性别)$C=\\{C_1,\\cdots,C_n\\}$的身高变量$Y=\\{Y_1,\\cdots,Y_n\\}$，并且$Y$和$C$是独立同分布的，我们的参数是$\\mu_M$和$\\mu_F$。\n\n在我们真正推导算法之前，我们需要一个关键结论：`Jensen's inequation`(琴声不等式)。在这个例子中我们需要的琴声不等式的特例：\n$$\n\\log(\\mathbb{E}[X])\\ge \\mathbb{E}[\\log(X)]\n$$\n下图是关于琴声不等式的几何直观：\n\n![](https://raw.githubusercontent.com/HFC666/image/master/img/EM.png)\n\n> 琴声不等式的特例说明：对于任何随机变量$X$，$\\mathbb{E}[\\log X]\\le\\log\\mathbb{E}[X]$。令$X$的PDF为红色的曲线。令$Z=\\log X$。中间和右边的图展示了怎么构建$Z$的PDF(蓝色曲线)：因为$\\log$，与$X$的PDF相比，它倾向于更小的值。$\\log\\mathbb{E}(X)$是由中间黑色曲线或$\\mathbb{E}(Z)$给出的点。但是，$\\mathbb{E}[\\log X]$或者$\\mathbb{E}[Z]$，将始终较小(或至少永远不会较大)因为对数挤压了分布较大的一端(其中$Z$比较大)并拉伸了较小的一端(其中$Z$较小)。\n\n> 关于琴声不等式\n>\n> 对于一个实函数$\\phi(x)$，在区间$I$内它是凸的($\\frac{d^2\\phi(x)}{d^2x}>0, \\forall x\\in I$)，那么它满足下面关系：\n> $$\n> \\phi(\\sum_{i=1}^Np_ix_i)\\le \\sum_{i=1}^Np_i\\phi(x_i)\n> $$\n> 其中$p_i\\ge0,\\sum_{i=1}^Np_i=1$，且$x_i\\in I,(i=1,\\cdots,N)$。\n>\n> **证明**：令$A=\\sum_{i=1}^Np_ix_i$，显然$A\\in I$。取\n> $$\n> \\begin{aligned}\n> S &= \\sum_{i=1}^N\\phi(x_i) - \\phi(A)\\\\\n> &= \\sum_{i=1}^Np_i[\\phi(x_i)-\\phi(A)]\\\\\n> &= \\sum_{i=1}^N p_i\\int_A^{x_i}\\phi^{\\prime}(x)dx\n> \\end{aligned}\n> $$\n> 若$A\\le x_i$，因为$\\phi^{\\prime}(x)$在区间$I$上是递增的，所以\n> $$\n> \\int_A^{x_i}\\phi^{\\prime}(x)dx\\ge \\phi^{\\prime}(A)(x_i-A)\n> $$\n> 若$A_i>x_i$，则\n> $$\n> \\int_A^{x_i}\\phi^{\\prime}(x)dx = -\\int_{x_i}^A\\phi^{\\prime}(x)dx\\ge-(A-x_i)\\phi(A) = (x_i-A)\\phi^{\\prime}(A)\n> $$\n> 所以：\n> $$\n> \\begin{aligned}\n> S &\\ge \\sum_{i=1}^Np_i\\phi^{\\prime}(A)(x_i-A)\\\\\n> &= \\phi^{\\prime}(A)[\\sum_{i=1}^Np_i(x_i-A)]\\\\\n> &= \\phi^{\\prime}(A)(A-A)\\\\\n> &=0\n> \\end{aligned}\n> $$\n> 证毕。\n\n在本文中，求期望相当于加权平均，而$\\ln(x)$的二阶导小于$0$，故与上文提到的不等式符号相反。\n\n#### The EM Algorithm\n\n我们想要去最大化似然函数。我们通过边缘化$C$来计算对数似然函数：\n$$\n\\log p_Y(y;\\theta) = \\log\\left(\\sum_cp_{Y,C}(y,c)\\right)\n$$\n我们现在也遭受了困境，我们无法对和取对数。如果我们交换它们的顺序不是更好吗？对，期望是一种特殊的求和，并且琴声不等式可以让我们互换它们如果我们有期望的话。因此，我们将要引进一个隐变量$C$的新的分布$q_C$：\n$$\n\\begin{aligned} \\log p_{Y}(y ; \\theta) & \\\\ \\left.\\text { (Marginalizing over } C \\text { and introducing } q_{C}(c) / q_{C}(c)\\right) &=\\log \\left(\\sum_{c} q_{C}(c) \\frac{p_{Y, C}(y, c ; \\theta)}{q_{C}(c)}\\right) \\\\ \\text { (Rewriting as an expectation) } &=\\log \\left(\\mathbb{E}_{q_{C}}\\left[\\frac{p_{Y, C}(y, C ; \\theta)}{q_{C}(C)}\\right]\\right) \\\\ \\text { (Using Jensen's inequality) } & \\geq \\mathbb{E}_{q_{C}}\\left[\\log \\frac{p_{Y, C}(y, C ; \\theta)}{q_{C}(C)}\\right](2) \\\\ \\text { Using definition of conditional probability } &=\\mathbb{E}_{q_{C}}\\left[\\log \\frac{p_{Y}(y ; \\theta) p_{C \\mid Y}(C \\mid y ; \\theta)}{q_{C}(C)}\\right](3) \\end{aligned}\n$$\n\n\n现在我们有可以很容易优化的$\\log p_Y(y;\\theta)$的下界了。因为我们已经引入了$q_C$，我们想要在$\\theta$和$q_C$上实施最大化。\n\n我们将使用(2)和(3)分别进行优化。首先先用(2)寻找最好的参数：\n$$\n\\mathbb{E}_{q_{C}}\\left[\\log \\frac{p_{Y, C}(y, C ; \\theta)}{q_{C}(C)}\\right] = \\mathbb{E}_{q_C}[\\log p_{Y, C}(y, C ; \\theta)] - \\mathbb{E}_{q_C}[\\log q_C(C)]\n$$\n因为$q_C$不依赖于$\\theta$，因此我们可以只优化第一项：\n$$\n\\widehat{\\theta} \\leftarrow \\underset{\\theta}{\\operatorname{argmax}} \\mathbb{E}_{q_{C}}\\left[\\log p_{Y, C}(y, C ; \\theta)\\right]\n$$\n这被称作`M-step`：`M`代表最大化，因为我们正在最大化参数。现在，我们用(3)来找到最好的$q_C$：\n$$\n\\mathbb{E}_{q_{C}}\\left[\\log \\frac{p_{Y}(y ; \\theta) p_{C \\mid Y}(C \\mid y ; \\theta)}{q_{C}(C)}\\right]=\\mathbb{E}_{q_{C}}\\left[\\log p_{Y}(y ; \\theta)\\right]+\\mathbb{E}_{q_{C}}\\left[\\log \\frac{p_{C \\mid Y}(C \\mid y ; \\theta)}{q_{C}(C)}\\right]\n$$\n第一项不依赖于$c$，并且第二项看起来像KL散度：\n$$\n\\begin{aligned}\n&= \\log p_Y(y;\\theta) - \\mathbb{E}_{q_C}[\\log\\frac{q_C(C)}{p_{C|Y}(C|y;\\theta)}]\\\\\n&= \\log p_Y(y;\\theta) - D(q_C(\\cdot)||p_{C|Y}(\\cdot|y;\\theta))\n\\end{aligned}\n$$\n因此，当最大化上述值时，我们想要最小化KL散度。KL散度总是大于等于零，当两个分布完全一样时取零。因此，最优化的$q_C$即为$p_{C|Y}(c|y;\\theta)$：\n$$\n\\hat{q}_C(c) \\leftarrow p_{C|Y}(c|y;\\theta)\n$$\n这被称为`E-step`：`E`代表期望，因为我们正在计算$q_C$以便我们可以将其用于期望值。\n\n#### The algorithm\n\n![](https://raw.githubusercontent.com/HFC666/image/master/img/EM_a.png)\n\n> 信息熵可以表达数据的信息量大小，是信息处理的一个非常重要的概念。对于离散型随机变量，信息熵公式如下：\n> $$\n> H(p) = H(X) = \\mathrm{E}_{x\\sim p(x)}[-\\log p(x)] = -\\sum_{i=1}^n p(x)\\log p(x)\n> $$\n> 对于连续性随机变量，信息熵公式如下：\n> $$\n> H(p) = H(X) = E_{x\\sim p(x)}[-\\log p(x)] = -\\int p(x)\\log p(x)dx\n> $$\n> 接下来我们来说一下相对熵，又被称为KL散度或信息散度，是两个概率分布差异的非对称度量。在信息论中，相对熵等价于两个概率分布的信息熵的插值，若其中一个概率分布为真实分布，另一个为理论分布，则此时相对熵等于交叉熵与真实分布的信息熵之差，表示使用理论分布拟合真实分布时产生的信息损耗。公式为：\n> $$\n> D_{\\operatorname{KL}}(p||q) = \\sum_{i=1}^N[p(x_i)\\log p(x_i) - p(x_i)\\log q(x_i)]\n> $$\n> 上面的$p(x_i)$为真实事件的概率分布，$q(x_i)$为理论拟合出来的该事件的概率分布。当拟合出来的事件概率分布跟真实的一模一样时，相对熵等于零，而拟合出来不一样时，相对熵大于零。\n>\n> 最后我们来证明一下相对熵公式只有在$p(x_i)$等于$q(x_i)$的时候等于$0$，其他时候大于$0$。\n>\n> 要证：\n> $$\n> D_{\\operatorname{KL}}(p||q) = \\sum_{i=1}^N[p(x_i)\\log p(x_i)-p(x_i)\\log q(x_i)]\\ge 0\n> $$\n> 即证：\n> $$\n> \\sum_{i=1}^Np(x_i)\\log\\frac{q(x_i)}{p(x_i)}\\le 0\n> $$\n> 又$\\ln(x)\\le x-1$，当且仅当$x=1$时等号成立\n>\n> 故\n> $$\n> \\sum_{i=1}^Np(x_i)\\log\\frac{q(x_i)}{p(x_i)}\\le \\sum_{i=1}^Np(x_i)(\\frac{q(x_i)}{p(x_i)}-1) = \\sum_{i=1}^N[p(x_i)-q(x_i)]=0\n> $$\n> 上面式子中$=$只在$p(x_i)=q(x_i)$时成立。\n\n### Example: Applying the general algorithm to GMMS\n\n现在，让我们重现看一下关于身高的GMM模型，看看我们如何应用这两个步骤。我们有观测变量$Y=\\{Y_1,\\cdots,Y_n\\}$和隐变量$C=\\{C_1,\\cdots,C_n\\}$。对于`E-Step`，我们计算后验概率$p_{C|Y}(c|y)$，之前已经计算过了。对于`M-Step`，我们得去计算联合概率：\n$$\n\\begin{aligned}\n\\mathbb{E}_{q_C}[\\ln p_{Y,C}(y,C)] &= \\mathbb{E}[\\ln p_{Y|C}(y|C)p_C(C)]\\\\\n&= \\mathbb{E}_{q_C}\\left[\\ln \\prod_{i=1}^n\\prod_{c\\in \\{M,F\\}}(\\pi_c\\mathcal{N}(y_i;\\mu_c,\\sigma^2))^{\\mathbb{1}(C_i=c)}\\right]\\\\\n&= \\mathbb{E}_{q_C}\\left[\\sum_{i=1}^n\\sum_{c\\in \\{M,F\\}}\\mathbb{1}(C_i=c)(\\ln\\pi_c+\\ln\\mathcal{N}(y_i;\\mu_c,\\sigma^2))\\right]\\\\\n&= \\sum_{i=1}^n\\sum_{c\\in \\{M,F\\}}\\mathbb{E}_{q_C}[\\mathbb{1}(C_i=c)]\\left(\\ln\\pi_c + \\ln\\frac{1}{\\sigma\\sqrt{2\\pi}}-\\frac{(y_i-\\mu_c)^2}{2\\sigma^2}\\right)\n \\end{aligned}\n$$\n$\\mathbb{E}_{q_C}[\\mathbb{1}(C_i=c)]$是$C_i$为$c$的概率。现在我们对$\\mu_M$求导：\n$$\n\\frac{d}{d \\mu_{M}} \\mathbb{E}_{q_{C}}\\left[\\ln p_{Y \\mid C}(y \\mid C) p_{C}(C)\\right]=\\sum_{i=1}^{n} q_{C_{i}}(M)\\left(\\frac{y_{i}-\\mu_{M}}{\\sigma^{2}}\\right)=0\n$$\n得：\n$$\n\\mu_{M}=\\frac{\\sum_{i=1}^{n} q_{C_{i}}(M) y_{i}}{\\sum_{i=1}^{n} q_{C_{i}}(M)}\n$$\n\n同理可得：\n$$\n\\mu_{F}=\\frac{\\sum_{i=1}^{n} q_{C_{i}}(F) y_{i}}{\\sum_{i=1}^{n} q_{C_{i}}(F)}\n$$\n","tags":["机器学习"],"categories":["算法"]},{"title":"贝叶斯数据分析","url":"/2021/08/11/贝叶斯数据分析/","content":"\n当初为什么要用英文写笔记？？\n{% pdf https://hfcouc.work/pdfs/Bayesian_Data_Analysis.pdf %}","tags":["贝叶斯数据分析"],"categories":["贝叶斯机器学习"]},{"title":"微分方程","url":"/2021/08/09/微分方程/","content":"\n微分方程学习笔记\n\n{% pdf https://hfcouc.work/pdfs/Differential_equation.pdf %}\n","tags":["微分方程"],"categories":["数学"]},{"title":"数值分析笔记","url":"/2021/07/30/数值分析/","content":"\n学习数值分析的笔记\n\n{% pdf https://hfcouc.work/pdfs/Numerical_analysis.pdf %}\n\n","tags":["数值分析"],"categories":["数学"]},{"title":"统计学习基础","url":"/2021/07/28/统计学习精要笔记/","content":"\n自己学习统计学习基础的笔记。\n\n{% pdf https://hfcouc.work/pdfs/ESLII.pdf %}\n\n","tags":["机器学习"],"categories":["机器学习"]},{"title":"只争朝夕，不负韶华","url":"/2021/07/13/index/","content":"\n{% meting \"33378114\" \"netease\" \"song\" \"theme:#555\" \"mutex:true\" \"listmaxheight:340px\" \"preload:auto\" %}\n\n<img src=\"https://cdn.jsdelivr.net/gh/HFC666/image@master/20210713/3.178msyh9rg68.png\" style=\"zoom:80%;\" />\n\n\n\n> 现状是没那么容易改变的\n> 即便是足够努力\n> 也很难在短时间内看出效果\n> 所以有时你认为的无法改变\n> 也可能只是暂时没看出效果而己\n> 而不是不够努力\n\n<img src=\"https://cdn.jsdelivr.net/gh/HFC666/image@master/20210713/4.36iwffkaky00.png\" style=\"zoom:80%;\" />\n\n\n\n> 不能再浑浑噩噩\n> 如果不把眼皮用力抬起看个真切\n> 或许就会错过人生中按下快门的良机\n\n\n\n**即使是一个人，也需要好好吃饭，这是治愈自己的一种方式**\n\n\n\n“不被时间和社会所束缚，幸福的填饱肚子，短时间内变得随心所欲，变得自由，不被打扰；毫不费神的吃东西的这种孤高行为，是现代人，都平等的拥有的最高治愈。”\n\n{% raw %}\n\n<div style=\"position: relative; width: 100%; height: 0; padding-bottom: 75%;\"><iframe \nsrc=\"//player.bilibili.com/player.html?\taid=73923411&bvid=BV1AE411a7Nw&cid=126453908&page=1\" scrolling=\"no\" border=\"0\" \nframeborder=\"no\" framespacing=\"0\" allowfullscreen=\"true\" style=\"position: absolute; width: 100%; \nheight: 100%; left: 0; top: 0;\"> </iframe></div>\n\n\n{% endraw %}\n\n\n\n<div style=\"position: relative; width: 100%; height: 0; padding-bottom: 75%;\"><iframe \nsrc=\"//player.bilibili.com/player.html?aid=52761403&bvid=BV1r4411776v&cid=92328539&page=1\" scrolling=\"no\" border=\"0\" \nframeborder=\"no\" framespacing=\"0\" allowfullscreen=\"true\" style=\"position: absolute; width: 100%; \nheight: 100%; left: 0; top: 0;\"> </iframe></div>\n\n\n\n\n"}]