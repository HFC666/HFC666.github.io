<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>独自赏晴雨</title>
  
  
  <link href="https://www.hfcouc.work/atom.xml" rel="self"/>
  
  <link href="https://www.hfcouc.work/"/>
  <updated>2022-03-02T10:30:51.125Z</updated>
  <id>https://www.hfcouc.work/</id>
  
  <author>
    <name>HFC</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>贝叶斯数据分析第二周-采样方法</title>
    <link href="https://www.hfcouc.work/2022/03/02/%E8%B4%9D%E5%8F%B6%E6%96%AF%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E7%AC%AC%E4%BA%8C%E5%91%A8/"/>
    <id>https://www.hfcouc.work/2022/03/02/%E8%B4%9D%E5%8F%B6%E6%96%AF%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E7%AC%AC%E4%BA%8C%E5%91%A8/</id>
    <published>2022-03-02T09:01:09.000Z</published>
    <updated>2022-03-02T10:30:51.125Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="\assets\js\Meting.min.js"></script><p>Coursera贝叶斯统计第一周学习笔记</p><span id="more"></span><h3 id="Metropolis-Hastings"><a href="#Metropolis-Hastings" class="headerlink" title="Metropolis-Hastings"></a>Metropolis-Hastings</h3><p>假设我们的目标分布为$g(\theta)$，我们从$q(\theta)$中采样：M-H采样其步骤为：</p><ol><li>选择一个初始值$\theta_0$</li><li>对于$i=1,\cdots,m$，重复<ol><li>从分布$\theta^\star\sim q(\theta^\star|\theta_{i-1})$中采样</li><li>计算<script type="math/tex">\alpha = \frac{g(\theta^\star)q(\theta_{i-1}|\theta^\star)}{g(\theta_{i-1})q(\theta^\star|\theta_{i-1})}</script>，当$q$为对称的分布时(如正态分布)，则$\alpha = \frac{g(\theta^\star)}{g(\theta_{i-1})}$</li><li>若$\alpha\ge 1$，则接受$\theta^\star$，并把$\theta_i$设置为$\theta^\star$；若$0&lt;\alpha&lt;1$，则以概率$\alpha$接受$\theta^\star$并将$\theta_i$设置为$\theta^\star$，同时以$1-\alpha$的概率拒绝$\theta^\star$并且将$\theta_i$设置为$\theta_{i-1}$。</li></ol></li></ol><h4 id="案例"><a href="#案例" class="headerlink" title="案例"></a>案例</h4><p>假设我们的后验分布为：</p><script type="math/tex; mode=display">p(\mu|y_1,\cdots,y_n)\propto \frac{\exp[n(\overline{y}\mu-\mu^2/2)]}{1+\mu^2}</script><p>为了数值稳定性，我们计算上述分布的对数：</p><script type="math/tex; mode=display">\log(g(\mu)) = n(\overline{y}\mu-\mu^2/2) - \log(1+\mu^2)</script><figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line">lg &lt;- <span class="keyword">function</span>(mu ,n ,ybar) &#123;</span><br><span class="line">  mu2 &lt;- mu^<span class="number">2</span></span><br><span class="line">  n * (ybar * mu - mu2 / <span class="number">2.0</span>) - <span class="built_in">log</span>(<span class="number">1.0</span> + mu2)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">mh &lt;- <span class="keyword">function</span>(n, ybar, n_iter, mu_init, cand_sd) &#123;</span><br><span class="line">  mu_out &lt;- numeric(n_iter)</span><br><span class="line">  accept &lt;- 0</span><br><span class="line">  mu_now = mu_init</span><br><span class="line">  lg_now = lg(mu=mu_now, n=n, ybar=ybar)</span><br><span class="line">  </span><br><span class="line">  <span class="keyword">for</span> (i <span class="keyword">in</span> <span class="number">1</span>:n_iter) &#123;</span><br><span class="line">    mu_cand &lt;- rnorm(<span class="number">1</span>, mean = mu_now, sd = cand_sd)</span><br><span class="line">    </span><br><span class="line">    lg_cand &lt;- lg(mu = mu_cand, n = n, ybar = ybar)</span><br><span class="line">    lalpha &lt;- lg_cand - lg_now</span><br><span class="line">    alpha &lt;- <span class="built_in">exp</span>(lalpha)</span><br><span class="line">    </span><br><span class="line">    u &lt;- runif(<span class="number">1</span>)</span><br><span class="line">    <span class="keyword">if</span> (u &lt; alpha) &#123;</span><br><span class="line">      mu_now &lt;- mu_cand</span><br><span class="line">      accept &lt;- accept + <span class="number">1</span></span><br><span class="line">      lg_now &lt;- lg_cand</span><br><span class="line">    &#125;</span><br><span class="line">    mu_out[i] &lt;- mu_now</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="built_in">list</span>(mu=mu_out, accept=accept/n_iter)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 数据</span></span><br><span class="line">y &lt;- <span class="built_in">c</span>(<span class="number">1.2</span>, <span class="number">1.4</span>, -<span class="number">0.5</span>, <span class="number">0.3</span>, <span class="number">0.9</span>, <span class="number">2.3</span>, <span class="number">1.0</span>, <span class="number">0.1</span>, <span class="number">1.3</span>, <span class="number">1.9</span>)</span><br><span class="line"></span><br><span class="line">ybar &lt;- mean(y)</span><br><span class="line">n&lt;- <span class="built_in">length</span>(y)</span><br><span class="line"></span><br><span class="line">hist(y, freq=<span class="literal">FALSE</span>, xlim=<span class="built_in">c</span>(-<span class="number">1.0</span>, <span class="number">3.0</span>))</span><br><span class="line">points(y, <span class="built_in">rep</span>(<span class="number">0.0</span>, n))</span><br><span class="line">points(ybar, <span class="number">0.0</span>, pch=<span class="number">19</span>)</span><br><span class="line">curve(dt(x, df=<span class="number">1</span>), lty=<span class="number">2</span>, add=<span class="literal">TRUE</span>) <span class="comment"># 我们的先验为t分布，绘制先验分布</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># MH采样</span></span><br><span class="line">set.seed(<span class="number">43</span>)</span><br><span class="line"><span class="comment"># 一般cand_sd即方差越大，接受率越小，而方差过小，每次移动的步伐太短，需要很长时间才能走遍后验分布。</span></span><br><span class="line">post &lt;- mh(n = n, ybar = ybar, n_iter = <span class="number">1e3</span>, mu_init = <span class="number">0.0</span>, cand_sd = <span class="number">3</span>)</span><br><span class="line">str(post)</span><br><span class="line"></span><br><span class="line">library(coda)</span><br><span class="line">traceplot(as.mcmc(post$mu))</span><br></pre></td></tr></table></figure><p><img src="https://static01.imgkr.com/temp/d8f150dff9f64e9a8990a9c7c807bfd0.png" alt=""></p><p>如果我们设置的初始值不当，如：</p><figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">post &lt;- mh(n = n, ybar = ybar, n_iter = <span class="number">1e3</span>, mu_init = <span class="number">30.0</span>, cand_sd = <span class="number">3</span>)</span><br><span class="line">str(post)</span><br><span class="line"></span><br><span class="line">library(coda)</span><br><span class="line">traceplot(as.mcmc(post$mu))</span><br></pre></td></tr></table></figure><p><img src="https://static01.imgkr.com/temp/a17c3fedd89944ad87cad9d10835067f.png" alt=""></p><p>可以看到经过多次迭代后才达到平稳分布，我们需要删除之前未达到平稳的样本。</p><h3 id="JAGS"><a href="#JAGS" class="headerlink" title="JAGS"></a>JAGS</h3><p>JAGS为一个用于MCMC的工具。我们下面看如何使用JAGS：</p><p>假设我们的模型为：</p><script type="math/tex; mode=display">\begin{aligned}&y_{i} \mid \mu \stackrel{\mathrm{iid}}{\sim} \mathrm{N}(\mu, 1), \quad i=1, \ldots, n \\&\mu \sim \mathrm{t}(0,1,1)\end{aligned}</script><figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line">library(rjags)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 1. Specify the model</span></span><br><span class="line">mod_string &lt;- <span class="string">&quot;model &#123;</span></span><br><span class="line"><span class="string">  for (i in 1:n) &#123;</span></span><br><span class="line"><span class="string">    y[i] ~ dnorm(mu, 1.0/sig2)</span></span><br><span class="line"><span class="string">  &#125;</span></span><br><span class="line"><span class="string">  mu ~ dt(0.0, 1.0/1.0, 1)</span></span><br><span class="line"><span class="string">  sig2 = 1.0</span></span><br><span class="line"><span class="string">&#125;&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 2. Set up the model</span></span><br><span class="line">set.seed(<span class="number">50</span>)</span><br><span class="line"></span><br><span class="line">y &lt;- <span class="built_in">c</span>(<span class="number">1.2</span>, <span class="number">1.4</span>, -<span class="number">0.5</span>, <span class="number">0.3</span>, <span class="number">0.9</span>, <span class="number">2.3</span>, <span class="number">1.0</span>, <span class="number">0.1</span>, <span class="number">1.3</span>, <span class="number">1.9</span>)</span><br><span class="line">n &lt;- <span class="built_in">length</span>(y)</span><br><span class="line"></span><br><span class="line">data_jags &lt;- <span class="built_in">list</span>(y=y, n=n)</span><br><span class="line">params &lt;- <span class="built_in">c</span>(<span class="string">&quot;mu&quot;</span>)</span><br><span class="line"></span><br><span class="line">inits = <span class="keyword">function</span>() &#123;</span><br><span class="line">  inits &lt;- <span class="built_in">list</span>(<span class="string">&quot;mu&quot;</span>=<span class="number">0.0</span>)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">mod &lt;- jags.model(textConnection(mod_string), data = data_jags, inits = inits)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 3. Run the MCMC sampler</span></span><br><span class="line"></span><br><span class="line">update(mod, <span class="number">500</span>) <span class="comment"># 先迭代500次，达到平稳分布</span></span><br><span class="line"></span><br><span class="line">mod_sim &lt;- coda.samples(model = mod, variable.names = params, n.iter = <span class="number">1000</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 4. Post processing</span></span><br><span class="line">library(coda)</span><br><span class="line">plot(mod_sim)</span><br></pre></td></tr></table></figure><p><img src="https://static01.imgkr.com/temp/ce2e97df9a4d40cf993e91c68718f27c.png" alt=""></p><h3 id="Gibbs-Sampling"><a href="#Gibbs-Sampling" class="headerlink" title="Gibbs Sampling"></a>Gibbs Sampling</h3><p>假设我们的后验分布为$p(\theta,\phi|y)$，Gibbs采样的算法为：</p><ol><li>初始$\theta_0,\phi_0$</li><li>对于$i=1,\cdots,m$，重复：<ol><li>使用$\phi_{i-1}$产生$\theta_i\sim p(\theta|\phi_{i-1},y)$</li><li>使用$\theta_i$产生$\phi_i\sim p(\phi|\theta_i,y)$</li></ol></li></ol><p>例子：</p><p>假设我们有如下模型：</p><script type="math/tex; mode=display">\begin{aligned}y_i|\mu,\sigma^2&\stackrel{\mathrm{iid}}{\sim} \mathcal{N}(\mu,\sigma^2)\\\mu&\sim \mathcal{N}(\mu_0,\sigma_0^2)\\\sigma^2&\sim \operatorname{IG}(\nu_0,\beta_0)\end{aligned}</script><p>我们的联合概率分布为：</p><script type="math/tex; mode=display">\begin{aligned}&p\left(\mu, \sigma^{2} \mid y_{1}, \ldots, y_{n}\right) \propto p\left(y_{1}, \ldots, y_{n} \mid \mu_{1} \sigma^{2}\right) p(\mu) p\left(\sigma^{2}\right)\\&=\prod_{i=1}^{n}\left[N\left(y_{i} \mid \mu, \sigma^{2}\right)\right] N\left(\mu \mid \mu_{0}, \sigma_{0}^{2}\right) I G\left(\sigma^{2} \mid \nu_{0}, \beta_{0}\right)\\&=\prod_{i=1}^{n}\left[\frac{1}{\sqrt{2 \pi \sigma^{2}}} \exp \left(-\frac{1}{2 \sigma^{2}}\left(y_{i}-\mu^{n}\right)\right)\right] \frac{1}{\sqrt{2 \pi \sigma_{0}^{2}}} \exp \left(-\frac{1}{2 \sigma_{0}^{2}}\left(\mu-\mu_{0}\right)^{2}\right) \frac{\beta_{0}^{\nu_{0}}}{\Gamma\left(N_{0}\right)}\left(\sigma_{0}^{-(\nu_0+1)} \exp \left(\frac{-\beta_{0}}{\sigma^{2}}\right)\right.\\&\propto\left(\sigma^{2}\right)^{-\frac{n}{2}} \exp \left[-\frac{1}{2 \sigma^{2}} \sum_{i=1}^{n}\left(y_{i}-\mu\right)^{2}\right] \exp \left[-\frac{1}{2 \sigma_{0}^{2}}\left(\mu-\mu_{0}\right)^{2}\right]\left(\sigma^{2}\right)^{-\left(\nu_{0}+1\right)} \exp \left[\frac{-\beta_{0}}{\sigma^{2}}\right]\end{aligned}</script><p>下面我们计算其条件概率作为转移概率：</p><script type="math/tex; mode=display">\begin{aligned}p\left(\mu \mid \sigma^{2}, y_{1}, \ldots, y_{n}\right) & \propto P\left(\mu, \sigma^{2} \mid y_{1}, \ldots, y_{n}\right) \\& \propto \exp \left[-\frac{1}{2 \sigma^{2}} \sum_{i=1}^{n}\left(y_{i}-\mu\right)^{2}\right] \exp \left[-\frac{1}{2 \sigma_{0}^{2}}\left(\mu-\mu_{0}\right)^{2}\right] \\&=\exp \left[-\frac{1}{2}\left(\frac{\sum\left(y_{i}-\mu\right)^{2}}{\sigma^{2}}+\frac{\left(\mu-\mu_{0}\right)^{2}}{\sigma_{0}^{2}}\right)\right] \\& \propto N\left(\mu \mid \frac{n \bar{y} / \sigma^{2}+\mu_{0} / \sigma_{0}^{2}}{n / \sigma^{2}+1 / \sigma_{0}^{2}}, \frac{1}{n / \sigma^{2}+1 / \sigma_{0}^{2}}\right)\end{aligned}</script><script type="math/tex; mode=display">\begin{aligned}p\left(\sigma^{2} \mid \mu, y_{1}, \ldots, y_{n}\right) & \propto p\left(\mu, \sigma^{2} \mid y_{1}, \ldots, y_{n}\right) \\& \propto\left(\sigma^{2}\right)^{-\frac{n}{2}} \exp \left[-\frac{1}{2 \sigma^{2}} \Sigma\left(y_{i}-\mu\right)^{2}\right]\left(\sigma^{2}\right)^{-\left(\nu_{0}+1\right)} \exp \left[\frac{-\beta_{0}}{\sigma^{2}}\right] \\&=\left(\sigma^{2}\right)^{-\left(\nu_{0}+\frac{n}{2}+1\right)} \exp \left[-\frac{1}{\sigma^{2}}\left(\beta_{0}+\frac{\sum_{1=1}^{n}\left(y_{i}-\mu\right)^{2}}{2}\right)\right] \\& \propto I G\left(\sigma^{2} \mid \nu_{0}+\frac{n}{2}, \beta_{0}+\frac{\sum_{i=1}^{n}\left(y_{i}-\mu\right)^{2}}{2}\right)\end{aligned}</script><figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br></pre></td><td class="code"><pre><span class="line">update_mu = <span class="keyword">function</span>(n, ybar, sig2, mu_0, sig2_0) &#123;</span><br><span class="line">  sig2_1 = <span class="number">1.0</span> / (n / sig2 + <span class="number">1.0</span> / sig2_0)</span><br><span class="line">  mu_1 = sig2_1 * (n * ybar / sig2 + mu_0 / sig2_0)</span><br><span class="line">  rnorm(n=<span class="number">1</span>, mean=mu_1, sd=<span class="built_in">sqrt</span>(sig2_1))</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">update_sig2 = <span class="keyword">function</span>(n, y, mu, nu_0, beta_0) &#123;</span><br><span class="line">  nu_1 = nu_0 + n / <span class="number">2.0</span></span><br><span class="line">  sumsq = <span class="built_in">sum</span>( (y - mu)^<span class="number">2</span> ) <span class="comment"># vectorized</span></span><br><span class="line">  beta_1 = beta_0 + sumsq / <span class="number">2.0</span></span><br><span class="line">  out_gamma = rgamma(n=<span class="number">1</span>, shape=nu_1, rate=beta_1) <span class="comment"># rate for gamma is shape for inv-gamma</span></span><br><span class="line">  <span class="number">1.0</span> / out_gamma <span class="comment"># reciprocal of a gamma random variable is distributed inv-gamma</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">gibbs = <span class="keyword">function</span>(y, n_iter, init, prior) &#123;</span><br><span class="line">  ybar = mean(y)</span><br><span class="line">  n = <span class="built_in">length</span>(y)</span><br><span class="line">  </span><br><span class="line">  <span class="comment">## initialize</span></span><br><span class="line">  mu_out = numeric(n_iter)</span><br><span class="line">  sig2_out = numeric(n_iter)</span><br><span class="line">  </span><br><span class="line">  mu_now = init$mu</span><br><span class="line">  </span><br><span class="line">  <span class="comment">## Gibbs sampler</span></span><br><span class="line">  <span class="keyword">for</span> (i <span class="keyword">in</span> <span class="number">1</span>:n_iter) &#123;</span><br><span class="line">    sig2_now = update_sig2(n=n, y=y, mu=mu_now, nu_0=prior$nu_0, beta_0=prior$beta_0)</span><br><span class="line">    mu_now = update_mu(n=n, ybar=ybar, sig2=sig2_now, mu_0=prior$mu_0, sig2_0=prior$sig2_0)</span><br><span class="line">    </span><br><span class="line">    sig2_out[i] = sig2_now</span><br><span class="line">    mu_out[i] = mu_now</span><br><span class="line">  &#125;</span><br><span class="line">  </span><br><span class="line">  cbind(mu=mu_out, sig2=sig2_out)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">y = <span class="built_in">c</span>(<span class="number">1.2</span>, <span class="number">1.4</span>, -<span class="number">0.5</span>, <span class="number">0.3</span>, <span class="number">0.9</span>, <span class="number">2.3</span>, <span class="number">1.0</span>, <span class="number">0.1</span>, <span class="number">1.3</span>, <span class="number">1.9</span>)</span><br><span class="line">ybar = mean(y)</span><br><span class="line">n = <span class="built_in">length</span>(y)</span><br><span class="line"></span><br><span class="line"><span class="comment">## prior</span></span><br><span class="line">prior = <span class="built_in">list</span>()</span><br><span class="line">prior$mu_0 = <span class="number">0.0</span></span><br><span class="line">prior$sig2_0 = <span class="number">1.0</span></span><br><span class="line">prior$n_0 = <span class="number">2.0</span> <span class="comment"># prior effective sample size for sig2</span></span><br><span class="line">prior$s2_0 = <span class="number">1.0</span> <span class="comment"># prior point estimate for sig2</span></span><br><span class="line">prior$nu_0 = prior$n_0 / <span class="number">2.0</span> <span class="comment"># prior parameter for inverse-gamma</span></span><br><span class="line">prior$beta_0 = prior$n_0 * prior$s2_0 / <span class="number">2.0</span> <span class="comment"># prior parameter for inverse-gamma</span></span><br><span class="line"></span><br><span class="line">hist(y, freq=<span class="literal">FALSE</span>, xlim=<span class="built_in">c</span>(-<span class="number">1.0</span>, <span class="number">3.0</span>)) <span class="comment"># histogram of the data</span></span><br><span class="line">curve(dnorm(x=x, mean=prior$mu_0, sd=<span class="built_in">sqrt</span>(prior$sig2_0)), lty=<span class="number">2</span>, add=<span class="literal">TRUE</span>) <span class="comment"># prior for mu</span></span><br><span class="line">points(y, <span class="built_in">rep</span>(<span class="number">0</span>,n), pch=<span class="number">1</span>) <span class="comment"># individual data points</span></span><br><span class="line">points(ybar, <span class="number">0</span>, pch=<span class="number">19</span>) <span class="comment"># sample mean</span></span><br><span class="line"></span><br><span class="line">set.seed(<span class="number">53</span>)</span><br><span class="line"></span><br><span class="line">init = <span class="built_in">list</span>()</span><br><span class="line">init$mu = <span class="number">0.0</span></span><br><span class="line"></span><br><span class="line">post = gibbs(y=y, n_iter=<span class="number">1e3</span>, init=init, prior=prior)</span><br><span class="line"></span><br><span class="line">library(<span class="string">&quot;coda&quot;</span>)</span><br><span class="line">plot(as.mcmc(post))</span><br></pre></td></tr></table></figure><p><img src="https://static01.imgkr.com/temp/d644d2a3edd8434884e861f1f8aa0717.png" alt=""></p><h3 id="Assessing-Convergence"><a href="#Assessing-Convergence" class="headerlink" title="Assessing Convergence"></a>Assessing Convergence</h3><h4 id="trace-plots"><a href="#trace-plots" class="headerlink" title="trace plots"></a>trace plots</h4><p>trace plots可以展示样本迭代的过程，下面就是一个很可能收敛的链的例子：</p><figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">set.seed(<span class="number">61</span>)</span><br><span class="line">post0 &lt;- mh(n=n, ybar=ybar, n_iter=<span class="number">10e3</span>, mu_init=<span class="number">0.0</span>, cand_sd=<span class="number">0.9</span>)</span><br><span class="line">coda::traceplot(as.mcmc(post0$mu[-<span class="built_in">c</span>(<span class="number">1</span>:<span class="number">500</span>)]))</span><br></pre></td></tr></table></figure><p><img src="https://static01.imgkr.com/temp/987ac99478704339b4018abdfd543f7a.png" alt=""></p><p>如果链是平稳的，它就不应该有长期的趋势。链的平均值应该大致相平。不应该像下面的例子那样徘徊：</p><figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">set.seed(<span class="number">61</span>)</span><br><span class="line">post1 &lt;- mh(n=n, ybar=ybar, n_iter=<span class="number">1e3</span>, mu_init=<span class="number">0.0</span>, cand_sd=<span class="number">0.04</span>)</span><br><span class="line">coda::traceplot(as.mcmc(post1$mu[-<span class="built_in">c</span>(<span class="number">1</span>:<span class="number">500</span>)]))</span><br></pre></td></tr></table></figure><p>在这种情况下，我们应该迭代更多次：</p><figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">set.seed(<span class="number">61</span>)</span><br><span class="line">post2 = mh(n=n, ybar=ybar, n_iter=<span class="number">100e3</span>, mu_init=<span class="number">0.0</span>, cand_sd=<span class="number">0.04</span>)</span><br><span class="line">coda::traceplot(as.mcmc(post2$mu))</span><br></pre></td></tr></table></figure><p><img src="https://static01.imgkr.com/temp/65092d69de2946f9b3fdbd7c00b6ddea.png" alt=""></p><h4 id="Monte-Carlo-effective-sample-size"><a href="#Monte-Carlo-effective-sample-size" class="headerlink" title="Monte Carlo effective sample size"></a>Monte Carlo effective sample size</h4><p>我们所观察的这两条链之间的一个主要区别是每条链的自相关程度。这里的自相关程度指的是与之前样本之间的关系：</p><figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">coda::autocorr.plot(as.mcmc(post0$mu))</span><br></pre></td></tr></table></figure><p><img src="https://static01.imgkr.com/temp/256220abc1914c9faf63d5a0e2c9c9ba.png" alt=""></p><figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">coda::autocorr.diag(as.mcmc(post0$mu))</span><br></pre></td></tr></table></figure><figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">##             [,1]</span></span><br><span class="line"><span class="comment">## Lag 0  1.0000000</span></span><br><span class="line"><span class="comment">## Lag 1  0.9850078</span></span><br><span class="line"><span class="comment">## Lag 5  0.9213126</span></span><br><span class="line"><span class="comment">## Lag 10 0.8387333</span></span><br><span class="line"><span class="comment">## Lag 50 0.3834563</span></span><br></pre></td></tr></table></figure><p>自相关很重要，因为它告诉我们在马尔科夫链中有多少信息可用。从高度相关的马尔可夫链中抽样1000次迭代所得到的关于平稳分布的信息，要比我们从平稳分布中独立抽取的1000个样本所得到的信息少。</p><p>自相关是计算蒙特卡洛有效样本的一个重要组成。蒙特卡洛有效样本指的是你需要从平稳分布抽取的样本的数量，使这些样本包含的信息与你的马尔科夫链包含的信息相同。</p><figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">coda::effectiveSize(as.mcmc(post2$mu))</span><br></pre></td></tr></table></figure><figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">##    var1 </span></span><br><span class="line"><span class="comment">## 373.858</span></span><br></pre></td></tr></table></figure><p>讲样本减少至自相关系数为$0$。这将会给你近似的独立样本。这些样本的数量近似于有效样本。</p><figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">coda::autocorr.plot(as.mcmc(post2$mu), lag.max=<span class="number">500</span>)</span><br></pre></td></tr></table></figure><p><img src="https://static01.imgkr.com/temp/7e891bcfd129407fa7e4fc96147cff61.png" alt=""></p><figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">thin_interval = <span class="number">400</span> <span class="comment"># how far apart the iterations are for autocorrelation to be essentially 0.</span></span><br><span class="line">thin_indx = seq(from=thin_interval, to=<span class="built_in">length</span>(post2$mu), by=thin_interval)</span><br><span class="line">head(thin_indx)</span><br><span class="line"></span><br><span class="line">post2mu_thin = post2$mu[thin_indx]</span><br><span class="line">traceplot(as.mcmc(post2$mu))</span><br></pre></td></tr></table></figure><p><img src="https://static01.imgkr.com/temp/8469da465d81475ca225f314830da20b.png" alt=""></p><figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">traceplot(as.mcmc(post2mu_thin))</span><br></pre></td></tr></table></figure><p><img src="https://static01.imgkr.com/temp/c6c48f26167b46fb81f9f878b7c2643e.png" alt=""></p><figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">coda::autocorr.plot(as.mcmc(post2mu_thin), lag.max=<span class="number">10</span>)</span><br></pre></td></tr></table></figure><p><img src="https://static01.imgkr.com/temp/63546d9e120f4d9995d1563e3c9eed35.png" alt=""></p><p>检查链的蒙特卡罗有效样本大小通常是一个好主意。如果你所寻求的只是一个后验均值估计，那么几百到几千的有效样本量就足够了。然而，如果你想创建一个$95\%$的后验区间，你可能需要成千上万的有效样本来产生分布外边缘的可靠估计。</p><figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">raftery.diag(as.mcmc(post0$mu))</span><br><span class="line"><span class="comment">## </span></span><br><span class="line"><span class="comment">## Quantile (q) = 0.025</span></span><br><span class="line"><span class="comment">## Accuracy (r) = +/- 0.005</span></span><br><span class="line"><span class="comment">## Probability (s) = 0.95 </span></span><br><span class="line"><span class="comment">##                                        </span></span><br><span class="line"><span class="comment">##  Burn-in  Total Lower bound  Dependence</span></span><br><span class="line"><span class="comment">##  (M)      (N)   (Nmin)       factor (I)</span></span><br><span class="line"><span class="comment">##  12       13218 3746         3.53</span></span><br></pre></td></tr></table></figure><figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">raftery.diag(as.mcmc(post0$mu), q=<span class="number">0.005</span>, r=<span class="number">0.001</span>, s=<span class="number">0.95</span>)</span><br><span class="line"><span class="comment">## </span></span><br><span class="line"><span class="comment">## Quantile (q) = 0.005</span></span><br><span class="line"><span class="comment">## Accuracy (r) = +/- 0.001</span></span><br><span class="line"><span class="comment">## Probability (s) = 0.95 </span></span><br><span class="line"><span class="comment">## </span></span><br><span class="line"><span class="comment">## You need a sample size of at least 19112 with these values of q, r and s</span></span><br></pre></td></tr></table></figure><h4 id="Burn-in"><a href="#Burn-in" class="headerlink" title="Burn-in"></a>Burn-in</h4><p>我们还看到了链的初值如何影响链的收敛速度。如果我们的初始值离后验分布的大部分很远，那么这条链可能需要一段时间才能到达那里。</p><figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">set.seed(<span class="number">62</span>)</span><br><span class="line">post3 &lt;- mh(n=n, ybar=ybar, n_iter=<span class="number">500</span>, mu_init=<span class="number">10.0</span>, cand_sd=<span class="number">0.3</span>)</span><br><span class="line">coda::traceplot(as.mcmc(post3$mu))s</span><br></pre></td></tr></table></figure><p><img src="https://static01.imgkr.com/temp/448eff5804354950ab8f32a86900884d.png" alt=""></p><p>显然，前100次左右的迭代不能反映平稳分布的结果，因此在我们使用这个链进行蒙特卡罗估计之前，应该抛弃它们。这就是所谓的burn-in期。</p><h3 id="Multiple-chains-Gelman-Rubin"><a href="#Multiple-chains-Gelman-Rubin" class="headerlink" title="Multiple chains, Gelman-Rubin"></a>Multiple chains, Gelman-Rubin</h3><p>如果我们想要更确信我们已经收敛到真正的平稳分布，我们可以模拟多个链，每个链有不同的初始值。</p><figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line">set.seed(<span class="number">61</span>)</span><br><span class="line"></span><br><span class="line">nsim = <span class="number">500</span></span><br><span class="line">post1 = mh(n=n, ybar=ybar, n_iter=nsim, mu_init=<span class="number">15.0</span>, cand_sd=<span class="number">0.4</span>)</span><br><span class="line">post1$accpt</span><br><span class="line"></span><br><span class="line">post2 = mh(n=n, ybar=ybar, n_iter=nsim, mu_init=-<span class="number">5.0</span>, cand_sd=<span class="number">0.4</span>)</span><br><span class="line">post2$accpt</span><br><span class="line"></span><br><span class="line">post3 = mh(n=n, ybar=ybar, n_iter=nsim, mu_init=<span class="number">7.0</span>, cand_sd=<span class="number">0.1</span>)</span><br><span class="line">post3$accpt</span><br><span class="line"></span><br><span class="line">post4 = mh(n=n, ybar=ybar, n_iter=nsim, mu_init=<span class="number">23.0</span>, cand_sd=<span class="number">0.5</span>)</span><br><span class="line">post4$accpt</span><br><span class="line"></span><br><span class="line">post5 = mh(n=n, ybar=ybar, n_iter=nsim, mu_init=-<span class="number">17.0</span>, cand_sd=<span class="number">0.4</span>)</span><br><span class="line">post5$accpt</span><br><span class="line"></span><br><span class="line">pmc = mcmc.list(as.mcmc(post1$mu), as.mcmc(post2$mu), </span><br><span class="line">                as.mcmc(post3$mu), as.mcmc(post4$mu), as.mcmc(post5$mu))</span><br><span class="line">str(pmc)</span><br><span class="line"></span><br><span class="line"><span class="comment">## List of 5</span></span><br><span class="line"><span class="comment">##  $ :Class &#x27;mcmc&#x27;  atomic [1:500] 14.8 14 14 13.8 13.8 ...</span></span><br><span class="line"><span class="comment">##   .. ..- attr(*, &quot;mcpar&quot;)= num [1:3] 1 500 1</span></span><br><span class="line"><span class="comment">##  $ :Class &#x27;mcmc&#x27;  atomic [1:500] -5 -5 -5 -5 -4.89 ...</span></span><br><span class="line"><span class="comment">##   .. ..- attr(*, &quot;mcpar&quot;)= num [1:3] 1 500 1</span></span><br><span class="line"><span class="comment">##  $ :Class &#x27;mcmc&#x27;  atomic [1:500] 7 7 7 6.94 6.94 ...</span></span><br><span class="line"><span class="comment">##   .. ..- attr(*, &quot;mcpar&quot;)= num [1:3] 1 500 1</span></span><br><span class="line"><span class="comment">##  $ :Class &#x27;mcmc&#x27;  atomic [1:500] 23 21.9 21.9 21.8 21.8 ...</span></span><br><span class="line"><span class="comment">##   .. ..- attr(*, &quot;mcpar&quot;)= num [1:3] 1 500 1</span></span><br><span class="line"><span class="comment">##  $ :Class &#x27;mcmc&#x27;  atomic [1:500] -17 -17 -16.9 -16.2 -15.7 ...</span></span><br><span class="line"><span class="comment">##   .. ..- attr(*, &quot;mcpar&quot;)= num [1:3] 1 500 1</span></span><br><span class="line"><span class="comment">##  - attr(*, &quot;class&quot;)= chr &quot;mcmc.list&quot;</span></span><br></pre></td></tr></table></figure><figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">coda::traceplot(pmc)</span><br></pre></td></tr></table></figure><p><img src="https://static01.imgkr.com/temp/92cc83b7959d48b5a684e5fc84da94f6.png" alt=""></p><p>我们可以用格尔曼和鲁宾的诊断来支持我们的视觉结果。这个诊断统计量计算链内的可变性，并将其与链间的可变性进行比较。如果所有的链都趋同于平稳分布，链之间的变异应该相对较小，诊断报告的潜在尺度缩减因子应该接近于1。如果这些值远远大于1，那么我们就会得出结论，这些链还没有收敛。</p><figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">coda::gelman.diag(pmc)</span><br><span class="line"></span><br><span class="line"><span class="comment">## Potential scale reduction factors:</span></span><br><span class="line"><span class="comment">## </span></span><br><span class="line"><span class="comment">##      Point est. Upper C.I.</span></span><br><span class="line"><span class="comment">## [1,]       1.01       1.02</span></span><br></pre></td></tr></table></figure><figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">coda::gelman.plot(pmc)</span><br></pre></td></tr></table></figure><p><img src="https://static01.imgkr.com/temp/804a2f8c5243411aa31b7044fe8f360f.png" alt=""></p><figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">nburn = <span class="number">1000</span> <span class="comment"># remember to discard early iterations</span></span><br><span class="line">post0$mu_keep = post0$mu[-<span class="built_in">c</span>(<span class="number">1</span>:<span class="number">1000</span>)]</span><br><span class="line">summary(as.mcmc(post0$mu_keep))</span><br><span class="line"></span><br><span class="line"><span class="comment">## </span></span><br><span class="line"><span class="comment">## Iterations = 1:9000</span></span><br><span class="line"><span class="comment">## Thinning interval = 1 </span></span><br><span class="line"><span class="comment">## Number of chains = 1 </span></span><br><span class="line"><span class="comment">## Sample size per chain = 9000 </span></span><br><span class="line"><span class="comment">## </span></span><br><span class="line"><span class="comment">## 1. Empirical mean and standard deviation for each variable,</span></span><br><span class="line"><span class="comment">##    plus standard error of the mean:</span></span><br><span class="line"><span class="comment">## </span></span><br><span class="line"><span class="comment">##           Mean             SD       Naive SE Time-series SE </span></span><br><span class="line"><span class="comment">##       0.889449       0.304514       0.003210       0.006295 </span></span><br><span class="line"><span class="comment">## </span></span><br><span class="line"><span class="comment">## 2. Quantiles for each variable:</span></span><br><span class="line"><span class="comment">## </span></span><br><span class="line"><span class="comment">##   2.5%    25%    50%    75%  97.5% </span></span><br><span class="line"><span class="comment">## 0.2915 0.6825 0.8924 1.0868 1.4890</span></span><br></pre></td></tr></table></figure>]]></content>
    
    
    <summary type="html">&lt;p&gt;Coursera贝叶斯统计第一周学习笔记&lt;/p&gt;</summary>
    
    
    
    <category term="贝叶斯机器学习" scheme="https://www.hfcouc.work/categories/%E8%B4%9D%E5%8F%B6%E6%96%AF%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
    <category term="贝叶斯数据分析" scheme="https://www.hfcouc.work/tags/%E8%B4%9D%E5%8F%B6%E6%96%AF%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90/"/>
    
  </entry>
  
  <entry>
    <title>贝叶斯数据分析第一周-统计模型</title>
    <link href="https://www.hfcouc.work/2022/03/02/%E8%B4%9D%E5%8F%B6%E6%96%AF%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E7%AC%AC%E4%B8%80%E5%91%A8/"/>
    <id>https://www.hfcouc.work/2022/03/02/%E8%B4%9D%E5%8F%B6%E6%96%AF%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E7%AC%AC%E4%B8%80%E5%91%A8/</id>
    <published>2022-03-02T09:01:00.000Z</published>
    <updated>2022-03-02T10:30:45.468Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="\assets\js\Meting.min.js"></script><p>Coursera贝叶斯统计第一周学习笔记</p><span id="more"></span><h2 id="统计模型"><a href="#统计模型" class="headerlink" title="统计模型"></a>统计模型</h2><p>统计模型是用来模拟或近似数据生成的过程。</p><p>贝叶斯统计中的三个重要组成部分：</p><ul><li>先验</li><li>似然</li><li>后验</li></ul><p>关于分册模型，我们看之前讲过的一个例子：</p><p>假设</p><script type="math/tex; mode=display">y_i|\mu,\sigma^2\sim \mathrm{N}(\mu,\sigma^2)</script><script type="math/tex; mode=display">\mu|\sigma^2\sim \mathrm{N}(\mu_0,\frac{\sigma^2}{w_0})</script><script type="math/tex; mode=display">\sigma^2\sim \Gamma^{-1}(\nu_0,\beta_0)</script><p>那么我们的联合概率分布可以写为：</p><script type="math/tex; mode=display">\begin{aligned}p(y_1,\cdots,y_n,\mu,\sigma^2) &= p(y_1,\cdots,p_n|\mu,\sigma^2)p(\mu|\sigma^2)p(\sigma^2)\\&=\prod_{i=1}^n\left[\mathrm{N}(y_i|\mu,\sigma^2)\right]\times \mathrm{N}(\mu|\mu_0,\frac{\sigma^2}{w_0})\times \Gamma^{-1}(\sigma^2|\nu_0,\beta_0)\\&\propto p(\mu,\sigma^2|y_1,\cdots,y_n)\end{aligned}</script><h3 id="常见的概率分布"><a href="#常见的概率分布" class="headerlink" title="常见的概率分布"></a>常见的概率分布</h3><h4 id="离散"><a href="#离散" class="headerlink" title="离散"></a>离散</h4><h5 id="均匀分布"><a href="#均匀分布" class="headerlink" title="均匀分布"></a>均匀分布</h5><script type="math/tex; mode=display">X\sim \operatorname{Uniform}(N)</script><script type="math/tex; mode=display">P(X=x|N) = 1/N,x = 1,2,\cdots,N</script><script type="math/tex; mode=display">\mathrm{E}[X] = \frac{N+1}{2}</script><script type="math/tex; mode=display">\operatorname{Var}[X] = \frac{N^2-1}{12}</script><h5 id="伯努利分布"><a href="#伯努利分布" class="headerlink" title="伯努利分布"></a>伯努利分布</h5><script type="math/tex; mode=display">X\sim \operatorname{Bern}(p)</script><script type="math/tex; mode=display">P(X=x|p) = p^x(1-p)^{1-x},x=0,1</script><script type="math/tex; mode=display">\mathrm{E}[X] = p</script><script type="math/tex; mode=display">\operatorname{Var}[X] = p(1-p)</script><h5 id="二项分布"><a href="#二项分布" class="headerlink" title="二项分布"></a>二项分布</h5><script type="math/tex; mode=display">Y\sim \operatorname{Binom}(n,p)</script><script type="math/tex; mode=display">P(Y=y|n,p) = C_n^pp^y(1-p)^{n-y}</script><script type="math/tex; mode=display">\mathrm{E}[Y] = np</script><script type="math/tex; mode=display">\operatorname{Var} = np(1-p)</script><h5 id="泊松分布"><a href="#泊松分布" class="headerlink" title="泊松分布"></a>泊松分布</h5><p>泊松分布主要用来计数。参数$\lambda&gt;0$是我们期望观察到我们正在计数的事物的比率。</p><script type="math/tex; mode=display">X\sim \operatorname{Pois}(\lambda)</script><script type="math/tex; mode=display">P(X=x|\lambda) = \frac{\lambda^x\exp(-\lambda)}{x!}</script><script type="math/tex; mode=display">\mathrm{E}[X] = \lambda</script><script type="math/tex; mode=display">\operatorname{Var}[X] = \lambda</script><h5 id="几何分布"><a href="#几何分布" class="headerlink" title="几何分布"></a>几何分布</h5><p>指的是在达到成功时的失败的次数。</p><script type="math/tex; mode=display">X\sim \operatorname{Geo}(p)</script><script type="math/tex; mode=display">P(X=x|p) = p(1-p)^x</script><script type="math/tex; mode=display">\mathrm{E}[X] = \frac{1-p}{p}</script><h5 id="负二项分布"><a href="#负二项分布" class="headerlink" title="负二项分布"></a>负二项分布</h5><p>是对几何分布的推广，指的是在达到$r$次成功时失败的次数。</p><script type="math/tex; mode=display">Y\sim \operatorname{NegBinom}(r,p)</script><script type="math/tex; mode=display">P(Y=y|r,p) = C_{r+y-1}^yp^r(1-p)^y</script><script type="math/tex; mode=display">\mathrm{E}[Y] = \frac{r(1-p)}{p}</script><script type="math/tex; mode=display">\operatorname{Var}[Y] = \frac{r(1-p)}{p^2}</script><h5 id="多项式分布"><a href="#多项式分布" class="headerlink" title="多项式分布"></a>多项式分布</h5><p>为二项分布的推广，有$k$中结果，第$k$个结果出现$x_k$次，出现的概率为$p_k$。</p><script type="math/tex; mode=display">f(x_1,\cdots,x_k|p_1,\cdots,p_k) = \frac{n!}{x_1!\cdots x_k!}p^{x_1}\cdots p^{x_k}</script><h4 id="连续"><a href="#连续" class="headerlink" title="连续"></a>连续</h4><h5 id="均匀分布-1"><a href="#均匀分布-1" class="headerlink" title="均匀分布"></a>均匀分布</h5><script type="math/tex; mode=display">X\sim \operatorname{Uniform}(a,b)</script><script type="math/tex; mode=display">f(x|a,b) = \frac{1}{b-a}I_{\{a\le x\le b\}}</script><script type="math/tex; mode=display">\mathrm{E}[X] = \frac{a+b}{2}</script><script type="math/tex; mode=display">\operatorname{Var}[X] = \frac{(b-a)^2}{12}</script><h5 id="Beta"><a href="#Beta" class="headerlink" title="Beta"></a>Beta</h5><script type="math/tex; mode=display">X\sim \operatorname{Beta}(\alpha,\beta)</script><script type="math/tex; mode=display">f(x|\alpha,\beta) = \frac{\Gamma(\alpha+\beta)}{\Gamma(\alpha)\Gamma(\beta)}x^{\alpha-1}(1-x)^{\beta-1}I_{\{0<x<1\}}</script><script type="math/tex; mode=display">\mathrm{E}[X] = \frac{\alpha}{\alpha+\beta}</script><script type="math/tex; mode=display">\operatorname{Var}[X] = \frac{\alpha\beta}{(\alpha+\beta)^2(\alpha+\beta+1)}</script><h5 id="指数分布"><a href="#指数分布" class="headerlink" title="指数分布"></a>指数分布</h5><p>指数分布常用来对随机事件之间的等待时间进行建模。</p><script type="math/tex; mode=display">X\sim \operatorname{Exp}(\lambda)</script><script type="math/tex; mode=display">f(x|\lambda) = \lambda e^{-\lambda x}I_{\{x\ge0\}}</script><script type="math/tex; mode=display">\mathrm{E}[X] = \frac{1}{\lambda}</script><script type="math/tex; mode=display">\operatorname{Var}[X] = \frac{1}{\lambda^2}</script><h5 id="双指数分布"><a href="#双指数分布" class="headerlink" title="双指数分布"></a>双指数分布</h5><p>是指数分布的推广，可以取负数。</p><script type="math/tex; mode=display">X\sim \operatorname{DExp}(\lambda)</script><script type="math/tex; mode=display">f(x|\mu,\tau) = \frac{\tau}{2}e^{-\tau|x-\mu|}</script><script type="math/tex; mode=display">\mathrm{E}[X] = \mu</script><script type="math/tex; mode=display">\operatorname{Var}[X] = \frac{1}{2\tau^2}</script><h5 id="Gamma"><a href="#Gamma" class="headerlink" title="Gamma"></a>Gamma</h5><p>当$X_1,X_2,\cdots,X_n$是相继的事件之间的等待时间，$n$个事件总的等待时间$Y = \sum_{i=1}^n X_i$将服从shape参数$\alpha=n$，rate参数$\beta=\lambda$的Gamma分布。</p><script type="math/tex; mode=display">Y\sim \operatorname{Gamma}(\alpha,\beta)</script><script type="math/tex; mode=display">f(y|\alpha,\beta) = \frac{\beta^\alpha}{\Gamma(\alpha)}y^{\alpha-1}e^{-\beta y}I_{y\ge 0}</script><script type="math/tex; mode=display">\mathrm{E}[Y]=\frac{\alpha}{\beta}</script><script type="math/tex; mode=display">\operatorname{Var}[Y] = \frac{\alpha}{\beta^2}</script><blockquote><script type="math/tex; mode=display">\Gamma(n)=(n-1)!</script></blockquote><h5 id="Inverse-Gamma"><a href="#Inverse-Gamma" class="headerlink" title="Inverse-Gamma"></a>Inverse-Gamma</h5><p>当$X\sim \operatorname{Gamma}(\alpha,\beta)$，随机变量$Y=1/X\sim \operatorname{Inverse-Gamma}(\alpha,\beta)$，其中</p><script type="math/tex; mode=display">f(y) = \frac{\beta^\alpha}{\Gamma(\alpha)}y^{-(\alpha+1)}\exp\left(-\frac{\beta}{y}\right)I_{\{y>0\}}</script><script type="math/tex; mode=display">\mathrm{E}(Y) = \frac{\beta}{\alpha-1}</script><h5 id="t"><a href="#t" class="headerlink" title="t"></a>t</h5><p>如果我们有正态分布数据。我们易得：</p><script type="math/tex; mode=display">\frac{\bar{X}-\mu}{\sigma/\sqrt{n}}\sim \mathrm{N}(0,1)</script><p>但是我们可能不知道$\sigma$的值。我们用$S = \sqrt{\sum_{i}(X_i-\bar{X})^2/(n-1)}$来代替他，这就是自由度$\nu=n-1$的$t$分布。</p><script type="math/tex; mode=display">Y\sim t_{\nu}</script><script type="math/tex; mode=display">f(y) = \frac{\Gamma(\frac{\nu+1}{2})}{\Gamma(\frac{\nu}{2})\sqrt{\nu\pi}}\left(1+\frac{y^2}{\nu}\right)^{-\frac{\nu+1}{2}}</script><script type="math/tex; mode=display">\mathrm{E}[Y] = 0 \text{ if }\nu>1</script><script type="math/tex; mode=display">\operatorname{Var}[Y] = \frac{\nu}{\nu-2}\text{ if }\nu>2</script><h5 id="Dirichlet"><a href="#Dirichlet" class="headerlink" title="Dirichlet"></a>Dirichlet</h5><p>当似然函数为多项式分布时，Dirichlet分布是共轭的：</p><script type="math/tex; mode=display">f(y_1,y_2,\cdots,y_K|\alpha_1,\alpha_2,\cdots,\alpha_K) = \frac{\Gamma(\sum_{k=1}^K\alpha_k)}{\prod_{k=1}^K\Gamma(\alpha_k)}\cdot p_1^{\alpha_1-1}\cdot p_2^{\alpha_2-1}\cdots  p_K^{\alpha_K-1}</script><h3 id="蒙特卡洛估计"><a href="#蒙特卡洛估计" class="headerlink" title="蒙特卡洛估计"></a>蒙特卡洛估计</h3><h4 id="蒙特卡洛积分"><a href="#蒙特卡洛积分" class="headerlink" title="蒙特卡洛积分"></a>蒙特卡洛积分</h4><p>假设我们有函数$h(\theta)$，则其期望为</p><script type="math/tex; mode=display">\mathrm{E}[h(\theta)] = \int h(\theta)p(\theta)d\theta \approx \frac{1}{m}\sum_{i=1}^m h(\theta_i)</script><p>下面看一个例子：</p><p>假设$h(\theta)=I_{\theta&lt;5}(\theta)$，则</p><script type="math/tex; mode=display">\begin{aligned}\mathrm{E}(h(\theta))&= \int_0^\infty I_{\theta<5}(\theta)p(\theta)d\theta\\&= \int_0^51\cdot p(\theta)d\theta\\&=\Pr[0< \theta <5]\\&\approx \frac{1}{m}\sum_{i=1}^mI_{\theta<5}(\theta)\end{aligned}</script><p>所以可以用出现在$(0,5)$之间的样本比例来近似在此区间的概率。</p><h4 id="蒙特卡洛误差"><a href="#蒙特卡洛误差" class="headerlink" title="蒙特卡洛误差"></a>蒙特卡洛误差</h4><p>根据中心极限定理，我们知道：</p><script type="math/tex; mode=display">\bar{\theta}\sim \mathrm{N}(\mathrm{E}(\theta),\frac{\operatorname{Var}(\theta)}{m})</script><p>我们用样本方差来估计$\operatorname{Var}(\theta)$，我们知道总体方差的无偏估计为：</p><script type="math/tex; mode=display">\hat{\operatorname{Var}}(\theta) = \frac{1}{m-1}\sum_{i=1}^m(\theta_i-\bar{\theta})^2</script><p>则</p><script type="math/tex; mode=display">\sqrt{\frac{\hat{\operatorname{Var}(\theta)}}{m}} = \text{standard error}</script><p>为蒙特卡洛误差。</p><p>假设有分层模型：</p><script type="math/tex; mode=display">y|\phi\sim \operatorname{Bin}(10,\phi)</script><script type="math/tex; mode=display">\phi\sim \operatorname{Beta}(2,2)</script><p>我们需要根据联合分布$p(y,\theta)=p(\theta|\phi)p(\phi)$来产生数据。我们可以根据以下步骤生成数据：</p><ol><li>从Beta分布中生成数据$\phi_i$</li><li>在给定$\phi_i$的条件下生成$y_i^\star\sim \operatorname{Bin}(10,\phi)$</li></ol><p>这样就生成了独立的样本$(y_i,\phi_i)$。这种采样让边缘化更加简单，若按照以往，我们需要对联合概率分布进行积分，将$\phi$积分消去后得到边缘分布再对$y$采样，但是现在我们只需要忽略掉样本中的$\phi$即可，将$y_i$视为从其边缘分布采样得到的。</p><p>想了一下这就是<strong>吉布斯采样</strong>。:clap::clap:</p><figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 我们用上节课提到的例子</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 随机数种子</span></span><br><span class="line">set.seed(<span class="number">32</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 样本数</span></span><br><span class="line">m &lt;- 10000</span><br><span class="line"></span><br><span class="line"><span class="comment"># 参数</span></span><br><span class="line">a &lt;- 2.0</span><br><span class="line">b &lt;- 1.0 / <span class="number">3.0</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 采样</span></span><br><span class="line">theta &lt;- rgamma(n=m, shape = a, rate = b)</span><br><span class="line"></span><br><span class="line">hist(theta, freq = <span class="literal">FALSE</span>)</span><br><span class="line"></span><br><span class="line">curve(dgamma(x, shape = a, rate = b), col=<span class="string">&quot;blue&quot;</span>, add = <span class="literal">TRUE</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 我们可以估计其期望</span></span><br><span class="line">mean(theta)  </span><br><span class="line"><span class="comment"># 真实的期望为a / b</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 也可以估计其方差</span></span><br><span class="line">var(theta)</span><br><span class="line"><span class="comment"># 真实的方差为a / b^2</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 我们也可以用来估计其他的参数，比如theta&lt;5的概率</span></span><br><span class="line">ind &lt;- theta &lt; <span class="number">5</span></span><br><span class="line">mean(ind)</span><br><span class="line"><span class="comment"># 我们用pgamma函数计算真实值</span></span><br><span class="line">pgamma(q = <span class="number">5.0</span>, shape = a, rate = b)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 我们也可以计算其90%分位数</span></span><br><span class="line">quantile(theta, probs = <span class="number">0.9</span>)</span><br><span class="line"><span class="comment"># 真实值通过qgamma函数计算</span></span><br><span class="line">qgamma(p=<span class="number">0.9</span>, shape = a, rate = b)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 下面看我们之前举的分层模型的例子</span></span><br><span class="line">m &lt;- 1e5</span><br><span class="line">phi &lt;- rbeta(m, shape1 = <span class="number">2.0</span>, shape2 = <span class="number">2.0</span>)</span><br><span class="line">y &lt;- rbinom(m, size = <span class="number">10</span>, prob = phi)</span><br></pre></td></tr></table></figure>]]></content>
    
    
    <summary type="html">&lt;p&gt;Coursera贝叶斯统计第一周学习笔记&lt;/p&gt;</summary>
    
    
    
    <category term="贝叶斯机器学习" scheme="https://www.hfcouc.work/categories/%E8%B4%9D%E5%8F%B6%E6%96%AF%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
    <category term="贝叶斯数据分析" scheme="https://www.hfcouc.work/tags/%E8%B4%9D%E5%8F%B6%E6%96%AF%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90/"/>
    
  </entry>
  
  <entry>
    <title>图像的几何变换</title>
    <link href="https://www.hfcouc.work/2022/01/05/%E5%9B%BE%E7%89%87%E7%9A%84%E5%87%A0%E4%BD%95%E5%8F%98%E6%8D%A2/"/>
    <id>https://www.hfcouc.work/2022/01/05/%E5%9B%BE%E7%89%87%E7%9A%84%E5%87%A0%E4%BD%95%E5%8F%98%E6%8D%A2/</id>
    <published>2022-01-05T11:24:03.000Z</published>
    <updated>2022-01-05T11:28:51.432Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="\assets\js\Meting.min.js"></script><p>图像的空间域变换。</p><span id="more"></span><h3 id="空间域运算"><a href="#空间域运算" class="headerlink" title="空间域运算"></a>空间域运算</h3><h4 id="代数运算"><a href="#代数运算" class="headerlink" title="代数运算"></a>代数运算</h4><h5 id="加法运算"><a href="#加法运算" class="headerlink" title="加法运算"></a>加法运算</h5><p>生成图像叠加效果：对于两个图像$f(x,y)$和$(x,y)$的均值有：</p><script type="math/tex; mode=display">g(x,y) = \frac{1}{2}f(x,y)+\frac{1}{2}h(x,y)</script><p>会得到二次曝光的效果。推广这个公式为：</p><script type="math/tex; mode=display">g(x,y)=\alpha f(x,y)+\beta h(x,y),\quad \alpha+\beta=1</script><p>我们可以得到各种图像合成的效果，也可以用于两张图片的衔接。</p><h5 id="减法运算"><a href="#减法运算" class="headerlink" title="减法运算"></a>减法运算</h5><p>去除不需要的叠加性图案</p><p>设背景图像为$b(x,y)$，前景背景混合图像$f(x,y)$</p><script type="math/tex; mode=display">g(x,y) = f(x,y)-b(x,y)</script><p>$g(x,y)$为去除了背景的图像。电视制作的蓝屏技术就基于此。</p><p>检测同一场景两幅图像之间的变化</p><p>设时间$1$的图像为$T_1(x,y)$，时间$2$的图像为$T_2(x,y)$，则</p><script type="math/tex; mode=display">g(x,y) = T_2(x,y)-T_1(x,y)</script><h5 id="乘法运算"><a href="#乘法运算" class="headerlink" title="乘法运算"></a>乘法运算</h5><p>乘法的定义：</p><script type="math/tex; mode=display">C(x,y) = A(x,y)\times B(x,y)</script><p>主要应用：图像的局部显示(用二值蒙版图像与原图像做乘法)</p><h4 id="逻辑运算"><a href="#逻辑运算" class="headerlink" title="逻辑运算"></a>逻辑运算</h4><p>求反、异或、或、与</p><p>求反的定义：</p><script type="math/tex; mode=display">g(x,y) = R-f(x,y)</script><p>$R$为$f(x,y)$的灰度级，求反图像</p><p>与运算：</p><script type="math/tex; mode=display">g(x,y) = f(x,y)\wedge h(x,y)</script><p>求两个图像交集</p><h4 id="几何变换"><a href="#几何变换" class="headerlink" title="几何变换"></a>几何变换</h4><p>基本几何变换的定义：对于原图像$f(x,y)$，坐标变换函数</p><script type="math/tex; mode=display">x^{\prime}=a(x,y); y^{\prime}=b(x,y)</script><p>唯一确定了几何变换：</p><script type="math/tex; mode=display">g(x^{\prime},y^{\prime}) = f(a(x,y),b(x,y))</script><p>$g(x,y)$是目标图像。</p><h5 id="平移"><a href="#平移" class="headerlink" title="平移"></a>平移</h5><script type="math/tex; mode=display">a(x,y) = x+x_0\quad b(x,y) = y+y_0</script><script type="math/tex; mode=display">\begin{bmatrix}a(x,y)\\b(x,y)\\1\end{bmatrix} =\begin{bmatrix}1&0&x_0\\0&1&y_0\\0&0&1\end{bmatrix}\begin{bmatrix}x\\y\\1\end{bmatrix}</script><h5 id="放缩"><a href="#放缩" class="headerlink" title="放缩"></a>放缩</h5><script type="math/tex; mode=display">a(x,y)=cx\quad b(x,y) = dy</script><script type="math/tex; mode=display">\begin{bmatrix}a(x,y)\\b(x,y)\\1\end{bmatrix} =\begin{bmatrix}c&0&0\\0&d&0\\0&0&1\end{bmatrix}\begin{bmatrix}x\\y\\1\end{bmatrix}</script><h5 id="旋转"><a href="#旋转" class="headerlink" title="旋转"></a>旋转</h5><script type="math/tex; mode=display">a(x,y) = x\cos(\theta)-y\sin(\theta)</script><script type="math/tex; mode=display">b(x,y) = x\sin(\theta)+y\cos(\theta)</script><script type="math/tex; mode=display">\begin{bmatrix}a(x,y)\\b(x,y)\\1\end{bmatrix} =\begin{bmatrix}\cos(\theta)&-\sin(\theta)&0\\\sin(\theta)&\cos(\theta)&0\\0&0&1\end{bmatrix}\begin{bmatrix}x\\y\\1\end{bmatrix}</script><p>证明过程如下：</p><p><img src="https://static01.imgkr.com/temp/57b93a78548f4b7495567ac1629c5440.png" alt=""></p><h5 id="其他几何变换"><a href="#其他几何变换" class="headerlink" title="其他几何变换"></a>其他几何变换</h5><p><img src="https://static01.imgkr.com/temp/18a1f0c5d579482ab4df31e2d4389e71.png" alt=""></p><h4 id="重采样"><a href="#重采样" class="headerlink" title="重采样"></a>重采样</h4><p>最近邻法：取最近的点。</p><p>特点：</p><ul><li>简单快速</li><li>灰度保真性好</li><li>误差较大</li><li>视觉特性较差</li><li>马赛克效应</li></ul><p>双线性插值(一阶插值)：根据四个相邻点的灰度值，分别在$x$和$y$方向上进行两次插值。插值可以根据距离的远近进行加权，一般采用双曲抛物面方程进行插值：</p><script type="math/tex; mode=display">f(x,y) = ax+by+cxy+d</script><p>将四个点代入可得：</p><script type="math/tex; mode=display">\begin{aligned}a &= f(1,0)-f(0,0)\\b&= f(0,1)-f(0,0)\\c&= f(1,1)+f(0,0)-f(0,1)-f(1,0)\\d&=f(0,0)\end{aligned}</script><p><img src="https://static01.imgkr.com/temp/92766afdbe36417894b87ba6713b6633.png" alt=""></p><p>高阶插值：</p><p>利用三次多项式来近似理论上的最佳插值函数$\operatorname{sinc}(x)$：</p><script type="math/tex; mode=display">S(x) = \begin{cases}1-2|x|^2+|x|^3&|x|<1\\4-8|x|+5|x|^2-|x|^3&1\le |x|\le 2\\0&|x|>2\end{cases}</script><p>由此形成常用的三次卷积插值算法，又称为三次内插法，两次立方法、CC插值法等。</p><p>其形状为：</p><p><img src="https://static01.imgkr.com/temp/dbf533512a5e42eba69c50c7406057b8.png" alt=""></p><p>对于待插值的像素点$(x,y)$，取其附近的$4\times 4$邻域点$(x_i,y_j),i,j=0,1,2,3$按如下公式进行插值计算：</p><script type="math/tex; mode=display">f(x,y) = \sum_{i=0}^3\sum_{j=0}^3f(x_i,y_j)W(x-x_i)W(y-y_j)</script><p>matlab写的旋转后双线性插值采样代码：</p><figure class="highlight matlab"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">function</span> <span class="title">img</span> = <span class="title">rotate</span><span class="params">(image, theta)</span></span></span><br><span class="line"><span class="comment">% 对图片进行旋转，采用二次线性插值</span></span><br><span class="line">[m,n] = <span class="built_in">size</span>(image);</span><br><span class="line">A = [<span class="built_in">cos</span>(theta) -<span class="built_in">sin</span>(theta)  <span class="number">0</span></span><br><span class="line">     <span class="built_in">sin</span>(theta) <span class="built_in">cos</span>(theta)   <span class="number">0</span></span><br><span class="line">     <span class="number">0</span>          <span class="number">0</span>            <span class="number">1</span></span><br><span class="line">];</span><br><span class="line">x = repelem(<span class="number">1</span>:n,<span class="number">1</span>,m);</span><br><span class="line">y = <span class="built_in">repmat</span>(<span class="number">1</span>:m,<span class="number">1</span>,n);</span><br><span class="line">z = <span class="built_in">ones</span>([<span class="number">1</span>,m*n]);</span><br><span class="line">position = [x;y;z];</span><br><span class="line">position_n = A*position;</span><br><span class="line">new_x = position_n(<span class="number">1</span>,:);</span><br><span class="line">new_y = position_n(<span class="number">2</span>,:);</span><br><span class="line">m_max = <span class="built_in">ceil</span>(<span class="built_in">max</span>(new_y));</span><br><span class="line">m_min = <span class="built_in">floor</span>(<span class="built_in">min</span>(new_y));</span><br><span class="line">n_max = <span class="built_in">ceil</span>(<span class="built_in">max</span>(new_x));</span><br><span class="line">n_min = <span class="built_in">floor</span>(<span class="built_in">min</span>(new_x));</span><br><span class="line">m_n = m_max-m_min+<span class="number">1</span>;</span><br><span class="line">n_n = n_max-n_min+<span class="number">1</span>;</span><br><span class="line">img = <span class="built_in">zeros</span>([m_n, n_n]);</span><br><span class="line"><span class="keyword">for</span> <span class="built_in">i</span> = <span class="number">1</span>:m_n</span><br><span class="line">    <span class="keyword">for</span> <span class="built_in">j</span> = <span class="number">1</span>:n_n</span><br><span class="line">        posi = A\[<span class="built_in">j</span>+n_min<span class="number">-1</span>;<span class="built_in">i</span>+m_min<span class="number">-1</span>;<span class="number">1</span>];</span><br><span class="line">        <span class="keyword">if</span> <span class="number">1</span>&lt;posi(<span class="number">1</span>)&amp;&amp;posi(<span class="number">1</span>)&lt;n &amp;&amp; <span class="number">1</span>&lt;posi(<span class="number">2</span>)&amp;&amp;posi(<span class="number">2</span>)&lt;m</span><br><span class="line">            <span class="comment">% 根据双曲抛物面方程进行插值：</span></span><br><span class="line">            a = image(<span class="built_in">floor</span>(posi(<span class="number">2</span>)),<span class="built_in">ceil</span>(posi(<span class="number">1</span>))) - image(<span class="built_in">floor</span>(posi(<span class="number">2</span>)),<span class="built_in">floor</span>(posi(<span class="number">1</span>)));</span><br><span class="line">            b = image(<span class="built_in">ceil</span>(posi(<span class="number">2</span>)),<span class="built_in">floor</span>(posi(<span class="number">1</span>))) - image(<span class="built_in">floor</span>(posi(<span class="number">2</span>)),<span class="built_in">floor</span>(posi(<span class="number">1</span>)));</span><br><span class="line">            c = image(<span class="built_in">ceil</span>(posi(<span class="number">2</span>)),<span class="built_in">ceil</span>(posi(<span class="number">1</span>))) + image(<span class="built_in">floor</span>(posi(<span class="number">2</span>)),<span class="built_in">floor</span>(posi(<span class="number">1</span>))) - image(<span class="built_in">ceil</span>(posi(<span class="number">2</span>)),<span class="built_in">floor</span>(posi(<span class="number">1</span>)))-image(<span class="built_in">floor</span>(posi(<span class="number">2</span>)),<span class="built_in">ceil</span>(posi(<span class="number">1</span>)));</span><br><span class="line">            d = image(<span class="built_in">floor</span>(posi(<span class="number">2</span>)),<span class="built_in">floor</span>(posi(<span class="number">1</span>)));</span><br><span class="line">            img(<span class="built_in">i</span>,<span class="built_in">j</span>) = (posi(<span class="number">1</span>)-<span class="built_in">floor</span>(posi(<span class="number">1</span>)))*a + (posi(<span class="number">2</span>)-<span class="built_in">floor</span>(posi(<span class="number">2</span>)))*b + c*(posi(<span class="number">1</span>)-<span class="built_in">floor</span>(posi(<span class="number">1</span>)))*(posi(<span class="number">2</span>)-<span class="built_in">floor</span>(posi(<span class="number">2</span>)))+d;</span><br><span class="line">        <span class="keyword">end</span></span><br><span class="line">    <span class="keyword">end</span></span><br><span class="line"><span class="keyword">end</span></span><br><span class="line">img = img/<span class="number">255</span>;</span><br></pre></td></tr></table></figure>]]></content>
    
    
    <summary type="html">&lt;p&gt;图像的空间域变换。&lt;/p&gt;</summary>
    
    
    
    <category term="图像处理" scheme="https://www.hfcouc.work/categories/%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86/"/>
    
    
    <category term="数字图像处理" scheme="https://www.hfcouc.work/tags/%E6%95%B0%E5%AD%97%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86/"/>
    
  </entry>
  
  <entry>
    <title>向量微积分第一周学习笔记</title>
    <link href="https://www.hfcouc.work/2022/01/04/%E5%90%91%E9%87%8F%E5%BE%AE%E7%A7%AF%E5%88%86/"/>
    <id>https://www.hfcouc.work/2022/01/04/%E5%90%91%E9%87%8F%E5%BE%AE%E7%A7%AF%E5%88%86/</id>
    <published>2022-01-04T14:43:38.000Z</published>
    <updated>2022-01-04T14:46:29.393Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="\assets\js\Meting.min.js"></script><p>向量微积分课程第一周的内容，不过还没学完，明天再学吧，我都快学死了。</p><span id="more"></span><h2 id="第一周"><a href="#第一周" class="headerlink" title="第一周"></a>第一周</h2><h3 id="向量"><a href="#向量" class="headerlink" title="向量"></a>向量</h3><h4 id="向量的叉乘"><a href="#向量的叉乘" class="headerlink" title="向量的叉乘"></a>向量的叉乘</h4><p>两个三维向量叉乘，得到一个新的向量，这个向量垂直于原来的两个向量，大小为原来两个向量围成的平行四边形的面积，向量方向的判断符合右手定理。</p><p>两个三维向量$A = (v_1,v_2,v_3),B=(w_1,w_2,w_3)$叉乘，可以用行列式来表示：</p><script type="math/tex; mode=display">A\times B = \det\begin{bmatrix}\vec{i}&\vec{j}&\vec{k}\\v_1&v_2&v_3\\w_1&w_2&w_3\end{bmatrix}</script><h3 id="解析几何"><a href="#解析几何" class="headerlink" title="解析几何"></a>解析几何</h3><h4 id="平面的解析几何"><a href="#平面的解析几何" class="headerlink" title="平面的解析几何"></a>平面的解析几何</h4><p>我们首先在平面上选择三个不共线的点$r_1,r_2,r_3$，我们可以通过这三个点构建两个位移向量，我们构建$s_1=r_2-r_1,s_2=r_3-r_2$。这样我们就有了平面上的两个向量，我们记这两个向量的叉乘为$n$，那么$n$垂直于这个平面。所以$n$与平面上所有向量的点乘都为$0$。那么对于平面上的任意一个向量$r$，我们都有</p><script type="math/tex; mode=display">\vec{n}\cdot (\vec{r}-\vec{r_1})=0</script><p>下面我们确定平面的表达式：</p><p>对于$\vec{r}$，我们设$\vec{r}=x\vec{i}+y\vec{j}+z\vec{k}$，我们设$\vec{n}=a\vec{i}+b\vec{j}+c\vec{k}$，因为$\vec{n}$和$\vec{r_1}$已知，我们假设其内积为$d$，所以我们就有：</p><script type="math/tex; mode=display">ax+by+cz=d</script><p>即为平面的方程。</p><h3 id="向量代数"><a href="#向量代数" class="headerlink" title="向量代数"></a>向量代数</h3><h4 id="Kronecker-Delta-and-Levi-Civita-Symbol"><a href="#Kronecker-Delta-and-Levi-Civita-Symbol" class="headerlink" title="Kronecker Delta and Levi-Civita Symbol"></a>Kronecker Delta and Levi-Civita Symbol</h4><p>关于Kronecker Delta(克罗内克函数)：</p><script type="math/tex; mode=display">\delta_{ij} = \begin{cases}1,& i=j\\0,&i\neq j\end{cases}</script><p>Levi-Civita Symbol(列维-奇维塔符号)：</p><script type="math/tex; mode=display">\epsilon_{ijk}=\begin{cases}1,&(i,j,k) = (1,2,3),(2,3,1),(3,1,2)\\-1,&(i,j,k) = (3,2,1),(2,1,3),(1,3,2)\\0,&i=j,j=k,k=i\end{cases}</script><p>Einstein summation convention(爱因斯坦求和约定)：所谓爱因斯坦求和约定就是略去求和式中的求和号，如下式：</p><script type="math/tex; mode=display">\sum_{i=1}^3\delta_{ii}=\delta_{ii}=3</script><p>还有以下例子：</p><script type="math/tex; mode=display">\epsilon_{ijk}\epsilon_{ijk}=6</script><p>我们对此有一个有用的公式：</p><script type="math/tex; mode=display">\epsilon_{i j k} \epsilon_{l m n}=\left|\begin{array}{ccc}\delta_{i l} & \delta_{i m} & \delta_{i n} \\\delta_{j l} & \delta_{j m} & \delta_{j n} \\\delta_{k l} & \delta_{k m} & \delta_{k n}\end{array}\right|=\delta_{i l}\left(\delta_{j m} \delta_{k n}-\delta_{j n} \delta_{k m}\right)-\delta_{i m}\left(\delta_{j l} \delta_{k n}-\delta_{j n} \delta_{k l}\right)+\delta_{i n}\left(\delta_{j l} \delta_{k m}-\delta_{j m} \delta_{k l}\right)</script><p>根据上述关系，我们还可以推出下列两条性质：</p><script type="math/tex; mode=display">\epsilon_{ijk}\epsilon_{imn} = \delta_{jm}\delta_{kn}-\delta_{jn}\delta_{km},\quad \epsilon_{ijk}\epsilon_{ijn}=2\delta_{kn}</script><p>我们对上面的式子进行证明：</p><script type="math/tex; mode=display">\begin{aligned}\epsilon_{i j k} \epsilon_{i m n} &=\delta_{i i}\left(\delta_{j m} \delta_{k n}-\delta_{j n} \delta_{k m}\right)-\delta_{i m}\left(\delta_{j i} \delta_{k n}-\delta_{j n} \delta_{k i}\right)+\delta_{i n}\left(\delta_{j i} \delta_{k m}-\delta_{j m} \delta_{k i}\right) \\&=3\left(\delta_{j m} \delta_{k n}-\delta_{j n} \delta_{k m}\right)-\left(\delta_{j m} \delta_{k n}-\delta_{j n} \delta_{k m}\right)+\left(\delta_{j n} \delta_{k m}-\delta_{j m} \delta_{k n}\right) \\&=\delta_{j m} \delta_{k n}-\delta_{j n} \delta_{k m}\end{aligned}</script><p>将上述标记与求解内积和叉乘联系起来，我们有：</p><script type="math/tex; mode=display">\vec{A}\cdot\vec{B} = A_iB_i</script><script type="math/tex; mode=display">(\vec{A}\times \vec{B})_i = \epsilon_{ijk}A_iB_k</script><h4 id="Vector-identities"><a href="#Vector-identities" class="headerlink" title="Vector identities"></a>Vector identities</h4><ul><li>Scalar triple product：<script type="math/tex">\vec{A}\cdot(\vec{B}\times \vec{C})=\vec{B}\cdot(\vec{C}\times \vec{A})=\vec{C}\cdot(\vec{A}\times\vec{B})</script></li><li>Vector triple product：<script type="math/tex">\vec{A}\times(\vec{B}\times\vec{C})=(\vec{A}\cdot\vec{C})\vec{B}-(\vec{A}\cdot\vec{B})\vec{C}</script></li><li>Scalar quadruple product：<script type="math/tex">(\vec{A}\times\vec{B})\cdot(\vec{C}\times\vec{D})=(\vec{A}\cdot\vec{C})(\vec{B}\cdot\vec{D})</script></li><li>Vector quadruple product：<script type="math/tex">(\vec{A}\times\vec{B})\times(\vec{C}\times\vec{D})=((\vec{A}\times\vec{B})\cdot\vec{D})\vec{C}-((\vec{A}\times\vec{B})\cdot\vec{C})\vec{D}</script></li></ul><p>我们需要证明上述结论，在此之前我们先看一下我们现在已有的结论：</p><script type="math/tex; mode=display">\epsilon_{ijk}=\epsilon_{jki}=\epsilon_{kij}</script><script type="math/tex; mode=display">\epsilon_{ijk}\epsilon_{imn} = \delta_{jm}\delta_{kn}-\delta_{jn}\delta_{km}</script><script type="math/tex; mode=display">\delta_{ij}A_j = A_i</script><script type="math/tex; mode=display">\vec{A}\cdot\vec{B}=A_iB_i</script><script type="math/tex; mode=display">[\vec{A}\times\vec{B}]_i = \epsilon_{ijk}A_jB_k</script><p>我们首先证明第一条：</p><script type="math/tex; mode=display">\begin{aligned}\vec{A}\cdot(\vec{B}\times\vec{C}) &= A_i[\vec{B}\times\vec{C}]_i=A_i\epsilon_{ijk}B_jC_k\\&=B_j\epsilon_{jki}C_kA_i = B_j[\vec{C}\times \vec{A}]_i\\&=\vec{B}\cdot(\vec{C}\times\vec{A})\\&= C_k\epsilon_{kij}A_iB_j = \vec{C}\cdot(\vec{A}\times\vec{B})\end{aligned}</script><p>从行列式的角度来理解，我们很容易得到：</p><script type="math/tex; mode=display">\vec{A}\cdot(\vec{B}\times\vec{C})=\vec{A}\cdot\det\begin{bmatrix}\vec{i}&\vec{j}&\vec{k}\\B_1&B_2&B_3\\C_1&C_2&C_3\end{bmatrix} = \det\begin{bmatrix}A_1&A_2&A_3\\B_1&B_2&B_3\\C_1&C_2&C_3\end{bmatrix}</script><p>所以性质一相当于交换行列式的行两次，相当于没变。同时该式子可以用来理解行列式的值的绝对值为体积，因为行列式的值相当于先求叉乘，得到方向与高相同或相反，大小等于底面积的向量，然后与另一个向量做内积相当于乘以高。</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;向量微积分课程第一周的内容，不过还没学完，明天再学吧，我都快学死了。&lt;/p&gt;</summary>
    
    
    
    <category term="数学" scheme="https://www.hfcouc.work/categories/%E6%95%B0%E5%AD%A6/"/>
    
    
    <category term="向量微积分" scheme="https://www.hfcouc.work/tags/%E5%90%91%E9%87%8F%E5%BE%AE%E7%A7%AF%E5%88%86/"/>
    
  </entry>
  
  <entry>
    <title>数字图像处理基本知识</title>
    <link href="https://www.hfcouc.work/2022/01/04/%E6%95%B0%E5%AD%97%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86%E5%9F%BA%E6%9C%AC%E7%9F%A5%E8%AF%86/"/>
    <id>https://www.hfcouc.work/2022/01/04/%E6%95%B0%E5%AD%97%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86%E5%9F%BA%E6%9C%AC%E7%9F%A5%E8%AF%86/</id>
    <published>2022-01-04T14:39:02.000Z</published>
    <updated>2022-01-05T11:25:34.932Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="\assets\js\Meting.min.js"></script><p>数字图像处理的基本知识。</p><span id="more"></span><h2 id="数字图像处理基本知识"><a href="#数字图像处理基本知识" class="headerlink" title="数字图像处理基本知识"></a>数字图像处理基本知识</h2><h3 id="数字图像和图像处理"><a href="#数字图像和图像处理" class="headerlink" title="数字图像和图像处理"></a>数字图像和图像处理</h3><p>图像可以表示为下述函数</p><script type="math/tex; mode=display">I = f(x,y,z,t)</script><p>一般我们考虑的图像是一个平面的，静止的图像，即</p><script type="math/tex; mode=display">I = f(x,y)</script><p>如果图像的$x,y$都是连续的，那么它就为<strong>模拟图像</strong>，然而在电脑的存储中不可能是连续的，而是<strong>离散的</strong>，称为<strong>数字图像</strong>。我们对$(x,y)$进行离散化称为取样，而对$f(x,y)$进行离散化称为量化，之后得到的图像称为数字图像。</p><p>对于一维的数据采样，采样的时间间隔$\Delta t\le \frac{1}{2f}$(香农定理)。</p><p>而对于我们的二维的数字图像，我们的采样要求：</p><script type="math/tex; mode=display">\begin{aligned}\Delta x&\le \frac{1}{2w_u}\\\Delta y&\le\frac{1}{2w_v}\end{aligned}</script><p>其中$w_u,w_v$为空间的变化频率，相当于灰度在$x,y$轴上变化的快慢。</p><p>图像分类：</p><ul><li>灰度图像</li><li>二值图像</li><li>RGB彩色图像</li></ul><h3 id="像素间的基本关系"><a href="#像素间的基本关系" class="headerlink" title="像素间的基本关系"></a>像素间的基本关系</h3><h4 id="邻域"><a href="#邻域" class="headerlink" title="邻域"></a>邻域</h4><p>坐标为$(x,y)$的像素$p$有两个垂直和两个水平邻近像素。</p><script type="math/tex; mode=display">(x+1, y), (x-1, y), (x,y+1), (x,y-1)</script><p>称为$p$的四邻域，表示为$N_4(p)$。</p><p>四个对角线邻域为：</p><script type="math/tex; mode=display">(x+1,y+1),(x+1,y-1),(x-1,y+1),(x-1,y-1)</script><p>称为$p$的对角线邻域，表示为$N_D(p)$。这八个点称为$p$的$8$邻域，表示为$N_8(p)$。点$p$的邻域的图像位置集合称为$p$的邻域。邻域如果包含$p$​则被称为闭的，反之开的。</p><h4 id="邻接与连通"><a href="#邻接与连通" class="headerlink" title="邻接与连通"></a>邻接与连通</h4><p>邻接必要条件：</p><ol><li>两像素是邻域</li><li>两像素的灰度值满足特定的相似规则</li></ol><p>假设$V$​​是用于定义邻接关系的一组灰度值。在二值图像中，如果我们指的是值为$1$的像素的邻接关系，则$V=\{1\}$。</p><p>我们将邻接关系分为$3$类：</p><ol><li>$4$邻接：两个像素$p$和$q$的值都来自$V$并且$q$位于$p$的$4$​邻域内。</li><li>$8$​邻接：两个像素$p$​和$q$​的值都来自$V$​并且$q$​位于$p$​的$8$​邻域内。</li><li>$m$邻接(也被称为混合邻接)：两个像素$p$和$q$的值都来自$V$并且<ul><li>$q$在$p$的$4$邻域内，或者</li><li>$q\in N_D(p)$并且$N_4(p)\cap N_4(q)$没有灰度值来自$V$​的像素。</li></ul></li></ol><p>那么$m$邻接和$8$邻接有什么不同呢？$m$邻接(混合邻接)是$8$邻接的改进，它的引入是为了消除使用$8$邻接可能导致的歧义。</p><p>如下：</p><p><img src="https://static01.imgkr.com/temp/a72cde8a2a644be6827e321f8217b96e.png" alt=""></p><p>$8$邻接有两条通路，形成了闭环，而$m$邻接则不存在此方面问题。</p><p>通路：</p><p>假设在点$p(x_0,y_0)$和点$q(x_{n+1},y_{n+1})$之间有一系列像素：</p><script type="math/tex; mode=display">(x_1,y_1),\cdots,(x_n,y_n)</script><p>并且$(x_{i-1},y_{i-1})$和$(x_i,y_i)$之间是邻接的，则说$p,q$是连通的，称为一条通路。如果$p,q$为同一个点，则称其为闭合通路。</p><p>如果$p$和$q$之间存在通路，则称$p$与$q$是连通的。对于一个像素集合$S$，如果$S$中所有像素都是连通的，则称$S$为连通集。</p><h4 id="距离度量"><a href="#距离度量" class="headerlink" title="距离度量"></a>距离度量</h4><p>对于像素点$p,q,s$，坐标分别为$(x,y),(u,v),(w,z)$，$D$为距离函数或度量如果</p><ol><li>$D(p,q)\ge 0, D(p,q)=0\iff p=q$</li><li>$D(p,q) = D(q,p)$</li><li>$D(p,s)\le D(p,q)+D(q,s)$</li></ol><p>则称$D$为距离或度量函数。</p><p>几种常用距离：</p><ol><li>$D_e(p, q) = [(x-u)^2 + (y-v)^2]^{1/2}$</li><li>$D_4(p,q) = |x-u|+|y-v|$</li><li>$D_8(p,q) = \max(|x-u|, |y-v|)$</li></ol><h3 id="数字图像的存储与读写"><a href="#数字图像的存储与读写" class="headerlink" title="数字图像的存储与读写"></a>数字图像的存储与读写</h3><h4 id="数字图像文件的存储格式"><a href="#数字图像文件的存储格式" class="headerlink" title="数字图像文件的存储格式"></a>数字图像文件的存储格式</h4><p>BMP格式(由4部分组成)：</p><ol><li>文件头</li><li>信息头</li><li>文件调色板</li><li>数据</li></ol><p>JPEG格式：有损压缩格式，颜色支持$24$位。</p><p>GIF格式：无损压缩格式</p><p>TIFF格式：非失真的压缩格式。</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;数字图像处理的基本知识。&lt;/p&gt;</summary>
    
    
    
    <category term="图像处理" scheme="https://www.hfcouc.work/categories/%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86/"/>
    
    
    <category term="数字图像处理" scheme="https://www.hfcouc.work/tags/%E6%95%B0%E5%AD%97%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86/"/>
    
  </entry>
  
  <entry>
    <title>条件随机场</title>
    <link href="https://www.hfcouc.work/2022/01/02/%E6%9D%A1%E4%BB%B6%E9%9A%8F%E6%9C%BA%E5%9C%BA/"/>
    <id>https://www.hfcouc.work/2022/01/02/%E6%9D%A1%E4%BB%B6%E9%9A%8F%E6%9C%BA%E5%9C%BA/</id>
    <published>2022-01-02T09:26:55.000Z</published>
    <updated>2022-01-02T09:51:50.627Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="\assets\js\Meting.min.js"></script><p>条件随机场感觉比HMM要难，虽然我看完了一遍，但是有些地方感觉还是云里雾里的，之后可能还得更详细的阅读。另外条件随机场需要由标注的数据，另外其参数设置也较复杂，实际应用中可能比较难。</p><span id="more"></span><h2 id="条件随机场"><a href="#条件随机场" class="headerlink" title="条件随机场"></a>条件随机场</h2><h3 id="HMM-VS-MEMM"><a href="#HMM-VS-MEMM" class="headerlink" title="HMM VS MEMM"></a>HMM VS MEMM</h3><p><img src="https://static01.imgkr.com/temp/0157b60e0d0d4391aeeaf10e3624b754.png" alt=""></p><p>HMM两大假设：</p><ol><li>齐次一阶马尔可夫假设：齐次指的是状态转移矩阵不变，与时间无关；一次指的是每一时刻的状态只与前一时刻的状态有关：$P(y_t|y_{1:t-1},x_{1:t-1})=P(y_t|y_{t-1})$</li><li>观测独立假设：$P(x_t|y_{1:t},x_{1:t-1})=P(x_t|y_t)$</li></ol><p>HMM是一个生成模型，其建模对象是$P(X,Y|\lambda)$。</p><p><img src="https://static01.imgkr.com/temp/a96d06aa1b7e405a95da3179f24e0853.png" alt=""></p><script type="math/tex; mode=display">\begin{aligned}P(X,Y|\lambda) &= P(x_{1:T},y_{1:T}|\lambda)\\&= P(x_T|x_{1:T-1},y_{1:T})P(y_T|y_{1:T-1},x_{1:T-1})P(y_{1:T-1},x_{1:T-1})\\&= P(x_T|y_T)P(y_T|y_{T-1})P(y_{1:T-1},x_{1:T-1})\\&\vdots\\&= \pi(y_1)P(x_1|y_1)\prod_{t=2}^{T}P(x_i|y_i)P(y_i|y_{i-1})\end{aligned}</script><p>MEMM打破了HMM的观测独立假设，如下图：</p><p><img src="https://static01.imgkr.com/temp/b4f4591ebb7446308706ef437ed0407d.png" alt=""></p><p>这时一个V型结构，<code>head-to-head</code>，在给定$Y_{t-1}$的条件下它们就不独立了。</p><blockquote><p>对于一个有向无环图，我们可以这样判断它的结点之间的关系：</p><p>形式一：<code>head-to-head</code></p><p><img src="https://static01.imgkr.com/temp/2e42113a75c2446bb147e07c1398155a.png" alt=""></p><p>在<strong>$c$未知的情况下，$a,b$是阻断的，是独立的，而在$c$已知的情况下，$a,b$是连通的，不独立。</strong>表示为：</p><script type="math/tex; mode=display">P(abc) = P(a)P(b)P(c|ab)</script><p>形式二：<code>tail-to-tail</code></p><p><img src="https://static01.imgkr.com/temp/e9bb6fbfe0d44673ab0437234f980389.png" alt=""></p><p>在<strong>$c$给定的条件下，$a,b$是被阻断的，是独立的</strong>，表示为</p><script type="math/tex; mode=display">P(abc) = P(a)P(c|a)P(b|ac) = P(a)P(c|a)P(b|c)</script><p>形式三：<code>head-to-tail</code></p><p><img src="https://static01.imgkr.com/temp/855b90da142e483abda8bf92a8da86bd.png" alt=""></p><p>在<strong>$c$给定的情况下，$a,b$是被阻断的，是独立的，</strong>表示为：</p><script type="math/tex; mode=display">P(abc) = P(a)P(c|a)P(b|c)</script></blockquote><p>MEMM对应于第一种情况，而HMM对应于第二种情况。</p><p>MEMM为判别模型，其是对条件概率建模：</p><script type="math/tex; mode=display">P(Y|X,\lambda) = \prod_{t=1}^TP(y_t|y_{t-1},x_{1:T},\lambda)</script><h3 id="MEMM-VS-CRF"><a href="#MEMM-VS-CRF" class="headerlink" title="MEMM VS CRF"></a>MEMM VS CRF</h3><p>MEMM会造成标注偏差问题。</p><blockquote><p>具体的我也没搞懂</p></blockquote><p>解决这个问题的方法是将有向图转换为了无向图：</p><p><img src="https://static01.imgkr.com/temp/d80727609dc342fd89101cc7d7d079be.png" alt=""></p><h3 id="CRF"><a href="#CRF" class="headerlink" title="CRF"></a>CRF</h3><p>条件随机场：</p><ul><li>条件：判别模型$P(Y|X)$</li><li>随机场：无向图模型</li></ul><p>定义(条件随机场)：设$X$与$Y$是随机变量，$P(Y|X)$是在给定$X$的条件下$Y$的条件概率分布。若随机变量$Y$构成一个由无向图$G=(V,E)$表示的马尔可夫随机场，即</p><script type="math/tex; mode=display">P(Y_v|X,Y_w,w\neq v) = P(Y_v|X,Y_w,w\sim v)</script><p>对任意结点$v$成立，则称条件概率分布$P(Y|X)$为条件随机场。式中$w\sim v$表示在图$G=(V,E)$中结点$v$有边连接的所有结点$w$，$w\neq v$表示结点$v$以外的所有结点。</p><p>在定义中并没有要求$X$和$Y$具有相同的结构。现实中，一般假设$X$和$Y$具有相同的图结构。我们在这里考虑的无向图为线性图，即</p><script type="math/tex; mode=display">G = (V=\{1,2,\cdots,n\},E = \{(i,i+1)\}),\quad i=1,2,\cdots,n-1</script><p>在此情况下，$X=(X_1,X_2,\cdots,X_n),Y=(Y_1,Y_2,\cdots,Y_n)$，最大团是相邻两个结点的集合。</p><p><img src="https://static01.imgkr.com/temp/0242442061844f8f8f51a69be9c8f7b3.png" alt=""></p><blockquote><p>$X$和$Y$有相同的图结构的线性链条条件随机场</p></blockquote><p>定义(线性链条件随机场)：设$X=(X_1,X_2,\cdots,X_n),Y=(Y_1,Y_2,\cdots,Y_n)$均为线性链表示的随机变量序列，若在给定随机变量序列$X$的条件下，随机变量序列$Y$的条件概率分布$P(Y|X)$构成条件随机场，即满足马尔可夫性</p><script type="math/tex; mode=display">P(Y_i|X,Y_1,\cdots,Y_{i-1},Y_{i+1},\cdots,Y_n) = P(Y_i|X,Y_{i-1},Y_{i+1})\quad i=1,2,\cdots,n</script><p>在$i=1,n$时只考虑单边。</p><h4 id="条件随机场的参数化形式"><a href="#条件随机场的参数化形式" class="headerlink" title="条件随机场的参数化形式"></a>条件随机场的参数化形式</h4><p>定义(线性链条件随机场的参数化形式)：设$P(Y|X)$为线性链条件随机场，则在随机变量$X$取值为$x$的条件下，随机变量$Y$取值为$y$的条件概率具有如下形式：</p><script type="math/tex; mode=display">p(y|x) = \frac{1}{Z(x)}\exp\left(\sum_{i,k}\lambda_kt_k(y_{i-1},y_i,x,i)+\sum_{i,l}\mu_ls_l(y_i,x,i)\right)</script><p>其中</p><script type="math/tex; mode=display">Z(x)=\sum_y\exp\left(\sum_{i,k}\lambda_kt_k(y_{i-1},y_i,x,i)+\sum_{i,l}\mu_ls_l(y_i,x,i)\right)</script><p>式中，$t_k$和$s_l$是特征函数，$\lambda_k$和$\mu_l$是对应的权值。$Z(x)$是规范化因子，求和是在所有可能的输出序列上进行的。</p><p>$t_k$是定义在边上的特征函数，称为转移特征，依赖于当前和前一个位置；$s_l$是定义在结点的特征函数，称为状态特征，依赖于当前位置。$t_k$和$s_l$都依赖于位置，是局部特征函数。通常，特征函数$t_k$和$s_l$取值为$1$或$0$；当满足特征条件时取值为$1$，否则为$0$。条件随机场完全由特征函数$t_k,s_l$和对应的权值$\lambda_k,\mu_l$确定。</p><h4 id="条件随机场的简化形式"><a href="#条件随机场的简化形式" class="headerlink" title="条件随机场的简化形式"></a>条件随机场的简化形式</h4><p>为了简便起见，首先将转移特征和状态特征及其权值用统一的符号表示。设有$K_1$个转移特征，$K_2$个状态特征，$K=K_1+K_2$，记</p><script type="math/tex; mode=display">f_k(y_{i-1},y_i,x,i) = \begin{cases}t_k(y_{i-1},y_i,x,i),\quad &k=1,2,\cdots,K_1\\s_l(y_i,x,i),&k=K_1+l;l=1,2,\cdots,K_2\end{cases}</script><p>然后，对转移与状态特征在各个位置$i$求和，记作</p><script type="math/tex; mode=display">f_k(y,x)=\sum_{i=1}^nf_k(y_{i-1},y_i,x,i),\quad k=1,2,\cdots,K</script><p>用$w_k$表示特征$f_k(y,x)$的权值，即</p><script type="math/tex; mode=display">w_k = \begin{cases}\lambda_k,\quad &k=1,2,\cdots,K_1\\\mu_l,&k=K_1+l;l=1,2,\cdots,K_2\end{cases}</script><p>于是我们的条件随机场简化为：</p><script type="math/tex; mode=display">\begin{aligned}P(y|x) &= \frac{1}{Z(x)}\exp\sum_{k=1}^Kw_kf_k(y,x)\\Z(x)&= \sum_y\exp\sum_{k=1}^Kw_kf_k(y,x)\end{aligned}</script><p>若以$w$表示权重向量，即</p><script type="math/tex; mode=display">w = (w_1,w_2,\cdots,w_K)^T</script><p>以$F(y,x)$表示全局特征向量，即</p><script type="math/tex; mode=display">F(y,x) = (f_1(y,x),f_2(y,x),\cdots,f_K(y,x))^T</script><p>则条件随机场可以写成向量$w$与$F(y,x)$的内积形式：</p><script type="math/tex; mode=display">P_w(y|x) = \frac{\exp(w\bullet F(y,x))}{Z_m(x)}</script><p>其中</p><script type="math/tex; mode=display">Z_w(x) = \sum_y\exp(w\bullet F(y,x))</script><h4 id="条件随机场的矩阵形式"><a href="#条件随机场的矩阵形式" class="headerlink" title="条件随机场的矩阵形式"></a>条件随机场的矩阵形式</h4><p>对观测序列$x$的每一个位置$i=1,2,\cdots,n+1$，由于$y_{i-1}$和$y_i$在$m$个标记中取值，可以定义一个$m$阶矩阵随机变量</p><script type="math/tex; mode=display">M_i(x) = [M_i(y_{i-1},y_i|x)]</script><p>矩阵随机变量的元素为</p><script type="math/tex; mode=display">\begin{aligned}M_i(y_{i-1},y_i|x)&= \exp(W_i(y_{i-1},y_i|x))\\W_i(y_{i-1},y_i|x)&= \sum_{k=1}^Kw_kf_k(y_{i-1},y_i,x,i)\end{aligned}</script><p>这样，给定观测序列$x$，相应标记序列$y$的非规范概率可以通过该序列$n+1$个矩阵的适当元素的乘积$\prod_{i=1}^{n+1}M_i(y_{i-1},y_i|x)$表示。于是，条件概率$P_w(y|x)$是</p><script type="math/tex; mode=display">P_w(y|x) = \frac{1}{Z_w(x)}\prod_{i=1}^{n+1}M_i(y_{i-1},y_i|x)</script><p>其中，$Z_w(x)$为规范化因子，是$n+1$个矩阵的乘积的元素，即</p><script type="math/tex; mode=display">Z_w(x) = [M_1(x)M_2(x)\cdots M_{n+1}(x)]_{\operatorname{start,stop}}</script><p>注意，$y_0=\operatorname{start}$与$y_{n+1}=\operatorname{stop}$表示开始状态和终止状态，规范化因子$Z_w(x)$是以$\operatorname{start}$为起点$\operatorname{stop}$为终点通过状态的所有路径$y_1y_2\cdots y_n$的非规范化概率$\prod_{i=1}^{n+1}M_i(y_{i-1},y_i|x)$之和。</p><h4 id="CRF要解决的问题"><a href="#CRF要解决的问题" class="headerlink" title="CRF要解决的问题"></a>CRF要解决的问题</h4><ul><li>学习问题：求解参数：$\hat{\theta}=\arg\max\prod_{i=1}^NP(y^{(i)}|x^{(i)})$</li><li>推断</li></ul><p>我们先讲推断问题：我们现在是假定参数已经学习好了，然后我们求解边缘概率：$P(y_t=i|x)$。</p><h4 id="推断"><a href="#推断" class="headerlink" title="推断"></a>推断</h4><h5 id="前向-后向算法"><a href="#前向-后向算法" class="headerlink" title="前向-后向算法"></a>前向-后向算法</h5><p>对每个指标$i=0,1,\cdots,n+1$，定义前向向量$\alpha_i(x)$：</p><script type="math/tex; mode=display">\alpha_0(y|x) = \begin{cases}1,\quad &y=\operatorname{start}\\0,&\text{否则}\end{cases}</script><p>递推公式为：</p><script type="math/tex; mode=display">\alpha_i^T(y_i|x) = \alpha_{i-1}^T(y_{i-1}|x)[M_i(y_{i-1},y_i|x)],\quad i=1,2,\cdots,n+1</script><p>又可以表示为</p><script type="math/tex; mode=display">\alpha_i^T(x) = \alpha_{i-1}^T(x)M_i(x)</script><p>因为$y_i$可取的值有$m$个，所以$\alpha_i(x)$是$m$维列向量。</p><p>同样，对每个指标$i=0,1,\cdots,n+1$，定义后向向量$\beta_i(x)$：</p><script type="math/tex; mode=display">\beta_{n+1}(y_{n+1}|x)=\begin{cases}1,\quad &y_{n+1}=\text{stop}\\0,&\text{否则}\end{cases}</script><script type="math/tex; mode=display">\beta_i(y_i|x) = [M_{i+1}(y_i,y_{i+1}|x)]\beta_{i+1}(y_{i+1}|x)</script><p>又可以表示为：</p><script type="math/tex; mode=display">\beta_i(x) = M_{i+1}(x)\beta_{i+1}(x)</script><h5 id="概率计算"><a href="#概率计算" class="headerlink" title="概率计算"></a>概率计算</h5><p>按照前向-后向向量的定义，很容易计算出标记序列在位置$i$是标记$y_i$的条件概率和在位置$i-1$与$i$是标记$y_{i-1}$和$y_i$的条件概率：</p><script type="math/tex; mode=display">P(Y_i=y_i|x) = \frac{\alpha_i^T(y_i|x)\beta_i(y_i|x)}{Z(x)}</script><script type="math/tex; mode=display">P(Y_{i-1}=y_{i-1},Y_i=y_i|x) = \frac{\alpha_{i-1}^T(y_{i-1}|x)M_i(y_{i-1},y_i|x)\beta_i(y_i|x)}{Z(x)}</script><p>其中</p><script type="math/tex; mode=display">Z(x)=\alpha_n^T(x)\textbf{1} = \textbf{1}\beta_1(x)</script><h5 id="期望值的计算"><a href="#期望值的计算" class="headerlink" title="期望值的计算"></a>期望值的计算</h5><p>利用前向-后向向量，可以计算特征函数关于联合分布$P(X,Y)$和条件分布$P(Y|X)$的数学期望。</p><p>特征函数$f_k$关于条件分布$P(Y|X)$的数学期望是</p><script type="math/tex; mode=display">\begin{aligned}E_{P(Y|X)}[f_k] &= \sum_yP(y|x)f_k(y,x)\\&=\sum_{i=1}^{n+1}\sum_{y_{i-1}y_i}f_k(y_{i-1},y_i,x,i)\frac{\alpha_{i-1}^T(y_{i-1}|x)M_i(y_{i-1},y_i|x)\beta_i(y_i|x)}{Z(x)}\end{aligned}</script><p>其中</p><script type="math/tex; mode=display">Z(x) = \alpha_n^T(x)\textbf{1}</script><p>假设经验分布为$\tilde{P}(X)$，特征函数$f_k$关于联合分布$P(X,Y)$的数学期望是</p><script type="math/tex; mode=display">\begin{aligned}E_{P(X, Y)}\left[f_{k}\right] &=\sum_{x, y} P(x, y) \sum_{i=1}^{n+1} f_{k}\left(y_{i-1}, y_{i}, x, i\right) \\&=\sum_{x} \tilde{P}(x) \sum_{y} P(y \mid x) \sum_{i=1}^{n+1} f_{k}\left(y_{i-1}, y_{i}, x, i\right) \\&=\sum_{x} \tilde{P}(x) \sum_{i=1}^{n+1} \sum_{y_{i-1} y_{i}} f_{k}\left(y_{i-1}, y_{i}, x, i\right) \frac{\alpha_{i-1}^{\mathrm{T}}\left(y_{i-1} \mid x\right) M_{i}\left(y_{i-1}, y_{i} \mid x\right) \beta_{i}\left(y_{i} \mid x\right)}{Z(x)}\end{aligned}</script><h4 id="学习"><a href="#学习" class="headerlink" title="学习"></a>学习</h4><p>在这里我们用如下向量表示条件随机场：</p><script type="math/tex; mode=display">p(y|x) = \frac{1}{Z(x,\lambda,\eta)}\exp\left(\sum_{t=1}^T\left[\lambda^Tf(y_{t_{t-1}},y_t,x)+\eta^Tg(y_t,x)\right]\right)</script><p>这里一个$x$对应于一个样本，一个样本就对应于一个序列。</p><p>所以我们要优化的对象为：</p><script type="math/tex; mode=display">\begin{aligned}\arg\max_{\lambda,\eta}\log\prod_{i=1}^NP(y^{(i)}|x^{(i)}) &= \arg\max_{\lambda,\eta}\sum_{i=1}^N\log P(y^{(i)}|x^{(i)})\\&=\arg\max_{\lambda,\eta}\sum_{i=1}^N\left(-\log Z(x^{(i)},\lambda,\eta)+\sum_{t=1}^T\left[\lambda^Tf(y_{t-1}^{(i)},y_t^{(i)},x^{(i)})+\eta^Tg(y_t^{(i)},x^{(i)})\right]\right)\\&= \arg\max_{\lambda,\eta}L(\lambda,\eta,x^{(i)})\end{aligned}</script><p>由于条件随机场是一个有监督的学习问题，所以我们知道$x,y$的值，对上述的式子我们可以采用梯度上升的方法求解，求解$\nabla_\lambda L,\nabla_\eta L$。</p><p>我们有：</p><script type="math/tex; mode=display">\begin{aligned}\nabla_\lambda L&= \sum_{i=1}^N\left(\sum_{t=1}^Tf(y_{t-1}^{(i)},y_t^{(i)},x^{(i)})\right)-\nabla_\lambda\log Z(x^{(i)},\lambda,\eta)\end{aligned}</script><p>而$\nabla_\lambda\log Z(x^{(i)},\lambda,\eta)$为<code>log-partition function</code>，它的导数为：</p><script type="math/tex; mode=display">\mathbb{E}_y\left[\sum_{t=1}^Tf(y_{t-1},y_t,x^{(i)})\right]</script><blockquote><p>下面我们来看一下<code>log-partition function</code>的求导：</p><script type="math/tex; mode=display">\begin{aligned}p(x;\theta) &= \exp\left(\theta^T\phi(x)-\Psi(\theta)\right)\\&=\frac{\exp\left(\theta^T\phi(x)\right)}{\exp(\Psi(\theta))}\end{aligned}</script><p>而我们的</p><script type="math/tex; mode=display">\Psi(\theta) = \log\sum_x\exp(\theta^T\phi(x))</script><p>相当于归一化因子。</p><p>我们对归一化因子的对数求导，即我们问题中的$\log Z(x^{(i)},\lambda,\eta)$求导，得</p><script type="math/tex; mode=display">\begin{aligned}\frac{\partial \Psi}{\partial \theta_{k}} &=\frac{\sum_{x} \exp \left\{\boldsymbol{\theta}^{T} \boldsymbol{\phi}(\boldsymbol{x})\right\} \phi_{k}(\boldsymbol{x})}{\sum_{x} \exp \left\{\boldsymbol{\theta}^{T} \boldsymbol{\phi}(\boldsymbol{x})\right\}} \\&=\frac{\sum_{x} \exp \left\{\boldsymbol{\theta}^{T} \boldsymbol{\phi}(\boldsymbol{x})\right\} \phi_{k}(\boldsymbol{x})}{\exp \Psi(\boldsymbol{\theta})} \\&=\sum_{\boldsymbol{x}} \exp \left\{\boldsymbol{\theta}^{T} \boldsymbol{\phi}(\boldsymbol{x})-\Psi(\boldsymbol{\theta})\right\} \phi_{k}(x) \\&=\sum_{\boldsymbol{x}} p(\boldsymbol{x} ; \boldsymbol{\theta}) \phi_{k}(\boldsymbol{x}) \\&=\mathbb{E}\left\{\phi_{k}(\boldsymbol{X})\right\} .\end{aligned}</script><p>即与我们上面得结果相同。</p></blockquote><p>所以</p><script type="math/tex; mode=display">\nabla_\lambda L= \sum_{i=1}^N\left(\sum_{t=1}^Tf(y_{t-1}^{(i)},y_t^{(i)},x^{(i)})\right) -\mathbb{E}_y\left[\sum_{t=1}^Tf(y_{t-1},y_t,x^{(i)})\right]</script><p>而期望的求法我们在上文中已经解决了。</p><h3 id="条件随机场的预测算法"><a href="#条件随机场的预测算法" class="headerlink" title="条件随机场的预测算法"></a>条件随机场的预测算法</h3><p>我们要找出使得$P_w(y|x)$最大的$y$。</p><script type="math/tex; mode=display">\begin{aligned}y^\star &= \arg\max_y P_w(y|x)\\&= \arg\max_y\frac{\exp(w\bullet F(y,x))}{Z_w(x)}\\&= \arg\max_y\exp(w\bullet F(y,x))\\&= \arg\max_y(w\bullet F(y,x))\end{aligned}</script><p>于是，条件随机场的预测问题称为求非规范化概率的最优路径问题：</p><script type="math/tex; mode=display">\max_y(w\bullet F(y,x))</script><p>这里，路径表示标记序列。其中</p><script type="math/tex; mode=display">w = (w_1,w_2,\cdots,w_K)^T</script><script type="math/tex; mode=display">F(y,x) = (f_1(y,x),f_2(y,x),\cdots,f_K(y,x))^T</script><script type="math/tex; mode=display">f_k(y,x) = \sum_{i=1}^nf_k(y_{i-1},y_i,x,i),\quad k=1,2,\cdots,K</script><p>为了求解最优路径，我们求解下式：</p><script type="math/tex; mode=display">\max_y\sum_{i=1}^nw\bullet F_i(y_{i-1},y_i,x)</script><p>其中</p><script type="math/tex; mode=display">F_i(y_{i-1},y_i,x) = (f_1(y_{i-1},y_i,x,i),f_2(y_{i-1},y_i,x,i),\cdots,f_K(y_{i-1},y_i,x,i))^T</script><p>是局部特征向量。</p><p>下面我们用维比特算法。首先求出位置$1$的各个标记$j=1,2,\cdots,m$的非规范化概率：</p><script type="math/tex; mode=display">\delta_1(j) = w\bullet F_1(y_0=\operatorname{start},y_1=j,x),\quad j=1,2,\cdots,m</script><p>一般地，由递推公式，求出到位置$i$的各个标记$l=1,2,\cdots,m$的非规范化概率的最大值，同时记录非规范化概率最大值的路径</p><script type="math/tex; mode=display">\delta_i(l) = \max_{1\le j\le m}\{\delta_{i-1}(j)+w\bullet F_i(y_{i-1}=j,y_i=l,x)\},l=1,2,\cdots,m</script><script type="math/tex; mode=display">\Psi_i(l) = \arg\max_{1\le j\le m}\{\delta_{i-1}(j)+w\bullet F_i(y_{i-1}=j,y_i=l,x)\},l=1,2,\cdots,m</script><p>直到$i=n$时终止。这时求得非规范化概率的最大值为</p><script type="math/tex; mode=display">\max_y(w\bullet F(y,x)) = \max_{i\le j\le m}\delta_n(j)</script><p>及最优路径的终点</p><script type="math/tex; mode=display">y_n^\star = \arg\max_{1\le j\le m}\delta_n(j)</script><p>由此最优路径返回</p><script type="math/tex; mode=display">y_i^\star=\Psi_{i+1}(y^\star_{i+1}),\quad i=n-1,n-2,\cdots,1</script><p>求得最优路径$y^\star = (y_1^\star,y_2^\star,\cdots,y^\star_n)^T$。</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;条件随机场感觉比HMM要难，虽然我看完了一遍，但是有些地方感觉还是云里雾里的，之后可能还得更详细的阅读。另外条件随机场需要由标注的数据，另外其参数设置也较复杂，实际应用中可能比较难。&lt;/p&gt;</summary>
    
    
    
    <category term="算法" scheme="https://www.hfcouc.work/categories/%E7%AE%97%E6%B3%95/"/>
    
    
    <category term="机器学习" scheme="https://www.hfcouc.work/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>连续观测数据下的HMM</title>
    <link href="https://www.hfcouc.work/2021/12/30/%E8%BF%9E%E7%BB%AD%E8%A7%82%E6%B5%8B%E6%95%B0%E6%8D%AE%E4%B8%8B%E7%9A%84HMM/"/>
    <id>https://www.hfcouc.work/2021/12/30/%E8%BF%9E%E7%BB%AD%E8%A7%82%E6%B5%8B%E6%95%B0%E6%8D%AE%E4%B8%8B%E7%9A%84HMM/</id>
    <published>2021-12-30T13:23:41.000Z</published>
    <updated>2022-01-02T09:51:00.447Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="\assets\js\Meting.min.js"></script><p>之前的HMM都是在离散观测的条件下，现在我们研究一下观测值连续时的HMM。</p><span id="more"></span><h2 id="连续观测下的HMM"><a href="#连续观测下的HMM" class="headerlink" title="连续观测下的HMM"></a>连续观测下的HMM</h2><p>在连续观测下，观测$o_t$是联系变量，矩阵$B$被概率密度函数代替。在给定状态$s_j$下的观测$o_t$的概率密度函数为：</p><script type="math/tex; mode=display">b_j(o_t) = p_j(o_t|\theta_j)</script><p>现在HMM的参数$\lambda=(a_{ij},\theta_j,\pi_j)$被称为连续观测HMM。</p><p>现在在连续的情况下，我们的前向概率变为：</p><script type="math/tex; mode=display">\alpha_1(i) = \pi_ip_i(o_1|\theta_i)</script><script type="math/tex; mode=display">\alpha_{t+1}(j) = \left(\sum_{i=1}^N a_{ij}\alpha_t(i)\right)p_j(o_{t+1}|\theta_j)</script><p>我们的后向概率为：</p><script type="math/tex; mode=display">\beta_T(i)=1\quad i=1,\cdots,N</script><script type="math/tex; mode=display">\beta_t(i) = \sum_{j=1}^Na_{ij}p_j(o_{t+1}|\theta_j)\beta_{t+1}(j)</script><p>我们有：</p><script type="math/tex; mode=display">P(O|\lambda) = \sum_{i}^N\alpha_t(i)\beta_t(i)</script><p>但是它的学习算法发生了改变。我们现在是要估计$\hat{\lambda} = (\hat{\alpha}_{ij},\hat{\theta}_j,\hat{\pi}_j)$，计算$\hat{\alpha}_{ij}$和$\hat{\pi}_j$的公式不变，为：</p><script type="math/tex; mode=display">\hat{a}_{ij} = \frac{\sum_{t=1}^{T-1}\alpha_t(i)\beta_{t+1}(j)a_{ij}p_j(o_{t+1}|\theta_j)}{\sum_{t=1}^{T-1}\alpha_t(i)\beta_t(i)}</script><script type="math/tex; mode=display">\hat{\pi}_i = \frac{\alpha_1(i)\beta_1(i)}{\sum_{i}^N\alpha_1(i)\beta_1(i)}</script><p>但是$o_t$为连续变量，所以对于确定的$o_t$其概率为$0$。所以我们必须在其$\epsilon$邻域内计算其概率，$\epsilon$事先指定为一个很小的数值。如果$p_j(o_t|\theta_j)$为正态分布，那么计算其$\epsilon$邻域内的概率将非常容易：</p><script type="math/tex; mode=display">\int_{o_t-\epsilon}^{o_t+\epsilon}p_j(o|\theta_j)do = \Phi\left(\frac{o_t+\epsilon-m_j}{\sigma_j}\right) - \Phi\left(\frac{o_t-\epsilon-m_j}{\sigma_j}\right)</script><p>下面我们就需要求解$\hat{\theta}$，我们有：</p><script type="math/tex; mode=display">\hat{\theta}=\arg\max_{\theta}\sum_{t=1}^T\sum_{i=1}^N\log p_i(o_t|\theta_i)\alpha_t(i)\beta_t(i)</script><p>对于一个确定的$\theta_j$，上式变为：</p><script type="math/tex; mode=display">\hat{\theta}_j = \arg\max_{\theta_j}\sum_{t=1}^T\log p_j(o_t|\theta_j)\alpha_t(j)\beta_t(j)</script><p>如果是正态分布，我们有：</p><script type="math/tex; mode=display">p_j(o_t|\theta_j) = \frac{1}{\sqrt{2\pi\sigma_j^2}}\exp\left(-\frac{1}{2}\frac{(o_t-m_j)^2}{\sigma_j^2}\right)</script><p>对$m_j$求导使导数为$0$我们有：</p><script type="math/tex; mode=display">\hat{m}_j = \frac{\sum_{t=1}^T\alpha_t(j)\beta_t(j)o_t}{\sum_{t=1}^T\alpha_t(j)\beta_t(j)}</script><p>同时对$\sigma_j^2$求导令其导数为零我们有：</p><script type="math/tex; mode=display">\hat{\sigma}_j^2 =\frac{\sum_{t=1}^T\alpha_t(j)\beta_t(j)(o_t-\hat{m}_j)^2}{\sum_{t=1}^T\alpha_t(j)\beta_t(j)}</script><h2 id="混合HMM"><a href="#混合HMM" class="headerlink" title="混合HMM"></a>混合HMM</h2><p>假设我们观测数据的概率密度函数为$K$个概率密度函数的混合，即：</p><script type="math/tex; mode=display">b_j(o_t) = p_j(o_t|\theta_j) = \sum_{k=1}^Kc_j^{(k)}p_j^{(k)}(o_t|\theta_j^{(k)})</script><p>其中$c_j^{(k)}$为非负归一化权重，$0\le c_j^{(k)}\le 1$使得</p><script type="math/tex; mode=display">\sum_{k=1}^Kc_j^{(k)}=1</script><p>这里优化的策略采用的是将混合分布拆成多个分布，每个对于一个优化的HMM对象(看了好久终于看懂了，这是因为当$x_1,x_2\in [0,1]$时，我们有$\log(x_1+x_2)\ge \log(x_1x_2)$)。</p><p>相较于之前，我们有：</p><script type="math/tex; mode=display">\alpha_{t+1}(j,k) = \left(\sum_{i=1}^n\alpha_t(i,k)a_{ij}\right)\int_{o_t+1-\epsilon}^{o_t+1+\epsilon}c_j^{(k)}p_j^{(k)}(o|\theta_j^{(k)})do</script><script type="math/tex; mode=display">\beta_t(i,k)=\sum_{j=1}^na_{ij}\left(\int_{o_t+1-\epsilon}^{o_t+1+\epsilon}c_j^{(k)}p_j^{(k)}(o|\theta_j^{(k)})do\right)\beta_{t+1}(j,k)</script><p>优化过程中$a,\pi$不变：</p><script type="math/tex; mode=display">\hat{\alpha}_{ij} = \frac{\sum_{t=1}^{T-1}\alpha_t(i)\beta_{t+1}(j)a_{ij}p_j(o_{t+1}|\theta_j)}{\sum_{t=1}^{T-1}\alpha_t(i)\beta_t(i)}</script><script type="math/tex; mode=display">\hat{\pi}_i = \frac{\alpha_1(i)\beta_1(i)}{\sum_{i}^N\alpha_1(i)\beta_1(i)}</script><p>优化$c$得：</p><script type="math/tex; mode=display">\hat{c}_j^{(k)} = \frac{\sum_{t=1}^T\alpha_t(j,k)\beta_t(j,k)}{\sum_{l=1}^K\sum_{t=1}^T\alpha_t(j,l)\beta_t(j,l)}</script><p>$\hat{m}_j^{(k)},\sigma^{2^{(k)}}_j$与之前相似，为：</p><script type="math/tex; mode=display">\hat{m}_j^{(k)} = \frac{\sum_{t=1}^T\alpha_t(j,k)\beta_t(j,k)o_t}{\sum_{t=1}^T\alpha_t(j,k)\beta_t(j,k)}</script><script type="math/tex; mode=display">\hat{\sigma}_j^{2^{(k)}} =\frac{\sum_{t=1}^T\alpha_t(j,k)\beta_t(j,k)(o_t-\hat{m}_j^{(k)})^2}{\sum_{t=1}^T\alpha_t(j,k)\beta_t(j,k)}</script><h2 id="多变量HMM"><a href="#多变量HMM" class="headerlink" title="多变量HMM"></a>多变量HMM</h2><p>假设我们的概率密度函数是多元正态分布：</p><script type="math/tex; mode=display">f(\mathrm{X}) = \frac{1}{\sqrt{(2\pi)^k}|\Sigma|^{\frac{1}{2}}}\exp\left(-\frac{1}{2}(\mathrm{x-\mu})^T\Sigma^{-1}(\mathrm{x-\mu})\right)</script><p>那么我们的概率计算也会发生相应的改变。</p><p>在优化过程中$a$和$\pi$仍然不变：</p><script type="math/tex; mode=display">\hat{a}_{ij} = \frac{\sum_{t=1}^{T-1}\alpha_t(i)\beta_{t+1}(j)a_{ij}p_j(o_{t+1}|\theta_j)}{\sum_{t=1}^{T-1}\alpha_t(i)\beta_t(i)}</script><script type="math/tex; mode=display">\hat{\pi}_i = \frac{\alpha_1(i)\beta_1(i)}{\sum_{i}^N\alpha_1(i)\beta_1(i)}</script><p>$\hat{m}$也大致相同，但是$o_t$变成了向量：</p><script type="math/tex; mode=display">\hat{m}_j = \frac{\sum_{t=1}^T\alpha_t(j)\beta_t(j)o_t}{\sum_{t=1}^T\alpha_t(j)\beta_t(j)}</script><p>$\hat{\Sigma}$也大致相同：</p><script type="math/tex; mode=display">\hat{\Sigma}_j=\frac{\sum_{t=1}^T\alpha_t(j)\beta_t(j)(o_t-\hat{m}_j)(o_t-\hat{m})^T}{\sum_{t=1}^T\alpha_t(j)\beta_t(j)}</script><p>如果是混合多变量HMM，那么有上面的混合HMM相似。</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;之前的HMM都是在离散观测的条件下，现在我们研究一下观测值连续时的HMM。&lt;/p&gt;</summary>
    
    
    
    <category term="算法" scheme="https://www.hfcouc.work/categories/%E7%AE%97%E6%B3%95/"/>
    
    
    <category term="机器学习" scheme="https://www.hfcouc.work/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>神经网络</title>
    <link href="https://www.hfcouc.work/2021/12/23/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    <id>https://www.hfcouc.work/2021/12/23/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/</id>
    <published>2021-12-23T12:44:07.000Z</published>
    <updated>2021-12-23T12:57:10.987Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="\assets\js\Meting.min.js"></script><p>参考自西瓜书、未更新完。还有一天就考研了，想好好休息一下准备考研了，考完研后再更新。</p><span id="more"></span><h2 id="神经网络"><a href="#神经网络" class="headerlink" title="神经网络"></a>神经网络</h2><h3 id="神经元模型"><a href="#神经元模型" class="headerlink" title="神经元模型"></a>神经元模型</h3><p>下图为M-P神经元模型。在这个模型中，神经元收到来自于$n$个其他神经元传递过来的输入信号，这些输入信号通过带权重的连接(connection)进行传递，神经元接收到的总输入值将与神经元的阈值进行比较，然后通过”激活函数”处理以产生神经元的输出。</p><p><img src="https://static01.imgkr.com/temp/b064a4e75dff4a56adc5a5300f3057fe.png" alt=""></p><p>理想的激活函数是下图所示的阶跃函数，它将输入值映射为输出值$0$或$1$，显然$1$对应于神经元兴奋，$0$对应于神经元抑制。然而，阶跃函数具有不连续、不光滑等不太好的性质，因此实际上常用Sigmoid函数作为激活函数。</p><p><img src="https://static01.imgkr.com/temp/eb8af338b48c43adb4f1185f4b041239.png" alt=""></p><p>把许多这样的神经元按一定层次结构连接起来，就得到了神经网络。</p><h3 id="感知机与多层网络"><a href="#感知机与多层网络" class="headerlink" title="感知机与多层网络"></a>感知机与多层网络</h3><p>感知机由两层神经元组成，如下图所示：</p><p><img src="https://static01.imgkr.com/temp/bc0f6f44f7614d91bbb10944597ac6b1.png" alt=""></p><p>但是感知机只适用于线性可分数据集，对于线性不可分数据集无法收敛。于是就有了多层神经网络。</p><h3 id="误差传播算法"><a href="#误差传播算法" class="headerlink" title="误差传播算法"></a>误差传播算法</h3><p>误差传播算法(BP算法)可以用来学习多层神经网络。</p><p>下面我们来看看BP算法究竟是什么样的。给定数据集$D=\{(x_1,y_1),(x_2,y_2),\cdots,(x_m,y_m)\},x_i\in \mathbb{R}^d,y_i\in \mathbb{R}^l$。设只有一个隐藏层，隐藏层具有$q$个神经元，其中输出层第$j$个神经元用阈值用$\theta_j$表示，隐含层第$h$个神经元的阈值用$\gamma_h$表示。输入层第$i$个神经元与隐含层第$h$个神经元之间的连接权为$v_{ih}$，隐含层第$h$个神经元与输出层第$j$个神经元之间的连接权重为$w_{hj}$。如下图所示：</p><p><img src="https://static01.imgkr.com/temp/78d1c347bbba40ffb71dddc518b0e877.png" alt=""></p><p>对训练例$(x_k,y_k)$，假定神经网络的输出为$\hat{y}_k = (\hat{y}_1^k,\hat{y}_2^k,\cdots,\hat{y}_l^k)$，即</p><script type="math/tex; mode=display">\hat{y}_j^k = f(\beta_j-\theta_j)</script><p>则网络在$(x_k,y_k)$上的均方误差为：</p><script type="math/tex; mode=display">E_k = \frac{1}{2}\sum_{j=1}^l(\hat{y}_j^k-y^k_j)^2</script>]]></content>
    
    
    <summary type="html">&lt;p&gt;参考自西瓜书、未更新完。还有一天就考研了，想好好休息一下准备考研了，考完研后再更新。&lt;/p&gt;</summary>
    
    
    
    <category term="算法" scheme="https://www.hfcouc.work/categories/%E7%AE%97%E6%B3%95/"/>
    
    
    <category term="机器学习" scheme="https://www.hfcouc.work/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>决策树</title>
    <link href="https://www.hfcouc.work/2021/12/21/%E5%86%B3%E7%AD%96%E6%A0%91/"/>
    <id>https://www.hfcouc.work/2021/12/21/%E5%86%B3%E7%AD%96%E6%A0%91/</id>
    <published>2021-12-21T14:12:55.000Z</published>
    <updated>2022-01-12T10:38:16.014Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="\assets\js\Meting.min.js"></script><p>以下主要参考李航老师《统计学习方法》。已更新决策树和回归树代码，CART算法代码待补全。</p><span id="more"></span><h2 id="决策树"><a href="#决策树" class="headerlink" title="决策树"></a>决策树</h2><p>决策树学习的三个阶段</p><ol><li>特征选择</li><li>决策树的生成</li><li>决策树的修剪</li></ol><h3 id="特征选择"><a href="#特征选择" class="headerlink" title="特征选择"></a>特征选择</h3><p>特征选择的准则时信息增益或信息增益比。</p><h4 id="信息增益"><a href="#信息增益" class="headerlink" title="信息增益"></a>信息增益</h4><p>我们先介绍熵的定义。设$X$是一个取有限个值的离散随机变量，其概率分布为：</p><script type="math/tex; mode=display">P(X=x_i)=p_i,i=1,2,\cdots,n</script><p>则随机变量$X$的熵的定义为</p><script type="math/tex; mode=display">H(X) = -\sum_{i=1}^np_i\log p_i</script><p>若$p_i=0$，我们定义$0\log0=0$。对数若以$2$为底，单位为比特，以$e$为底，单位为纳特。熵只与$X$的分布有关而与$X$的取值无关，因此我们将$X$的熵记作$H(p)$，即</p><script type="math/tex; mode=display">H(p) =-\sum_{i=1}^np_i\log p_i</script><p>熵越大，随机变量的不确定性就越大。从定义可验证：</p><script type="math/tex; mode=display">0\le H(p)\le \log n</script><p>设有随机变量$(X,Y)$，其联合概率密度分布为：</p><script type="math/tex; mode=display">P(X=x_i,Y=y_j) = p_{ij},\quad i=1,2,\cdots,n;\quad j=1,2,\cdots,m</script><p>条件熵$H(Y|X)$表示在已知随机变量$X$的条件下随机变量$Y$的不确定性。随机变量$X$给定的条件下随机变量$Y$的条件熵$H(Y|X)$，定义为$X$给定条件下$Y$的条件概率分布的熵对$X$的数学期望</p><script type="math/tex; mode=display">H(Y|X) = \sum_{i=1}^n p_iH(Y|X=x_i)</script><p>这里，$p_i = P(X=x_i),i=1,2,\cdots,n$。</p><p>信息增益表示得知特征$X$的信息而使得类$Y$的信息不确定性减少的程度。</p><p>定义(信息增益)：特征$A$对训练数据$D$的信息增益$g(D,A)$，定义为集合$D$的经验熵$H(D)$与特征$A$给定条件下$D$的经验条件熵$H(D|A)$之差，即</p><script type="math/tex; mode=display">g(D,A) = H(D)-H(D|A)</script><p>一般地，熵$H(Y)$与条件熵$H(Y|X)$之差称为互信息。决策树学习中的信息增益等价于训练数据集中类与特征的互信息。</p><p>设训练数据集为$D$，$|D|$表示其样本容量，即样本个数。设有$K$个类$C_k,k=1,2,\cdots,K$，$|C_k|$为属于类$C_k$的样本数。设特征$A$有$n$个不同的取值$\{a_1,a_2,\cdots,a_n\}$，根据特征$A$的取值将$D$划分为$n$个子集$D_1,D_2,\cdots,D_n$，$|D_i|$表示$D_i$的样本数。记子集$D_i$中属于类$C_k$的样本集合为$D_{ik}$，即$D_{ik}=D_i\cap C_k$。</p><p>信息增益的算法：</p><p>输入：训练数据集$D$和特征$A$</p><p>输出：特征$A$对训练数据集$D$的信息增益$g(D,A)$</p><ol><li>计算数据集的经验熵$H(D)$：<script type="math/tex">H(D)=-\sum_{k=1}^K\frac{|C_k|}{|D|}\log_2\frac{|C_k|}{|D|}</script></li><li>计算特征$A$对数据集$D$的经验条件熵$H(D|A)$：<script type="math/tex">H(D|A)=\sum_{i=1}^n\frac{|D_i|}{D}H(D_i)=-\sum_{i=1}^n\frac{|D_i|}{|D|}\sum_{k=1}^K\frac{|D_{ik}|}{|D_i|}\log2\frac{|D_{ik}|}{|D_i|}</script></li><li>计算信息增益：<script type="math/tex">g(D,A)=H(D)-H(D|A)</script></li></ol><h4 id="信息增益比"><a href="#信息增益比" class="headerlink" title="信息增益比"></a>信息增益比</h4><p>以信息增益作为划分训练数据集的特征，存在偏向于选择取值的较多的特征的问题，使用信息增益比可以对这一问题进行校正。</p><p>定义(信息增益)：特征$A$对训练数据集$D$信息增益比$g_R(D,A)$定义为其信息增益$g(D,A)$与训练数据集$D$关于特征$A$的值的熵$H_A(D)$之比，即</p><script type="math/tex; mode=display">g_R(D,A) = \frac{g(D,A)}{H_A(D)}</script><p>。</p><h3 id="决策树的生成"><a href="#决策树的生成" class="headerlink" title="决策树的生成"></a>决策树的生成</h3><h4 id="ID3算法"><a href="#ID3算法" class="headerlink" title="ID3算法"></a>ID3算法</h4><p>输入：训练数据集$D$，特征值阈值$\epsilon$</p><p>输出：决策树$T$</p><ol><li>若$D$中所有实例属于同一类$C_k$，则$T$为单节点树，并将类$C_k$作为该结点的类标记，返回$T$；</li><li>若$A=\varnothing$，则$T$为单结点树，并将$D$中实例树最大的类$C_k$作为该结点的类标记，返回$T$；</li><li>否则，计算$A$中各特征值对$D$的信息增益，选择信息增益最大的特征$A_g$；</li><li>如果$A_g$的信息增益小于阈值$\epsilon$，则置$T$为单节点树，并将$D$中实例数最大的类$C_k$作为该结点的类标记，返回$T$</li><li>否则，对$A_g$的每一可能值$a_i$，依$A_g=a_i$将$D$分割为若干非空子集$D_i$，将$D_i$中实例数最大的类作为标记，构建子结点，由结点及其子节点构成数$T$，返回$T$；</li><li>对第$i$个子结点，以$D_i$为训练集，以$A-\{A_g\}$为特征集，递归调用步骤1到5，得到子树$T_i$，返回$T_i$。</li></ol><p>ID3算法只有数的生成，所以该算法生成的树容易产生过拟合。</p><h4 id="C4-5的生成算法"><a href="#C4-5的生成算法" class="headerlink" title="C4.5的生成算法"></a>C4.5的生成算法</h4><p>与ID3算法的差别仅在于使用信息增益比来选择特征。</p><ol><li>若$D$中所有实例属于同一类$C_k$，则$T$为单节点树，并将类$C_k$作为该结点的类标记，返回$T$；</li><li>若$A=\varnothing$，则$T$为单结点树，并将$D$中实例树最大的类$C_k$作为该结点的类标记，返回$T$；</li><li>否则，计算$A$中各特征值对$D$的信息增益比，选择信息增益比最大的特征$A_g$；</li><li>如果$A_g$的信息增益比小于阈值$\epsilon$，则置$T$为单节点树，并将$D$中实例数最大的类$C_k$作为该结点的类标记，返回$T$</li><li>否则，对$A_g$的每一可能值$a_i$，依$A_g=a_i$将$D$分割为若干非空子集$D_i$，将$D_i$中实例数最大的类作为标记，构建子结点，由结点及其子节点构成数$T$，返回$T$；</li><li>对第$i$个子结点，以$D_i$为训练集，以$A-\{A_g\}$为特征集，递归调用步骤1到5，得到子树$T_i$，返回$T_i$。</li></ol><h3 id="决策树的剪枝"><a href="#决策树的剪枝" class="headerlink" title="决策树的剪枝"></a>决策树的剪枝</h3><p>决策树的剪枝往往通过极小化决策树整体的损失函数来实现。设数$T$的叶结点个数为$|T|$，$t$是数$T$的叶结点，该结点有$N_t$个样本点，其中$k$类的样本点有$N_{tk}$个，$H_t(T)$为叶结点$t$上的经验熵，$\alpha\ge0$为超参数，则决策树学习的损失函数可以定义为</p><script type="math/tex; mode=display">C_{\alpha}(T) = \sum_{t=1}^{|T|}N_tH_t(T)+\alpha|T|</script><p>其中经验熵为：</p><script type="math/tex; mode=display">H_t(T) = -\sum_k\frac{N_{tk}}{N_t}\log\frac{N_{tk}}{N_t}</script><p>在损失函数中，常将右端的第一项记作：</p><script type="math/tex; mode=display">C(T) =\sum_{t=1}^{|T|}N_tH_t(T) = -\sum_{t=1}^{|T|}\sum_{k=1}^K\frac{N_{tk}}{N_t}\log\frac{N_{tk}}{N_t}</script><p>这时有：</p><script type="math/tex; mode=display">C_{\alpha}(T) = C(T)+\alpha|T|</script><p>算法(数的剪枝算法)</p><p>输入：生成算法产生的整个数$T$，参数$\alpha$</p><p>输出：修剪后的子树$T_{\alpha}$</p><ol><li>计算每个结点的经验熵。</li><li>递归地从数地叶结点向上回缩。设一组叶结点回缩到其父结点之前与之后的整体数分别为$T_B$和$T_A$，其对应的损失函数分别是$C_{\alpha}(T_B)$与$C_{\alpha}(T_A)$，其对应的损失函数值分别是$C_{\alpha}(T_B)$与$C_{\alpha}(T_A)$，如果<script type="math/tex">C_{\alpha}(T_A)\le C_{\alpha}(T_B)</script>，则进行剪枝，即将父结点变为新的叶结点。</li><li>返回2，直至不能继续为止，得到损失函数最小的子树$T_{\alpha}$。</li></ol><h3 id="CART算法"><a href="#CART算法" class="headerlink" title="CART算法"></a>CART算法</h3><p>分类与回归树(classfication and regression tree, CART)。</p><p>决策树就是递归地构建二叉决策树的过程。对回归树用平方误差最小化准则，对分类树用基尼系数最小化准则，进行特征选择，生成二叉树。</p><h4 id="回归树的生成"><a href="#回归树的生成" class="headerlink" title="回归树的生成"></a>回归树的生成</h4><p>假设$X$和$Y$分别为输入和输出变量，并且$Y$是连续变量，给定训练数据集</p><script type="math/tex; mode=display">D= \{(x_1,y_1),(x_2,y_2),\cdots,(x_N,y_N)\}</script><p>考虑如何生成回归树。</p><p>假设树将输入空间划分为$M$个单元$R_1,R_2,\cdots,R_M$，并且在每个单元$R_m$上有一个固定的输出值$c_m$，于是回归树模型可表示为：</p><script type="math/tex; mode=display">f(x) = \sum_{m=1}^Mc_mI(x\in R_m)</script><p>当输入空间的划分确定时，可以用平方误差$\sum_{x_i\in R_m}(y_i-f(x_i))^2$来表示回归树对于训练数据的预测误差，求解每个单元上的最优输出值。易知：</p><script type="math/tex; mode=display">\hat{c}_m = \operatorname{ave}(y_i|x_i\in R_m)</script><p>问题是怎么对输入空间进行划分。我们采用启发式的方法，选择第$j$个变量$x^{(j)}$和它的取值$s$，作为切分变量和切分点，并定义两个区域：</p><script type="math/tex; mode=display">R_1(j,s) = \{x|x^{(j)}\le s\}\quad\text{和}R_2(j,s) = \{x|x^{(j)}>s\}</script><p>然后寻找最优切分点和最优切分变量：</p><script type="math/tex; mode=display">\min_{j,s}\left[\min_{c_1}\sum_{x_i\in R_1(j,s)}(y_i-c_1)^2+\min_{c_2}\sum_{x_i\in R_2(j,s)}(y_i-c_2)^2\right]</script><p>对于固定输入变量$j$可以找到最优切分点$s$</p><script type="math/tex; mode=display">\hat{c}_1 = \operatorname{ave}(y_i|x_i\in R_1(j,s))\quad\text{和}\quad \hat{c}_2 = \operatorname{ave}(y_i|x_i\in R_2(j,s))\quad</script><p>算法为：</p><p>输入：训练数据集$D$</p><p>输出：回归树$f(x)$</p><p>在训练数据集中所在的输入空间中，递归地将每个区域划分为两个子区域并决定每个子区域上的输出值，构建二叉决策树：</p><ol><li><p>选择最优切分变量$j$和切分点$s$，求解：</p><script type="math/tex; mode=display">\min_{j,s}\left[\min_{c_1}\sum_{x_i\in R_1(j,s)}(y_i-c_1)^2+\min_{c_2}\sum_{x_i\in R_2(j,s)}(y_i-c_2)^2\right]</script><p>遍历变量$j$，对固定的切分变量$j$扫描切分点$s$，选择使上式达到最小值的对$(j,s)$。</p></li><li><p>对选定的对$(j,s)$划分区域并决定相应的输出值</p><script type="math/tex; mode=display">\begin{gathered}R_{1}(j, s)=\left\{x \mid x^{(j)} \leqslant s\right\}, \quad R_{2}(j, s)=\left\{x \mid x^{(j)}>s\right\} \\\hat{c}_{m}=\frac{1}{N_{m}} \sum_{x_{i} \in R_{m}(j, s)} y_{i}, \quad x \in R_{m}, \quad m=1,2\end{gathered}</script></li><li><p>继续对两个子区域调用步骤1，2，直到满足停止条件。</p></li><li><p>将输入空间划分为$M$个区域$R_1,R_2,\cdots,R_M$，生成决策树：</p><script type="math/tex; mode=display">f(x) = \sum_{m=1}^M\hat{c}_mI(x\in R_m)</script></li></ol><h4 id="分类树的生成"><a href="#分类树的生成" class="headerlink" title="分类树的生成"></a>分类树的生成</h4><p>分类树用基尼指数选择最优特征，同时决定该特征的最优二值切分点。</p><p>定义(基尼系数)：分类问题中，假设有$K$个类，样本点属于第$k$类的概率为$p_k$，则概率分布的基尼指数定义为：</p><script type="math/tex; mode=display">\operatorname{Gini}(p) = \sum_{k=1}^Kp_k(1-p_k) = 1-\sum_{k=1}^Kp_k^2</script><p>对于二分类问题，若样本点属于第$1$个类的概率为$p$，则概率分布的基尼指数为：</p><script type="math/tex; mode=display">\operatorname{Gini}(p) = 2p(1-p)</script><p>对于给定样本集合$D$，其基尼指数为：</p><script type="math/tex; mode=display">\operatorname{Gini}(D) = 1-\sum_{k=1}^K\left(\frac{|C_k|}{|D|}\right)^2</script><p>基尼系数越大，样本集合的不确定性越大。</p><p>如果样本集合$D$根据特征$A$是否取某一可能值$a$被分割成$D_1$和$D_2$两部分，即</p><script type="math/tex; mode=display">D_1=\{(x,y)\in D|A(x)=a\},\quad D_2=D-D_1</script><p>则在特征$A$的条件下，集合$D$的基尼指数定义为</p><script type="math/tex; mode=display">\operatorname{Gini}(D,A) = \frac{|D_1|}{|D|}\operatorname{Gini}(D_1)+\frac{|D_2|}{|D|}\operatorname{Gini}(D_2)</script><p>算法(CART生成算法)</p><p>输入：训练数据集$D$，停止计算的条件</p><p>输出：CART决策树</p><ol><li>设结点的训练数据集为$D$，计算现有特征对该数据集的基尼系数。此时，对于每一个特征$A$，对其可能取的每个值$a$，根据样本点$A=a$的测试为”是”或”否”将$D$分割成$D_1$和$D_2$两个部分，计算$A=a$时的基尼指数。</li><li>在所有可能的特征$A$以及它们所有可能的切分点$a$中，选择基尼指数最小的特征及其对应的切分点作为最优特征与最优切分点。依最优特征与最优切分点，从现结点生成两个子结点，将训练数据集依特征分配到两个子结点中去。</li><li>对两个子结点递归地调用1，2，直到满足终止条件。</li><li>生成CART决策树。</li></ol><h4 id="CART剪枝"><a href="#CART剪枝" class="headerlink" title="CART剪枝"></a>CART剪枝</h4><p>CART剪枝算法由两步组成：首先从生成算法产生的决策树$T_0$低端开始不断剪枝，直到$T_0$的根结点，形成一个子树序列$\{T_0,T_1,\cdots,T_n\}$；然后通过交叉验证法在独立的验证数据集上对子树序列进行测试，从中选择最优子树。</p><p>可以利用递归的方法对数进行剪枝。将$\alpha$从小增大，$0=\alpha_0&lt;\alpha_1&lt;\cdots&lt;\alpha_n&lt;+\infty$，产生一系列的区间$[\alpha_i,\alpha_{i+1}),i=0,1,\cdots,n$的最优子树序列$\{T_0,T_1,\cdots,T_n\}$，序列中的子树是嵌套的。</p><p>具体地，从整体树$T_0$开始剪枝。对$T_0$的任意内部结点$t$，以$t$为单结点数的损失函数是：</p><script type="math/tex; mode=display">C_{\alpha}(t) = C(t) + \alpha</script><p>以$t$为根结点的子树$T_t$的损失函数是：</p><script type="math/tex; mode=display">C_{\alpha}(T_t) = C(T_t)+\alpha|T_t|</script><p>当$\alpha=0$及$\alpha$充分小时，有不等式</p><script type="math/tex; mode=display">C_{\alpha}(T_t)<C_{\alpha}(t)</script><p>当$\alpha$增大时，不等式反向。只要$\alpha=\frac{C(t)-C(T_t)}{|T_t|-1}$，$T_t$与$t$有相同的损失函数值，而$t$的结点少，因此$t$比$T_t$更可取，对$T_t$进行剪枝。</p><p>为此，对$T_0$中每一内部结点$t$，计算</p><script type="math/tex; mode=display">g(t) = \frac{C(t)-C(T_t)}{|T_t|-1}</script><p>它表示剪枝后整体损失函数减少的程度。在$T_0$中剪去$g(t)$最小的$T_t$，将得到的子树作为$T_1$，同时将最小的$g(t)$设为$\alpha_1$。==$T_1$为区间$[\alpha_1,\alpha_2)$的最优子树。==</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 决策树生成</span></span><br><span class="line"><span class="comment"># 定义节点类 二叉树</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Node</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, root=<span class="literal">True</span>, label=<span class="literal">None</span>, feature_name=<span class="literal">None</span>, feature=<span class="literal">None</span></span>):</span></span><br><span class="line">        self.root = root</span><br><span class="line">        self.label = label</span><br><span class="line">        self.feature_name = feature_name</span><br><span class="line">        self.feature = feature</span><br><span class="line">        self.tree = &#123;&#125;</span><br><span class="line">        self.result = &#123;</span><br><span class="line">            <span class="string">&#x27;label:&#x27;</span>: self.label,</span><br><span class="line">            <span class="string">&#x27;feature:&#x27;</span>: self.feature,</span><br><span class="line">            <span class="string">&#x27;tree:&#x27;</span>: self.tree</span><br><span class="line">        &#125;</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__repr__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="keyword">return</span> <span class="string">&#x27;&#123;&#125;&#x27;</span>.<span class="built_in">format</span>(self.result)</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">add_node</span>(<span class="params">self, val, node</span>):</span></span><br><span class="line">        self.tree[val] = node</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">predict</span>(<span class="params">self, features</span>):</span></span><br><span class="line">        <span class="keyword">if</span> self.root <span class="keyword">is</span> <span class="literal">True</span>:</span><br><span class="line">            <span class="keyword">return</span> self.label</span><br><span class="line">        <span class="keyword">return</span> self.tree[features[self.feature]].predict(features)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DTree</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, epsilon=<span class="number">0.1</span></span>):</span></span><br><span class="line">        self.epsilon = epsilon</span><br><span class="line">        self._tree = &#123;&#125;</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 熵</span></span><br><span class="line"><span class="meta">    @staticmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">calc_ent</span>(<span class="params">datasets</span>):</span></span><br><span class="line">        data_length = <span class="built_in">len</span>(datasets)</span><br><span class="line">        label_count = &#123;&#125;</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(data_length):</span><br><span class="line">            label = datasets[i][-<span class="number">1</span>]</span><br><span class="line">            <span class="keyword">if</span> label <span class="keyword">not</span> <span class="keyword">in</span> label_count:</span><br><span class="line">                label_count[label] = <span class="number">0</span></span><br><span class="line">            label_count[label] += <span class="number">1</span></span><br><span class="line">        ent = -<span class="built_in">sum</span>([(p / data_length) * log(p / data_length, <span class="number">2</span>)</span><br><span class="line">                    <span class="keyword">for</span> p <span class="keyword">in</span> label_count.values()])</span><br><span class="line">        <span class="keyword">return</span> ent</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 经验条件熵</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">cond_ent</span>(<span class="params">self, datasets, axis=<span class="number">0</span></span>):</span></span><br><span class="line">        data_length = <span class="built_in">len</span>(datasets)</span><br><span class="line">        feature_sets = &#123;&#125;</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(data_length):</span><br><span class="line">            feature = datasets[i][axis]</span><br><span class="line">            <span class="keyword">if</span> feature <span class="keyword">not</span> <span class="keyword">in</span> feature_sets:</span><br><span class="line">                feature_sets[feature] = []</span><br><span class="line">            feature_sets[feature].append(datasets[i])</span><br><span class="line">        cond_ent = <span class="built_in">sum</span>([(<span class="built_in">len</span>(p) / data_length) * self.calc_ent(p)</span><br><span class="line">                        <span class="keyword">for</span> p <span class="keyword">in</span> feature_sets.values()])</span><br><span class="line">        <span class="keyword">return</span> cond_ent</span><br><span class="line">    <span class="comment">#信息增益</span></span><br><span class="line"><span class="meta">    @staticmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">info_gain</span>(<span class="params">ent, cond_ent</span>):</span></span><br><span class="line">        <span class="keyword">return</span> ent - cond_ent</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">info_gain_train</span>(<span class="params">self, datasets</span>):</span></span><br><span class="line">        count = <span class="built_in">len</span>(datasets[<span class="number">0</span>]) - <span class="number">1</span></span><br><span class="line">        ent = self.calc_ent(datasets)</span><br><span class="line">        best_feature = []</span><br><span class="line">        <span class="keyword">for</span> c <span class="keyword">in</span> <span class="built_in">range</span>(count):</span><br><span class="line">            c_info_gain = self.info_gain(ent, self.cond_ent(datasets, axis=c))</span><br><span class="line">            best_feature.append((c, c_info_gain))</span><br><span class="line">        <span class="comment"># 比较大小</span></span><br><span class="line">        best_ = <span class="built_in">max</span>(best_feature, key=<span class="keyword">lambda</span> x: x[-<span class="number">1</span>])</span><br><span class="line">        <span class="keyword">return</span> best_</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">train</span>(<span class="params">self, train_data</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        input:数据集D(DataFrame格式)，特征集A，阈值eta</span></span><br><span class="line"><span class="string">        output:决策树T</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        _, y_train, features = train_data.iloc[:,:-<span class="number">1</span>], train_data.iloc[:,-<span class="number">1</span>], train_data.columns[:-<span class="number">1</span>]</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 若D中实例属于同一类Ck，则T为单结点树，并将类Ck作为结点的类标记，返回T</span></span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">len</span>(y_train.value_counts()) == <span class="number">1</span>:</span><br><span class="line">            <span class="keyword">return</span> Node(root=<span class="literal">True</span>, label=y_train.iloc[<span class="number">0</span>])</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 若A为空，则T为单结点树，将D中实例树最大的类Ck作为该结点的标记，返回T</span></span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">len</span>(features) == <span class="number">0</span>:</span><br><span class="line">            <span class="keyword">return</span> Node(</span><br><span class="line">                root=<span class="literal">True</span>,</span><br><span class="line">                label=y_train.value_counts().sort_values(ascending=<span class="literal">False</span>).index[<span class="number">0</span>]</span><br><span class="line">            )</span><br><span class="line">        <span class="comment"># 计算最大信息增益 Ag为信息增益最大的特征</span></span><br><span class="line">        max_feature, max_info_gain = self.info_gain_train(np.array(train_data))</span><br><span class="line">        max_feature_name = features[max_feature]</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Ag的信息增益小于阈值eta，则置T为单结点树，并将D中实例数最大的类Ck作为该结点的类标记，返回T</span></span><br><span class="line">        <span class="keyword">if</span> max_info_gain &lt; self.epsilon:</span><br><span class="line">            <span class="keyword">return</span> Node(</span><br><span class="line">                root=<span class="literal">True</span>,</span><br><span class="line">                label=y_train.value_counts().sort_values(</span><br><span class="line">                ascending=<span class="literal">False</span>).index[<span class="number">0</span>]</span><br><span class="line">            )</span><br><span class="line">        <span class="comment"># 构建Ag子集</span></span><br><span class="line">        node_tree = Node(</span><br><span class="line">            root=<span class="literal">False</span>, feature_name=max_feature_name, feature=max_feature)</span><br><span class="line">        </span><br><span class="line">        feature_list = train_data[max_feature_name].value_counts().index</span><br><span class="line">        <span class="keyword">for</span> f <span class="keyword">in</span> feature_list:</span><br><span class="line">            sub_train_df = train_data.loc[train_data[max_feature_name]==f].drop([max_feature_name],axis=<span class="number">1</span>)</span><br><span class="line">            </span><br><span class="line">            <span class="comment"># 递归生成树</span></span><br><span class="line">            sub_tree = self.train(sub_train_df)</span><br><span class="line">            node_tree.add_node(f, sub_tree)</span><br><span class="line">        <span class="keyword">return</span> node_tree</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">fit</span>(<span class="params">self, train_data</span>):</span></span><br><span class="line">        self._tree = self.train(train_data)</span><br><span class="line">        <span class="keyword">return</span> self._tree</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">predict</span>(<span class="params">self, X_test</span>):</span></span><br><span class="line">        <span class="keyword">return</span> self._tree.predict(X_test)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 回归树</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">LeastSqRTree</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, train_X, y, epsilon</span>):</span></span><br><span class="line">        <span class="comment"># 训练集特征值</span></span><br><span class="line">        self.x = train_X</span><br><span class="line">        <span class="comment"># 类别</span></span><br><span class="line">        self.y = y</span><br><span class="line">        <span class="comment"># 特征总数</span></span><br><span class="line">        self.feature_count = train_X.shape[<span class="number">1</span>]</span><br><span class="line">        <span class="comment"># 损失阈值</span></span><br><span class="line">        self.epsilon = epsilon</span><br><span class="line">        <span class="comment"># 回归树</span></span><br><span class="line">        self.tree = <span class="literal">None</span></span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_fit</span>(<span class="params">self, x, y, feature_count, epsilon</span>):</span></span><br><span class="line">        <span class="comment"># 选择最优切分变量j和切分点s</span></span><br><span class="line">        (j, s, minval, c1, c2) = self._divide(x, y, feature_count)</span><br><span class="line">        <span class="comment"># 初始化树</span></span><br><span class="line">        tree = &#123;<span class="string">&#x27;feature&#x27;</span>:j, <span class="string">&quot;value&quot;</span>: x[s, j], <span class="string">&#x27;left&#x27;</span>:<span class="literal">None</span>, <span class="string">&#x27;right&#x27;</span>:<span class="literal">None</span>&#125;</span><br><span class="line">        <span class="keyword">if</span> minval &lt; self.epsilon <span class="keyword">or</span> <span class="built_in">len</span>(y[np.where(x[:,j]&lt;=x[s,j])]) &lt;=<span class="number">1</span>:</span><br><span class="line">            tree[<span class="string">&quot;left&quot;</span>] =c1</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            tree[<span class="string">&quot;left&quot;</span>] = self._fit(x[np.where(x[:,j]&lt;=x[s,j])],</span><br><span class="line">                                     y[np.where(x[:,j]&lt;=x[s,j])],</span><br><span class="line">                                    self.feature_count, self.epsilon)</span><br><span class="line">        <span class="keyword">if</span> minval &lt; self.epsilon <span class="keyword">or</span> <span class="built_in">len</span>(y[np.where(x[:, j] &gt; s)]) &lt;= <span class="number">1</span>:</span><br><span class="line">            tree[<span class="string">&quot;right&quot;</span>] = c2</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            tree[<span class="string">&quot;right&quot;</span>] = self._fit(x[np.where(x[:, j] &gt; x[s, j])],</span><br><span class="line">                                      y[np.where(x[:, j] &gt; x[s, j])],</span><br><span class="line">                                      self.feature_count, self.epsilon)</span><br><span class="line">        <span class="keyword">return</span> tree</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">fit</span>(<span class="params">self</span>):</span></span><br><span class="line">        self.tree = self._fit(self.x, self.y, self.feature_count, self.epsilon)</span><br><span class="line">    </span><br><span class="line"><span class="meta">    @staticmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_divide</span>(<span class="params">x, y, feature_count</span>):</span></span><br><span class="line">        <span class="comment"># 初始化损失误差</span></span><br><span class="line">        cost = np.zeros((feature_count, <span class="built_in">len</span>(x)))</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(feature_count):</span><br><span class="line">            <span class="keyword">for</span> k <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(x)):</span><br><span class="line">                value = x[k,i]</span><br><span class="line">                y1 = y[np.where(x[:,i]&lt;=value)]</span><br><span class="line">                c1 = np.mean(y1)</span><br><span class="line">                y2 = y[np.where(x[:,i]&gt;value)]</span><br><span class="line">                c2 = np.mean(y2)</span><br><span class="line">                y1[:] = y1[:] - c1</span><br><span class="line">                y2[:] = y2[:] - c2</span><br><span class="line">                cost[i,k] = np.<span class="built_in">sum</span>(y1 * y1) + np.<span class="built_in">sum</span>(y2 * y2)</span><br><span class="line">        <span class="comment"># 选取最优损失误差点</span></span><br><span class="line">        cost_index = np.where(cost==np.<span class="built_in">min</span>(cost))</span><br><span class="line">        <span class="comment"># 选取第几个特征值</span></span><br><span class="line">        j = cost_index[<span class="number">0</span>][<span class="number">0</span>]</span><br><span class="line">        <span class="comment"># 选取特征值的切分点</span></span><br><span class="line">        s = cost_index[<span class="number">1</span>][<span class="number">0</span>]</span><br><span class="line">        <span class="comment"># 求两个区域的均值c1,c2</span></span><br><span class="line">        c1 = np.mean(y[np.where(x[:, j] &lt;= x[s, j])])</span><br><span class="line">        c2 = np.mean(y[np.where(x[:, j] &gt; x[s, j])])</span><br><span class="line">        <span class="keyword">return</span> j, s, cost[cost_index], c1, c2</span><br></pre></td></tr></table></figure>]]></content>
    
    
    <summary type="html">&lt;p&gt;以下主要参考李航老师《统计学习方法》。已更新决策树和回归树代码，CART算法代码待补全。&lt;/p&gt;</summary>
    
    
    
    <category term="算法" scheme="https://www.hfcouc.work/categories/%E7%AE%97%E6%B3%95/"/>
    
    
    <category term="机器学习" scheme="https://www.hfcouc.work/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>极限</title>
    <link href="https://www.hfcouc.work/2021/12/19/%E6%9E%81%E9%99%90/"/>
    <id>https://www.hfcouc.work/2021/12/19/%E6%9E%81%E9%99%90/</id>
    <published>2021-12-19T14:22:41.000Z</published>
    <updated>2021-12-22T07:21:39.867Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="\assets\js\Meting.min.js"></script><p>梅加强老师《数学分析》</p><span id="more"></span><h3 id="数列极限"><a href="#数列极限" class="headerlink" title="数列极限"></a>数列极限</h3><h4 id="数列极限的定义"><a href="#数列极限的定义" class="headerlink" title="数列极限的定义"></a>数列极限的定义</h4><p>定义(数列极限)。设$\{a_n\}$为数列，$A\in \mathbb{R}$.如果任给$\epsilon&gt;0$,都存在正整数$N(\epsilon)$，使得当$n&gt;N$时，有</p><script type="math/tex; mode=display">    |a_n-A|<\epsilon</script><p>则称$\{a_n\}$以$A$为极限，或称$\{a_n\}$收敛于$A$，记为</p><script type="math/tex; mode=display">    \lim_{n\rightarrow\infty}a_n=A\text{ 或 }a_n\rightarrow A(n\rightarrow \infty)</script><p>当然我们也可以用$\epsilon-N$语言给出数列$\{a_n\}$不以$A$为极限的定义：如果存在$\epsilon_0&gt;0$，使得任给正数$N$，均存在$n_0&gt;N$满足不等式$|a_{n_0}-A|\ge\epsilon_0$，则$\{a_n\}$不以$A$为极限。</p><p>命题：如果数列$\{a_n\}$有极限，则其极限是唯一的。</p><p>定理(夹逼定理).设$\{a_n\},\{b_n\},\{c_n\}$均为数列，且</p><script type="math/tex; mode=display">    a_n\le b_n\le c_n,\forall n\ge N_0</script><p>其中$N_0$为一整数，如果</p><script type="math/tex; mode=display">    \lim_{n\rightarrow\infty}a_n=A=\lim_{n\rightarrow\infty}c_n</script><p>则$\lim_{n\rightarrow\infty}b_n=A$。</p><p>例题：<br>考虑无限循环小数$A=0.99999\cdots$，问：$A$是否小于$1$？<br>解：我们可以将$A$视为一列有限小数$\{a_n\}$的极限，其中$a_n = 0.99\cdots9(n\text{个}9)$。由于：</p><script type="math/tex; mode=display">    |a_n-1| = 10^{-n}</script><p>根据夹逼定理</p><script type="math/tex; mode=display">a_n\le A\le 1</script><p>而</p><script type="math/tex; mode=display">\lim_{n\rightarrow\infty}a_n=1</script><p>所以</p><script type="math/tex; mode=display">A=1</script><p>例：<br>设$0&lt;\alpha&lt;1$，证明$\lim_{n\rightarrow\infty}[(n+1)^{\alpha}-n^{\alpha}]=0$<br>证明：当$n\ge1$时，有</p><script type="math/tex; mode=display">    \begin{aligned}        0<(n+1)^{\alpha}-n^{\alpha} &= n^{\alpha}[(1+\frac{1}{n})^{\alpha}-1]\\        &\le n^{\alpha}[(1+\frac{1}{n})-1]=\frac{1}{n^{1-\alpha}}    \end{aligned}</script><p>根据夹逼定理我们有$\lim_{n\rightarrow\infty}[(n+1)^{\alpha}-n^{\alpha}]=0$。</p><p>例：设$\alpha&gt;0,a&gt;1$，则$\lim_{n\rightarrow\infty}\frac{n^{\alpha}}{a^n}=0$</p><p>思路：由于分子分母同时含有$n$，因此我们很难进行判断，我们想要做的是根据放缩消去一个$n$，而留下的$n$很容易处理，因此我们对$a^n$进行放缩处理。<br>我们记$a^{\frac{1}{\alpha}}=1+\beta,\beta&gt;0$。由于$n&gt;1$，有</p><script type="math/tex; mode=display">(1+\beta)^n = 1 + n\beta+\frac{1}{2}n(n-1)\beta^2+\cdots+\beta^n>\frac{1}{2}n(n-1)\beta^2</script><p>故</p><script type="math/tex; mode=display">0<\frac{n^{\alpha}}{a^n} = \left[\frac{n}{(1+\beta)^n}\right]^{\alpha} < \left[\frac{2}{(n-1)\beta^2}\right]^\alpha</script><p>由夹逼原理可知$\lim_{n\rightarrow\infty}\frac{n^{\alpha}}{a^n}=0$。</p><p>例：证明$\lim_{n\rightarrow\infty}\frac{1}{\sqrt[n]{n!}}=0$</p><p>注意到当$1\le k\le n$时$(k-1)(n-k)\ge0$，从而$k(n-k+1)\ge n$，我们就有：</p><script type="math/tex; mode=display">    (n!)^2 = (1\cdot n)(2(n-1))\cdots(k(n-k+1))\cdots(n\cdot1)\ge n^n,\forall n\ge1</script><p>因此</p><script type="math/tex; mode=display">    0<\frac{1}{\sqrt[n]{n!}}\le\frac{1}{\sqrt{n}},\forall n\ge1</script><p>由夹逼原理可得：$\lim_{n\rightarrow\infty}\frac{1}{\sqrt[n]{n!}}=0$</p><p>例：证明$\lim_{n\rightarrow\infty}\sqrt[n]{n}=1$<br>证明：记$\sqrt[n]{n}=1+\alpha_n$，当$n&gt;1$时，</p><script type="math/tex; mode=display">    n = (1+\alpha_n)^n=1+n\alpha_n+\frac{1}{2}n(n-1)\alpha_n^2+\cdots+\alpha_n^n > \frac{1}{2}n(n-1)\alpha_n^2</script><p>从而有估计</p><script type="math/tex; mode=display">0<\alpha_n<\sqrt{\frac{2}{n-1}}</script><p>因此，当$n&gt;1$时，有</p><script type="math/tex; mode=display">1<\sqrt[n]{n} = 1+\alpha_n<1+\sqrt{\frac{2}{n-1}}</script><p>由夹逼原理即得：$\lim_{n\rightarrow\infty}\sqrt[n]{n}=1$。</p><p>下面两个为比较重要的例题：</p><p>设$\lim_{n\rightarrow\infty}a_n=A$，证明$\lim_{n\rightarrow\infty}\frac{a_1+a_2+\cdots+a_n}{n}=A$。<br>证明：任给$\epsilon&gt;0$，因为$\lim_{n\rightarrow\infty}a_n=A$，故存在$N_0$，使得当$n&gt;N_0$时，有</p><script type="math/tex; mode=display">    |a_n-A|<\frac{\epsilon}{2}</script><p>令</p><script type="math/tex; mode=display">N>\max\{N_0,2\epsilon^{-1}|a_1+\cdots+a_{N_0}-N_0A|\}</script><p>则当$n&gt;N$时，有</p><script type="math/tex; mode=display">\begin{aligned}&\left|\frac{a_{1}+\cdots+a_{n}}{n}-A\right|=\left|\frac{a_{1}+\cdots+a_{N_{0}}-N_{0} A}{n}+\frac{\left(a_{N_{0}+1}-A\right)+\cdots+\left(a_{n}-A\right)}{n}\right| \\&\leqslant \frac{\left|a_{1}+\cdots+a_{N_{0}}-N_{0} A\right|}{n}+\frac{\left|a_{N_{0}+1}-A\right|+\cdots+\left|a_{n}-A\right|}{n} \\&\leqslant \frac{\left|a_{1}+\cdots+a_{N_{0}}-N_{0} A\right|}{n}+\frac{n-N_{0}}{n} \frac{\varepsilon}{2} \\&<\frac{\varepsilon}{2}+\frac{\varepsilon}{2}=\varepsilon\end{aligned}</script><p>这说明：$\lim_{n\rightarrow\infty}\frac{a_1+a_2+\cdots+a_n}{n}=A$。</p><p>我们再来证明一个跟这个差不多的例题：</p><p>$\lim_{n\rightarrow\infty}a_n=A$，则$\sqrt[n]{a_1\cdots a_n}=A$。</p><p>证明：</p><p>我们可以取对数利用上面那个题的结论即可证明。</p><p>例：<strong>任何实数都是某个有理数列的极限</strong>。<br>证明：设$A$为实数。如果$A$为有理数，则令$a_n=A(n\ge1)$即可。如果$A$为无理数，令</p><script type="math/tex; mode=display">a_n = \frac{[nA]}{n},\forall n\ge1</script><p>其中$[x]$表示不超过$x$的最大整数，因此$a_n$都是有理数。因为$A$不是有理数，故：</p><script type="math/tex; mode=display">nA-1<[nA]<nA,\forall n\ge1</script><p>即</p><script type="math/tex; mode=display">A-\frac{1}{n}<a_n=\frac{[nA]}{n}<A,\forall n\ge1</script><p>由夹逼定律可知$\lim_{n\rightarrow\infty} a_n=A$</p><h4 id="数列极限的基本性质"><a href="#数列极限的基本性质" class="headerlink" title="数列极限的基本性质"></a>数列极限的基本性质</h4><p>命题(有界性)：设数列$\{a_n\}$收敛，则$\{a_n\}$有界<br>由此命题立知，无界数列必定发散。如果$\{a_n\}$发散到$+\infty$，则称$\{a_n\}$发散到$\infty$，记为</p><script type="math/tex; mode=display">    \lim_{n\rightarrow\infty}a_n=\infty,\text{ 或}a_n\rightarrow\infty(n\rightarrow\infty)</script><p>命题(绝对值性质)。设数列$\{a_n\}$收敛到$A$，则$\{|a_n|\}$收敛到$|A|$。</p><p>推论：数列$\{a_n\}$收敛到$0$当且仅当$|a_n|$收敛到$0$；数列$\{a_n\}$收敛到$A$当且仅当$|a_n-A|$收敛到$0$。</p><p>命题(保序性质)。设数列$\{a_n\}$收敛到$A$，$\{b_n\}$收敛到$B$，则有</p><ol><li>如果存在$N_0$，当$n&gt;N_0$时$a_n\ge b_n$，则$A\ge B$。</li><li>反之，如果$A&gt;B$，则存在$N$，使得当$n&gt;N$时$a_n&gt;b_n$。</li></ol><p>推论：设$\lim_{n\rightarrow\infty}a_n=A$，如果$A\neq0$，则存在$N$，使得当$n&gt;N$时，有</p><script type="math/tex; mode=display">    \frac{1}{2}|A|<|a_n|<\frac{3}{2}|A|</script><p>例：设$q&gt;1$，则$\lim_{n\rightarrow\infty}\frac{\log_qn}{n}=0$<br>解：任给$\epsilon&gt;0$，利用之前的结论，有</p><script type="math/tex; mode=display">\lim_{n\rightarrow\infty}\sqrt[n]{n}=1<q^{\epsilon}</script><p>由极限的保序性质，存在$N$，当$n&gt;N$时，有</p><script type="math/tex; mode=display">    \sqrt[n]{n}<q^{\epsilon}</script><p>即</p><script type="math/tex; mode=display">    \frac{\log_qn}{n}<\epsilon,\forall n>N</script><p>这说明：$\lim_{n\rightarrow\infty}a_n=A$.</p><p>命题(四则运算)。设数列$\{a_n\}$收敛到$A$，$\{b_n\}$收敛到$B$，则有：</p><ol><li>$\{\alpha a_n+\beta b_n\}$收敛到$\alpha A+\beta B$，其中$\alpha,\beta$为常数</li><li>$\{a_nb_n\}$收敛到$AB$</li><li>当$B\neq0$时，$\{a_n/b_n\}$收敛到$A/B$</li></ol><p>下面我们引入数列的子列的概念，并研究数列的极限和其子列的极限之间的关系，设</p><script type="math/tex; mode=display">    a_1,a_2,\cdots,a_n,\cdots</script><p>是数列，如果</p><script type="math/tex; mode=display">    1\le n_1<n_2<\cdots<n_k<\cdots</script><p>是一列严格递增的正整数，则称数列</p><script type="math/tex; mode=display">    a_{n_1},a_{n_2},\cdots,a_{n_k},\cdots</script><p>为原数列$\{a_n\}$的子列，记为$\{a_{n_k}\}$。两个特殊的子列$\{a_{2k}\}$和$\{a_{2k-1}\}$分别为偶子列和奇子列。</p><p>命题</p><ol><li>设$\{a_n\}$收敛到$A$，则它的任何子列$\{a_{n_k}\}$也收敛到$A$</li><li>如果$\{a_n\}$的偶子列与奇子列收敛到$A$，则$\{a_n\}$也收敛到$A$</li></ol><h4 id="例题"><a href="#例题" class="headerlink" title="例题"></a>例题</h4><p>设$\lim_{n\rightarrow \infty}(a_{n+1}-a_n)=A$，则$\lim_{n\rightarrow\infty}\frac{a_n}{n}=A$</p><script type="math/tex; mode=display">\frac{a_n}{n} = \frac{a_1+(a_2-a_1)+(a_3-a_2)+\cdots+(a_n-a_{n-1})}{n}</script><p>因为$\lim_{n\rightarrow \infty}(a_{n+1}-a_n)=A$，利用已知例题的结论即可证明得到。</p><p>设$\lim_{n\rightarrow\infty}a_n=A$，证明：</p><script type="math/tex; mode=display">\lim_{n\rightarrow\infty}\frac{1}{n^2}(a_1+2a_2+\cdots+na_n)=\frac{1}{2}A</script><p>对上式变换得到</p><script type="math/tex; mode=display">\frac{1}{n^2}(a_1+2a_2+\cdots+na_n) = \sum_{i=1}^n\frac{i(a_i-A)}{n^2}+\frac{n(n+1)}{2n^2}A</script><p>这样就好证明了：</p><p>因为：</p><script type="math/tex; mode=display">\lim_{n\rightarrow\infty}\frac{i}{n}=0\quad\lim_{n\rightarrow\infty}(a_i-A)=0\Rightarrow\lim_{n\rightarrow\infty}\frac{i(a_i-A)}{n}=0</script><p>所以：</p><script type="math/tex; mode=display">\lim_{n\rightarrow0}\sum_{i=1}^n\frac{i(a_i-A)}{n^2}=0</script><p>有因为：</p><script type="math/tex; mode=display">\lim_{n\rightarrow0}\frac{n(n+1)}{2n^2}A=\frac{1}{2}A</script><p>得证。</p><h4 id="单调数列的极限"><a href="#单调数列的极限" class="headerlink" title="单调数列的极限"></a>单调数列的极限</h4><p>确定原理：非空数集如果有上界则必有上确界，如果有下界则必有下确界。</p><p>设$\{a_n\}$为实数列，如果</p><script type="math/tex; mode=display">a_1\le a_2\le\cdots\le a_n\le \cdots</script><p>则称$\{a_n\}$是单调递增序列，当上式中的$\le$号换成$&lt;$时称$\{a_n\}$是严格单调递增的。</p><p>定理(单调数列的极限)：设$\{a_n\}$为单调数列</p><ol><li>如果$\{a_n\}$为单调递增的数列，则$\lim_{n\rightarrow\infty}a_n=\sup\{a_k|k\ge1\}$</li><li>如果$\{a_n\}$为单调递减序列，则$\lim_{n\rightarrow\infty}a_n=\inf\{a_k|k\ge1\}$</li></ol><p>证明：记$M=\sup\{a_k|k\ge1\}$，先考虑$M$有限的情形。任给$\epsilon&gt;0$，存在$a_N$，使得</p><script type="math/tex; mode=display">M-\epsilon<a_N\le M</script><p>因为$\{a_n\}$是单调递增序列，故当$n&gt;N$时</p><script type="math/tex; mode=display">M-\epsilon<a_N\le a_n\le M<M+\epsilon</script><p>由数列的极限定义可知：</p><script type="math/tex; mode=display">\lim_{n\rightarrow\infty}a_n=M=\sup\{a_k|k\ge1\}</script><p>如果$M=+\infty$，则任给$A&gt;0$，由上确界的定义，存在$a_N$，使得$a_N&gt;A$。由于$\{a_n\}$是单调递增序列，故当$n&gt;N$时有$a_n\ge a_N&gt;A$，从而</p><script type="math/tex; mode=display">\lim_{n\rightarrow\infty}a_n=+\infty=\sup\{a_k|k\ge1\}</script><p>由上面的推理我们也可以得到：单调有界的数列必有极限。</p><p>任何收敛数列都有单调的收敛子列。</p><p>例：设$a_1&gt;0,a_{n+1}=\frac{1}{2}(a_n+\frac{1}{a_n}),n\ge1$。研究数列$\{a_n\}$的极限。</p><p>对于$a_n&gt;0,\forall n\ge1$。我们有：</p><script type="math/tex; mode=display">a_{n+1} = \frac{1}{2}(a_n+\frac{1}{a_n})\ge\frac{1}{2}\cdot2(a_n\cdot\frac{1}{a_n})=1</script><p>故我们有：</p><script type="math/tex; mode=display">a_{n+1} = \frac{1}{2}(a_n+\frac{1}{a_n})\le\frac{1}{2}(a_n+a_n)=a_n</script><p>所以单调递减，而又有下界。因此收敛，记其极限为$A$，则$A\ge1$。另一方面：</p><script type="math/tex; mode=display">A = \lim_{n\rightarrow\infty}a_{n+1}=\lim_{n\rightarrow\infty}\frac{1}{2}(a_n+\frac{1}{a_n}) = \frac{1}{2}(A+\frac{1}{A})</script><p>故$A=1$.</p><p>我们现在讨论几个重要的极限：</p><script type="math/tex; mode=display">a_n = (1+\frac{1}{n})^n, b_n = (1+\frac{1}{n})^{n+1},n\ge1</script><blockquote><p>详细证明过程见49页</p></blockquote><p>我们可以证明$\{a_n\}$单调递增，$\{b_n\}$单调递减。两者均收敛于$e$。</p><script type="math/tex; mode=display">\lim_{n\rightarrow\infty}b_n = \lim_{n\rightarrow\infty}a_n(1+\frac{1}{n})=\lim_{n\rightarrow\infty}a_n=e</script><p>我们可以得到下述重要的不等式：</p><script type="math/tex; mode=display">\left(1+\frac{1}{n}\right)^{n}<\left(1+\frac{1}{n+1}\right)^{n+1}<e<\left(1+\frac{1}{n+1}\right)^{n+2}<\left(1+\frac{1}{n}\right)^{n+1}, \quad \forall n \geqslant 1</script><p>我们令$c_n=1+\frac{1}{2}+\cdots+\frac{1}{n}-\ln n$，可以证明$\{c_n\}$收敛，其极限为$\gamma$，称为Euler常数，计算表明</p><script type="math/tex; mode=display">\gamma = 0.5772156649\cdots</script><p>下面，我们利用单调数列来研究一般的有界数列。设$\{a_n\}$为有界数列，我们要研究它的收敛性。我们不知道$a_n$是否逐渐趋于某个数，一个好的想法就是取考虑$n$很大时$\{a_n\}$中最大的最小的项，看看它们是否相近。当然，最大和最小项不一定存在，但是我们可以利用上确界和下确界来分别代替它们。为此，令：</p><script type="math/tex; mode=display">\underline{a}_{n}=\inf \left\{a_{k} \mid k \geqslant n\right\}, \quad \bar{a}_{n}=\sup \left\{a_{k} \mid k \geqslant n\right\}</script><p>$\{\underline{a}_n\}$和$\{\bar{a}_n\}$分别是单调递增和单调递减的序列，且：</p><script type="math/tex; mode=display">\underline{a}_n\le a_n\le \bar{a}_n</script><p>单调数列$\{\underline{a}_n\}$和$\{\bar{a}_n\}$的极限分别称为$\{a_n\}$的<strong>下极限</strong>和<strong>上极限</strong>，记为：</p><script type="math/tex; mode=display">\varliminf_{n \rightarrow \infty} a_{n}=\lim _{n \rightarrow \infty} \underline{a}_{n}, \quad \varlimsup_{n \rightarrow \infty} a_{n}=\lim _{n \rightarrow \infty} \bar{a}_{n}</script><p>定理：设$\{a_n\}$为有界数列，则下列命题等价：</p><ol><li>$\{a_n\}$收敛</li><li>$\{a_n\}$的上极限和下极限相等</li><li>$\lim_{n\rightarrow\infty}(\bar{a}_n-\underline{a}_n)=0$</li></ol><p>命题：设$\{a_n\},\{b_n\}$为有界数列</p><ol><li>如果存在$N_0$，当$n&gt;N_0$时$a_n\ge b_n$，则<script type="math/tex">\varliminf_{n \rightarrow \infty} a_{n} \geqslant \varliminf_{n \rightarrow \infty} b_{n}, \quad \varlimsup_{n \rightarrow \infty} a_{n} \geqslant \varlimsup_{n \rightarrow \infty} b_{n}</script></li><li><script type="math/tex; mode=display">\overline{\lim}_{n\rightarrow\infty}(a_n+b_n)\le\bar{\lim}_{n\rightarrow\infty}a_n+\bar{\lim}_{n\rightarrow\infty}b_n</script></li></ol><p>设$a_n&gt;0,a_n\rightarrow A(n\rightarrow \infty)$。记$b_n = \sqrt[n]{a_1a_2\cdots a_n}$，则</p><script type="math/tex; mode=display">\lim_{n\rightarrow\infty}b_n=A</script><p>由已知条件，任取$\epsilon&gt;0$，则存在$N$，当$n&gt;N$时，</p><script type="math/tex; mode=display">0<a_n<A+\epsilon</script><p>于是当$n&gt;N$时，有</p><script type="math/tex; mode=display">b_n\le\sqrt[n]{a_1a_2\cdots a_N}(A+\epsilon)^{\frac{n-N}{n}} = \sqrt[n]{a_1a_2\cdots a_N(A+\epsilon)^{-N}}(A+\epsilon)</script><p>令$n\rightarrow\infty$，得</p><script type="math/tex; mode=display">\overline{\lim}_{n\rightarrow\infty}\le A+\epsilon</script><p>同理可证</p><script type="math/tex; mode=display">\underline{\lim}_{n\rightarrow\infty}b_n\ge A-\epsilon</script><p>因为$\epsilon$是任取的，从而必有</p><script type="math/tex; mode=display">\varlimsup_{n \rightarrow \infty} b_{n}=A=\varliminf_{n \rightarrow \infty} b_{n}</script><p>这说明$\{b_n\}$收敛到$A$。</p><h4 id="Cauchy准则"><a href="#Cauchy准则" class="headerlink" title="Cauchy准则"></a>Cauchy准则</h4><p>定义：设$\{a_n\}$为数列，如果任给$\epsilon&gt;0$，均存在$N=N(\epsilon)$，当$m,n&gt;N$时，有</p><script type="math/tex; mode=display">|a_m-a_n|<\epsilon</script><p>则$\{a_n\}$为Cauchy数列或基本列。</p><p>例：对于$n\ge1$，定义</p><script type="math/tex; mode=display">a_n =1+\frac{1}{2}+\cdots+\frac{1}{n}</script><p>则$\{a_n\}$不是Cauchy列。</p><p>证明：对于$n\ge1$，我们有：</p><script type="math/tex; mode=display">\begin{aligned}a_{2n}-a_n &= \frac{1}{n+1}+\cdots+\frac{1}{2n}\\&\ge\frac{1}{2n}+\cdots+\frac{1}{2n}=n\frac{1}{2n}=\frac{1}{2} \end{aligned}</script><p>由此定义即知$\{a_n\}$不是Cauchy数列。</p><p>命题：Cauchy数列必是有界数列。</p><p>证明：按定义，取$\epsilon=1$，则存在$N$，当$m,n&gt;N$时，有</p><script type="math/tex; mode=display">|a_m-a_n|<1</script><p>令$M=\max\{|a_k|+1|1\le k\le N+1\}$，则当$n\le N$时显然$|a_n|\le M$；而当$n&gt;N$时，有：</p><script type="math/tex; mode=display">|a_n|\le |a_n-a_{N+1}|+|a_{N+1}|<1+|a_{N+1}|\le M</script><p>说明$\{a_n\}$是有界数列。</p><p>定理(Cauchy准则)：$\{a_n\}$为Cauchy数列当且仅当它是收敛的。</p><p>证明：</p><p>充分性：设$\{a_n\}$收敛到$A$。则任给$\epsilon&gt;0$，存在$N$，当$n&gt;N$时，有：</p><script type="math/tex; mode=display">|a_n-A|<\frac{1}{2}\epsilon</script><p>因此，当$m,n&gt;N$时，有</p><script type="math/tex; mode=display">|a_m-a_n|\le |a_m-A|+|A-a_n|<\frac{1}{2}\epsilon+\frac{1}{2}\epsilon = \epsilon</script><p>这说明$\{a_n\}$为Cauchy数列。</p><p>必要性：设$\{a_n\}$为Cauchy数列，由上面的命题可知$\{a_n\}$是有界数列，记$A$为其上极限。我们来说明$\{a_n\}$收敛到$A$。</p><p>事实上，由于$\{a_n\}$为Cauchy数列，任给$\epsilon&gt;0$，存在$N$，当$m,n&gt;N$时，有</p><script type="math/tex; mode=display">|a_m-a_n|<\frac{1}{2}\epsilon</script><p>即</p><script type="math/tex; mode=display">-\frac{1}{2}\epsilon<a_m-a_n<\frac{1}{2}\epsilon,\forall m,n>N</script><p>在上式中令$m\rightarrow\infty$，利用上极限的保序性，得</p><script type="math/tex; mode=display">-\frac{1}{2}\epsilon\le \overline{\lim_{m\rightarrow\infty}}a_m-a_n\le\frac{1}{2}\epsilon,\forall n>N</script><p>即</p><script type="math/tex; mode=display">|A-a_n|\le\frac{1}{2}\epsilon<\epsilon,\forall n>N</script><p>这说明，$\{a_n\}$收敛到$A$。</p><p>例：</p><p>设数列$\{a_n\}$满足，存在正数$M$，对于一个$n$都有：</p><script type="math/tex; mode=display">A_n = |a_2-a_1|+|a_3-a_2|+\cdots+|a_{n}-a_{n-1}|\le M</script><p>证明数列$\{a_n\}$是Cauchy数列，从而是收敛的。</p><p>证明：</p><script type="math/tex; mode=display">A_{n+1}-A_n=|a_{n+1}-a_{n}|\ge0</script><p>所以数列$A_n$为递增数列，又因为$0\le A_n \le M$，故$A_n$单调有界，所以数列$A_n$一定有极限(收敛)。因为$A_n$是收敛的，那么它一定是cauthy数列，根据cauthy收敛准则：$\forall \epsilon&gt;0,\exists N$当$m,n&gt;N$时，有：</p><script type="math/tex; mode=display">|A_{m}-A_{n}|<\epsilon</script><p>因为</p><script type="math/tex; mode=display">\epsilon>|A_m-A_n| = |a_{n+1}-a_n|+\cdots + |a_m-a_{m-1}|\ge |a_{m}-a_{n}|</script><p>故$a_n$为Cauchy序列，故$a_n$收敛。</p><h4 id="Stolz公式"><a href="#Stolz公式" class="headerlink" title="Stolz公式"></a>Stolz公式</h4><p>引理：设$b_k&gt;0(1\le k\le n)$，且</p><script type="math/tex; mode=display">m\le \frac{a_k}{b_k}\le M,\forall 1\le k\le n</script><p>则有</p><script type="math/tex; mode=display">m\le \frac{a_1+a_2+\cdots+a_n}{b_1+b_2+\cdots+b_n}\le M</script><p>定理(Stolz公式之一)：设$\{x_n\},\{y_n\}$为数列，且$\{y_n\}$严格单调地趋于$+\infty$，如果</p><script type="math/tex; mode=display">\lim_{n\rightarrow\infty}\frac{x_n-x_{n-1}}{y_n-y_{n-1}}=A</script><p>则</p><script type="math/tex; mode=display">\lim_{n\rightarrow\infty}\frac{x_n}{y_n}=A</script><blockquote><p>证明见课本60页</p></blockquote><p>定理(Stolz公式之二)：设数列$\{y_n\}$严格单调递减趋于$0$，数列$\{x_n\}$也收敛到$0$，如果：</p><script type="math/tex; mode=display">\lim_{n\rightarrow\infty}\frac{x_n-x_{n-1}}{y_n-y_{n-1}}=A</script><p>则</p><script type="math/tex; mode=display">\lim_{n\rightarrow\infty}\frac{x_n}{y_n}=A</script>]]></content>
    
    
    <summary type="html">&lt;p&gt;梅加强老师《数学分析》&lt;/p&gt;</summary>
    
    
    
    <category term="数学" scheme="https://www.hfcouc.work/categories/%E6%95%B0%E5%AD%A6/"/>
    
    
    <category term="数学分析" scheme="https://www.hfcouc.work/tags/%E6%95%B0%E5%AD%A6%E5%88%86%E6%9E%90/"/>
    
  </entry>
  
</feed>
