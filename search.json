[{"title":"贝叶斯统计分析","url":"/2021/12/05/贝叶斯统计/","content":"\n{% pdf https://hfcouc.work/pdfs/Bayesian.pdf %}","tags":["贝叶斯数据分析"],"categories":["贝叶斯机器学习"]},{"title":"马尔科夫链蒙特卡洛方法","url":"/2021/12/04/MCMC/","content":"\n## 蒙特卡洛方法\n\n### MC实质：随机抽样\n\n为什么要抽样？\n\n假设我们有关于$x$的一个正态分布的概率密度：\n$$\nf(x) = \\frac{1}{\\sqrt{2\\pi}\\sigma}\\exp\\left(-\\frac{(x-\\mu)^2}{2\\sigma^2}\\right)\n$$\n我们很容易得到其期望。但是如果我们要求$g(x)$的期望，我们要用到：\n$$\nE(g(X)) = \\int_{-\\infty}^{+\\infty}f(x)g(x)dx\n$$\n如果$g(x)=x^2$，那么我们将很难求得其期望值，那么我们应该怎么办呢？我们可以抽取样本$X$，然后计算$g(X)$，进而计算其平均：\n$$\nE(g(X)) = \\frac{1}{N}\\sum_{i=1}^Ng(x_i)\n$$\n但是给定我们一个分布，我们怎么求得符合这个分布的样本值呢？\n\n我们可以对概率密度函数进行积分，得到累积分布函数：\n$$\nF(x) = \\int_{-\\infty}^xf(t)dt\n$$\n累计分布函数为递增函数，其值域为$[0,1]$，因此我们可以在$[0,1]$上均匀采样，假设采样的值为$y\\in[0,1]$，则求$x = F^{-1}(y)$即为我们抽样的点。\n\n但是$F(x)$真的可求吗？对于复杂的$f(x)$，$F(x)$可能不好求，于是就有下面的取舍采样法：\n\n对于复杂的概率密度函数$f(x)$，我们可以找到一个简单的可求累积分布函数的概率密度函数$q(x)$，对于常数$m$，都有$mq(x)\\ge f(x)$。这样我们可以求得$q(x)$的累计分布函数$Q(x)$，在$Q(x)$上进行采样。假设采的样本为$x_i$，则：\n\n+ 以概率$P = \\frac{f(x)}{mq(x)}$接受\n+ 以概率$1-P$拒绝\n\n但是合适的$q(x)$好找吗？其实在高维情况下并不好找。\n\n于是我们便有了马尔科夫链蒙特卡洛采样\n\n### MCMC\n\n假设状态序列为：$x_{t-2},x_{t-1},x_t,x_{t+1},x_{t+2}$，则\n$$\nP(x_{t+1}|\\cdots,x_{t-2},x_{t-1},x_t) = P(x_{t+1}|x_t)\n$$\n我们有一个状态转移矩阵$P$，表示各个状态之间的转移概率。\n\n马尔科夫链有一个好的性质，就是其初始概率分布乘以状态转移矩阵$P$多次后会收敛达到==稳定的概率分布==。\n\n所以假设我们的概率分布为$\\pi^0,\\pi^1,\\cdots,\\pi^m$，假设经过$m$步后收敛，那么在$m$步之后我们都有：$\\pi P = \\pi$。\n\n那我们如何找到这个$P$呢？\n\n我们采用一个更强的条件来找$P$，即细致平衡条件：\n$$\n\\pi(i)P(i,j) = \\pi(j)P(j,i)\n$$\n有细致平衡条件可以推出$\\pi P = \\pi$，但是反过来不一定成立。\n$$\n\\sum_{i=1}^\\infty \\pi(i)P(i,j) = \\sum_{i=1}^\\infty\\pi(j)P(j,i) = \\pi(j)\\sum_{i=1}^\\infty P(j,i) = \\pi(j)\n$$\n由此可以推出$\\pi P = \\pi$。\n\n但是是不是所有的$Q$都满足这个条件呢？显然不是，对任意$Q$，有\n$$\n\\pi(i)Q(i,j)\\neq \\pi(j)Q(j,i)\n$$\n既然随便一个矩阵$Q$不行，那么我们引入$\\alpha$，使得\n$$\n\\pi(i)Q(i,j)\\alpha(i,j) = \\pi(j)Q(j,i)\\alpha(j,i)\n$$\n\n很容易得到，使得这个等式成立的$\\alpha$为：\n$$\n\\begin{aligned}\n\\alpha(i,j) &= \\pi(j)Q(j,i)\\\\\n\\alpha(j,i) &= \\pi(i)Q(i,j)\n\\end{aligned}\n$$\n由此可以将$\\alpha(i,j)$看作是一个概率，$\\alpha\\in [0,1]$。\n\n则\n$$\nP(i,j) = Q(i,j)\\alpha(i,j)\n$$\n\n> 在这里我有个疑问，如果$\\alpha(i,j)\\in [0,1]$，那么这个式子是不太可能成立的，因为$P(i,j)\\le Q(i,j)$，且两者都为概率函数，所以上式应该为：$P(i,j) = mQ(i,j)\\alpha(i,j),m>1$。这就与接受-拒绝采样差不多了。\n\n### Metropolis-Hastings采样\n\n因为$\\alpha$的值通常较小，我们用Metropolis-Hastings采样算法来解决这个问题：\n\n核心的公式仍然没变：\n$$\n\\pi(i)Q(i,j)\\alpha(i,j) = \\pi(j)Q(j,i)\\alpha(j,i)\n$$\n只是我让两边的$\\alpha$值同时扩大相同的倍数，等式仍然成立，直到其中一侧的$\\alpha$值扩大为了$1$。\n\n假设右边的大一点，原来是：\n$$\n\\pi_iQ(i,j)\\times 0.01 = \\pi_jQ(j,i)\\times 0.05\n$$\n\n\n现在是：\n$$\n\\pi_iQ(i,j)\\times 0.2 = \\pi_jQ(j,i)\\times 1\n$$\n\n\n这样我们的接受率实际是做了如下改进，即：\n$$\n\\alpha(i,j) = \\min\\{\\frac{\\pi(j)Q(j,i)}{\\pi(i)Q(i,j)},1\\}\n$$\n很多时候，我们选择的马尔科夫链状态转移矩阵如果是对称的，即满足$Q(i,j)=Q(j,i)$，这时我们的接受率可以进一步化简：\n$$\n\\alpha(i,j) = \\min\\{\\frac{\\pi(j)}{\\pi(i)}\\}\n$$\n\n#### 例\n\n假设目标平稳分布是一个均值为$10$，标准差为$5$的正态分布，而选择的马尔科夫链状态转移矩阵$Q(i,j)$的条件转移概率是以$i$为均值，方差为$1$的正态分布在位置$j$的值。\n\n~~~python\nimport random\nimport math\nfrom scipy.stats import norm\nimport matplotlib.pyplot as plt\n\ndef norm_dist_prob(theta):\n    y = norm.pdf(theta, loc=10, scale=5)\n    return y\n\nT = 5000\npi = [0 for i in range(T)]\nsigma = 1\nt = 0\nwhile t < T-1:\n    t = t + 1\n    pi_star = norm.rvs(loc=pi[t-1], scale=sigma, size=1, random_state=None)\n    alpha = min(1, norm_dist_prob(pi_star[0]) / norm_dist_prob(pi[t-1]))\n    \n    u = random.uniform(0, 1)\n    if u < alpha:\n        pi[t] = pi_star[0]\n    else:\n        pi[t] = pi[t-1]\n\nplt.scatter(pi, norm.pdf(pi, loc=10, scale=5),label='Target Distribution', c= 'red')\nnum_bins = 50\nplt.hist(pi, num_bins, density=1, facecolor='green', alpha=0.7,label='Samples Distribution')\nplt.legend()\nplt.show()\n~~~\n\n再假设目标平稳是一个$a = 2.37, b =0.627$的$\\beta$分布，而选择的马尔可夫转移矩阵$Q(i,j)$的条件概率是以$i$为均值，方差为$1$的正态分布在位置$i$的值。\n\n~~~python\nfrom scipy.stats import beta\na, b = 2.31, 0.627\n\ndef beta_dist_prob(theta):\n    y = beta(2.31, 0.627).pdf(theta)\n    return y\n\nT = 5000\npi = [0 for i in range(T)]\nsigma = 1\nt = 0\nwhile t < T-1:\n    t = t + 1\n    pi_star = norm.rvs(loc=pi[t-1], scale=sigma, size=1, random_state=None)\n    alpha = min(1, beta_dist_prob(pi_star[0]) / beta_dist_prob(pi[t-1]))\n    \n    u = random.uniform(0, 1)\n    if u < alpha:\n        pi[t] = pi_star[0]\n    else:\n        pi[t] = pi[t - 1]\n\nplt.scatter(pi, beta(2.31, 0.627).pdf(pi),label='Target Distribution', c= 'red')\nnum_bins = 50\nplt.hist(pi, num_bins, density=1, facecolor='green', alpha=0.7,label='Samples Distribution')\nplt.legend()\nplt.show()\n~~~\n\n### 吉布斯采样\n\nM-H采样在高维时计算时间较长，算法效率较低。而且，很多时候我们甚至很难求出目标的各特征维度联合分布，但是可以方便求出各个特征之间的条件概率分布。所以我们希望对条件概率分布进行抽样，得到样本的序列。\n\n对于二维概率密度函数：\n\n![](https://static01.imgkr.com/temp/7b4a4951c849420390f5190505229c78.png)\n\n假设我们取两个点$A$和$B$，我们有\n$$\n\\pi(A) = \\pi(x_1,y_1) = \\pi(x_1)\\pi(y_1|x_1),\n\\pi(B) = \\pi(x_1,y_2) = \\pi(x_1)\\pi(y_2|x_1)\n$$\n变化一下，得：\n$$\n\\pi(A)\\pi(y_2|x_1) = \\pi(x_1)\\pi(y_1|x_1)\\pi(y_2|x_1),\n\\pi(B)\\pi(y_1|x_1) = \\pi(x_1)\\pi(y_2|x_1)\\pi(y_1|x_1)\n$$\n所以\n$$\n\\pi(A)\\pi(y_2|x_1) = \\pi(B)\\pi(y_1|x_1)\n$$\n这与细致平衡条件非常相像：\n\n我们令$\\pi(y_2|x_1)$为状态转移概率$P(A\\rightarrow B)$。\n\n则\n$$\n\\pi(A)P(A\\rightarrow B) = \\pi(B)P(B\\rightarrow A)\n$$\n假设我们有第三个点$C$：\n\n则同理\n$$\n\\pi(A)\\pi(y_1|x_2) = \\pi(C)\\pi(y_1|x_1)\n$$\n即\n$$\n\\pi(A)P(A\\rightarrow C) = \\pi(C)P(C\\rightarrow A)\n$$\n那么对于所有$A^{\\prime}$都有：\n$$\n\\pi(A)P(A\\rightarrow A^{\\prime}) = \\pi(A)P(A^{\\prime}\\rightarrow A)\n$$\n![](https://static01.imgkr.com/temp/c09972e12900423281c5fcad2216d535.png)\n\n因为我们的状态转移矩阵$P(A\\rightarrow B)=\\pi(y_2|x_1)$已知，因此我们不存在拒绝采样的问题。\n\n但是我们前面的推理都是基于有一个坐标相等，如果每个坐标都不想等怎么办？\n\n![](https://static01.imgkr.com/temp/2b8c1e58ea604756b4836abb3331502d.png)\n\n如上图的$D$点，我们规定：\n$$\nP(A\\rightarrow D)=0\n$$\n即我们只允许在**平行坐标轴**上采样。\n\n\n\n吉布斯采样步骤：\n\n+ 给定平稳分布$\\pi(x_1,x_2)$\n+ $t=0$随机产生一个初始状态$(x_1^{(0)},x_2^{(0)})$\n+ 从条件概率分布$P(x_2|x_1^{(0)})$中采样$(x_1^{(0)},x_2^{(1)})$\n+ 从条件概率分布$P(x_1|x_2^{(1)})$中采样$(x_1^{(1)},x_2^{(1)})$\n+ 不停轮换坐标轴，采取指定数量样本为止\n\n#### 例\n\n假设我们要采样的是一个二维正态分布$N(\\mu,\\Sigma)$，其中：$\\mu=(\\mu_1,\\mu_2) = (5,-1),\\Sigma=\\left(\\begin{array}{cc} \\sigma_{1}^{2} & \\rho \\sigma_{1} \\sigma_{2} \\\\ \\rho \\sigma_{1} \\sigma_{2} & \\sigma_{2}^{2} \\end{array}\\right)=\\left(\\begin{array}{ll} 1 & 1 \\\\ 1 & 4\\end{array}\\right)$。\n\n首先要求得：采样过程中需要的状态转移条件分布：\n$$\nP(x_1|x_2) = N(\\mu_1+\\rho\\sigma_1/\\sigma_2(x_2-\\mu_2),(1-\\rho^2)\\sigma_1^2),\nP(x_2|x_1) = N(\\mu_2+\\rho\\sigma_2/\\sigma_1(x_1-\\mu_1),(1-\\rho^2)\\sigma_2^2)\n$$\n\n~~~python\nimport random\nimport math\nfrom scipy.stats import beta\nfrom scipy.stats import norm\nimport matplotlib.pyplot as plt\n\nfrom mpl_toolkits.mplot3d import Axes3D\nfrom scipy.stats import multivariate_normal\n\nsamplesource = multivariate_normal(mean=[5,-1], cov=[[1,1],[1,4]])\n\ndef p_ygivenx(x, m1, m2, s1, s2):\n    return (random.normalvariate(m2+0.5*s2/s1*(x-m1), math.sqrt(1-0.5**2)*s2))\n\ndef p_xgiveny(y, m1, m2, s1, s2):\n    return (random.normalvariate(m1+0.5*s1/s2*(y-m2), math.sqrt(1-0.5**2)*s1))\n\nN = 5000\nK = 50\nx_res = []\ny_res = []\nz_res = []\nm1 = 5\nm2 = -1\ns1 = 1\ns2 = 2\ny = m2\n\nfor i in range(N):\n    for j in range(K):\n        x = p_xgiveny(y, m1, m2, s1, s2)   #y给定得到x的采样\n        y = p_ygivenx(x, m1, m2, s1, s2)   #x给定得到y的采样\n        z = samplesource.pdf([x,y])\n        x_res.append(x)\n        y_res.append(y)\n        z_res.append(z)\n\nnum_bins = 50\nplt.scatter(x_res, norm.pdf(x_res, loc=5, scale=1),label='Target Distribution x', c= 'green')\nplt.scatter(y_res, norm.pdf(y_res, loc=-1, scale=2),label='Target Distribution y', c= 'orange')\nplt.hist(x_res, num_bins, density=1, facecolor='Cyan', alpha=0.5,label='x')\nplt.hist(y_res, num_bins, density=1, facecolor='magenta', alpha=0.5,label='y')\nplt.title('Histogram')\nplt.legend()\nplt.show()\n~~~\n\n","tags":["机器学习"],"categories":["算法"]},{"title":"SVM算法","url":"/2021/11/26/SVM算法/","content":"\n## 支持向量机(SVM)\n### 线性模型\n#### 线性可分训练集\n一个训练数据集线性可分是指：$\\{(x_i,y_i)\\}_{i=1\\sim N},\\exists(w,b)$，使对$\\forall i=1\\sim N$，有\n+ 若$y_i=+1$，则$w^Tx_i+b\\ge0$\n+ 若$y_i=-1$，则$w^Tx_i+b<0$\n\n即$y_i[w^Tx_i+b]\\ge0$(公式1)\n\n对于线性可分的数据集，我们需要划一条线来分割两类不同的样本。但是分割的线有无数条，我们怎么判断哪一条线更好呢？\n![](https://static01.imgkr.com/temp/bf661c3ba75549abacf5b4e9dde0e254.png)\n很多人认为第二条线是最好的。但是因为根据免费午餐定理，这三条曲线是一样好的。那么我们为什么会认为第二条曲线是最好的呢？这是因为我们在研究问题之前，对此问题存在先验假设。有很多先验假设认为第二条直线比其余两条要好。我们只考虑其中一种假设，即假设*训练样本的位置在特征空间有测量误差*。如下图所示：\n![](https://static01.imgkr.com/temp/51e3dc9f3d9847298c80f36fc1d22328.png)\n假设红色的叉和圆圈的真实位置为红色虚线圆圈，则线3和1都会分类错误，而2不会，这说明2号线更能抵御训练样本位置的误差。\n\n那么2号线是怎么画出来的呢？\n支持向量机的创造者Vapnik是这样回答的，它首先将直线向一侧平行移动，直到它叉到一个或几个样本为止；之后再向另一侧移动，直到叉到一个或多个样本未知。\n![](https://static01.imgkr.com/temp/a184f5d78b3b4cd78df273fbd4f0adfd.png)\n我们要找的2号线找的是使得间隔最大的且位于间隔中间的线。\n**在多维的情况下，直线变为超平面**。\n\n之后我们将支持向量机转化为一个优化问题，优化问题为：\n$$\n\\begin{aligned}\n\t&\\min \\frac{1}{2}||w||^2\\\\\n\t&\\operatorname{s.t.} \\quad y_i[w^T_i+b]\\ge1\n\\end{aligned}\n$$\n\n那么这是怎么得到的呢？那面我们详细讨论一下：\n\n事实一：$w^Tb+b=0$与$aw^Tx+ab=0$是同一个平面，$a\\in R^+$。即若$(w,b)$满足公式1，则$(aw,ab)$也满足公式一。\n> 公式1：$y_i[w^Tx_i+b]\\ge0$\n\n事实二：点到平面的距离公式。\n向量$x_0$到超平面$w^Tx+b=0$的距离：\n$$\nd = \\frac{|w^Tx_0+b|}{||w||}\n$$\n当$x_0$为支持向量时，我们要做的就是最大化$d$。\n根据事实1，我们可以用$a$去缩放：\n$$\n(w,b)\\rightarrow (aw,ab)\n$$\n最终使在支持向量上$x_0$上，有：\n$$\n\t|w^Tx_0+b|=1\n$$\n此时支持向量与平面距离：\n$$\n d = \\frac{1}{||w||}\n$$\n因为最大化$\\frac{1}{||w||}$相当于最小化$||w||^2$，所以得到上述的目标函数。\n\n下面看约束条件是如何得到的。\n因为在上面的描述中我们有，对于所有的支持向量，我们有\n$$\n|w^Tx_0+b|=1\n$$\n所以对于非支持向量，我们有：\n$$\n|w^Tx_0+b|>1\n$$\n又因为：$y_i[w^Tx_i+b]\\ge0$，所以综上我们有\n$$\ny_i[w^Tx_i+b] = |w^Tx_0+b|\\ge 1\n$$\n这样我们就得到了上面提到的优化问题。\n\n这个优化问题为凸优化问题中的*二次优化问题*。\n二次规划问题：\n1. 目标函数是二次项\n2. 限制条件是一次项\n\n这样就会导致要么无解，要么只有一个极值。\n### 非线性可分\n我们改写目标函数和约束条件，使其变为：\n$$\n\\begin{aligned}\n&\\min&\\quad\\frac{1}{2}||w||^2+C\\sum_{i=1}^N\\xi_i\\\\\n&\\operatorname{s.t.}\\quad &y_i[w^Tx_i+b]\\ge 1-\\xi_i\\\\\n&\\quad &\\xi_i\\ge0\n\\end{aligned}\n$$\n其中$\\xi_i$称为松弛变量，$C\\sum_{i=1}^N\\xi_i$称为正则项。\n### 非线性问题\n对于下图所示的问题，我们不能找到一个很好的直线将两类分开：\n![](https://static01.imgkr.com/temp/491f1d06bdce47cf8cff0ed74a575eb3.png)\n但是我们可以将其映射到高维空间中的点，然后在高维空间中寻找直线。\n我们定义一个从低维到高维的映射$\\phi(x)$：\n$$\nx\\xrightarrow{\\phi}\\phi(x)\n$$\n其中$x$为低维向量，而$\\phi(x)$为一个高维映射。\n\n下面我们举一个例子：如下图所示的异或问题\n![](https://static01.imgkr.com/temp/c0bcf66b2ef143ae9b7e93f699654ddb.png)\n这个问题我们在二维空间里无法找到一条直线将其分开。\n在上图中我们令四个点分别为：\n$$\nx_1 = \\begin{bmatrix}0\\\\0\\end{bmatrix},x_2 = \\begin{bmatrix}1\\\\1\\end{bmatrix}\\in C_1\n$$\n$$\nx_3 = \\begin{bmatrix}1\\\\0\\end{bmatrix},x_4 = \\begin{bmatrix}0\\\\1\\end{bmatrix}\\in C_2\n$$\n我们令\n$$\n\\phi(x): x = \\begin{bmatrix}a\\\\b\\end{bmatrix}\\xrightarrow{\\phi}\\phi(x) = \\begin{bmatrix}a^2\\\\b^2\\\\a\\\\b\\\\ab\\end{bmatrix}\n$$\n则经过映射得到：\n$$\n\\phi(x_1)= \\begin{bmatrix}0\\\\0\\\\0\\\\0\\\\0\\end{bmatrix},\\phi(x_2)= \\begin{bmatrix}1\\\\1\\\\1\\\\1\\\\1\\end{bmatrix}\n$$\n$$\n\\phi(x_3)= \\begin{bmatrix}1\\\\0\\\\1\\\\0\\\\0\\end{bmatrix},\\phi(x_4)= \\begin{bmatrix}0\\\\1\\\\0\\\\1\\\\0\\end{bmatrix}\n$$\n我们可以令\n$$\nw= \\begin{bmatrix}-1\\\\-1\\\\-1\\\\-1\\\\6\\end{bmatrix},b=1\n$$\n来达到区分的目的。\n\n有证明显示：在越高维度情况下，找打一个线性超平面来将样本分开的概率越大。我们如何选取$\\phi$，我们将$\\phi(x)$选择为无限维。但是$\\phi(x)$为无限维，$w$将为无限维，优化问题将不可做。\n\n我们可以不知道无限维映射$\\phi(x)$的显式表达，我们只要知道一个核函数：\n$$\nK(x_1,x_2) = \\phi(x_1)^T\\phi(x_2)\n$$\n下面的优化问题：\n$$\n\\begin{aligned}\n&\\min&\\quad\\frac{1}{2}||w||^2+C\\sum_{i=1}^N\\xi_i\\\\\n&\\operatorname{s.t.}\\quad &y_i[w^T\\phi(x_i)+b]\\ge 1-\\xi_i\\\\\n&\\quad &\\xi_i\\ge0\n\\end{aligned}\n$$\n仍然可解。\n\n在SVM中常用的核函数：\n$$\nK(x_1,x_2) = e^{-\\frac{||x_1-x_2||^2}{2\\sigma^2}} = \\phi(x_1)^T\\phi(x_2)\n$$\n为高斯核函数\n$$\nK(x_1,x_2) = (x_1^Tx_2+1)^d = \\phi(x_1)^T\\phi(x_2)\n$$\n为多项式核函数，$d$为阶数。\n\n核函数$K$必须满足某些条件才能被拆成内积的形式：\n$K(x_1,x_2)$能被写成$\\phi(x_1)^T\\phi(x_2)$的充要条件为：\n1. $K(x_1,x_2)=K(x_2,x_1)$\n2. $\\forall c_i\\in R,x_i(i=1\\sim N)$，有$$\\sum_{i=1}^N\\sum_{i=1}^Nc_ic_jK(x_i,x_j)\\ge 0$$\n\n### 原问题和对偶问题\n#### 原问题\n最小化：$f(w)$\n限制条件：\n+ $g_i(w)\\le0(i=1\\sim K)$\n+ $h_i(w)=0(i=1\\sim M)$\n\n#### 对偶问题\n定义：\n$$\n\\begin{aligned}\nL(w,\\alpha,\\beta) &= f(w) + \\sum_{i=1}^K\\alpha_ig_i(w)+\\sum_{i=1}^M\\beta_ih_i(w)\\\\\n&= f(w) + \\alpha^Tg(w)+\\beta^Th(w)\n\\end{aligned}\n$$\n\n\n对偶问题的定义\n最大化：$\\theta(\\alpha,\\beta)=\\inf_w(w,\\alpha,\\beta)$，$\\inf$表示下界\n限制条件：$\\alpha_i\\ge 0,\\beta_i\\ge0$\n\n#### 原问题和对偶问题解的关系\n定理：如果$w^\\star$是原问题的解，而$\\alpha^\\star,\\beta^\\star$是对偶问题的解，则有：\n$$\nf(w^\\star)\\ge \\theta(\\alpha^\\star,\\beta^\\star)\n$$\n证明：\n$$\n\\begin{aligned}\n\\theta(\\alpha^\\star,\\beta^\\star) &= \\inf_w L(w,\\alpha^\\star,\\beta^\\star)\\\\\n&\\le L(w^\\star,\\alpha^\\star,\\beta^\\star) = f(w^\\star) + \\sum_{i=1}^K\\alpha^\\star_ig_i(w^\\star)+\\sum_{i=1}^M\\beta^\\star_ih_i(w^\\star)\\\\\n&\\le f(w^\\star)\n\\end{aligned}\n$$\n\n定义：\n$$\nG = f(w^\\star) - \\theta(\\alpha^\\star,\\beta^\\star)\\ge0\n$$\n$G$叫做原问题与对偶问题的间距。对于某些特定优化问题，可以证明：$G=0$。\n\n强对偶定理：若$f(w)$为凸函数，且$g(w)=Aw+b,h(w)=Cw+d$，则此优化问题的原问题与对偶问题的间距为$0$。即\n$$\nf(w^\\star) = \\theta(\\alpha^\\star,\\beta^\\star)\n$$\n此时我们易得对$\\forall i=1\\sim K$：\n+ 或者$\\alpha^\\star_i=0$\n+ 或者$g^{\\star}_i(w^\\star)=0$\n\n这被称为**KKT条件**。\n### 利用对偶问题求解SVM\n我们先复习一下原问题：\n$$\n\\begin{aligned}\n&\\min&\\quad\\frac{1}{2}||w||^2+C\\sum_{i=1}^N\\xi_i\\\\\n&\\operatorname{s.t.}\\quad &y_i[w^T\\phi(x_i)+b]\\ge 1-\\xi_i\\\\\n&\\quad &\\xi_i\\ge0\n\\end{aligned}\n$$\n\n根据原问题的定义形式，我们将上述问题改写：\n$$\n\\begin{aligned}\n&\\min&\\quad\\frac{1}{2}||w||^2-C\\sum_{i=1}^N\\xi_i\\\\\n&\\operatorname{s.t.}&\\quad 1+\\xi_i-y_i w^T\\phi(x_i)-y_ib\\le 0\\\\\n&\\quad &\\xi_i\\le0\n\\end{aligned}\n$$\n\n\n\n凸函数定义：\n$$\nf(\\lambda x_1+(1-\\lambda)x_2)\\le \\lambda f(x_1)+(1-\\lambda)f(x_2)\n$$\n\n我们SVM的对偶问题为：\n最大化：\n$$\n\\theta(\\alpha,\\beta) = \\inf_{(w,\\xi_i,b)}\\{\\frac{1}{2}||w||^2-C\\sum_{i=1}^N\\xi_i+\\sum_{i=1}^N\\beta_i\\xi_i+\\sum_{i=1}^N\\alpha_i[1+\\xi_i-y_i w^T\\phi(x_i)-y_ib]\\}\n$$\n限制条件：\n+ $\\alpha_i\\ge 0$\n+ $\\beta_i\\ge 0$\n\n我们要想求得$\\theta(\\alpha,\\beta)$，首先要最优化$w,\\xi_i,b$，对$L$函数求偏导：\n$$\n\\begin{aligned}\n\\frac{\\partial L}{\\partial w} = 0&\\Rightarrow w = \\sum_{i=1}^N\\alpha_iy_i\\phi(x_i)\\\\\n\\frac{\\partial L}{\\partial \\xi_i} = 0&\\Rightarrow \\beta_i+\\alpha_i=C\\\\\n\\frac{\\partial L}{\\partial b}=0&\\Rightarrow \\sum_{i=1}^N\\alpha_iy_i=0\n\\end{aligned}\n$$\n将其代入，得到：\n$$\n\\theta(\\alpha,\\beta) = \\sum_{i=1}^N\\alpha_i-\\frac{1}{2}\\sum_{i=1}^N\\sum_{j=1}^N\\alpha_i\\alpha_jy_iy_jK(x_i,x_j) \n$$\n其中$K(x_i,x_j) = \\phi(x_i)^T\\phi(x_j)$。\n\n所以对偶优化问题变为：\n最大化：\n$$\n\\theta(\\alpha) = \\sum_{i=1}^N\\alpha_i-\\frac{1}{2}\\sum_{i=1}^N\\sum_{j=1}^N\\alpha_i\\alpha_jy_iy_jK(x_i,x_j) \n$$\n限制条件：\n1. $0\\le\\alpha_i\\le C$\n2. $\\sum_{i=1}^N\\alpha_iy_i=0$\n\n这也是一个凸优化问题，解这个问题有一个标准的算法(SMO)算法。\n\n这样我们就可以求出$\\alpha_i,i=1\\sim N$，但是我们要求的是$w$和$b$，那么我们应该如何求出$w,b$呢，我们可以用之前得到的$w = \\sum_{i=1}^N\\alpha_iy_i\\phi(x_i)$，但问题是我们并不知道$\\phi(x_i)$。\n\n但是在判断样本属于哪一类的时候我们并不需要知道$w$，假设有测试样本$x$，我们知道：\n+ 若$w^T\\phi(x)+b\\ge0$，则$y=+1$\n+ 若$w^T\\phi(x)+b<0$，则$y=-1$\n\n而\n$$\n\\begin{aligned}\nw^T\\phi(x) &= \\sum_{i=1}^N\\alpha_iy_i\\phi(x_i)^T\\phi(x)\\\\\n&= \\sum_{i=1}^N\\alpha_iy_iK(x_i,x)\n\\end{aligned}\n$$\n\n但是$b$应该怎么算呢？\n应用KKT条件，我们有\n+ 要么$\\beta_i=0$，要么$\\xi_i=0$\n+ 要么$\\alpha_i=0$，要么$1+\\xi_i-y_iw^T\\phi(x_i)-y_ib=0$\n\n我们取一个$0<\\alpha_i<C\\Rightarrow \\beta_i=C-\\alpha_i>0$，\n\n此时$\\beta_i\\neq0\\Rightarrow\\xi_i=0$，因为$\\alpha_i\\neq0\\Rightarrow b = y_i - \\sum_{j=1}^N\\alpha_jy_jK(x_i,x_j)$。也可以找到所有不等于$0$的$\\alpha_i$，求得$b$取平均。\n\n### SMO算法\n最大化：\n$$\n\\theta(\\alpha) = \\sum_{i=1}^N\\alpha_i-\\frac{1}{2}\\sum_{i=1}^N\\sum_{j=1}^N\\alpha_i\\alpha_jy_iy_jK(x_i,x_j) \n$$\n限制条件：\n1. $0\\le\\alpha_i\\le C$\n2. $\\sum_{i=1}^N\\alpha_iy_i=0$\n\n因为我们要优化的变量($\\alpha_i$)很多，所以我们每次迭代只选择几个变量进行更新；又因为我们的是有约束的优化问题，所以每次更新可能会破坏我们的约束条件，为了不破坏我们的约束条件，我们每次至少选择两个变量进行优化。因此我们每次选择两个变量来进行优化。\n\n#### 两个变量二次规划的求解过程\n+ 选择两个变量，其他变量固定\n+ SMO将对偶问题转化成一系列子问题\n\n$$\n\\begin{aligned}\n\n\\min _{\\alpha_{1}, \\alpha_{2}} & W\\left(\\alpha_{1}, \\alpha_{2}\\right)=\\frac{1}{2} K_{11} \\alpha_{1}^{2}+\\frac{1}{2} K_{22} \\alpha_{2}^{2}+y_{1} y_{2} K_{12} \\alpha_{1} \\alpha_{2} \\\\\n\n& -\\left(\\alpha_{1}+\\alpha_{2}\\right)+y_{1} \\alpha_{1} \\sum_{i=3}^{N} y_{i} \\alpha_{i} K_{i 1}+y_{2} \\alpha_{2} \\sum_{i=3}^{N} y_{i} \\alpha_{i} K_{i 2} \\\\\n\n\\text { s.t. } & \\alpha_{1} y_{1}+\\alpha_{2} y_{2}=-\\sum_{i=3}^{N} y_{i} \\alpha_{i}=\\zeta \\\\\n\n& 0 \\leq \\alpha_{i} \\leq C, i=1,2\n\\end{aligned}\n$$\n+ 根据约束条件，$\\alpha_2$可以表示为$\\alpha_1$的函数\n+ 优化问题有解析解\n+ 基于初始可行解$\\alpha_1^{old},\\alpha_2^{old}$，可以得到$\\alpha_1^{new},\\alpha_2^{new}$\n\n两个变量，约束条件用二维空间中的图形表示：\n![](https://static01.imgkr.com/temp/fb18fa3a67f14d0eaecf9214ecf952ec.png)\n下面首先考虑第一种情况，根据不等式条件$\\alpha_2^{new}$的取值范围：\n$$\n\tL\\le \\alpha_2^{new} \\le H\n$$\n其中\n$$\nL = \\max(0,\\alpha_2^{old}-\\alpha_1^{old})\\quad H = \\min(C,C+\\alpha_2^{old}-\\alpha_1^{old})\n$$\n\n同理对于第二种情况，根据不等式条件$\\alpha_2^{new}$的取值范围：\n$$\n\tL\\le \\alpha_2^{new} \\le H\n$$\n其中\n$$\nL = \\max(0,\\alpha_2^{old}+\\alpha_1^{old}-C)\\quad H = \\min(C,\\alpha_2^{old}+\\alpha_1^{old})\n$$\n\n下面开始求解，求得的过程为：\n+ 先求沿着约束方向未经剪辑时的$\\alpha_2^{new,unc}$\n+ 再求剪辑后的$\\alpha_2^{new}$\n\n我们记\n$g(x)=\\sum_{i=1}^N\\alpha_iy_iK(x_i,x)+b$，即我们的判别表达式，令：\n$$\nE_i = g(x_i)-y_i = \\left(\\sum_{j=1}^N\\alpha_jy_jK(x_j,x_i)+b\\right)-y_i\n$$\n为输入$x$的预测值和真实输出$y$的差。\n为了简便，引进记号：\n$$\nv_i = \\sum_{j=3}^N\\alpha_jy_jK(x_i,x_j) = g(x_i) - \\sum_{j=1}^2\\alpha_jy_jK(x_i,x_j)-b\n$$\n目标函数写成：\n$$\n\\begin{aligned}\n\nW\\left(\\alpha_{1}, \\alpha_{2}\\right)=& \\frac{1}{2} K_{11} \\alpha_{1}^{2}+\\frac{1}{2} K_{22} \\alpha_{2}^{2}+y_{1} y_{2} K_{12} \\alpha_{1} \\alpha_{2} \\\\\n\n&-\\left(\\alpha_{1}+\\alpha_{2}\\right)+y_{1} v_{1} \\alpha_{1}+y_{2} v_{2} \\alpha_{2}\n\n\\end{aligned}\n$$\n由$\\alpha_1y_1 = \\zeta-\\alpha_2y_2$及$y_i^2=1$，我们得$\\alpha_1 = (\\zeta-y_2\\alpha_2)y_1$，代入上式得到只是$\\alpha_2$的函数的目标函数：\n$$\n\\begin{aligned}\nW(\\alpha_2) &= \\frac{1}{2}K_{11}(\\zeta-\\alpha_2y_2)^2 + \\frac{1}{2}K_{22}\\alpha_2^2+y_2K_{12}(\\zeta-\\alpha_2y_2)\\alpha_2\\\\\n&-(\\zeta-\\alpha_2y_2)y_1-\\alpha_2+v_1(\\zeta-\\alpha_2y_2)+y_2v_2\\alpha_2\n\\end{aligned}\n$$\n对$\\alpha_2$求导并令其等于$0$，得：\n$$\n\\begin{aligned}\n\n&\\left(K_{11}+K_{22}-2 K_{12}\\right) \\alpha_{2}=y_{2}\\left(y_{2}-y_{1}+\\zeta K_{11}-\\zeta K_{12}+v_{1}-v_{2}\\right) \\\\\n\n&=y_{2}\\left[y_{2}-y_{1}+\\zeta K_{11}-\\zeta K_{12}+\\left(g\\left(x_{1}\\right)-\\sum_{j=1}^{2} y_{j} \\alpha_{j} K_{1 j}-b\\right)-\\left(g\\left(x_{2}\\right)-\\sum_{j=1}^{2} y_{j} \\alpha_{j} K_{2 j}-b\\right)\\right]\n\n\\end{aligned}\n$$\n将$\\zeta=\\alpha_1^{old}y_1+\\alpha_2^{old}y_2$代入：\n$$\n\\begin{aligned}\n\n\\left(K_{11}+K_{22}-2 K 12\\right) \\alpha_{2}^{n e w, u n c} &\\left.=y_{2}\\left(\\left(K_{11}+K_{22}-2 K_{12}\\right) \\alpha_{2}^{\\text {old }} y_{2}+y_{2}-y_{1}+g\\left(x_{1}\\right)-g\\left(x_{2}\\right)\\right)\\right) \\\\\n\n&=\\left(K_{11}+K_{22}-2 K_{12}\\right) \\alpha_{2}^{\\text {old }}+y_{2}\\left(E_{1}-E_{2}\\right)\n\n\\end{aligned}\n$$\n将$\\eta = K_{11}+K_{22}-2K_{12}$代入：\n$$\n\t\\alpha_2^{new,unc} = \\alpha_2^{old}+\\frac{y_2(E_1-E_2)}{\\eta}\n$$\n我们之后对解进行剪辑：\n$$\n\\begin{cases}\nH,\\quad &\\alpha_2^{new,unc}>H\\\\\n\\alpha_2^{new,unc},\\quad&L\\le \\alpha_2^{new,unc}\\le H\\\\\nL,\\quad& \\alpha_2^{new,unc}<L\n\\end{cases}\n$$\n得到$\\alpha_1$的解：\n$$\n\\alpha_1^{new} = \\alpha_1^{old}+y_1y_2(\\alpha_2^{old}-\\alpha_2^{new})\n$$\n关于KKT条件，我们有：\n$$\n\\begin{array}{r}\n\n\\alpha_{i}=0 \\Leftrightarrow y_{i} g\\left(x_{i}\\right) \\geqslant 1 \\\\\n\n0<\\alpha_{i}<C \\Leftrightarrow y_{i} g\\left(x_{i}\\right)=1 \\\\\n\n\\alpha_{i}=C \\Leftrightarrow y_{i} g\\left(x_{i}\\right) \\leqslant 1\n\n\\end{array}\n$$\n下面我们计算阈值$b$和$E_i$\n由KKT条件，如果$0<\\alpha_1^{new}<C$，则\n$$\n\\sum_{i=1}^N\\alpha_iy_iK_{i1}+b=y_1\n$$\n$$\nb_1^{new} = y_1-\\sum_{i=3}^N\\alpha_iy_iK_{i1}-\\alpha_1^{new}y_1K_{11}-\\alpha_2^{new}y_2K_{21}\n$$\n$$\nE_i = g(x_i)-y_i = \\left(\\sum_{j=1}^N\\alpha_jy_jK(x_j,x_i)+b\\right)-y_i\n$$\n$$\nE_1 = \\sum_{i=3}^N\\alpha_iy_iK_{i1}+\\alpha_1^{old}y_1K_{11}+\\alpha_2^{old}y_2K_{21}+b^{old}-y_1\n$$\n$E_1$的表达式与$b_1^{new}$相结合，得：\n$$\nb_1^{new} = -E_1-y_1K_{11}(\\alpha_1^{new}-\\alpha_1^{old})-y_2K_{21}(\\alpha_2^{new}-\\alpha_2^{old})+b^{old}\n$$\n同理，如果$0<\\alpha_2^{new}<C$，则\n$$\n  \n\n\\begin{aligned}\n\n&0<\\alpha_{2}^{\\text {new }}<C \\\\\n\n&b_{2}^{\\text {new }}=-E_{2}-y_{1} K_{12}\\left(\\alpha_{1}^{\\text {new }}-\\alpha_{1}^{\\text {old }}\\right)-y_{2} K_{22}\\left(\\alpha_{2}^{\\text {new }}-\\alpha_{2}^{\\text {old }}\\right)+b^{\\text {old }} \\\\\n\n&E_{i}^{\\text {new }}=\\sum_{S} y_{j} \\alpha_{j} K\\left(x_{i}, x_{j}\\right)+b^{\\text {new }}-y_{i}\n\n\\end{aligned}\n$$\n如果$\\alpha_1^{new},\\alpha_2^{new}$同时满足条件$0<\\alpha_i^{new}<C$，那么$b_1^{new}=b_2^{new}$。如果$\\alpha_1^{new},\\alpha_2^{new}$是$0$或者$C$，那么$b_1^{new},b_2^{new}$以及它们之间的数都是符合KKT条件的阈值，这时选择它们的中点作为$b^{new}$。\n在每次完成两个变量的优化之后，还必须更新对应的$E_i$值，并将它们保存在列表中。$E_i$值的更新要用到$b^{new}$值，以及所有支持向量对应的$\\alpha_j$：\n$$\nE_i^{new} = \\sum_{S}y_ja_jK(x_i,x_j)+b^{new}-y_i\n$$\n其中，$S$是所有支持向量的集合。\n[关于此方面的解释](https://zhuanlan.zhihu.com/p/62367247)\n#### 变量的启发式选择\nSMO算法在每个子问题中选择两个变量优化，其中至少一个变量是违反KKT条件的。\n1. 第一个变量的选择：外循环\n\t1. 违反KKT条件最严重的样本点\n\t2. 检验样本点是否满足KKT条件：\n\t3. $$\\begin{array}{r}\n\t\\alpha_{i}=0 \\Leftrightarrow y_{i} g\\left(x_{i}\\right) \\geqslant 1 \\\\ 0<\\alpha_{i}<C \\Leftrightarrow y_{i} g\\left(x_{i}\\right)=1 \\\\ \\alpha_{i}=C \\Leftrightarrow y_{i} g\\left(x_{i}\\right) \\leqslant 1\n\t\\end{array}$$\n\n该检验是在$\\epsilon$范围内进行的。在检验过程中，外层循环首先遍历满足条件$0<\\alpha_i<C$的样本点，即在间隔边界上的支持向量点，检验它们是否满足KKT条件，如果这些样本点都满足KKT条件，那么遍历整个训练集，检验他们是否满足KKT条件。\n1. 第二个变量的检查：内循环\n\t1. 选择的标准是希望能使目标函数有足够大的变化\n\t\t1. 即对应$|E_1-E_2|$最大\n\t2. 如果内循环通过上述方法找到的点不能使目标函数有足够大的下降，则：遍历间隔边界上的样本点，测试目标函数下降\n\t\t1. 如果下降不大，则遍历所有样本点\n\t\t2. 如果依然下降不大，则丢弃外循环点，重新选择\n\n[算法实现](https://zhuanlan.zhihu.com/p/138556326)\n~~~python\ndef kernal(a, b):\n    # 高斯核, s为方差\n    return a.dot(b)\nclass SVM:\n    def __init__(self,data,label):\n        self.data = data\n        self.label = label\n        self.len = data.shape[0]\n        self.E = np.zeros((self.len,1))\n        self.alpha = np.random.random((self.len,1))\n        self.b = 0\n    def f(self,i):\n        # 传入索引\n        s = 0\n        for k in range(self.len):\n            s += self.alpha[k]*self.label[k]*kernal(self.data[i,:],self.data[k,:])\n        return s + self.b\n    def error(self):\n        # 计算E\n        for j in range(self.len):\n            E = 0\n            for i in range(self.len):\n                E += self.alpha[i]*self.label[i]*kernal(self.data[i,:],self.data[j,:])\n            self.E[j] = E + self.b - self.label[j]\n        \n    def bound(self, i,j,alpha_i, alpha_j, C):\n        # 传入两个索引，为需要优化的alpha的索引\n        # 求解alpha_j的范围\n        # C为正则化项系数\n        if self.label[i] != self.label[j]:\n            L, H = np.max([0, alpha_j-alpha_i]), np.min([C, C+alpha_j-alpha_i])\n        else:\n            L, H = np.max([0, alpha_j+alpha_i-C]), np.min([C, alpha_j+alpha_i])\n        return (L,H)\n    \n    def update(self, i,j, C):\n        # 更新alpha和b\n        # 传入索引\n        eta = kernal(self.data[i,:],self.data[i,:]) + kernal(self.data[j,:],self.data[j,:]) - 2*kernal(self.data[i,:],self.data[j,:])\n        \n        # 下面需要补充逻辑关系\n        alpha_old_i = self.alpha[i]\n        alpha_old_j = self.alpha[j]\n        self.alpha[j] = self.alpha[j] + self.label[j]*(self.E[i]-self.E[j])/eta\n        L, H = self.bound(i,j,alpha_old_i,alpha_old_j,C)\n        if self.alpha[j] >= H:\n            self.alpha[j] = H\n        elif self.alpha[j] <= L:\n            self.alpha[j] = L\n        else:\n            self.alpha[j] = self.alpha[j]\n        self.alpha[i] = self.alpha[i] + self.label[i]*self.label[j]*(alpha_old_j-self.alpha[j])\n        \n        b1 = self.b - self.E[i] - self.label[i]*(self.alpha[i]-alpha_old_i)*kernal(self.data[i,:],self.data[i,:])-self.label[j]*(self.alpha[j]-alpha_old_j)*kernal(self.data[j,:],self.data[i,:])\n        b2 = self.b - self.E[j] - self.label[i]*(self.alpha[i]-alpha_old_i)*kernal(self.data[i,:],self.data[j,:])-self.label[j]*(self.alpha[j]-alpha_old_j)*kernal(self.data[j,:],self.data[j,:])\n        \n        self.b = (b1+b2)/2\n        self.error()\n    def smo(self, epsilon, max_iter, C):\n        for k in range(max_iter):\n            I = np.intersect1d(np.argwhere(self.alpha>0),np.argwhere(self.alpha<C))\n            d = []\n            for i in I:\n                d.append(np.abs(self.label[i]*self.f(i)-1))\n            i = np.argmax(np.array(d))\n            j = np.argmax(self.E-self.E[i])\n            self.update(i,j,C)\n        \n            ge = np.array([i  for i in range(self.len) if self.alpha[i]>=1])\n            eq = np.array([i  for i in range(self.len) if self.alpha[i]==1])\n            le = np.array([i  for i in range(self.len) if self.alpha[i]<=1])\n            Ge = []\n            Eq = []\n            Le = []\n            for ig in ge:\n                Ge.appned(self.label[ig]*self.f(ig))\n            for ie in eq:\n                Eq.append(self.label[ie]*self.f(ie))\n            for il in le:\n                Le.append(self.label[il]*self.f(il))\n            \n            Ge = np.array(Ge)\n            Eq = np.array(Eq)\n            Le = np.array(Le)\n            \n            ne = np.sum(np.abs(Eq-1)>epsilon)\n            ng1 = np.sum(np.abs(Ge-1)<0)\n            ng2 = np.sum(np.abs(Ge-1)>epsilon)\n            nl1 = np.sum(np.abs(Le-1)>0)\n            nl2 = np.sum(np.abs(Le-1)>epsilon)\n            if (ne+ng1+ng2+nl1+nl2)==0:\n                break\n    def predict(self, x):\n        s = 0\n        for k in range(self.len):\n            s += self.alpha[k]*self.label[k]*kernal(self.data[k,:],x)\n        s = s + self.b\n        if s >=0:\n            return 1\n        else:\n            return -1\n~~~\n","tags":["机器学习"],"categories":["算法"]},{"title":"计算机视觉中的线性代数第一章","url":"/2021/10/31/Linear-algebra-in-cv/","content":"\n# 向量空间、基、线性映射\n## 线性组合、线性独立和秩\n在线性优化问题中，我们经常会遇到线性方程组。例如，考虑求解下列具有三个变量$x_1,x_2,x_2\\mathbb{R}$的三个线性方程组：\n$$\n\\begin{aligned}\nx_1 + 2x_2 - x_3 &= 1\\\\\n2x_1 + x_2 + x_3 &= 2\\\\\nx_1 - 2x_2 - 2x_3 &= 3\n\\end{aligned}\n$$\n解决这个问题的一个方法是引入向量$u,v,w$和$b$，为\n$$u=\\left(\\begin{array}{l}1 \\\\ 2 \\\\ 1\\end{array}\\right) \\quad v=\\left(\\begin{array}{c}2 \\\\ 1 \\\\ -2\\end{array}\\right) \\quad w=\\left(\\begin{array}{c}-1 \\\\ 1 \\\\ -2\\end{array}\\right) \\quad b=\\left(\\begin{array}{l}1 \\\\ 2 \\\\ 3\\end{array}\\right)$$\n所以我们的线性系统可以写为：\n$$\nx_1u + x_2v + x_3w = b\n$$\n> 我们通常将列向量写为$\\mathbb{R}^{3\\times1}$表示$3$行$1$列，而行向量写作$\\mathbb{R}^3$。\n\n下面的公式\n$$\nx_1u + x_2v + x_3w\n$$\n其中$u,v,w$为向量并且$x_i\\in \\mathbb{R}$被称为线性映射。使用这种符号，我们线性系统的解\n$$\nx_1u + x_2v + x_3w\n$$\n等价于确定$b$是否可以被写成$u,v,w$的线性组合的形式。\n如果$u,v,w$是线性独立的，着意味着不存在三元组$(x_1,x_2,x_3)\\neq(0,0,0)$使得\n$$\nx_1u + x_2v + x_3w = 0\n$$\n可以证明所有属于$\\mathbb{R}^{3\\times 1}$的向量可以写成$u,v,w$的线性组合。\n事实上，任何向量$z\\in \\mathbb{R}^{3\\times 1}$可以被唯一地写成下列线性组合的形式：\n$$\nz = x_1u+x_2v+x_3w\n$$\n这是因为：\n\n> 假设\n> $$\n> z = x_1u + x_2v + x_3w = y_1u+y_2v+y_2w\n> $$\n> 移项，得：\n> $$\n> (y_1-x_1)u + (y_2-x_2)v + (y_3-x_3)w = 0\n> $$\n> 通过线性独立，我们得到：\n> $$\n> y_1-x_1 = y_2-x_2 = y_3-x_3=0\n> $$\n> 这意味着$z$只有一种线性组合的表示方法。\n\n但是我们如何确定一些向量是否是线性独立的呢？\n一个办法是计算数值$\\det(u,v,w)$，称作$(u,v,w)$的秩，并检查其是否为零，不为零说明线性独立。\n我们也可以将我们的线性系统写为矩阵的形式。我们的线性系统以矩阵形式表示为$Ax=b$的观点强调了这样一个事实，即映射$x\\mapsto Ax$是一个线性映射。这意味着：\n$$\nA(\\lambda x) = \\lambda(Ax)\n$$\n对于任何$x\\in \\mathbb{R}^{3\\times 1}$和$\\lambda \\in \\mathbb{R}$，并且\n$$\nA(u+v) = Au + Av\n$$\n对于任何$u,v\\in \\mathbb{R}^{3\\times 1}$。我们可以将矩阵$A$看作是表达从$\\mathbb{R}^{3\\times1}$到$\\mathbb{R}^{3\\times1}$的线性映射并且求解系统$Ax=b$相当于确定$b$是否为此线性映射的像(image)。\n考虑一个$3\\times3$的矩阵$A$，\n$$\nA =\\left(\\begin{array}{lll}a_{11} & a_{12} & a_{13} \\\\ a_{21} & a_{22} & a_{23} \\\\ a_{31} & a_{32} & a_{33}\\end{array}\\right)\n$$\n它的列是三个向量，表示为$A^1,A^2,A^3$，给定任意向量$x = (x_1,x_2,x_3)$，我们将乘积$Ax$定义为线性组合的形式\n$$\n  \n\nA x=x_{1} A^{1}+x_{2} A^{2}+x_{3} A^{3}=\\left(\\begin{array}{l}\n\na_{11} x_{1}+a_{12} x_{2}+a_{13} x_{3} \\\\\n\na_{21} x_{1}+a_{22} x_{2}+a_{23} x_{3} \\\\\n\na_{31} x_{1}+a_{32} x_{2}+a_{33} x_{3}\n\n\\end{array}\\right)\n$$\n。常见的模式是，$Ax$的第$i$个坐标由行向量($A$的第$i$行)乘以列向量$x$的某种称为内积的乘积给出：\n$$\n\\left(\\begin{array}{lll}a_{i 1} & a_{i 2} & a_{i 3}\\end{array}\\right) \\cdot\\left(\\begin{array}{l}x_{1} \\\\ x_{2} \\\\ x_{3}\\end{array}\\right)=a_{i 1} x_{1}+a_{i 2} x_{2}+a_{i 3} x_{3}\n$$\n内积非常重要。首先，我们量化\n$$\n ||x||_2= \\sqrt{x\\cdot x} = (x_1^2+\\cdots+x_n^2)^{1/2}\n$$\n为向量长度的推广，称为欧几里得范数，或者$\\ell^2$范数。另外，可以证明以下不等式\n$$\n|x\\cdot y| \\le ||x||\\cdot||y||\n$$\n如果$x,y\\neq0$，比值$(x\\cdot y)/(||x||\\cdot||y||)$可以被看作是角度的余弦值。\n\n保持内积不变的矩阵(正交矩阵)$Q$，即对所有的$x,y\\in \\mathbb{R}^n, <Qx, Qy>=<x,y>$也有着非常重要的作用。它们可以被认为是广义的旋转。 ^zkwecp\n\n返回到矩阵，如果$A$为一个包含$n$列$A^1,\\cdots,A^n\\in \\mathbb{R}^m$的$m\\times n$矩阵，并且$B$为一个包含$p$列$B^1,\\cdots,B^p\\in \\mathbb{R}^n$的$n\\times p$矩阵，我们可以生成$p$个向量：  ^614093\n$$\nAB^1,\\cdots,AB^p\n$$\n这$p$个向量构成$m\\times p$矩阵$AB$，其第$j$列为$AB^j$。但是我们知道$AB^j$的第$i$个坐标是$A$的第$i$行和$B$的第$j$列的内积：\n$$\n\\left(\\begin{array}{llll}a_{i1}&a_{i2}&\\cdots&a_{in}\\end{array}\\right)\\cdot\\left(\\begin{array}{c}b_{1j}\\\\b_{2j}\\\\\\vdots\\\\b_{nj}\\end{array}\\right) = \\sum_{k=1}^na_{ik}b_{kj}\n$$\n所以我们定义了矩阵的乘积\n$$\n(AB)_{ij} = \\sum_{k=1}^na_{ik}b_{kj}\n$$\n假设$A$是一个$n\\times n$矩阵并且我们想要求解线性系统\n$$\nAx = b\n$$\n其中$b\\in \\mathbb{R}^n$。假设我们可以找到一个$n\\times n$的矩阵$B$使得\n$$\nBA^i = e_i,\\quad i=1,\\cdots,n\n$$\n其中$e_i = (0,\\cdots,0,1,0,\\cdots,0)$只有第$i$个位置的数据为$1$。我们称$n\\times n$矩阵\n$$\n  \n\nI_{n}=\\left(\\begin{array}{cccccc}1 & 0 & 0 & \\cdots & 0 & 0 \\\\ 0 & 1 & 0 & \\cdots & 0 & 0 \\\\ 0 & 0 & 1 & \\cdots & 0 & 0 \\\\ \\vdots & \\vdots & \\vdots & \\ddots & \\vdots & \\vdots \\\\ 0 & 0 & 0 & \\cdots & 1 & 0 \\\\ 0 & 0 & 0 & \\cdots & 0 & 1\\end{array}\\right)\n$$\n为单位矩阵，它的第$i$列为$e_i$，则上面等价于\n$$\nBA = I_n\n$$\n如果$Ax = b$，在等式的两边左乘以$B$，我们有\n$$\nB(Ax) = Bb\n$$\n因为$B(Ax) = (BA)x = I_nx = x$，因此我们有\n$$\nx = Bb\n$$\n矩阵$B$为矩阵$A$的逆，常被表示为$A^{-1}$。很容易证明存在唯一的矩阵使得：\n$$\nAA^{-1} = A^{-1}A = I_n\n$$\n如果一个方阵有逆矩阵，则说明他是可逆或者非奇异的，反之为奇异的。\n总之，如果$A$是一个可逆方阵，则线性系统$Ax = b$有唯一解$x=A^{-1}b$。但是在实践中，这并不是求解线性系统的一个很好的方法，因为计算$A^{-1}$的代价太大了。一个求解线性系统的实用的方法是高斯消去。其他的求解线性系统$Ax = b$的实用的方法利用$A$的因式分解(QR分解，SVD分解)，用到下面定义的正交矩阵的概念。\n给定一个$m\\times n$的矩阵$A=(a_{kl})$，$n\\times m$矩阵$A^T = (a_{ij}^T)$的第$i$行是$A$的第$i$列，这意味着$a_{ij}^T=a_{ji},i=1,\\cdots,n,\\quad j=1,\\cdots,m$称为$A$的转置。一个$n\\times n$矩阵$Q$使得\n$$\nQQ^T = Q^TQ = I_n\n$$\n被称为正交矩阵。正交矩阵$Q$的逆$Q^{-1}$等于其转置$Q$。正交矩阵具有非常重要的作用。在几何上，它们对应于保持长度不变的线性变换。线性代数的一个重要的结果表明，每个$m\\times n$矩阵$A$都可以表示为：\n$$\nA = V\\Sigma U^T\n$$\n其中$V$是一个$m\\times m$的正交矩阵，$U$是一个$n\\times n$的正交矩阵，$\\Sigma$是一个$m\\times n$的矩阵，其非零项为非负对角线项$\\sigma_1\\ge \\sigma_2\\ge \\cdots\\ge\\sigma_p$，其中$p = \\min(m,n)$，称为$A$的奇异值。分解$A = V\\Sigma U^T$被称为$A$的奇异值分解，或SVD。\nSVD分解可以用来\"求解\"线性系统$Ax=b$其中$A$为一个$m\\times n$矩阵，甚至当此系统无解的时候。这个在公式的数量大于变量的数量的情况下发生($m>n$)，这种情况下系统被称为超定的。\n当然并不存在奇迹使一个无法解决的系统有解。但是我们可以寻找一个好的近似解，即最小化误差$Ax-b$的某个度量方法的向量$x$。我们可以使用误差的平方欧几里得范数$||Ax-b||_2^2$。这个度量是可导的，并且存在唯一的向量$x^+$最小化$||Ax-b||_2^2$。此外，$x^+$由表达式$x^+=A^+b$给出，其中$A^+$为$A$的伪逆，并且$A^+$可以由$A$的SVD分解$A = V\\Sigma U^T$计算。事实上，$A^+=U\\Sigma^+V^T$，其中$\\Sigma^+$是通过将$\\Sigma$的所有非零奇异值变为$\\sigma_i^{-1}$，零值不变并转置。\n除了寻找使欧几里得范数$||Ax-b||_2^2$最小的向量外，我们可以添加一个惩罚项$K||x||_2^2(K>0)$来最小化$||Ax-b||_2^2+K||x||_2^2$。这种方法被称为岭回归。事实证明，存在唯一的极小化$x^+=(A^TA+KI_n)^{-1}A^Tb$。\n另一种方法是用$K||x||_1$替换惩罚项$K||x||_2^2$，其中$||x||_1=|x_1|+\\cdots+|x_n|$($x$的$\\ell^1$范数)。值得注意的是，使$||Ax-b||_2^2+K||x||_1$最小的$x$为稀疏的，这意味着$x$的很多元素是零。这种方法被称为laoss。\n在现实世界中，线性代数被证明非常有效的另一个很好的例子是数据压缩问题，也就是说，使用小得多的存储量来表示非常大的数据集。\n一般数据集表示为一个$m\\times n$的矩阵$A$其中每一行表示一个$n$维数据点并且一般$m\\ge n$。在大多数的应用中，数据不是独立的因此$A$的秩比$\\min(m,n)$小的多，并且低秩分解的目标是将$A$分解为两个矩阵$B$和$C$，其中$B$为$m\\times k$矩阵，$C$为$k\\times n$矩阵，$k\\ll\\min(a,b)$：\n$$\n\\left(\\begin{array}{c}A \\\\ m \\times n\\end{array}\\right)=\\left(\\begin{array}{c}B \\\\ m \\times k\\end{array}\\right)\\left(\\begin{array}{c}C \\\\ k \\times n\\end{array}\\right)\n$$\n现在要找到如上所述的精确因式分解通常成本太高，所以我们寻找一个低秩矩阵$A^{\\prime}$，它是$A$的“好”近似值。为了使这一陈述精确，我们需要定义一种机制来确定两个矩阵的距离。这可以使用矩阵范数来完成。我们的目标是找到一个低秩矩阵$A^{\\prime}$来最小化范数\n$$\n||A-A^{\\prime}||^2\n$$\n在秩最大为$k$的矩阵$A^{\\prime}$上。\n## 向量空间\n### 群\n一个实向量空间是具有两种运算的集合$E$：$+:E\\times E\\rightarrow E$和$\\cdot:\\mathbb{R}\\times E\\rightarrow E$，称为加法和数乘法，并且满足一些简单的性质。首先，加法下的$E$必须是可交换的。\n```ad-note\n注意，向量空间不仅是代数对象；它们也是几何对象。\n```\n定义：\n群是具有二元运算$\\cdot:G\\times G\\rightarrow G$的集合$G$，它将元素$a\\cdot b\\in G$与每一对元素$a,b\\in G$相关联，并且具有如下性质：满足结合律、有一个单位元$e\\in G$，并且$G$中的每个元素都有逆元。即满足下列三条性质：\n\n1. $a\\cdot(b\\cdot c) = (a\\cdot b)\\cdot c$\n2. $a\\cdot e = e\\cdot a = a$\n3. 对于每一个$a\\in G$，存在$a^{-1}\\in G$使得$a\\cdot a^{-1}=a^{-1}\\cdot a = e$\n\n一个群是可交换的如果：\n$$\na\\cdot b = b\\cdot a\n$$\n具有运算$\\cdot:M\\times M\\rightarrow M$的集合$M$，并且元素$e$只满足条件1和条件2，被称为一个幺半群。例如，集合$\\mathbb{N} = \\{0,1,\\cdots,n,\\cdots\\}$是以$0$为单位元的加法幺半群，但不是群。\n\n群的例子\n\n> 这个群没看懂什么意思：\n>\n> 给定任意非空集$S$，双射集$f:S\\rightarrow S$，也称为$S$的置换，是函数合成下的群。单位元为单位函数$\\operatorname{id}_S$。只要$S$中有两个以上的元素，这个群就不是交换群。\n\n具有实或复数元素的$n\\times n$矩阵集合是矩阵加法下的交换群，单位元为零矩阵。符号表示为$\\mathrm{M}_n(\\mathbb{R})$或者$\\mathrm{M}_n(\\mathbb{C})$。\n\n具有实数或复数元素的$n\\times n$可逆矩阵集合是矩阵乘法下的群，单位元为单位矩阵$I_n$。这个群被称为一般线性群，通常用$\\mathbf{GL}(n,\\mathbb{R})$或者$\\mathbf{GL}(n,\\mathbb{C})$表示。\n\n具有实数或复数元素并且行列式为$+1$的$n\\times n$可逆矩阵的集合矩阵乘法下的群，单位元为单位矩阵$I_n$。这个群被称为特殊线性群并且用$\\mathbf{SL}(n,\\mathbb{R})$或$\\mathbf{SL}(n,\\mathbb{C})$表示。\n\n具有实数元素并且满足$RR^T = R^TR = I_n$的并且行列式为$+1$的可逆矩阵集合为矩阵乘法下的群，被称为特殊正交群，用符号$\\mathbf{SO}(n)$表示。它与$\\mathbb{R}$上的旋转相对应。\n\n给定开区间$(a,b)$和连续函数：$f:(a,b)\\rightarrow \\mathbb{R}$的集合$\\mathcal{C}(a,b)$是一个运算$f+g$下的交换群：\n$$\n(f+g)(x) = f(x) + g(x)\n$$\n\n通常用$+$表示交换群$G$的运算，在这种情况下，元素$a^{-1}$的逆用$-a$表示。\n\n群的单位元是唯一的。事实上，我们可以证明一个更一般的命题：\n\n命题：如果一个二元运算：$\\cdot:M\\times M\\rightarrow M$为结合的并且$e^{\\prime}$为左单位元，$e^{\\prime\\prime}$为右单位元，这意味着：\n$$\ne^{\\prime}\\cdot a = a\\quad \\forall a\\in M\n$$\n并且\n$$\na\\cdot e^{\\prime\\prime} = a\\quad \\forall a\\in M\n$$\n则\n$$\ne^{\\prime} = e^{\\prime\\prime}\n$$\n> 如果我们令$a = e^{\\prime\\prime}$，则\n> $$\n> e^{\\prime}\\cdot e^{\\prime\\prime} = e^{\\prime\\prime}\n> $$\n> 并且令$a = e^{\\prime}$，我们有\n> $$\n> e^{\\prime}\\cdot e^{\\prime\\prime}\n> $$\n> 因此\n> $$\n> e^{\\prime} = e^{\\prime}\\cdot e^{\\prime\\prime} = e^{\\prime\\prime}\n> $$\n\n命题：在一个单位元为$e$的幺半群$M$中，如果元素$a\\in M$有左逆$a^{\\prime}\\in M$和右逆$a^{\\prime\\prime}\\in M$，则$a^{\\prime} = a^{\\prime\\prime}$。\n\n> $$\n> (a^{\\prime}\\cdot a)\\cdot a^{\\prime\\prime} = e\\cdot a^{\\prime\\prime} = a^{\\prime\\prime}\n> $$\n>\n> $$\n> a^{\\prime}\\cdot (a\\cdot a^{\\prime\\prime}) = a^{\\prime\\prime}\\cdot e = a^{\\prime}\n> $$\n> 所以\n> $$\n> a^{\\prime} = a^{\\prime\\prime}\n> $$\n\n### 环\n向量空间是具有标量乘法$\\cdot:K\\times E\\rightarrow E$的交换群，它允许$K$中的元素对$E$的向量进行重新缩放。集合$K$本身是一个称为域的代数结构。域是环的一种特殊的结构。下面我们先介绍环。\n\n环的定义：环是具有两种运算$+:A\\times A\\rightarrow A$(加法)和$*:A\\times A\\rightarrow A$(乘法)的集合$A$，并满足下列性质：\n1. $A$是加法上的交换群\n2. $*$是可结合的并且有单位元$1\\in A$\n3. $*$对于$+$是满足分配律的\n\n加法的单位元用$0$表示，$a$的加法逆元用$-a$表示。具体地，环具有如下性质：\n1. $a+(b+c) = (a+b)+c$：加法结合律\n2. $a+b = b+a$：加法交换律\n3. $a + 0 = 0 + a$：零\n4. $a + (-a) = (-a) + a = 0$：加法逆元\n5. $a*(b*c) = (a*b)*c$：乘法结合律\n6. $a*1 = 1*a=a$：乘法单位元\n7. $(a+b)*c = a*b + b*c$：乘法分配律\n8. $a*(b+c) = a*b+a*c$：乘法分配律\n\n环$A$是交换的如果\n$$\na*b = b*a\\quad \\forall a,b\\in A\n$$\n我们还能推出它的两条性质：$0\\cdot \\alpha = \\alpha\\cdot 0=0,(-\\alpha)\\cdot v = \\alpha\\cdot(-v)=-(\\alpha v)$\n\n> $$\n> \\begin{aligned}\n> a*0 &= 0*a = 0\\\\\n> a*(-b)&= (-a)*b = -(a*b)\n> \\end{aligned}\n> $$\n> $$\n> a*0 = a*(0+0) = a*0 + a*0\\rightarrow a*0=0\n> $$\n> 对于第二个性质假设$\\forall a,b\\in A$\n> $$\n> a*(-b) + a*b = a*(b+(-b)) = a*0 = 0\n> $$\n> $$\n> (-a)*b + a*b = ((-a)+a)*b = 0*b = 0\n> $$\n> 所以$(-a)*b$和$a*(-b)$都为$ab$的加法逆元即为$-ab$\n\n上式意味着如果$1=0$，则$a=0,\\forall a\\in A$，那么$A = \\{0\\}$。环$A=\\{0\\}$被称为**平凡环**。$1\\neq0$的环被称为非平凡环。\n### 域\n域是一个交换环$K$，满足$K-\\{0\\}$是乘法下的群。\n\n定义：集合$K$是一个域如果它是满足下列性质的环：\n1. $0\\neq1$\n2. $K^*=K-\\{0\\}$是乘法下的群\n3. $*$是交换的\n\n如果只满足1和2但是不满足3我们称其为**斜域**或非交换域。\n### 向量空间\n\n定义\n一个实向量空间是向量集合$E$，集合$E$上有两个运算：$+:E\\times E\\rightarrow E$(称为向量加法)和$\\cdot:\\mathbb{R}\\times E\\rightarrow E$(数乘法)并满足下列条件，对于$\\alpha,\\beta\\in \\mathbb{R}$和$u,v\\in E$：\n\n1. $E$为加法上的交换群，单位元为$0$\n2. $\\alpha\\cdot(u+v) = (\\alpha\\cdot u)+(\\alpha\\cdot v)$\n3. $(\\alpha+\\beta)\\cdot u = (\\alpha\\cdot u)+(\\beta\\cdot u)$\n4. $(\\alpha*\\beta)\\cdot u = \\alpha\\cdot(\\beta\\cdot u)$\n5. $1\\cdot u = u$\n\n从上面的性质我们可以推出：$\\alpha$\n\n其中$*$表示$\\mathbb{R}$上的乘法。\n\n> 对于任何$u\\in E$和任何$\\lambda\\in E$，如果$\\lambda \\neq 0$并且$\\lambda\\cdot u=0$，则$u=0$。\n>\n> 事实上，因为$\\lambda\\neq0$，它有乘法逆元$\\lambda^{-1}$，因此从$\\lambda\\cdot u=0$，我们有\n> $$\n> \\lambda^{-1}\\cdot(\\lambda\\cdot u) = \\lambda^{-1}\\cdot 0 \n> $$\n> 但是，我们可以得到$\\lambda^{-1}\\cdot0=0$，所以\n> $$\n> \\lambda^{-1}\\cdot(\\lambda\\cdot u) = (\\lambda^{-1}\\lambda)\\cdot u = 1\\cdot u = u\n> $$\n> 所以$u=0$。\n\n向量空间的一个非常重要的例子是两个向量空间之间的线性映射集。令$X$为一个非空集合并且$E$为一个向量空间。所有函数$f:X\\rightarrow E$的集合可以构成如下向量空间：给定两个任意函数$f:X\\rightarrow E$和$g:X\\rightarrow E$，令$(f+g):X\\rightarrow E$定义为\n$$\n(f+g)(x) = f(x)+g(x)\n$$\n并且对于任何$\\lambda\\in \\mathbb{R}$，令$\\lambda f:X\\rightarrow E$定义为\n$$\n(\\lambda f)(x) = \\lambda f(x)\n$$\n。\n## 索引族；求和符号$\\sum_{i\\in I}a_i$\n给定一个集合$A$，回想一下序列是一个有序的$n$元组$(a_1,\\cdots,a_n)\\in A^n$，其元素来自于$A$，对于某些自然数$n$。序列的元素不需要不同并且顺序是很重要的。例如，$(a_1,a_2,a_1)$和$(a_2,a_1,a_1)$是$A^3$上两个不同的序列。它们的基本集合是$\\{a_1,a_2\\}$。\n我们刚才定义的是**有限**序列，这可以被看作是从$\\{1,2,\\cdots,n\\}$到集合$A$的函数，序列$(a_1,\\cdots,a_n)$的第$i$个元素是$i$在此函数下的象。这一观点是非常非常有用的，因为它允许我们将有限序列定义为函数：$s:\\mathbb{N}\\rightarrow A$。但是，我们为什么要把自己限制在像$\\{1,2,\\cdots\\}$或$\\mathbb{N}$这样的索引集内呢？\n索引集的主要作用是对每个元素进行唯一标记，尽管方便，但标记的顺序并不重要。\n定义：给定集合$A$，$A$中元素的一个$I-$索引集为一个函数$a:I\\rightarrow A$，其中$I$是可以被看作索引集的任何集合。因为函数$a$由它的图所确定：\n$$\n\\{(i,a(i))|i\\in I\\}\n$$\n族$a$可以被看作对$a=\\{(i,a(i))|i\\in I\\}$的集合。为了符号上的简单我们用$a_i$表示$a(i)$，用$(a_i)_{i\\in I}$表示族$a=\\{(i,a(i))|i\\in I\\}$。\n\n如果索引集合$I$为完全有序的，族$(a_i)_{i\\in I}$常被称为$I-$序列。有趣的是，集合$A$可以被看作$A-$索引族$\\{(a,a)|a\\in I\\}$对应于单位函数。\n\n\n我们也需要注意一个问题，那就是定义形式$\\sum_{i\\in I}a_i$的和，其中$I$是任何有限的索引集并且$(a_i)_{i\\in I}$为具有二元运算$+:A\\times A\\rightarrow A$的一些集合$A$中的一族元素，并且$+$是结合和交换的。\n\n问题是$+$运算只告诉我们如何去计算两个元素之间的$a_1+a_2$，没有告诉我们如何进行两个元素以上的运算。我们要做的是通过序列来定义$a_1+a_2+a_3$，每一步包含两个元素。并且如果$+$是结合和交换的，很冥想$\\sum_{i\\in I}a_i$的和并不依赖于运算的顺序。\n\n首先我们定义和$\\sum_{i\\in I}a_i$，其中$I$为有限不同的自然数的序列，写作$I=(i_1,\\cdots,i_m)$。如果$I=(i_1,\\cdots,i_m)$其中$m\\ge2$，我们用$I-\\{i_1\\}$表示序列$(i_2,\\cdots,i_m)$。我们对$I$的大小$m$进行归纳。令\n$$\n\\begin{aligned}\n\\sum_{i\\in I}a_i&=a_{i1},\\quad \\text{if }m=1\\\\\n\\sum_{i\\in I}a_i &= a_{i1}+\\left(\\sum_{i\\in I-\\{i_1\\}}a_i\\right),\\quad \\text{if }m>1\n\\end{aligned}\n$$\n例如，如果$I=(1,2,3,4)$，我们有\n$$\n\\sum_{i\\in I}a_i = a_1 + (a_2+(a_3+a_4))\n$$\n如果$+$不满足结合律的话，那么不同组的划分将得到不同的答案。\n\n但是，如果$+$是结合的，只要元素的顺序保持不变，则$\\sum_{i\\in I}a_i$不依赖于组的划分。例如，如果$I=(1,2,3,4,5)$，$J_1=(1,2)$和$J_2=(3,4,5)$，我们希望\n$$\n\\sum_{i\\in I}a_i = \\left(\\sum_{j\\in J_1}a_j\\right) + \\left(\\sum_{j\\in J_2}a_j\\right)\n$$\n这个是成立的，因为我们有以下性质。\n\n性质：给定任何具有结合二元运算$+:A\\times A\\rightarrow A$的非空集合$A$，对于任何具有不同自然数的非空有限序列$I$和对于将$I$分为$p$个非空序列$I_{k_1},\\cdots,I_{k_p}$的任何分割，对于具有不同自然数的非空序列$K=(k_1,\\cdots,k_p)$，使得$k_i<k_j$暗示$\\alpha<\\beta$对于所有$\\alpha\\in I_{k_i}$和所有$\\beta\\in I_{k_j}$，对于$A$中元素$(a_i)_{i\\in I}$的每一个序列，我们有：\n$$\n\\sum_{\\alpha\\in I}a_{\\alpha} = \\sum_{k\\in K}(\\sum_{\\alpha\\in I_k}a_{\\alpha})\n$$\n\n证明：我们对大小为$n$的$I$进行归纳。\n如果$n=1$，则我们一定有$p=1$并且$I_{k1}=I$，因此性质一定成立。\n之后，假设$n>1$。如果$p=1$则$I_{k1}=I$并且公式是容易解决的，因此假设$p\\ge2$，并且令$J=(k_2,\\cdots,k_p)$。这样有两种情况：\n情况1：序列$I_{k_1}$有单个元素，为$\\beta$，为$I$的第一个元素。在这种情况下，$C$表示$I$去除掉第一个元素$\\beta$后的序列。通过定义：\n$$\n\\sum_{\\alpha\\in I}a_{\\alpha} = a_{\\beta} + \\left(\\sum_{\\alpha\\in C}a_{\\alpha}\\right)\n$$\n和\n$$\n\\sum_{k\\in K}\\left(\\sum_{\\alpha\\in I_k}a_{\\alpha}\\right) = a_{\\beta}+\\left(\\sum_{j\\in J}\\left(\\sum_{\\alpha\\in I_j}a_{\\alpha}\\right)\\right)\n$$\n因为$|C|=n-1$，通过归纳法的假设，我们有：\n$$\n\\left(\\sum_{\\alpha\\in C}a_{\\alpha}\\right)= \\sum_{j\\in J}\\left(\\sum_{\\alpha\\in I_j}a_{\\alpha}\\right)\n$$\n这就证明了上述情况。\n\n情况2：序列$I_{k_1}$至少有两个元素。在这种情况下，令$\\beta$为$I$的第一个元素(也是$I_{k_1}$的)，令$I^{\\prime}$为去除它的第一个元素$\\beta$后的序列；$I^{\\prime}_{k_1}$为$I_{k_1}$去除$\\beta$后的序列，并且令$I^{\\prime}_{k_i}=I_{k_i},i=2,\\cdots,p$。序列$I^{\\prime}$有$n-1$个元素，因此将归纳假设应用到$I^{\\prime}$和$I^{\\prime}_{k_i}$，我们得到：\n$$\n\\sum_{\\alpha\\in I^{\\prime}}a_{\\alpha} = \\sum_{k\\in K}\\left(\\sum_{\\alpha\\in I^{\\prime}_k}a_{\\alpha}\\right) = \\left(\\sum_{\\alpha\\in I^{\\prime}_{k_1}}a_{\\alpha}\\right)+\\sum_{j\\in J}\\left(\\sum_{\\alpha\\in I_j}a_{\\alpha}\\right)\n$$\n如果我们把左边加到$\\alpha_{\\beta}$上，通过定义我们得到：\n$$\n\\sum_{\\alpha\\in I}a_{\\alpha}\n$$\n如果我们把右边加到$a_{\\beta}$上，使用结合性和索引和的定义：\n$$\n\\begin{aligned}\na_{\\beta}+\\left(\\left(\\sum_{\\alpha \\in I_{k_{1}}^{\\prime}} a_{\\alpha}\\right)+\\left(\\sum_{j \\in J}\\left(\\sum_{\\alpha \\in I_{j}} a_{\\alpha}\\right)\\right)\\right) \\\\\n&=\\left(a_{\\beta}+\\left(\\sum_{\\alpha \\in I_{k_{1}}^{\\prime}} a_{\\alpha}\\right)\\right)+\\left(\\sum_{j \\in J}\\left(\\sum_{\\alpha \\in I_{j}} a_{\\alpha}\\right)\\right) \\\\\n&=\\left(\\sum_{\\alpha \\in I_{k_{1}}} a_{\\alpha}\\right)+\\left(\\sum_{j \\in J}\\left(\\sum_{\\alpha \\in I_{j}} a_{\\alpha}\\right)\\right)=\\sum_{k \\in K}\\left(\\sum_{\\alpha \\in I_{k}} a_{\\alpha}\\right)\n\\end{aligned}\n$$\n得证。\n如果$I=(1,\\cdots,n)$，我们用$\\sum_{i=1}^n$代替$\\sum_{i\\in I}a_i$。因为$+$是结合的，上述性质证明和$\\sum_{i\\in I}a_i$与元素的分组无关，这说明了符号$a_1+\\cdots+a_n$的正确性(没有括号)。\n如果我们假设我们$A$上的结合的二元运算符是交换的，那么我们可以证明和$\\sum_{i\\in I}a_i$不依赖于索引集$I$的顺序。\n","tags":["线性代数"],"categories":["数学"]},{"title":"算法学习笔记第一节","url":"/2021/10/31/算法学习笔记/","content":"\n{% raw %}\n\n<div style=\"position: relative; width: 100%; height: 0; padding-bottom: 75%;\"><iframe src=\"//player.bilibili.com/player.html?aid=933642480&bvid=BV1YT4y1o727&cid=428541775&page=2\" scrolling=\"no\" border=\"0\" frameborder=\"no\" framespacing=\"0\" allowfullscreen=\"true\" style=\"position: absolute; width: 100%; \nheight: 100%; left: 0; top: 0;\"> </iframe></div>\n\n{% endraw %}\n\n## 认识时间复杂度\n\n常数时间的操作：一个操作如果跟数据量没有关系，每次都是固定时间内完成的操作，叫做常数操作。\n时间复杂度为一个算法流程中，常数操作数量的质量，常用$O$来表示。具体来说，只要高阶项，不要低阶项，也不要高阶项的系数。剩下的部分记为$f(N)$，那么时间复杂度为$O(f(N))$。\n\n## 对数器\n有一个你想要测的方法$a$\n实现一个绝对正确但是复杂度不好的方法$b$\n实现一个随机样本产生器\n实现比对的方法\n把方法$a$和方法$b$比对很多次来验证方法$a$是否正确\n如果有一个样本使得比对出错，打印样本分析是哪个方法出错\n当样本数量很多时比对测试仍然正确，可以确定方法$a$已经正确\n\n## 冒泡排序\n先比较1和2位置的数，如果1大于2则位置交换，再比较2和3位置的数，以此类推，每进行一次都将最大的数放在最后一位；第二次循环的时候就不用管最后一位了，以此类推。\n\n~~~java\npublic class Bubble {\n    public static void bubbleSort(int[] arr) {\n        if (arr == null || arr.length < 2) {\n            return;\n        }\n        for (int end = arr.length-1; end > 0; end--) {\n            for (int i = 0; i < end; i++) {\n                if (arr[i] > arr[i+1]) {\n                    swap(arr, i, i+1);\n                }\n            }\n        }\n    }\n\n    public static void swap(int[] arr, int i, int j) {\n        int temp = arr[i];\n        arr[i] = arr[j];\n        arr[j] = temp;\n    }\n}\n~~~\n\n## 选择排序\n我们首先从0开始到$N-1$，找最小的数的下标并与$0$位置的数交换，只有再从$1$开始，找到从$1$到$N-1$的最小数的下标与$1$交换，以此类推。\n\n~~~java\npublic class Selection {\n    public static void selectionSort(int[] arr) {\n\n        if (arr == null || arr.length < 2) {\n            return;\n        }\n\n        for (int i = 0; i < arr.length-1; i++) {\n            int minindex = i;\n            for (int j=i; j < arr.length; j++) {\n                minindex = arr[j] < arr[minindex] ? j : minindex;\n            }\n            swap(arr, i, minindex);\n\n        }\n    }\n    public static void swap(int[] arr, int i, int j) {\n        int temp = arr[i];\n        arr[i] = arr[j];\n        arr[j] = temp;\n    }\n    \n}\n~~~\n\n## 插入排序\n我们首先认为第$0$位置的数是自己排好的，然后再看$0\\sim1$位置的数，如果$0$位置的数大于$1$位置的数则交换，现在$0\\sim1$位置的数是排好的，之后看$0\\sim2$位置的数，用$2$位置与$1$位置的数比较，$1$位置数大于$2$位置数交换，否则不交换；以此类推。\n\n~~~java\npublic class InsertionSort {\n    public static void insertionSort(int[] arr) {\n        if (arr == null || arr.length < 2) {\n            return;\n        }\n\n        for (int i = 1; i < arr.length; i++) {\n            for (int j = i-1; j>=0 && arr[j] > arr[j + 1]; j--) {\n                swap(arr, j, j+1);\n            }\n        }\n    }\n    public static void swap(int[] arr, int i, int j) {\n        int temp = arr[i];\n        arr[i] = arr[j];\n        arr[j] = temp;\n    }\n    \n}\n~~~\n\n## 递归\n现在假设我们用递归函数来返回数组的最大值。\n\n~~~java\npublic class GetMaxRe {\n    public static int getMax(int[] arr, int L, int R) {\n        if (L==R) {\n            return arr[L];\n        }\n        int mid = (L+R)/2;\n        int maxLeft = getMax(arr, L, mid);\n        int maxRight = getMax(arr, mid+1, R);\n        return Math.max(maxLeft, maxRight);\n    }\n    public static void main(String[] args) {\n        int[] arr = {1,2,3,4};\n        System.out.println(getMax(arr, 0, arr.length-1));\n    }\n    \n}\n~~~\n\n## 归并排序\n时间复杂度为$O(N\\log N)$，额外空间复杂度为$O(N)$。\n步骤：先将样本分为左右两个部分，分别排好序再后外排。\n所以其时间复杂度公式为：\n$$\nT(N) = 2T(N/2) + O(N)\n$$\n因为两边排完序后还要进行外排，外排需要遍历所有$N$个数据，所以其复杂度为$O(N)$。\n~~~java\npublic class MergeSort {\n\n    public static void sortProcess(int[] arr, int L, int R) {\n        if (L==R) {\n            return;\n        }\n\n        int mid = (L+R)/2;\n        sortProcess(arr, L, mid);\n        sortProcess(arr, mid+1, R);\n        merge(arr, L, mid, R);\n    }\n\n    public static void merge(int[] arr, int L, int mid, int R) {\n        int[] help = new int[R - L + 1];\n        int i = 0;\n        int p1 = L;\n        int p2 = mid + 1;\n        while(p1 <= mid && p2 <= R) {\n            help[i++] = arr[p1] < arr[p2] ? arr[p1++] : arr[p2++];\n        }\n\n        while (p1<=mid) {\n            help[i++] = arr[p1++];\n        }\n\n        while (p2<=R) {\n            help[i++] = arr[p2++];\n        }\n\n        for (i = 0; i < help.length; i++) {\n            arr[L+i] = help[i];\n        }\n    }\n    \n}\n\n~~~\n## 小和问题\n再一个数组中，每一个数左边必当前数小的数累加起来，叫做这个数的小和。求一个数组的小和。\n\n例子：\n[1,3,4,2,5]\n1的左边比1小的数，没有；\n3的左边比1小的数，1；\n4的左边比4小的数，1、3；\n2的左边比2小的数，1；\n5的左边比5小的数，1、3、4、2；\n所以小和为$1+1+3+1+1+3+4+2=16$\n\n~~~java\npublic class SmallSum {\n\n    public static int smallSum(int[] arr) {\n        if (arr == null || arr.length < 2) {\n            return 0;\n        }\n        return mergeSort(arr, 0, arr.length-1);\n    }\n\n    public static int mergeSort(int arr[], int l, int r) {\n        if (l==r) {\n            return 0;\n        }\n\n        int mid = (r+l)/2;\n        return mergeSort(arr, l, mid) + mergeSort(arr, mid+1, r) + merge(arr, l, mid, r);\n    }\n\n    public static int merge(int[] arr, int l, int m, int r) {\n        int[] help = new int[r - l + 1];\n        int i = 0;\n        int p1 = l;\n        int p2 = m+1;\n        int res = 0;\n        while (p1 <= m && p2 <= r) {\n            res += arr[p1] < arr[p2] ? (r-p2+1)*arr[p1] : 0;\n            help[i++] = arr[p1] < arr[p2] ? arr[p1++] : arr[p2++];\n        }\n        while (p1<=m) {\n            help[i++] = arr[p1++];\n        }\n\n        while (p2<=r) {\n            help[i++] = arr[p2++];\n        }\n\n        for (i = 0; i < help.length; i++) {\n            arr[l+i] = help[i];\n        }\n        return res;\n    }\n    \n}\n~~~\n\n> 在我们的代码中我们写`m = (l+r)/2`，这样的话可能会溢出，所以我们可以将其写为`m = l + (r-l)/2`，这样就不会溢出了，另外除以$2$的操作我们还可以写为位运算的形式，位运算要比算数运算快，`a/2 = a>>1`，相当于右移一位。所以`l+(r-l)/2 = l+(r-l)>>1`。\n\n## 逆序对问题\n\n在一个数组中，左边的数如果比右边的数大，则这两个数构成一个逆序对，请打印所有的逆序对。\n相当于是求右边有多少数比他小。\n\n~~~java\npublic class InversePair {\n\n    public static void inversePair(int[] arr) {\n        if (arr == null || arr.length < 2) {\n            return;\n        }\n        sortProcess(arr, 0, arr.length-1);\n    }\n\n    public static void sortProcess(int[] arr, int L, int R) {\n        if (L==R) {\n            return;\n        }\n\n        int mid = (L+R)/2;\n        sortProcess(arr, L, mid);\n        sortProcess(arr, mid+1, R);\n        merge(arr, L, mid, R);\n    }\n\n    public static void merge(int[] arr, int L, int mid, int R) {\n        int[] help = new int[R - L + 1];\n        int i = 0;\n        int p1 = L;\n        int p2 = mid + 1;\n        int res = 0;\n        while(p1 <= mid && p2 <= R) {\n            res = arr[p1] > arr[p2] ? (R-p2+1) : 0;\n            for (int k=0;k<res;k++){\n                System.out.println(\"[\"+arr[p1]+\" \"+arr[p2+k]+\"]\");\n            }\n            help[i++] = arr[p1] > arr[p2] ? arr[p1++] : arr[p2++];\n\n            \n        }\n\n        while (p1<=mid) {\n            help[i++] = arr[p1++];\n        }\n\n        while (p2<=R) {\n            help[i++] = arr[p2++];\n        }\n\n        for (i = 0; i < help.length; i++) {\n            arr[L+i] = help[i];\n        }\n    }\n\n    public static void main(String[] args) {\n        int[] arr = {5,3,1,4,2,8,9,7};\n        inversePair(arr);\n    }\n    \n}\n~~~\n","tags":["算法"],"categories":["算法"]},{"title":"线性代数应该这样学：线性空间","url":"/2021/09/12/线性空间/","content":"\n## 向量空间\n\n线性代数是研究有限维向量空间上的线性映射的学科。在线性代数中，如果研究复数和实数，会出现更好的定理和更多的洞察力。因此我们将从介绍复数和它们的基本概念开始。\n\n我们将平面和平凡空间(ordinary space)的例子推广到$\\mathbb{R}^n$和$\\mathbb{C}^n$，然后我们将它们推广到向量空间的概念。\n\n然后我们的下一个主题是子空间，它对于向量空间来说所扮演的角色等同于子集对于集合所扮演的角色。最后我们看一下子空间的和(等同于子集的并集)和子空间的直和(相当于不相交子集的并集)。\n\n### $\\mathbb{R}^n$ and $\\mathbb{C}^n$\n\n#### 复数\n\n定义\n\n一个复数就是一个有序数对$(a,b)$，其中$a,b\\in \\mathbb{R}$，不过我们将其写作$a+bi$。所有复数的集合表示为$\\mathbb{C} = \\{a+bi:a,b\\in \\mathbb{R}\\}$。\n\n#### 复数运算的性质\n\n交换律\n$$\n\\alpha + \\beta = \\beta + \\alpha,\\alpha\\beta = \\beta\\alpha,\\forall \\alpha,\\beta\\in \\mathbb{C}\n$$\n结合律\n$$\n(\\alpha+\\beta)+\\lambda = \\alpha+(\\beta+\\lambda),(\\alpha\\beta)\\lambda=\\alpha(\\beta\\lambda),\\forall\\alpha,\\beta\\in \\mathbb{C}\n$$\n单位元\n$$\n\\lambda+0=\\lambda,\\lambda1=\\lambda,\\forall \\lambda\\in \\mathbb{C}\n$$\n加法逆元\n\n对于任何$\\alpha\\in \\mathbb{C}$，存在唯一一个$\\beta\\in \\mathbb{C}$，使得$\\alpha+\\beta=0$。\n\n乘法逆元\n\n对于任何$\\alpha\\in \\mathbb{C}$，存在唯一的$\\beta\\in \\mathbb{C}$使得$\\alpha\\beta=1$。\n\n分配律\n$$\n\\lambda(\\alpha+\\beta) = \\lambda\\alpha+\\lambda\\beta,\\forall \\lambda,\\alpha,\\beta\\in \\mathbb{C}\n$$\n\n***\n\n#### 定义： $-\\alpha$，减法，$1/\\alpha$，除法\n\n$\\alpha,\\beta\\in \\mathbb{C}$\n\n+ 令$-\\alpha$表示$\\alpha$的加法逆元。因此$-\\alpha$是满足$\\alpha+(-\\alpha)=0$的唯一复数。\n+ 减法：$\\mathbb{C}$上的减法定义为：$\\beta-\\alpha=\\beta+(-\\alpha)$\n+ 对于$\\alpha\\neq 0$，令$1/\\alpha$表示$\\alpha$的乘法逆元。因此$1/\\alpha$为满足$\\alpha(1/\\alpha)=1$的唯一复数。\n+ $\\mathbb{C}$上的除法定义为：$\\beta/\\alpha=\\beta(1/\\alpha)$。\n\n***\n\n用$\\mathbb{F}$表示$\\mathbb{R}$或者$\\mathbb{C}$。对于$\\alpha\\in \\mathbb{F}$并且$m$为正数，我们定义$\\alpha^m$来表示$\\alpha$连乘$m$次：\n$$\n\\alpha^{m}=\\underbrace{\\alpha \\cdots \\alpha}_{m \\text { times }}\n$$\n很显然$(\\alpha^m)^n=\\alpha^{mn}$并且$(\\alpha\\beta)^m=\\alpha^m\\beta^m, \\forall \\alpha,\\beta\\in \\mathbb{F}$。\n\n***\n\n#### 定义：列表(list)，长度(length)\n\n假设$n$是一个非负整数。长度为$n$的列表是一个被括号包围用逗号分隔的$n$元有序数对。长度为$n$的列表如下：\n$$\n（x_1,\\cdots,x_n)\n$$\n两个列表是相等的当且仅当它们长度相等并且在相同的位置有相同的元素。\n\n> 长度无限的不能称为列表\n\n长度为零的列表像这样：$()$。我们将其当作列表以免不必要的例外情况。\n\n#### 定义：$\\mathbb{F}^n$\n\n$\\mathbb{F}^n$是所有元素来自$\\mathbb{F}$的$n$元有序数对的集合：\n$$\n\\mathbb{F}=\\{(x_1,\\cdots,x_n):x_j\\in \\mathbb{F},\\forall j=1,\\cdots,n\\}\n$$\n\n#### $\\mathbb{F}^n$上的加法\n\n$\\mathbb{F}^n$上的加法定义为相对应的元素相加：\n$$\n(x_1,\\cdots,x_n) + (y_1,\\cdots,y_n) = (x_1+y_1,\\cdots,x_n+y_n)\n$$\n\n#### $\\mathbb{F}^n$上加法的交换律\n\n如果$x,y\\in \\mathbb{F}^n$，则$x+y = y+x$\n\n证明：假设$x=(x_1,\\cdots,x_n)$和$y=(y_1,\\cdots,y_n)$。\n$$\n\\begin{aligned}\nx + y &= (x_1,\\cdots,x_n)+(y_1,\\cdots,y_n)\\\\\n&= (x_1+y_1,\\cdots,x_n+y_n)\\\\\n&= (y_1+x_1,\\cdots,y_n+x_n)\\\\\n&= (y_1,\\cdots,y_n) + (x_1,\\cdots,x_n)\\\\\n&= y+x\n\\end{aligned}\n$$\n\n#### 定义 $0$\n\n令$0$表示长度为$n$并且元素全部为$0$的列表：\n$$\n0 = (0,\\cdots,0)\n$$\n\n#### $\\mathbb{F}$上的加法逆元\n\n对于$x\\in \\mathbb{F}^n$，$x$的加法逆元，表示为$-x$，为向量$-x \\in \\mathbb{F}^n$使得\n$$\nx + (-x) = 0\n$$\n换句话说，如果$x = (x_1,\\cdots,x_n)$，则$-x = (-x_1,\\cdots,-x_n)$。\n\n#### $\\mathbb{F}$上的数乘\n\n数字$\\lambda$和$\\mathbb{F}^n$中向量的乘法通过用$\\lambda$乘以$\\mathbb{F}^n$中的每一个元素来完成。\n$$\n\\lambda(x_1,\\cdots,x_n) = (\\lambda x_1,\\cdots,\\lambda x_n)\n$$\n在这里$\\lambda\\in \\mathbb{F}$并且$(x_1,\\cdots,x_n)\\in \\mathbb{F}^n$.\n\n### 向量空间的定义\n\n定义向量空间的动机来自于$\\mathbb{F}^n$中的加法和标量乘法的性质：加法是可交换的、结合的并且有单位元。每个元素也都有加法逆元。标量乘法具有结合律。加法和数乘通过分配律相联系。\n\n我们把向量空间定义为在$\\mathbb{V}$上具有加法和数乘的集合$\\mathbb{V}$，该集合具有上面段落提到的性质。\n\n#### 加法、数乘\n\n集合$\\mathbb{V}$上的数乘是将元素$u+v\\in \\mathbb{V}$分配给每对元素$u,v\\in \\mathbb{V}$的函数。\n\n集合$\\mathbb{V}$上的数乘是将元素$\\lambda v\\in \\mathbb{V}$分配给每一个$\\lambda \\in \\mathbb{F}$和每一个$v\\in \\mathbb{V}$的函数。\n\n现在我们准备好给向量空间一个正式的定义。\n\n#### 定义：向量空间\n\n向量空间是具有$\\mathbb{V}$上加法和$\\mathbb{V}$上数乘的集合$\\mathbb{V}$，使得其满足以下性质：\n\n+ 交换律：$u+v = v+u,\\forall u,v\\in \\mathbb{V}$\n+ 结合律：$(u+v)+w = u+(v+w)$和$(ab)v = a(bv),\\forall u,v,w\\in \\mathbb{V}\\text{ and }a,b\\in \\mathbb{F}$\n+ 加法单位元：存在一个元素$0\\in \\mathbb{V}$使得$v+0=v,\\forall v\\in \\mathbb{V}$\n+ 加法逆元：对于任意的$v\\in \\mathbb{V}$，存在$w\\in \\mathbb{V}$，使得$v+w=0$\n+ 乘法单位元：$1v=v,\\forall v\\in \\mathbb{V}$\n+ 分配律：$a(u+v)=au+av$和$(a+b)v=av+bv,\\forall a,b\\in \\mathbb{F}\\text{ and }u,v\\in\\mathbb{V}$\n\n#### 向量、点\n\n向量空间的元素被称为向量或点。\n\n向量空间的数乘依赖于$\\mathbb{F}$。因当我们想要精确时，我们将会说$\\mathbb{V}$是$\\mathbb{F}$上的向量空间而不是说$\\mathbb{V}$是向量空间。\n\n#### 实向量空间、虚向量空间\n\n$\\mathbb{R}$上的向量空间被称为实向量空间。\n\n$\\mathbb{C}$上的向量空间被称为复向量空间。\n\n#### $\\mathbb{F}^S$\n\n如果$S$为一个集合，则$\\mathbb{F}^S$表示从$S$到$\\mathbb{F}$的函数的集合。\n\n对于$f,g\\in \\mathbb{F}^S$，和$f+g\\in \\mathbb{F}^S$是定义为\n$$\n(f+g)(x) = f(x) + g(x)\n$$\n的函数，对于任何$x\\in S$。\n\n对于$\\lambda \\in \\mathbb{F}$和$f\\in \\mathbb{F}^S$，乘积$\\lambda f\\in \\mathbb{F}^S$是定义为\n$$\n(\\lambda f)(x) = \\lambda f(x)\n$$\n对于所有$x\\in S$。\n\n作为上面定义的一个例子，如果$S$是区间$[0,1]$并且$\\mathbb{F} = \\mathbb{R}$，则$\\mathbb{R}^{[0,1]}$是区间$[0,1]$上的实值函数。\n\n\n\n$\\mathbb{F}^S$是向量空间：\n\n+ 如果$S$是一个非空集合，则$\\mathbb{F}^S$(有上面定义的加法和数乘运算)是$\\mathbb{F}$上的向量空间。\n+ $\\mathbb{F}^S$的加法单位元为函数：$0:S\\rightarrow \\mathbb{F}$定义为：$0(x)=0,\\forall x\\in S$\n+ 对于$f \\in \\mathbb{F}^S$，$f$的加法逆元为函数$-f$：$S\\rightarrow \\mathbb{F}$定义为：$(-f)(x)=-f(x),\\forall x \\in S$\n\n> 我们之前定义的$\\mathbb{F}^n$也可以看作是向量空间$\\mathbb{F}^S$的一个特例，我们可以认为$\\mathbb{F}^n$是从$\\{1,2,\\cdots,n\\}$到$\\mathbb{F}$的函数，即将$\\mathbb{F}^n$看作$\\mathbb{F}^{\\{1,2,\\cdots,n\\}}$。我的理解是例如$n=2$，则$\\mathbb{F}^2$可以看作从$\\{1,2\\}$到$\\mathbb{F}$的函数，当取$1$时从$\\mathbb{F}$中取一个数，等于$2$时再从$\\mathbb{F}$中取一个数。\n\n#### 加法单位元的唯一性\n\n一个向量空间有唯一的加法逆元。\n\n证明：假设$0$和$0^{\\prime}$都是一些向量空间$\\mathbb{V}$的加法单位元。则：\n$$\n0^{\\prime} = 0^{\\prime} + 0 = 0+ 0^{\\prime} = 0\n$$\n因此$0^{\\prime} = 0$，证明$\\mathbb{V}$只有一个加法单位元。\n\n#### 加法逆元的唯一性\n\n在向量空间的每一个元素都有唯一的加法逆元。\n\n证明：假设$\\mathbb{V}$是一个向量空间。令$v\\in \\mathbb{V}$。假设$w$和$w^{\\prime}$都是$v$的逆。则：\n$$\nw = w +0 = w+(v+w^{\\prime}) = (w+v)+w^{\\prime} = 0+w^{\\prime}=w^{\\prime}\n$$\n因此$w = w^{\\prime}$，即只有一个加法逆元。\n\n#### $-v,w-v$\n\n令$v,w\\in \\mathbb{V}$。则\n\n+ $-v$表示$v$的加法逆元\n+ $w-v$定义为$w+(-v)$\n\n#### 数字$0$乘以一个向量\n\n$0v=0,\\forall v\\in \\mathbb{V}$\n\n证明：\n$$\n0v = (0+0)v = 0v + 0v\n$$\n两边同时加上$0v$的逆元，得到$0=0v$。\n\n#### 数字乘以向量$0$\n\n$a0=0$对于任何$a\\in \\mathbb{F}$。\n\n证明：$a\\in \\mathbb{F}$，我们有\n$$\na0 = a(0+0)=a0+a0\n$$\n两边同时加上$a0$的加法逆元，即可得到$0=a0$。\n\n#### 数字$-1$乘以一个向量\n\n$(-1)v=-v$对于任意$v\\in \\mathbb{V}$。\n\n证明：对于$v\\in \\mathbb{V}$，我们有：\n$$\nv + (-1)v = 1v+(-1)v = (1+(-1))v = 0v = 0\n$$\n因此$(-1)v$是$v$的加法逆元。\n\n### 子空间\n\n$V$的子集$U$被称为是子空间如果$U$也是向量空间。\n\n> 一些数学家使用属于线性子空间来描述子空间。\n\n#### 子空间条件\n\n子集$U$是$V$的子空间当且仅当$U$满足下列三个条件：\n\n+ 加法单位元：$0\\in U$\n+ 加法封闭：$u,w\\in U\\rightarrow u+w\\in U$\n+ 数乘封闭：$a\\in \\mathbb{F},u\\in U\\rightarrow au\\in U$\n\n#### 例子\n\n如果$b\\in \\mathbb{F}$，则：\n$$\n\\{(x_1,x_2,x_3,x_4)\\}\\in \\mathbb{F}^4:x_3=5x_4+b\n$$\n为$\\mathbb{F}^4$的子空间，当且仅当$b=0$。\n\n***\n\n区间$[0,1]$上的连续实值函数的几何是$\\mathbb{R}^{[0,1]}$的子空间。\n\n***\n\n$\\mathbb{R}$上的可导实值函数的集合是$\\mathbb{R}^{\\mathbb{R}}$的子空间。\n\n***\n\n使$f^{\\prime}(2)=b$成立的在区间$(0,3)$上的实值可导函数$f$的集合是$\\mathbb{R}^{(0,3)}$的子空间，当且仅当$b=0$。\n\n***\n\n所有极限为0的复数序列的集合是$\\mathbb{C}^{\\infty}$的子空间。\n\n***\n\n> 很容易发现$\\{0\\}$是$\\mathbb{V}$的最小的子空间，$\\mathbb{V}$是$\\mathbb{V}$最大的子空间。\n\n#### 子集的和\n\n假设$U_1,\\cdots,U_m$是$V$的子集。$U_1,\\cdots,U_m$的和，表示为$U_1+\\cdots+U_m$，是$U_1,\\cdots,U_m$的所有可能的元素和的集合。更具体地：\n$$\nU_1+\\cdots+U_m = \\{u_1+\\cdots+u_m:u_1\\in U_1,\\cdots,u_m\\in U_m\\}\n$$\n\n#### Sum of subspaces is the smallest containing subspace\n\n假设$U_1,\\cdots,U_m$是$V$的子空间。则$U_1+\\cdots+U_m$是$V$包含$U_1,\\cdots,U_m$的最小子空间。\n\n证明：\n\n很容易发现$0\\in U_1+\\cdots+U_m$并且$U_1+\\cdots+U_m$对加法和数乘法封闭。因此它为子空间。\n\n显然，$U_1,\\cdots,U_m$都包含在$U_1+\\cdots+U_m$中。相反地，$V$的每个包含$U_1,\\cdots,U_m$的子空间都包含$U_1+\\cdots+U_m$，因此$U_1+\\cdots+U_m$是包含$U_1,\\cdots,U_m$的$V$的最小的子空间。\n\n#### 直和\n\n假设$U_1,\\cdots,U_m$是$V$的子空间。$U_1+\\cdots+U_m$的每一个元素都可以被写为以下形式：\n$$\nu_1+\\cdots+u_m\n$$\n其中$u_j$在$U_j$里。我们可能对$U_1+\\cdots+U_m$中每一个元素都可以用唯一一种上述形式表示的例子感兴趣。这种情况非常重要以至于我们给它一个特殊的名字：直和。\n\n***\n\n定义：直和\n\n假设$U_1,\\cdots,U_m$是$V$的子空间\n\n+ 和$U_1+\\cdots+U_m$被称为直和如果$U_1+\\cdots+U_m$的每一个元素可以唯一地表示为$u_1+\\cdots+u_m$，其中每一个$u_j$在$U_j$内。\n+ 如果$U_1+\\cdots+U_m$是直和，则$U_1\\oplus \\cdots \\oplus U_m$表示$U_1+\\cdots+U_m$，用符号$\\oplus$来表示这是直和。\n\n#### 直和的条件\n\n假设$U_1,\\cdots,U_m$是$V$的子空间。则$U_1+\\cdots+U_m$是直和当且仅当将$0$写为一个和$u_1+\\cdots+u_m$的唯一形式是让每一个$u_j$等于$0$，其中$u_j$位于$U_j$中。\n\n证明：\n\n首先假设$U_1+\\cdots+U_m$是一个直和。则直和的定义意味着将$0$写为$u_1+\\cdots+u_m$的和的唯一形式是通过使每个$u_j$等于零。\n\n现在假设将零写为$u_1+\\cdots+u_m$的唯一形式是令每一个$u_j$等于零。为了证明$U_1+\\cdots+U_m$是一个直和，令$v\\in U_1+\\cdots+U_m$。我们可以写为\n$$\nv = u_1+\\cdots+u_m\n$$\n为了证明这种表示是唯一的，假设我们有\n$$\nv = v_1+\\cdots+v_m\n$$\n将两个方程相减，我们有\n$$\n0 = (u_1-v_1)+\\cdots+(u_m-v_m)\n$$\n因此$u_1=v_1,\\cdots,u_m=v_m$。\n\n证毕。\n\n#### 两个子空间的直和\n\n假设$U$和$W$是$V$的子空间。则$U+W$是直和当且仅当$U\\cap W = \\{0\\}$。\n\n证明\n\n首先假设$U+W$是直和。如果$v\\in U\\cap W$，则$0 = v+(-v)$，其中$v\\in U, -v\\in W$。\n\n> 注：这一步是因为$v\\in U\\cap W$，则$v\\in U\\text{且} v\\in W$，所以$-v \\in W$。\n\n通过$0$的唯一的作为$U$中向量和$V$中向量的和表示方式，我们有$v=0$。因此$U\\cap W = \\{0\\}$，这完成了一个方向的证明。\n\n为了证明另一个方向，现在假设$U\\cap W = \\{0\\}$。为了证明$U+W$是直和，假设$u\\in U, w\\in W$，并且：\n$$\n0 = u+w\n$$\n为了完成证明，我们只需要证明$u=w=0$。上面的方程意味着$u = -w\\in W$。因此$u \\in U\\cap W$。所以$u=0$，这也意味着$w=0$，得证。\n\n证毕。\n\n","tags":["线性代数"],"categories":["数学"]},{"title":"高斯混合模型和EM算法","url":"/2021/08/31/高斯混合模型和EM算法/","content":"## Gaussian mixture models and the EM algorithm\n\n我们使用简写符号$X_1^n$来表示$X_1,X_2,\\cdots,X_n$，相似地，$x_1^n$表示$x_1,x_2,\\cdots,x_n$。\n\n### The model\n\n假设我们有有编号的人$i=1,\\cdots,n$。我们观察表示每个人的身高的随机变量$Y_i\\in \\mathbb{R}$，同时假设有一个观测不到的标签$C_i\\in \\{\\operatorname{M,F}\\}$表示人的性别。在这了，小写字母$c$代表\"class\"。我们也假设两组具有相同的已知方差$\\sigma^2$，但不同的未知均值$\\mu_M$和$\\mu_F$。类标签符合伯努利分布：\n$$\np_{C_i}(c_i) = q^{\\mathbb{1}(c_i=M)}(1-q)^{\\mathbb{1}(c_i=F)}\n$$\n我们也假设$q$是已知的。为了简化符号，我们令$\\pi_M=q$和$\\pi_F=1-q$，因此我们可以写作：\n$$\np_{C_i}(c_i) = \\prod_{c\\in \\{M,F\\}}\\pi_c^{\\mathbb{1}(c_i=c)}\n$$\n每一类的条件分布都为高斯分布：\n$$\np_{Y_i|C_i}(y_i|c_i) = \\prod_c\\mathcal{N}(y_i;\\mu_c,\\sigma^2)^{\\mathbb{1}(c_i=c)}\n$$\n\n### Parameter estimation: a first attempt\n\n假设我们观测到独立同分布的身高$Y_1=y_1,\\cdots,Y_n=y_n$，并且我们想要去找到参数$\\mu_M,\\mu_F$的极大似然估计。这是一个非监督问题：我们不知道我们数据的性别标签，但是我们想根据这些标签学习参数。\n\n根据上文提到的模型设计，计算所有数据点$P_{Y_1,\\cdots,Y_n}$的联合密度，以$\\mu_M,\\mu_F,\\sigma,q$表示。取$\\log$后计算$\\log$似然，然后对$\\mu_M$进行求导。为什么优化这么困难？\n\n我们先对单个数据点$Y_i=y_i$求密度：\n$$\n\\begin{aligned}\nP_{Y_i}(y_i) &= \\sum_{c_i}p_{C_i}(c_i)p_{Y_i|C_i}(y_i|c_i)\\\\\n&= \\sum_{c_i}(\\pi_c\\mathcal{N}(y_i;\\mu_C,\\sigma^2))^{\\mathbb{1}(c_i=c)}\\\\\n&= q\\mathcal{N}(y_i;\\mu_M,\\sigma^2) + (1-q)\\mathcal{N}(y_i;\\mu_F,\\sigma^2)\n\\end{aligned}\n$$\n现在，所有观测的联合分布为\n$$\nP_{Y_1^n}(y_1^n) = \\prod_{i=1}^n(q\\mathcal{N}(y_i;\\mu_M,\\sigma^2) + (1-q)\\mathcal{N}(y_i;\\mu_F,\\sigma^2))\n$$\n则$\\log$似然函数为：\n$$\n\\ln p_{Y_1^n}(y_1^n) = \\sum_{i=1}^n\\ln(\\pi_M\\mathcal{N}(y_i;\\mu_M,\\sigma^2) + \\pi_F\\mathcal{N}(y_i;\\mu_F,\\sigma^2))\n$$\n我们已经遇到了一个问题，和的形式阻止我们将$\\log$应用于内部的正态分布密度函数。通过对称性，我们只需要看其中一个均值，另一个将遵循相同的过程。在我们深入区分之前，我们注意到：\n$$\n\\begin{aligned}\n\\frac{d}{d\\mu}\\mathcal{N}(x;\\mu,\\sigma^2) &= \\frac{d}{d\\mu}[\\frac{1}{\\sigma\\sqrt{2\\pi}}e^{-\\frac{(x-\\mu)^2}{2\\sigma^2}}]\\\\\n&= \\frac{1}{\\sigma\\sqrt{2\\pi}}e^{-\\frac{-(x-\\mu)^2}{2\\sigma^2}}\\cdot\\frac{2(x-\\mu)}{2\\sigma^2}\\\\\n&= \\mathcal{N}(x;\\mu,\\sigma^2)\\cdot\\frac{(x-\\mu)}{\\sigma^2}\n\\end{aligned}\n$$\n对数似然函数对$\\mu_M$进行求导，得\n\n[^1]: $\\sum_{i=1}^n\\frac{1}{\\pi_M\\mathcal{N}(y_i;\\mu_M,\\sigma^2)+\\pi_F\\mathcal{N}(y_i;\\mu_F,\\sigma^2)}\\pi_M\\mathcal{N}(y_i;\\mu_M,\\sigma^2)\\frac{y_i-\\mu_M}{\\sigma^2}=0$\n\n$$\n\\sum_{i=1}^n\\frac{1}{\\pi_M\\mathcal{N}(y_i;\\mu_M,\\sigma^2)+\\pi_F\\mathcal{N}(y_i;\\mu_F,\\sigma^2)}\\pi_M\\mathcal{N}(y_i;\\mu_M,\\sigma^2)\\frac{y_i-\\mu_M}{\\sigma^2}=0\n$$\n\n对于这个表达式我们无法求解。\n\n### Using hidden variables and the EM Algorithm\n\n退一步，什么会使这个计算容易。如果我们知道隐变量$C_i$的值，则对参数们做最大似然估计就会很容易：我们将会取所有$C_i=M$的点用来估计$\\mu_M$，然后对所有$C_i=F$的点重复此过程来估计$\\mu_F$。受此启发，我们尝试计算给定观测下$C_i$的分布。我们将从贝叶斯规则开始：\n$$\n\\begin{aligned}\np_{C_i|Y_i}(c_i|y_i) &= \\frac{p_{Y_i|C_i}(y_i|c_i)p_{C_i}(c_i)}{p_{Y_i}(y_i)}\\\\\n&= \\frac{\\prod_{c\\in \\{M,F\\}}(\\pi_c\\mathcal{N}(y_i;\\mu_c,\\sigma^2))^{\\mathbb{1}(c=c_i)}}{\\pi_M\\mathcal{N}(y_i;\\mu_M,\\sigma^2)+\\pi_F\\mathcal{N}(y_i;\\mu_F,\\sigma^2)} = q_{C_i}(c_i)\n\\end{aligned}\n$$\n我们看一下$C_i=M$的后验概率：\n$$\np_{C_i|Y_i}(M|y_i) = \\frac{\\pi_M\\mathcal{N}(y_i;\\mu_M,\\sigma^2)}{\\pi_M\\mathcal{N}(y_i;\\mu_M,\\sigma^2)+\\pi_F\\mathcal{N}(y_i;\\mu_F,\\sigma^2)} = q_{C_i}(M)\n$$\n这个看起来很熟悉，这是式[^1]中的一部分，我们可以将式[^1]用$q_{C_i}$重写，并且假定其跟$\\mu_M$无关\n$$\n\\sum_{i=1}^nq_{C_i}(M)\\frac{y_i-\\mu_M}{\\sigma^2}=0\\\\\n\\mu_M = \\frac{\\sum_{i=1}^nq_{C_i}(M)y_i}{\\sum_{i=1}^nq_{C_i}(M)}\n$$\n这样就看起来好多了：$\\mu_M$是身高的加权平均值，其中每个身高都根据该人是男性的可能性进行加权。\n\n因此现在我们形成了一个循环，如果我们知道参数我们将会很容易计算出$C_1^n$的后验概率，并且如果我们知道后验概率我们将很容易地估计出参数，这就陷入了死循环。这就暗示了以下策略，我们可以固定一个来求解另一个。这种方法通常被称为`EM`算法。它的工作原理大致如下：\n\n+ 首先，我们固定参数(在这种情况下为高斯分布的均值$\\mu_M$和$\\mu_F$)并且求解隐变量的后验分布(在这种情况下记为$q_{C_i}$)。\n+ 之后，我们固定隐变量的后验分布，利用隐变量的期望值来最优化参数。\n+ 重复两个步骤直到收敛。\n\n### The EM Algorithm: a more formal look\n\n正如我们将在几个短步骤中展示的那样，EM算法实际上是最大化对数似然的下限(换句话说，每一步都保证改进我们的答案直到收敛)。\n\n假设我们观测到了随机变量$Y$。现在假设我们也有一些隐变量$C$并且$Y$依赖于$C$。我们可以说$C$和$Y$的分布中有一些我们不知道的参数$\\theta$，并且我们有兴趣找到它们。\n\n在我们上一个例子中，我们观测到有隐变量(性别)$C=\\{C_1,\\cdots,C_n\\}$的身高变量$Y=\\{Y_1,\\cdots,Y_n\\}$，并且$Y$和$C$是独立同分布的，我们的参数是$\\mu_M$和$\\mu_F$。\n\n在我们真正推导算法之前，我们需要一个关键结论：`Jensen's inequation`(琴声不等式)。在这个例子中我们需要的琴声不等式的特例：\n$$\n\\log(\\mathbb{E}[X])\\ge \\mathbb{E}[\\log(X)]\n$$\n下图是关于琴声不等式的几何直观：\n\n![](https://raw.githubusercontent.com/HFC666/image/master/img/EM.png)\n\n> 琴声不等式的特例说明：对于任何随机变量$X$，$\\mathbb{E}[\\log X]\\le\\log\\mathbb{E}[X]$。令$X$的PDF为红色的曲线。令$Z=\\log X$。中间和右边的图展示了怎么构建$Z$的PDF(蓝色曲线)：因为$\\log$，与$X$的PDF相比，它倾向于更小的值。$\\log\\mathbb{E}(X)$是由中间黑色曲线或$\\mathbb{E}(Z)$给出的点。但是，$\\mathbb{E}[\\log X]$或者$\\mathbb{E}[Z]$，将始终较小(或至少永远不会较大)因为对数挤压了分布较大的一端(其中$Z$比较大)并拉伸了较小的一端(其中$Z$较小)。\n\n> 关于琴声不等式\n>\n> 对于一个实函数$\\phi(x)$，在区间$I$内它是凸的($\\frac{d^2\\phi(x)}{d^2x}>0, \\forall x\\in I$)，那么它满足下面关系：\n> $$\n> \\phi(\\sum_{i=1}^Np_ix_i)\\le \\sum_{i=1}^Np_i\\phi(x_i)\n> $$\n> 其中$p_i\\ge0,\\sum_{i=1}^Np_i=1$，且$x_i\\in I,(i=1,\\cdots,N)$。\n>\n> **证明**：令$A=\\sum_{i=1}^Np_ix_i$，显然$A\\in I$。取\n> $$\n> \\begin{aligned}\n> S &= \\sum_{i=1}^N\\phi(x_i) - \\phi(A)\\\\\n> &= \\sum_{i=1}^Np_i[\\phi(x_i)-\\phi(A)]\\\\\n> &= \\sum_{i=1}^N p_i\\int_A^{x_i}\\phi^{\\prime}(x)dx\n> \\end{aligned}\n> $$\n> 若$A\\le x_i$，因为$\\phi^{\\prime}(x)$在区间$I$上是递增的，所以\n> $$\n> \\int_A^{x_i}\\phi^{\\prime}(x)dx\\ge \\phi^{\\prime}(A)(x_i-A)\n> $$\n> 若$A_i>x_i$，则\n> $$\n> \\int_A^{x_i}\\phi^{\\prime}(x)dx = -\\int_{x_i}^A\\phi^{\\prime}(x)dx\\ge-(A-x_i)\\phi(A) = (x_i-A)\\phi^{\\prime}(A)\n> $$\n> 所以：\n> $$\n> \\begin{aligned}\n> S &\\ge \\sum_{i=1}^Np_i\\phi^{\\prime}(A)(x_i-A)\\\\\n> &= \\phi^{\\prime}(A)[\\sum_{i=1}^Np_i(x_i-A)]\\\\\n> &= \\phi^{\\prime}(A)(A-A)\\\\\n> &=0\n> \\end{aligned}\n> $$\n> 证毕。\n\n在本文中，求期望相当于加权平均，而$\\ln(x)$的二阶导小于$0$，故与上文提到的不等式符号相反。\n\n#### The EM Algorithm\n\n我们想要去最大化似然函数。我们通过边缘化$C$来计算对数似然函数：\n$$\n\\log p_Y(y;\\theta) = \\log\\left(\\sum_cp_{Y,C}(y,c)\\right)\n$$\n我们现在也遭受了困境，我们无法对和取对数。如果我们交换它们的顺序不是更好吗？对，期望是一种特殊的求和，并且琴声不等式可以让我们互换它们如果我们有期望的话。因此，我们将要引进一个隐变量$C$的新的分布$q_C$：\n$$\n\\begin{aligned} \\log p_{Y}(y ; \\theta) & \\\\ \\left.\\text { (Marginalizing over } C \\text { and introducing } q_{C}(c) / q_{C}(c)\\right) &=\\log \\left(\\sum_{c} q_{C}(c) \\frac{p_{Y, C}(y, c ; \\theta)}{q_{C}(c)}\\right) \\\\ \\text { (Rewriting as an expectation) } &=\\log \\left(\\mathbb{E}_{q_{C}}\\left[\\frac{p_{Y, C}(y, C ; \\theta)}{q_{C}(C)}\\right]\\right) \\\\ \\text { (Using Jensen's inequality) } & \\geq \\mathbb{E}_{q_{C}}\\left[\\log \\frac{p_{Y, C}(y, C ; \\theta)}{q_{C}(C)}\\right](2) \\\\ \\text { Using definition of conditional probability } &=\\mathbb{E}_{q_{C}}\\left[\\log \\frac{p_{Y}(y ; \\theta) p_{C \\mid Y}(C \\mid y ; \\theta)}{q_{C}(C)}\\right](3) \\end{aligned}\n$$\n\n\n现在我们有可以很容易优化的$\\log p_Y(y;\\theta)$的下界了。因为我们已经引入了$q_C$，我们想要在$\\theta$和$q_C$上实施最大化。\n\n我们将使用(2)和(3)分别进行优化。首先先用(2)寻找最好的参数：\n$$\n\\mathbb{E}_{q_{C}}\\left[\\log \\frac{p_{Y, C}(y, C ; \\theta)}{q_{C}(C)}\\right] = \\mathbb{E}_{q_C}[\\log p_{Y, C}(y, C ; \\theta)] - \\mathbb{E}_{q_C}[\\log q_C(C)]\n$$\n因为$q_C$不依赖于$\\theta$，因此我们可以只优化第一项：\n$$\n\\widehat{\\theta} \\leftarrow \\underset{\\theta}{\\operatorname{argmax}} \\mathbb{E}_{q_{C}}\\left[\\log p_{Y, C}(y, C ; \\theta)\\right]\n$$\n这被称作`M-step`：`M`代表最大化，因为我们正在最大化参数。现在，我们用(3)来找到最好的$q_C$：\n$$\n\\mathbb{E}_{q_{C}}\\left[\\log \\frac{p_{Y}(y ; \\theta) p_{C \\mid Y}(C \\mid y ; \\theta)}{q_{C}(C)}\\right]=\\mathbb{E}_{q_{C}}\\left[\\log p_{Y}(y ; \\theta)\\right]+\\mathbb{E}_{q_{C}}\\left[\\log \\frac{p_{C \\mid Y}(C \\mid y ; \\theta)}{q_{C}(C)}\\right]\n$$\n第一项不依赖于$c$，并且第二项看起来像KL散度：\n$$\n\\begin{aligned}\n&= \\log p_Y(y;\\theta) - \\mathbb{E}_{q_C}[\\log\\frac{q_C(C)}{p_{C|Y}(C|y;\\theta)}]\\\\\n&= \\log p_Y(y;\\theta) - D(q_C(\\cdot)||p_{C|Y}(\\cdot|y;\\theta))\n\\end{aligned}\n$$\n因此，当最大化上述值时，我们想要最小化KL散度。KL散度总是大于等于零，当两个分布完全一样时取零。因此，最优化的$q_C$即为$p_{C|Y}(c|y;\\theta)$：\n$$\n\\hat{q}_C(c) \\leftarrow p_{C|Y}(c|y;\\theta)\n$$\n这被称为`E-step`：`E`代表期望，因为我们正在计算$q_C$以便我们可以将其用于期望值。\n\n#### The algorithm\n\n![](https://raw.githubusercontent.com/HFC666/image/master/img/EM_a.png)\n\n> 信息熵可以表达数据的信息量大小，是信息处理的一个非常重要的概念。对于离散型随机变量，信息熵公式如下：\n> $$\n> H(p) = H(X) = \\mathrm{E}_{x\\sim p(x)}[-\\log p(x)] = -\\sum_{i=1}^n p(x)\\log p(x)\n> $$\n> 对于连续性随机变量，信息熵公式如下：\n> $$\n> H(p) = H(X) = E_{x\\sim p(x)}[-\\log p(x)] = -\\int p(x)\\log p(x)dx\n> $$\n> 接下来我们来说一下相对熵，又被称为KL散度或信息散度，是两个概率分布差异的非对称度量。在信息论中，相对熵等价于两个概率分布的信息熵的插值，若其中一个概率分布为真实分布，另一个为理论分布，则此时相对熵等于交叉熵与真实分布的信息熵之差，表示使用理论分布拟合真实分布时产生的信息损耗。公式为：\n> $$\n> D_{\\operatorname{KL}}(p||q) = \\sum_{i=1}^N[p(x_i)\\log p(x_i) - p(x_i)\\log q(x_i)]\n> $$\n> 上面的$p(x_i)$为真实事件的概率分布，$q(x_i)$为理论拟合出来的该事件的概率分布。当拟合出来的事件概率分布跟真实的一模一样时，相对熵等于零，而拟合出来不一样时，相对熵大于零。\n>\n> 最后我们来证明一下相对熵公式只有在$p(x_i)$等于$q(x_i)$的时候等于$0$，其他时候大于$0$。\n>\n> 要证：\n> $$\n> D_{\\operatorname{KL}}(p||q) = \\sum_{i=1}^N[p(x_i)\\log p(x_i)-p(x_i)\\log q(x_i)]\\ge 0\n> $$\n> 即证：\n> $$\n> \\sum_{i=1}^Np(x_i)\\log\\frac{q(x_i)}{p(x_i)}\\le 0\n> $$\n> 又$\\ln(x)\\le x-1$，当且仅当$x=1$时等号成立\n>\n> 故\n> $$\n> \\sum_{i=1}^Np(x_i)\\log\\frac{q(x_i)}{p(x_i)}\\le \\sum_{i=1}^Np(x_i)(\\frac{q(x_i)}{p(x_i)}-1) = \\sum_{i=1}^N[p(x_i)-q(x_i)]=0\n> $$\n> 上面式子中$=$只在$p(x_i)=q(x_i)$时成立。\n\n### Example: Applying the general algorithm to GMMS\n\n现在，让我们重现看一下关于身高的GMM模型，看看我们如何应用这两个步骤。我们有观测变量$Y=\\{Y_1,\\cdots,Y_n\\}$和隐变量$C=\\{C_1,\\cdots,C_n\\}$。对于`E-Step`，我们计算后验概率$p_{C|Y}(c|y)$，之前已经计算过了。对于`M-Step`，我们得去计算联合概率：\n$$\n\\begin{aligned}\n\\mathbb{E}_{q_C}[\\ln p_{Y,C}(y,C)] &= \\mathbb{E}[\\ln p_{Y|C}(y|C)p_C(C)]\\\\\n&= \\mathbb{E}_{q_C}\\left[\\ln \\prod_{i=1}^n\\prod_{c\\in \\{M,F\\}}(\\pi_c\\mathcal{N}(y_i;\\mu_c,\\sigma^2))^{\\mathbb{1}(C_i=c)}\\right]\\\\\n&= \\mathbb{E}_{q_C}\\left[\\sum_{i=1}^n\\sum_{c\\in \\{M,F\\}}\\mathbb{1}(C_i=c)(\\ln\\pi_c+\\ln\\mathcal{N}(y_i;\\mu_c,\\sigma^2))\\right]\\\\\n&= \\sum_{i=1}^n\\sum_{c\\in \\{M,F\\}}\\mathbb{E}_{q_C}[\\mathbb{1}(C_i=c)]\\left(\\ln\\pi_c + \\ln\\frac{1}{\\sigma\\sqrt{2\\pi}}-\\frac{(y_i-\\mu_c)^2}{2\\sigma^2}\\right)\n \\end{aligned}\n$$\n$\\mathbb{E}_{q_C}[\\mathbb{1}(C_i=c)]$是$C_i$为$c$的概率。现在我们对$\\mu_M$求导：\n$$\n\\frac{d}{d \\mu_{M}} \\mathbb{E}_{q_{C}}\\left[\\ln p_{Y \\mid C}(y \\mid C) p_{C}(C)\\right]=\\sum_{i=1}^{n} q_{C_{i}}(M)\\left(\\frac{y_{i}-\\mu_{M}}{\\sigma^{2}}\\right)=0\n$$\n得：\n$$\n\\mu_{M}=\\frac{\\sum_{i=1}^{n} q_{C_{i}}(M) y_{i}}{\\sum_{i=1}^{n} q_{C_{i}}(M)}\n$$\n\n同理可得：\n$$\n\\mu_{F}=\\frac{\\sum_{i=1}^{n} q_{C_{i}}(F) y_{i}}{\\sum_{i=1}^{n} q_{C_{i}}(F)}\n$$\n","tags":["机器学习"],"categories":["算法"]},{"title":"贝叶斯数据分析","url":"/2021/08/11/贝叶斯数据分析/","content":"\n当初为什么要用英文写笔记？？\n{% pdf https://hfcouc.work/pdfs/Bayesian_Data_Analysis.pdf %}","tags":["贝叶斯数据分析"],"categories":["贝叶斯机器学习"]},{"title":"微分方程","url":"/2021/08/09/微分方程/","content":"\n微分方程学习笔记\n\n{% pdf https://hfcouc.work/pdfs/Differential_equation.pdf %}\n","tags":["微分方程"],"categories":["数学"]},{"title":"数值分析笔记","url":"/2021/07/30/数值分析/","content":"\n学习数值分析的笔记\n\n{% pdf https://hfcouc.work/pdfs/Numerical_analysis.pdf %}\n\n","tags":["数值分析"],"categories":["数学"]},{"title":"统计学习基础","url":"/2021/07/28/统计学习精要笔记/","content":"\n自己学习统计学习基础的笔记。\n\n{% pdf https://hfcouc.work/pdfs/ESLII.pdf %}\n\n","tags":["机器学习"],"categories":["机器学习"]},{"title":"只争朝夕，不负韶华","url":"/2021/07/13/index/","content":"\n{% meting \"346089\" \"netease\" \"song\" \"theme:#555\" \"mutex:true\" \"listmaxheight:340px\" \"preload:auto\" %}\n\n\n\n<img src=\"https://cdn.jsdelivr.net/gh/HFC666/image@master/20210713/3.178msyh9rg68.png\" style=\"zoom:80%;\" />\n\n\n\n> 现状是没那么容易改变的\n> 即便是足够努力\n> 也很难在短时间内看出效果\n> 所以有时你认为的无法改变\n> 也可能只是暂时没看出效果而己\n> 而不是不够努力\n\n<img src=\"https://cdn.jsdelivr.net/gh/HFC666/image@master/20210713/4.36iwffkaky00.png\" style=\"zoom:80%;\" />\n\n\n\n> 不能再浑浑噩噩\n> 如果不把眼皮用力抬起看个真切\n> 或许就会错过人生中按下快门的良机\n\n\n\n**即使是一个人，也需要好好吃饭，这是治愈自己的一种方式**\n\n\n\n“不被时间和社会所束缚，幸福的填饱肚子，短时间内变得随心所欲，变得自由，不被打扰；毫不费神的吃东西的这种孤高行为，是现代人，都平等的拥有的最高治愈。”\n\n{% raw %}\n\n<div style=\"position: relative; width: 100%; height: 0; padding-bottom: 75%;\"><iframe \nsrc=\"//player.bilibili.com/player.html?\taid=73923411&bvid=BV1AE411a7Nw&cid=126453908&page=1\" scrolling=\"no\" border=\"0\" \nframeborder=\"no\" framespacing=\"0\" allowfullscreen=\"true\" style=\"position: absolute; width: 100%; \nheight: 100%; left: 0; top: 0;\"> </iframe></div>\n\n\n{% endraw %}\n\n\n\n<div style=\"position: relative; width: 100%; height: 0; padding-bottom: 75%;\"><iframe \nsrc=\"//player.bilibili.com/player.html?aid=52761403&bvid=BV1r4411776v&cid=92328539&page=1\" scrolling=\"no\" border=\"0\" \nframeborder=\"no\" framespacing=\"0\" allowfullscreen=\"true\" style=\"position: absolute; width: 100%; \nheight: 100%; left: 0; top: 0;\"> </iframe></div>\n\n\n\n\n"}]