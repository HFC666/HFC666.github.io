<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>独自赏晴雨</title>
  
  
  <link href="https://www.hfcouc.work/atom.xml" rel="self"/>
  
  <link href="https://www.hfcouc.work/"/>
  <updated>2022-01-05T11:28:51.432Z</updated>
  <id>https://www.hfcouc.work/</id>
  
  <author>
    <name>HFC</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>图像的几何变换</title>
    <link href="https://www.hfcouc.work/2022/01/05/%E5%9B%BE%E7%89%87%E7%9A%84%E5%87%A0%E4%BD%95%E5%8F%98%E6%8D%A2/"/>
    <id>https://www.hfcouc.work/2022/01/05/%E5%9B%BE%E7%89%87%E7%9A%84%E5%87%A0%E4%BD%95%E5%8F%98%E6%8D%A2/</id>
    <published>2022-01-05T11:24:03.000Z</published>
    <updated>2022-01-05T11:28:51.432Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="\assets\js\Meting.min.js"></script><p>图像的空间域变换。</p><span id="more"></span><h3 id="空间域运算"><a href="#空间域运算" class="headerlink" title="空间域运算"></a>空间域运算</h3><h4 id="代数运算"><a href="#代数运算" class="headerlink" title="代数运算"></a>代数运算</h4><h5 id="加法运算"><a href="#加法运算" class="headerlink" title="加法运算"></a>加法运算</h5><p>生成图像叠加效果：对于两个图像$f(x,y)$和$(x,y)$的均值有：</p><script type="math/tex; mode=display">g(x,y) = \frac{1}{2}f(x,y)+\frac{1}{2}h(x,y)</script><p>会得到二次曝光的效果。推广这个公式为：</p><script type="math/tex; mode=display">g(x,y)=\alpha f(x,y)+\beta h(x,y),\quad \alpha+\beta=1</script><p>我们可以得到各种图像合成的效果，也可以用于两张图片的衔接。</p><h5 id="减法运算"><a href="#减法运算" class="headerlink" title="减法运算"></a>减法运算</h5><p>去除不需要的叠加性图案</p><p>设背景图像为$b(x,y)$，前景背景混合图像$f(x,y)$</p><script type="math/tex; mode=display">g(x,y) = f(x,y)-b(x,y)</script><p>$g(x,y)$为去除了背景的图像。电视制作的蓝屏技术就基于此。</p><p>检测同一场景两幅图像之间的变化</p><p>设时间$1$的图像为$T_1(x,y)$，时间$2$的图像为$T_2(x,y)$，则</p><script type="math/tex; mode=display">g(x,y) = T_2(x,y)-T_1(x,y)</script><h5 id="乘法运算"><a href="#乘法运算" class="headerlink" title="乘法运算"></a>乘法运算</h5><p>乘法的定义：</p><script type="math/tex; mode=display">C(x,y) = A(x,y)\times B(x,y)</script><p>主要应用：图像的局部显示(用二值蒙版图像与原图像做乘法)</p><h4 id="逻辑运算"><a href="#逻辑运算" class="headerlink" title="逻辑运算"></a>逻辑运算</h4><p>求反、异或、或、与</p><p>求反的定义：</p><script type="math/tex; mode=display">g(x,y) = R-f(x,y)</script><p>$R$为$f(x,y)$的灰度级，求反图像</p><p>与运算：</p><script type="math/tex; mode=display">g(x,y) = f(x,y)\wedge h(x,y)</script><p>求两个图像交集</p><h4 id="几何变换"><a href="#几何变换" class="headerlink" title="几何变换"></a>几何变换</h4><p>基本几何变换的定义：对于原图像$f(x,y)$，坐标变换函数</p><script type="math/tex; mode=display">x^{\prime}=a(x,y); y^{\prime}=b(x,y)</script><p>唯一确定了几何变换：</p><script type="math/tex; mode=display">g(x^{\prime},y^{\prime}) = f(a(x,y),b(x,y))</script><p>$g(x,y)$是目标图像。</p><h5 id="平移"><a href="#平移" class="headerlink" title="平移"></a>平移</h5><script type="math/tex; mode=display">a(x,y) = x+x_0\quad b(x,y) = y+y_0</script><script type="math/tex; mode=display">\begin{bmatrix}a(x,y)\\b(x,y)\\1\end{bmatrix} =\begin{bmatrix}1&0&x_0\\0&1&y_0\\0&0&1\end{bmatrix}\begin{bmatrix}x\\y\\1\end{bmatrix}</script><h5 id="放缩"><a href="#放缩" class="headerlink" title="放缩"></a>放缩</h5><script type="math/tex; mode=display">a(x,y)=cx\quad b(x,y) = dy</script><script type="math/tex; mode=display">\begin{bmatrix}a(x,y)\\b(x,y)\\1\end{bmatrix} =\begin{bmatrix}c&0&0\\0&d&0\\0&0&1\end{bmatrix}\begin{bmatrix}x\\y\\1\end{bmatrix}</script><h5 id="旋转"><a href="#旋转" class="headerlink" title="旋转"></a>旋转</h5><script type="math/tex; mode=display">a(x,y) = x\cos(\theta)-y\sin(\theta)</script><script type="math/tex; mode=display">b(x,y) = x\sin(\theta)+y\cos(\theta)</script><script type="math/tex; mode=display">\begin{bmatrix}a(x,y)\\b(x,y)\\1\end{bmatrix} =\begin{bmatrix}\cos(\theta)&-\sin(\theta)&0\\\sin(\theta)&\cos(\theta)&0\\0&0&1\end{bmatrix}\begin{bmatrix}x\\y\\1\end{bmatrix}</script><p>证明过程如下：</p><p><img src="https://static01.imgkr.com/temp/57b93a78548f4b7495567ac1629c5440.png" alt=""></p><h5 id="其他几何变换"><a href="#其他几何变换" class="headerlink" title="其他几何变换"></a>其他几何变换</h5><p><img src="https://static01.imgkr.com/temp/18a1f0c5d579482ab4df31e2d4389e71.png" alt=""></p><h4 id="重采样"><a href="#重采样" class="headerlink" title="重采样"></a>重采样</h4><p>最近邻法：取最近的点。</p><p>特点：</p><ul><li>简单快速</li><li>灰度保真性好</li><li>误差较大</li><li>视觉特性较差</li><li>马赛克效应</li></ul><p>双线性插值(一阶插值)：根据四个相邻点的灰度值，分别在$x$和$y$方向上进行两次插值。插值可以根据距离的远近进行加权，一般采用双曲抛物面方程进行插值：</p><script type="math/tex; mode=display">f(x,y) = ax+by+cxy+d</script><p>将四个点代入可得：</p><script type="math/tex; mode=display">\begin{aligned}a &= f(1,0)-f(0,0)\\b&= f(0,1)-f(0,0)\\c&= f(1,1)+f(0,0)-f(0,1)-f(1,0)\\d&=f(0,0)\end{aligned}</script><p><img src="https://static01.imgkr.com/temp/92766afdbe36417894b87ba6713b6633.png" alt=""></p><p>高阶插值：</p><p>利用三次多项式来近似理论上的最佳插值函数$\operatorname{sinc}(x)$：</p><script type="math/tex; mode=display">S(x) = \begin{cases}1-2|x|^2+|x|^3&|x|<1\\4-8|x|+5|x|^2-|x|^3&1\le |x|\le 2\\0&|x|>2\end{cases}</script><p>由此形成常用的三次卷积插值算法，又称为三次内插法，两次立方法、CC插值法等。</p><p>其形状为：</p><p><img src="https://static01.imgkr.com/temp/dbf533512a5e42eba69c50c7406057b8.png" alt=""></p><p>对于待插值的像素点$(x,y)$，取其附近的$4\times 4$邻域点$(x_i,y_j),i,j=0,1,2,3$按如下公式进行插值计算：</p><script type="math/tex; mode=display">f(x,y) = \sum_{i=0}^3\sum_{j=0}^3f(x_i,y_j)W(x-x_i)W(y-y_j)</script><p>matlab写的旋转后双线性插值采样代码：</p><figure class="highlight matlab"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">function</span> <span class="title">img</span> = <span class="title">rotate</span><span class="params">(image, theta)</span></span></span><br><span class="line"><span class="comment">% 对图片进行旋转，采用二次线性插值</span></span><br><span class="line">[m,n] = <span class="built_in">size</span>(image);</span><br><span class="line">A = [<span class="built_in">cos</span>(theta) -<span class="built_in">sin</span>(theta)  <span class="number">0</span></span><br><span class="line">     <span class="built_in">sin</span>(theta) <span class="built_in">cos</span>(theta)   <span class="number">0</span></span><br><span class="line">     <span class="number">0</span>          <span class="number">0</span>            <span class="number">1</span></span><br><span class="line">];</span><br><span class="line">x = repelem(<span class="number">1</span>:n,<span class="number">1</span>,m);</span><br><span class="line">y = <span class="built_in">repmat</span>(<span class="number">1</span>:m,<span class="number">1</span>,n);</span><br><span class="line">z = <span class="built_in">ones</span>([<span class="number">1</span>,m*n]);</span><br><span class="line">position = [x;y;z];</span><br><span class="line">position_n = A*position;</span><br><span class="line">new_x = position_n(<span class="number">1</span>,:);</span><br><span class="line">new_y = position_n(<span class="number">2</span>,:);</span><br><span class="line">m_max = <span class="built_in">ceil</span>(<span class="built_in">max</span>(new_y));</span><br><span class="line">m_min = <span class="built_in">floor</span>(<span class="built_in">min</span>(new_y));</span><br><span class="line">n_max = <span class="built_in">ceil</span>(<span class="built_in">max</span>(new_x));</span><br><span class="line">n_min = <span class="built_in">floor</span>(<span class="built_in">min</span>(new_x));</span><br><span class="line">m_n = m_max-m_min+<span class="number">1</span>;</span><br><span class="line">n_n = n_max-n_min+<span class="number">1</span>;</span><br><span class="line">img = <span class="built_in">zeros</span>([m_n, n_n]);</span><br><span class="line"><span class="keyword">for</span> <span class="built_in">i</span> = <span class="number">1</span>:m_n</span><br><span class="line">    <span class="keyword">for</span> <span class="built_in">j</span> = <span class="number">1</span>:n_n</span><br><span class="line">        posi = A\[<span class="built_in">j</span>+n_min<span class="number">-1</span>;<span class="built_in">i</span>+m_min<span class="number">-1</span>;<span class="number">1</span>];</span><br><span class="line">        <span class="keyword">if</span> <span class="number">1</span>&lt;posi(<span class="number">1</span>)&amp;&amp;posi(<span class="number">1</span>)&lt;n &amp;&amp; <span class="number">1</span>&lt;posi(<span class="number">2</span>)&amp;&amp;posi(<span class="number">2</span>)&lt;m</span><br><span class="line">            <span class="comment">% 根据双曲抛物面方程进行插值：</span></span><br><span class="line">            a = image(<span class="built_in">floor</span>(posi(<span class="number">2</span>)),<span class="built_in">ceil</span>(posi(<span class="number">1</span>))) - image(<span class="built_in">floor</span>(posi(<span class="number">2</span>)),<span class="built_in">floor</span>(posi(<span class="number">1</span>)));</span><br><span class="line">            b = image(<span class="built_in">ceil</span>(posi(<span class="number">2</span>)),<span class="built_in">floor</span>(posi(<span class="number">1</span>))) - image(<span class="built_in">floor</span>(posi(<span class="number">2</span>)),<span class="built_in">floor</span>(posi(<span class="number">1</span>)));</span><br><span class="line">            c = image(<span class="built_in">ceil</span>(posi(<span class="number">2</span>)),<span class="built_in">ceil</span>(posi(<span class="number">1</span>))) + image(<span class="built_in">floor</span>(posi(<span class="number">2</span>)),<span class="built_in">floor</span>(posi(<span class="number">1</span>))) - image(<span class="built_in">ceil</span>(posi(<span class="number">2</span>)),<span class="built_in">floor</span>(posi(<span class="number">1</span>)))-image(<span class="built_in">floor</span>(posi(<span class="number">2</span>)),<span class="built_in">ceil</span>(posi(<span class="number">1</span>)));</span><br><span class="line">            d = image(<span class="built_in">floor</span>(posi(<span class="number">2</span>)),<span class="built_in">floor</span>(posi(<span class="number">1</span>)));</span><br><span class="line">            img(<span class="built_in">i</span>,<span class="built_in">j</span>) = (posi(<span class="number">1</span>)-<span class="built_in">floor</span>(posi(<span class="number">1</span>)))*a + (posi(<span class="number">2</span>)-<span class="built_in">floor</span>(posi(<span class="number">2</span>)))*b + c*(posi(<span class="number">1</span>)-<span class="built_in">floor</span>(posi(<span class="number">1</span>)))*(posi(<span class="number">2</span>)-<span class="built_in">floor</span>(posi(<span class="number">2</span>)))+d;</span><br><span class="line">        <span class="keyword">end</span></span><br><span class="line">    <span class="keyword">end</span></span><br><span class="line"><span class="keyword">end</span></span><br><span class="line">img = img/<span class="number">255</span>;</span><br></pre></td></tr></table></figure>]]></content>
    
    
    <summary type="html">&lt;p&gt;图像的空间域变换。&lt;/p&gt;</summary>
    
    
    
    <category term="图像处理" scheme="https://www.hfcouc.work/categories/%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86/"/>
    
    
    <category term="数字图像处理" scheme="https://www.hfcouc.work/tags/%E6%95%B0%E5%AD%97%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86/"/>
    
  </entry>
  
  <entry>
    <title>向量微积分第一周学习笔记</title>
    <link href="https://www.hfcouc.work/2022/01/04/%E5%90%91%E9%87%8F%E5%BE%AE%E7%A7%AF%E5%88%86/"/>
    <id>https://www.hfcouc.work/2022/01/04/%E5%90%91%E9%87%8F%E5%BE%AE%E7%A7%AF%E5%88%86/</id>
    <published>2022-01-04T14:43:38.000Z</published>
    <updated>2022-01-04T14:46:29.393Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="\assets\js\Meting.min.js"></script><p>向量微积分课程第一周的内容，不过还没学完，明天再学吧，我都快学死了。</p><span id="more"></span><h2 id="第一周"><a href="#第一周" class="headerlink" title="第一周"></a>第一周</h2><h3 id="向量"><a href="#向量" class="headerlink" title="向量"></a>向量</h3><h4 id="向量的叉乘"><a href="#向量的叉乘" class="headerlink" title="向量的叉乘"></a>向量的叉乘</h4><p>两个三维向量叉乘，得到一个新的向量，这个向量垂直于原来的两个向量，大小为原来两个向量围成的平行四边形的面积，向量方向的判断符合右手定理。</p><p>两个三维向量$A = (v_1,v_2,v_3),B=(w_1,w_2,w_3)$叉乘，可以用行列式来表示：</p><script type="math/tex; mode=display">A\times B = \det\begin{bmatrix}\vec{i}&\vec{j}&\vec{k}\\v_1&v_2&v_3\\w_1&w_2&w_3\end{bmatrix}</script><h3 id="解析几何"><a href="#解析几何" class="headerlink" title="解析几何"></a>解析几何</h3><h4 id="平面的解析几何"><a href="#平面的解析几何" class="headerlink" title="平面的解析几何"></a>平面的解析几何</h4><p>我们首先在平面上选择三个不共线的点$r_1,r_2,r_3$，我们可以通过这三个点构建两个位移向量，我们构建$s_1=r_2-r_1,s_2=r_3-r_2$。这样我们就有了平面上的两个向量，我们记这两个向量的叉乘为$n$，那么$n$垂直于这个平面。所以$n$与平面上所有向量的点乘都为$0$。那么对于平面上的任意一个向量$r$，我们都有</p><script type="math/tex; mode=display">\vec{n}\cdot (\vec{r}-\vec{r_1})=0</script><p>下面我们确定平面的表达式：</p><p>对于$\vec{r}$，我们设$\vec{r}=x\vec{i}+y\vec{j}+z\vec{k}$，我们设$\vec{n}=a\vec{i}+b\vec{j}+c\vec{k}$，因为$\vec{n}$和$\vec{r_1}$已知，我们假设其内积为$d$，所以我们就有：</p><script type="math/tex; mode=display">ax+by+cz=d</script><p>即为平面的方程。</p><h3 id="向量代数"><a href="#向量代数" class="headerlink" title="向量代数"></a>向量代数</h3><h4 id="Kronecker-Delta-and-Levi-Civita-Symbol"><a href="#Kronecker-Delta-and-Levi-Civita-Symbol" class="headerlink" title="Kronecker Delta and Levi-Civita Symbol"></a>Kronecker Delta and Levi-Civita Symbol</h4><p>关于Kronecker Delta(克罗内克函数)：</p><script type="math/tex; mode=display">\delta_{ij} = \begin{cases}1,& i=j\\0,&i\neq j\end{cases}</script><p>Levi-Civita Symbol(列维-奇维塔符号)：</p><script type="math/tex; mode=display">\epsilon_{ijk}=\begin{cases}1,&(i,j,k) = (1,2,3),(2,3,1),(3,1,2)\\-1,&(i,j,k) = (3,2,1),(2,1,3),(1,3,2)\\0,&i=j,j=k,k=i\end{cases}</script><p>Einstein summation convention(爱因斯坦求和约定)：所谓爱因斯坦求和约定就是略去求和式中的求和号，如下式：</p><script type="math/tex; mode=display">\sum_{i=1}^3\delta_{ii}=\delta_{ii}=3</script><p>还有以下例子：</p><script type="math/tex; mode=display">\epsilon_{ijk}\epsilon_{ijk}=6</script><p>我们对此有一个有用的公式：</p><script type="math/tex; mode=display">\epsilon_{i j k} \epsilon_{l m n}=\left|\begin{array}{ccc}\delta_{i l} & \delta_{i m} & \delta_{i n} \\\delta_{j l} & \delta_{j m} & \delta_{j n} \\\delta_{k l} & \delta_{k m} & \delta_{k n}\end{array}\right|=\delta_{i l}\left(\delta_{j m} \delta_{k n}-\delta_{j n} \delta_{k m}\right)-\delta_{i m}\left(\delta_{j l} \delta_{k n}-\delta_{j n} \delta_{k l}\right)+\delta_{i n}\left(\delta_{j l} \delta_{k m}-\delta_{j m} \delta_{k l}\right)</script><p>根据上述关系，我们还可以推出下列两条性质：</p><script type="math/tex; mode=display">\epsilon_{ijk}\epsilon_{imn} = \delta_{jm}\delta_{kn}-\delta_{jn}\delta_{km},\quad \epsilon_{ijk}\epsilon_{ijn}=2\delta_{kn}</script><p>我们对上面的式子进行证明：</p><script type="math/tex; mode=display">\begin{aligned}\epsilon_{i j k} \epsilon_{i m n} &=\delta_{i i}\left(\delta_{j m} \delta_{k n}-\delta_{j n} \delta_{k m}\right)-\delta_{i m}\left(\delta_{j i} \delta_{k n}-\delta_{j n} \delta_{k i}\right)+\delta_{i n}\left(\delta_{j i} \delta_{k m}-\delta_{j m} \delta_{k i}\right) \\&=3\left(\delta_{j m} \delta_{k n}-\delta_{j n} \delta_{k m}\right)-\left(\delta_{j m} \delta_{k n}-\delta_{j n} \delta_{k m}\right)+\left(\delta_{j n} \delta_{k m}-\delta_{j m} \delta_{k n}\right) \\&=\delta_{j m} \delta_{k n}-\delta_{j n} \delta_{k m}\end{aligned}</script><p>将上述标记与求解内积和叉乘联系起来，我们有：</p><script type="math/tex; mode=display">\vec{A}\cdot\vec{B} = A_iB_i</script><script type="math/tex; mode=display">(\vec{A}\times \vec{B})_i = \epsilon_{ijk}A_iB_k</script><h4 id="Vector-identities"><a href="#Vector-identities" class="headerlink" title="Vector identities"></a>Vector identities</h4><ul><li>Scalar triple product：<script type="math/tex">\vec{A}\cdot(\vec{B}\times \vec{C})=\vec{B}\cdot(\vec{C}\times \vec{A})=\vec{C}\cdot(\vec{A}\times\vec{B})</script></li><li>Vector triple product：<script type="math/tex">\vec{A}\times(\vec{B}\times\vec{C})=(\vec{A}\cdot\vec{C})\vec{B}-(\vec{A}\cdot\vec{B})\vec{C}</script></li><li>Scalar quadruple product：<script type="math/tex">(\vec{A}\times\vec{B})\cdot(\vec{C}\times\vec{D})=(\vec{A}\cdot\vec{C})(\vec{B}\cdot\vec{D})</script></li><li>Vector quadruple product：<script type="math/tex">(\vec{A}\times\vec{B})\times(\vec{C}\times\vec{D})=((\vec{A}\times\vec{B})\cdot\vec{D})\vec{C}-((\vec{A}\times\vec{B})\cdot\vec{C})\vec{D}</script></li></ul><p>我们需要证明上述结论，在此之前我们先看一下我们现在已有的结论：</p><script type="math/tex; mode=display">\epsilon_{ijk}=\epsilon_{jki}=\epsilon_{kij}</script><script type="math/tex; mode=display">\epsilon_{ijk}\epsilon_{imn} = \delta_{jm}\delta_{kn}-\delta_{jn}\delta_{km}</script><script type="math/tex; mode=display">\delta_{ij}A_j = A_i</script><script type="math/tex; mode=display">\vec{A}\cdot\vec{B}=A_iB_i</script><script type="math/tex; mode=display">[\vec{A}\times\vec{B}]_i = \epsilon_{ijk}A_jB_k</script><p>我们首先证明第一条：</p><script type="math/tex; mode=display">\begin{aligned}\vec{A}\cdot(\vec{B}\times\vec{C}) &= A_i[\vec{B}\times\vec{C}]_i=A_i\epsilon_{ijk}B_jC_k\\&=B_j\epsilon_{jki}C_kA_i = B_j[\vec{C}\times \vec{A}]_i\\&=\vec{B}\cdot(\vec{C}\times\vec{A})\\&= C_k\epsilon_{kij}A_iB_j = \vec{C}\cdot(\vec{A}\times\vec{B})\end{aligned}</script><p>从行列式的角度来理解，我们很容易得到：</p><script type="math/tex; mode=display">\vec{A}\cdot(\vec{B}\times\vec{C})=\vec{A}\cdot\det\begin{bmatrix}\vec{i}&\vec{j}&\vec{k}\\B_1&B_2&B_3\\C_1&C_2&C_3\end{bmatrix} = \det\begin{bmatrix}A_1&A_2&A_3\\B_1&B_2&B_3\\C_1&C_2&C_3\end{bmatrix}</script><p>所以性质一相当于交换行列式的行两次，相当于没变。同时该式子可以用来理解行列式的值的绝对值为体积，因为行列式的值相当于先求叉乘，得到方向与高相同或相反，大小等于底面积的向量，然后与另一个向量做内积相当于乘以高。</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;向量微积分课程第一周的内容，不过还没学完，明天再学吧，我都快学死了。&lt;/p&gt;</summary>
    
    
    
    <category term="数学" scheme="https://www.hfcouc.work/categories/%E6%95%B0%E5%AD%A6/"/>
    
    
    <category term="向量微积分" scheme="https://www.hfcouc.work/tags/%E5%90%91%E9%87%8F%E5%BE%AE%E7%A7%AF%E5%88%86/"/>
    
  </entry>
  
  <entry>
    <title>数字图像处理基本知识</title>
    <link href="https://www.hfcouc.work/2022/01/04/%E6%95%B0%E5%AD%97%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86%E5%9F%BA%E6%9C%AC%E7%9F%A5%E8%AF%86/"/>
    <id>https://www.hfcouc.work/2022/01/04/%E6%95%B0%E5%AD%97%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86%E5%9F%BA%E6%9C%AC%E7%9F%A5%E8%AF%86/</id>
    <published>2022-01-04T14:39:02.000Z</published>
    <updated>2022-01-05T11:25:34.932Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="\assets\js\Meting.min.js"></script><p>数字图像处理的基本知识。</p><span id="more"></span><h2 id="数字图像处理基本知识"><a href="#数字图像处理基本知识" class="headerlink" title="数字图像处理基本知识"></a>数字图像处理基本知识</h2><h3 id="数字图像和图像处理"><a href="#数字图像和图像处理" class="headerlink" title="数字图像和图像处理"></a>数字图像和图像处理</h3><p>图像可以表示为下述函数</p><script type="math/tex; mode=display">I = f(x,y,z,t)</script><p>一般我们考虑的图像是一个平面的，静止的图像，即</p><script type="math/tex; mode=display">I = f(x,y)</script><p>如果图像的$x,y$都是连续的，那么它就为<strong>模拟图像</strong>，然而在电脑的存储中不可能是连续的，而是<strong>离散的</strong>，称为<strong>数字图像</strong>。我们对$(x,y)$进行离散化称为取样，而对$f(x,y)$进行离散化称为量化，之后得到的图像称为数字图像。</p><p>对于一维的数据采样，采样的时间间隔$\Delta t\le \frac{1}{2f}$(香农定理)。</p><p>而对于我们的二维的数字图像，我们的采样要求：</p><script type="math/tex; mode=display">\begin{aligned}\Delta x&\le \frac{1}{2w_u}\\\Delta y&\le\frac{1}{2w_v}\end{aligned}</script><p>其中$w_u,w_v$为空间的变化频率，相当于灰度在$x,y$轴上变化的快慢。</p><p>图像分类：</p><ul><li>灰度图像</li><li>二值图像</li><li>RGB彩色图像</li></ul><h3 id="像素间的基本关系"><a href="#像素间的基本关系" class="headerlink" title="像素间的基本关系"></a>像素间的基本关系</h3><h4 id="邻域"><a href="#邻域" class="headerlink" title="邻域"></a>邻域</h4><p>坐标为$(x,y)$的像素$p$有两个垂直和两个水平邻近像素。</p><script type="math/tex; mode=display">(x+1, y), (x-1, y), (x,y+1), (x,y-1)</script><p>称为$p$的四邻域，表示为$N_4(p)$。</p><p>四个对角线邻域为：</p><script type="math/tex; mode=display">(x+1,y+1),(x+1,y-1),(x-1,y+1),(x-1,y-1)</script><p>称为$p$的对角线邻域，表示为$N_D(p)$。这八个点称为$p$的$8$邻域，表示为$N_8(p)$。点$p$的邻域的图像位置集合称为$p$的邻域。邻域如果包含$p$​则被称为闭的，反之开的。</p><h4 id="邻接与连通"><a href="#邻接与连通" class="headerlink" title="邻接与连通"></a>邻接与连通</h4><p>邻接必要条件：</p><ol><li>两像素是邻域</li><li>两像素的灰度值满足特定的相似规则</li></ol><p>假设$V$​​是用于定义邻接关系的一组灰度值。在二值图像中，如果我们指的是值为$1$的像素的邻接关系，则$V=\{1\}$。</p><p>我们将邻接关系分为$3$类：</p><ol><li>$4$邻接：两个像素$p$和$q$的值都来自$V$并且$q$位于$p$的$4$​邻域内。</li><li>$8$​邻接：两个像素$p$​和$q$​的值都来自$V$​并且$q$​位于$p$​的$8$​邻域内。</li><li>$m$邻接(也被称为混合邻接)：两个像素$p$和$q$的值都来自$V$并且<ul><li>$q$在$p$的$4$邻域内，或者</li><li>$q\in N_D(p)$并且$N_4(p)\cap N_4(q)$没有灰度值来自$V$​的像素。</li></ul></li></ol><p>那么$m$邻接和$8$邻接有什么不同呢？$m$邻接(混合邻接)是$8$邻接的改进，它的引入是为了消除使用$8$邻接可能导致的歧义。</p><p>如下：</p><p><img src="https://static01.imgkr.com/temp/a72cde8a2a644be6827e321f8217b96e.png" alt=""></p><p>$8$邻接有两条通路，形成了闭环，而$m$邻接则不存在此方面问题。</p><p>通路：</p><p>假设在点$p(x_0,y_0)$和点$q(x_{n+1},y_{n+1})$之间有一系列像素：</p><script type="math/tex; mode=display">(x_1,y_1),\cdots,(x_n,y_n)</script><p>并且$(x_{i-1},y_{i-1})$和$(x_i,y_i)$之间是邻接的，则说$p,q$是连通的，称为一条通路。如果$p,q$为同一个点，则称其为闭合通路。</p><p>如果$p$和$q$之间存在通路，则称$p$与$q$是连通的。对于一个像素集合$S$，如果$S$中所有像素都是连通的，则称$S$为连通集。</p><h4 id="距离度量"><a href="#距离度量" class="headerlink" title="距离度量"></a>距离度量</h4><p>对于像素点$p,q,s$，坐标分别为$(x,y),(u,v),(w,z)$，$D$为距离函数或度量如果</p><ol><li>$D(p,q)\ge 0, D(p,q)=0\iff p=q$</li><li>$D(p,q) = D(q,p)$</li><li>$D(p,s)\le D(p,q)+D(q,s)$</li></ol><p>则称$D$为距离或度量函数。</p><p>几种常用距离：</p><ol><li>$D_e(p, q) = [(x-u)^2 + (y-v)^2]^{1/2}$</li><li>$D_4(p,q) = |x-u|+|y-v|$</li><li>$D_8(p,q) = \max(|x-u|, |y-v|)$</li></ol><h3 id="数字图像的存储与读写"><a href="#数字图像的存储与读写" class="headerlink" title="数字图像的存储与读写"></a>数字图像的存储与读写</h3><h4 id="数字图像文件的存储格式"><a href="#数字图像文件的存储格式" class="headerlink" title="数字图像文件的存储格式"></a>数字图像文件的存储格式</h4><p>BMP格式(由4部分组成)：</p><ol><li>文件头</li><li>信息头</li><li>文件调色板</li><li>数据</li></ol><p>JPEG格式：有损压缩格式，颜色支持$24$位。</p><p>GIF格式：无损压缩格式</p><p>TIFF格式：非失真的压缩格式。</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;数字图像处理的基本知识。&lt;/p&gt;</summary>
    
    
    
    <category term="图像处理" scheme="https://www.hfcouc.work/categories/%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86/"/>
    
    
    <category term="数字图像处理" scheme="https://www.hfcouc.work/tags/%E6%95%B0%E5%AD%97%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86/"/>
    
  </entry>
  
  <entry>
    <title>条件随机场</title>
    <link href="https://www.hfcouc.work/2022/01/02/%E6%9D%A1%E4%BB%B6%E9%9A%8F%E6%9C%BA%E5%9C%BA/"/>
    <id>https://www.hfcouc.work/2022/01/02/%E6%9D%A1%E4%BB%B6%E9%9A%8F%E6%9C%BA%E5%9C%BA/</id>
    <published>2022-01-02T09:26:55.000Z</published>
    <updated>2022-01-02T09:51:50.627Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="\assets\js\Meting.min.js"></script><p>条件随机场感觉比HMM要难，虽然我看完了一遍，但是有些地方感觉还是云里雾里的，之后可能还得更详细的阅读。另外条件随机场需要由标注的数据，另外其参数设置也较复杂，实际应用中可能比较难。</p><span id="more"></span><h2 id="条件随机场"><a href="#条件随机场" class="headerlink" title="条件随机场"></a>条件随机场</h2><h3 id="HMM-VS-MEMM"><a href="#HMM-VS-MEMM" class="headerlink" title="HMM VS MEMM"></a>HMM VS MEMM</h3><p><img src="https://static01.imgkr.com/temp/0157b60e0d0d4391aeeaf10e3624b754.png" alt=""></p><p>HMM两大假设：</p><ol><li>齐次一阶马尔可夫假设：齐次指的是状态转移矩阵不变，与时间无关；一次指的是每一时刻的状态只与前一时刻的状态有关：$P(y_t|y_{1:t-1},x_{1:t-1})=P(y_t|y_{t-1})$</li><li>观测独立假设：$P(x_t|y_{1:t},x_{1:t-1})=P(x_t|y_t)$</li></ol><p>HMM是一个生成模型，其建模对象是$P(X,Y|\lambda)$。</p><p><img src="https://static01.imgkr.com/temp/a96d06aa1b7e405a95da3179f24e0853.png" alt=""></p><script type="math/tex; mode=display">\begin{aligned}P(X,Y|\lambda) &= P(x_{1:T},y_{1:T}|\lambda)\\&= P(x_T|x_{1:T-1},y_{1:T})P(y_T|y_{1:T-1},x_{1:T-1})P(y_{1:T-1},x_{1:T-1})\\&= P(x_T|y_T)P(y_T|y_{T-1})P(y_{1:T-1},x_{1:T-1})\\&\vdots\\&= \pi(y_1)P(x_1|y_1)\prod_{t=2}^{T}P(x_i|y_i)P(y_i|y_{i-1})\end{aligned}</script><p>MEMM打破了HMM的观测独立假设，如下图：</p><p><img src="https://static01.imgkr.com/temp/b4f4591ebb7446308706ef437ed0407d.png" alt=""></p><p>这时一个V型结构，<code>head-to-head</code>，在给定$Y_{t-1}$的条件下它们就不独立了。</p><blockquote><p>对于一个有向无环图，我们可以这样判断它的结点之间的关系：</p><p>形式一：<code>head-to-head</code></p><p><img src="https://static01.imgkr.com/temp/2e42113a75c2446bb147e07c1398155a.png" alt=""></p><p>在<strong>$c$未知的情况下，$a,b$是阻断的，是独立的，而在$c$已知的情况下，$a,b$是连通的，不独立。</strong>表示为：</p><script type="math/tex; mode=display">P(abc) = P(a)P(b)P(c|ab)</script><p>形式二：<code>tail-to-tail</code></p><p><img src="https://static01.imgkr.com/temp/e9bb6fbfe0d44673ab0437234f980389.png" alt=""></p><p>在<strong>$c$给定的条件下，$a,b$是被阻断的，是独立的</strong>，表示为</p><script type="math/tex; mode=display">P(abc) = P(a)P(c|a)P(b|ac) = P(a)P(c|a)P(b|c)</script><p>形式三：<code>head-to-tail</code></p><p><img src="https://static01.imgkr.com/temp/855b90da142e483abda8bf92a8da86bd.png" alt=""></p><p>在<strong>$c$给定的情况下，$a,b$是被阻断的，是独立的，</strong>表示为：</p><script type="math/tex; mode=display">P(abc) = P(a)P(c|a)P(b|c)</script></blockquote><p>MEMM对应于第一种情况，而HMM对应于第二种情况。</p><p>MEMM为判别模型，其是对条件概率建模：</p><script type="math/tex; mode=display">P(Y|X,\lambda) = \prod_{t=1}^TP(y_t|y_{t-1},x_{1:T},\lambda)</script><h3 id="MEMM-VS-CRF"><a href="#MEMM-VS-CRF" class="headerlink" title="MEMM VS CRF"></a>MEMM VS CRF</h3><p>MEMM会造成标注偏差问题。</p><blockquote><p>具体的我也没搞懂</p></blockquote><p>解决这个问题的方法是将有向图转换为了无向图：</p><p><img src="https://static01.imgkr.com/temp/d80727609dc342fd89101cc7d7d079be.png" alt=""></p><h3 id="CRF"><a href="#CRF" class="headerlink" title="CRF"></a>CRF</h3><p>条件随机场：</p><ul><li>条件：判别模型$P(Y|X)$</li><li>随机场：无向图模型</li></ul><p>定义(条件随机场)：设$X$与$Y$是随机变量，$P(Y|X)$是在给定$X$的条件下$Y$的条件概率分布。若随机变量$Y$构成一个由无向图$G=(V,E)$表示的马尔可夫随机场，即</p><script type="math/tex; mode=display">P(Y_v|X,Y_w,w\neq v) = P(Y_v|X,Y_w,w\sim v)</script><p>对任意结点$v$成立，则称条件概率分布$P(Y|X)$为条件随机场。式中$w\sim v$表示在图$G=(V,E)$中结点$v$有边连接的所有结点$w$，$w\neq v$表示结点$v$以外的所有结点。</p><p>在定义中并没有要求$X$和$Y$具有相同的结构。现实中，一般假设$X$和$Y$具有相同的图结构。我们在这里考虑的无向图为线性图，即</p><script type="math/tex; mode=display">G = (V=\{1,2,\cdots,n\},E = \{(i,i+1)\}),\quad i=1,2,\cdots,n-1</script><p>在此情况下，$X=(X_1,X_2,\cdots,X_n),Y=(Y_1,Y_2,\cdots,Y_n)$，最大团是相邻两个结点的集合。</p><p><img src="https://static01.imgkr.com/temp/0242442061844f8f8f51a69be9c8f7b3.png" alt=""></p><blockquote><p>$X$和$Y$有相同的图结构的线性链条条件随机场</p></blockquote><p>定义(线性链条件随机场)：设$X=(X_1,X_2,\cdots,X_n),Y=(Y_1,Y_2,\cdots,Y_n)$均为线性链表示的随机变量序列，若在给定随机变量序列$X$的条件下，随机变量序列$Y$的条件概率分布$P(Y|X)$构成条件随机场，即满足马尔可夫性</p><script type="math/tex; mode=display">P(Y_i|X,Y_1,\cdots,Y_{i-1},Y_{i+1},\cdots,Y_n) = P(Y_i|X,Y_{i-1},Y_{i+1})\quad i=1,2,\cdots,n</script><p>在$i=1,n$时只考虑单边。</p><h4 id="条件随机场的参数化形式"><a href="#条件随机场的参数化形式" class="headerlink" title="条件随机场的参数化形式"></a>条件随机场的参数化形式</h4><p>定义(线性链条件随机场的参数化形式)：设$P(Y|X)$为线性链条件随机场，则在随机变量$X$取值为$x$的条件下，随机变量$Y$取值为$y$的条件概率具有如下形式：</p><script type="math/tex; mode=display">p(y|x) = \frac{1}{Z(x)}\exp\left(\sum_{i,k}\lambda_kt_k(y_{i-1},y_i,x,i)+\sum_{i,l}\mu_ls_l(y_i,x,i)\right)</script><p>其中</p><script type="math/tex; mode=display">Z(x)=\sum_y\exp\left(\sum_{i,k}\lambda_kt_k(y_{i-1},y_i,x,i)+\sum_{i,l}\mu_ls_l(y_i,x,i)\right)</script><p>式中，$t_k$和$s_l$是特征函数，$\lambda_k$和$\mu_l$是对应的权值。$Z(x)$是规范化因子，求和是在所有可能的输出序列上进行的。</p><p>$t_k$是定义在边上的特征函数，称为转移特征，依赖于当前和前一个位置；$s_l$是定义在结点的特征函数，称为状态特征，依赖于当前位置。$t_k$和$s_l$都依赖于位置，是局部特征函数。通常，特征函数$t_k$和$s_l$取值为$1$或$0$；当满足特征条件时取值为$1$，否则为$0$。条件随机场完全由特征函数$t_k,s_l$和对应的权值$\lambda_k,\mu_l$确定。</p><h4 id="条件随机场的简化形式"><a href="#条件随机场的简化形式" class="headerlink" title="条件随机场的简化形式"></a>条件随机场的简化形式</h4><p>为了简便起见，首先将转移特征和状态特征及其权值用统一的符号表示。设有$K_1$个转移特征，$K_2$个状态特征，$K=K_1+K_2$，记</p><script type="math/tex; mode=display">f_k(y_{i-1},y_i,x,i) = \begin{cases}t_k(y_{i-1},y_i,x,i),\quad &k=1,2,\cdots,K_1\\s_l(y_i,x,i),&k=K_1+l;l=1,2,\cdots,K_2\end{cases}</script><p>然后，对转移与状态特征在各个位置$i$求和，记作</p><script type="math/tex; mode=display">f_k(y,x)=\sum_{i=1}^nf_k(y_{i-1},y_i,x,i),\quad k=1,2,\cdots,K</script><p>用$w_k$表示特征$f_k(y,x)$的权值，即</p><script type="math/tex; mode=display">w_k = \begin{cases}\lambda_k,\quad &k=1,2,\cdots,K_1\\\mu_l,&k=K_1+l;l=1,2,\cdots,K_2\end{cases}</script><p>于是我们的条件随机场简化为：</p><script type="math/tex; mode=display">\begin{aligned}P(y|x) &= \frac{1}{Z(x)}\exp\sum_{k=1}^Kw_kf_k(y,x)\\Z(x)&= \sum_y\exp\sum_{k=1}^Kw_kf_k(y,x)\end{aligned}</script><p>若以$w$表示权重向量，即</p><script type="math/tex; mode=display">w = (w_1,w_2,\cdots,w_K)^T</script><p>以$F(y,x)$表示全局特征向量，即</p><script type="math/tex; mode=display">F(y,x) = (f_1(y,x),f_2(y,x),\cdots,f_K(y,x))^T</script><p>则条件随机场可以写成向量$w$与$F(y,x)$的内积形式：</p><script type="math/tex; mode=display">P_w(y|x) = \frac{\exp(w\bullet F(y,x))}{Z_m(x)}</script><p>其中</p><script type="math/tex; mode=display">Z_w(x) = \sum_y\exp(w\bullet F(y,x))</script><h4 id="条件随机场的矩阵形式"><a href="#条件随机场的矩阵形式" class="headerlink" title="条件随机场的矩阵形式"></a>条件随机场的矩阵形式</h4><p>对观测序列$x$的每一个位置$i=1,2,\cdots,n+1$，由于$y_{i-1}$和$y_i$在$m$个标记中取值，可以定义一个$m$阶矩阵随机变量</p><script type="math/tex; mode=display">M_i(x) = [M_i(y_{i-1},y_i|x)]</script><p>矩阵随机变量的元素为</p><script type="math/tex; mode=display">\begin{aligned}M_i(y_{i-1},y_i|x)&= \exp(W_i(y_{i-1},y_i|x))\\W_i(y_{i-1},y_i|x)&= \sum_{k=1}^Kw_kf_k(y_{i-1},y_i,x,i)\end{aligned}</script><p>这样，给定观测序列$x$，相应标记序列$y$的非规范概率可以通过该序列$n+1$个矩阵的适当元素的乘积$\prod_{i=1}^{n+1}M_i(y_{i-1},y_i|x)$表示。于是，条件概率$P_w(y|x)$是</p><script type="math/tex; mode=display">P_w(y|x) = \frac{1}{Z_w(x)}\prod_{i=1}^{n+1}M_i(y_{i-1},y_i|x)</script><p>其中，$Z_w(x)$为规范化因子，是$n+1$个矩阵的乘积的元素，即</p><script type="math/tex; mode=display">Z_w(x) = [M_1(x)M_2(x)\cdots M_{n+1}(x)]_{\operatorname{start,stop}}</script><p>注意，$y_0=\operatorname{start}$与$y_{n+1}=\operatorname{stop}$表示开始状态和终止状态，规范化因子$Z_w(x)$是以$\operatorname{start}$为起点$\operatorname{stop}$为终点通过状态的所有路径$y_1y_2\cdots y_n$的非规范化概率$\prod_{i=1}^{n+1}M_i(y_{i-1},y_i|x)$之和。</p><h4 id="CRF要解决的问题"><a href="#CRF要解决的问题" class="headerlink" title="CRF要解决的问题"></a>CRF要解决的问题</h4><ul><li>学习问题：求解参数：$\hat{\theta}=\arg\max\prod_{i=1}^NP(y^{(i)}|x^{(i)})$</li><li>推断</li></ul><p>我们先讲推断问题：我们现在是假定参数已经学习好了，然后我们求解边缘概率：$P(y_t=i|x)$。</p><h4 id="推断"><a href="#推断" class="headerlink" title="推断"></a>推断</h4><h5 id="前向-后向算法"><a href="#前向-后向算法" class="headerlink" title="前向-后向算法"></a>前向-后向算法</h5><p>对每个指标$i=0,1,\cdots,n+1$，定义前向向量$\alpha_i(x)$：</p><script type="math/tex; mode=display">\alpha_0(y|x) = \begin{cases}1,\quad &y=\operatorname{start}\\0,&\text{否则}\end{cases}</script><p>递推公式为：</p><script type="math/tex; mode=display">\alpha_i^T(y_i|x) = \alpha_{i-1}^T(y_{i-1}|x)[M_i(y_{i-1},y_i|x)],\quad i=1,2,\cdots,n+1</script><p>又可以表示为</p><script type="math/tex; mode=display">\alpha_i^T(x) = \alpha_{i-1}^T(x)M_i(x)</script><p>因为$y_i$可取的值有$m$个，所以$\alpha_i(x)$是$m$维列向量。</p><p>同样，对每个指标$i=0,1,\cdots,n+1$，定义后向向量$\beta_i(x)$：</p><script type="math/tex; mode=display">\beta_{n+1}(y_{n+1}|x)=\begin{cases}1,\quad &y_{n+1}=\text{stop}\\0,&\text{否则}\end{cases}</script><script type="math/tex; mode=display">\beta_i(y_i|x) = [M_{i+1}(y_i,y_{i+1}|x)]\beta_{i+1}(y_{i+1}|x)</script><p>又可以表示为：</p><script type="math/tex; mode=display">\beta_i(x) = M_{i+1}(x)\beta_{i+1}(x)</script><h5 id="概率计算"><a href="#概率计算" class="headerlink" title="概率计算"></a>概率计算</h5><p>按照前向-后向向量的定义，很容易计算出标记序列在位置$i$是标记$y_i$的条件概率和在位置$i-1$与$i$是标记$y_{i-1}$和$y_i$的条件概率：</p><script type="math/tex; mode=display">P(Y_i=y_i|x) = \frac{\alpha_i^T(y_i|x)\beta_i(y_i|x)}{Z(x)}</script><script type="math/tex; mode=display">P(Y_{i-1}=y_{i-1},Y_i=y_i|x) = \frac{\alpha_{i-1}^T(y_{i-1}|x)M_i(y_{i-1},y_i|x)\beta_i(y_i|x)}{Z(x)}</script><p>其中</p><script type="math/tex; mode=display">Z(x)=\alpha_n^T(x)\textbf{1} = \textbf{1}\beta_1(x)</script><h5 id="期望值的计算"><a href="#期望值的计算" class="headerlink" title="期望值的计算"></a>期望值的计算</h5><p>利用前向-后向向量，可以计算特征函数关于联合分布$P(X,Y)$和条件分布$P(Y|X)$的数学期望。</p><p>特征函数$f_k$关于条件分布$P(Y|X)$的数学期望是</p><script type="math/tex; mode=display">\begin{aligned}E_{P(Y|X)}[f_k] &= \sum_yP(y|x)f_k(y,x)\\&=\sum_{i=1}^{n+1}\sum_{y_{i-1}y_i}f_k(y_{i-1},y_i,x,i)\frac{\alpha_{i-1}^T(y_{i-1}|x)M_i(y_{i-1},y_i|x)\beta_i(y_i|x)}{Z(x)}\end{aligned}</script><p>其中</p><script type="math/tex; mode=display">Z(x) = \alpha_n^T(x)\textbf{1}</script><p>假设经验分布为$\tilde{P}(X)$，特征函数$f_k$关于联合分布$P(X,Y)$的数学期望是</p><script type="math/tex; mode=display">\begin{aligned}E_{P(X, Y)}\left[f_{k}\right] &=\sum_{x, y} P(x, y) \sum_{i=1}^{n+1} f_{k}\left(y_{i-1}, y_{i}, x, i\right) \\&=\sum_{x} \tilde{P}(x) \sum_{y} P(y \mid x) \sum_{i=1}^{n+1} f_{k}\left(y_{i-1}, y_{i}, x, i\right) \\&=\sum_{x} \tilde{P}(x) \sum_{i=1}^{n+1} \sum_{y_{i-1} y_{i}} f_{k}\left(y_{i-1}, y_{i}, x, i\right) \frac{\alpha_{i-1}^{\mathrm{T}}\left(y_{i-1} \mid x\right) M_{i}\left(y_{i-1}, y_{i} \mid x\right) \beta_{i}\left(y_{i} \mid x\right)}{Z(x)}\end{aligned}</script><h4 id="学习"><a href="#学习" class="headerlink" title="学习"></a>学习</h4><p>在这里我们用如下向量表示条件随机场：</p><script type="math/tex; mode=display">p(y|x) = \frac{1}{Z(x,\lambda,\eta)}\exp\left(\sum_{t=1}^T\left[\lambda^Tf(y_{t_{t-1}},y_t,x)+\eta^Tg(y_t,x)\right]\right)</script><p>这里一个$x$对应于一个样本，一个样本就对应于一个序列。</p><p>所以我们要优化的对象为：</p><script type="math/tex; mode=display">\begin{aligned}\arg\max_{\lambda,\eta}\log\prod_{i=1}^NP(y^{(i)}|x^{(i)}) &= \arg\max_{\lambda,\eta}\sum_{i=1}^N\log P(y^{(i)}|x^{(i)})\\&=\arg\max_{\lambda,\eta}\sum_{i=1}^N\left(-\log Z(x^{(i)},\lambda,\eta)+\sum_{t=1}^T\left[\lambda^Tf(y_{t-1}^{(i)},y_t^{(i)},x^{(i)})+\eta^Tg(y_t^{(i)},x^{(i)})\right]\right)\\&= \arg\max_{\lambda,\eta}L(\lambda,\eta,x^{(i)})\end{aligned}</script><p>由于条件随机场是一个有监督的学习问题，所以我们知道$x,y$的值，对上述的式子我们可以采用梯度上升的方法求解，求解$\nabla_\lambda L,\nabla_\eta L$。</p><p>我们有：</p><script type="math/tex; mode=display">\begin{aligned}\nabla_\lambda L&= \sum_{i=1}^N\left(\sum_{t=1}^Tf(y_{t-1}^{(i)},y_t^{(i)},x^{(i)})\right)-\nabla_\lambda\log Z(x^{(i)},\lambda,\eta)\end{aligned}</script><p>而$\nabla_\lambda\log Z(x^{(i)},\lambda,\eta)$为<code>log-partition function</code>，它的导数为：</p><script type="math/tex; mode=display">\mathbb{E}_y\left[\sum_{t=1}^Tf(y_{t-1},y_t,x^{(i)})\right]</script><blockquote><p>下面我们来看一下<code>log-partition function</code>的求导：</p><script type="math/tex; mode=display">\begin{aligned}p(x;\theta) &= \exp\left(\theta^T\phi(x)-\Psi(\theta)\right)\\&=\frac{\exp\left(\theta^T\phi(x)\right)}{\exp(\Psi(\theta))}\end{aligned}</script><p>而我们的</p><script type="math/tex; mode=display">\Psi(\theta) = \log\sum_x\exp(\theta^T\phi(x))</script><p>相当于归一化因子。</p><p>我们对归一化因子的对数求导，即我们问题中的$\log Z(x^{(i)},\lambda,\eta)$求导，得</p><script type="math/tex; mode=display">\begin{aligned}\frac{\partial \Psi}{\partial \theta_{k}} &=\frac{\sum_{x} \exp \left\{\boldsymbol{\theta}^{T} \boldsymbol{\phi}(\boldsymbol{x})\right\} \phi_{k}(\boldsymbol{x})}{\sum_{x} \exp \left\{\boldsymbol{\theta}^{T} \boldsymbol{\phi}(\boldsymbol{x})\right\}} \\&=\frac{\sum_{x} \exp \left\{\boldsymbol{\theta}^{T} \boldsymbol{\phi}(\boldsymbol{x})\right\} \phi_{k}(\boldsymbol{x})}{\exp \Psi(\boldsymbol{\theta})} \\&=\sum_{\boldsymbol{x}} \exp \left\{\boldsymbol{\theta}^{T} \boldsymbol{\phi}(\boldsymbol{x})-\Psi(\boldsymbol{\theta})\right\} \phi_{k}(x) \\&=\sum_{\boldsymbol{x}} p(\boldsymbol{x} ; \boldsymbol{\theta}) \phi_{k}(\boldsymbol{x}) \\&=\mathbb{E}\left\{\phi_{k}(\boldsymbol{X})\right\} .\end{aligned}</script><p>即与我们上面得结果相同。</p></blockquote><p>所以</p><script type="math/tex; mode=display">\nabla_\lambda L= \sum_{i=1}^N\left(\sum_{t=1}^Tf(y_{t-1}^{(i)},y_t^{(i)},x^{(i)})\right) -\mathbb{E}_y\left[\sum_{t=1}^Tf(y_{t-1},y_t,x^{(i)})\right]</script><p>而期望的求法我们在上文中已经解决了。</p><h3 id="条件随机场的预测算法"><a href="#条件随机场的预测算法" class="headerlink" title="条件随机场的预测算法"></a>条件随机场的预测算法</h3><p>我们要找出使得$P_w(y|x)$最大的$y$。</p><script type="math/tex; mode=display">\begin{aligned}y^\star &= \arg\max_y P_w(y|x)\\&= \arg\max_y\frac{\exp(w\bullet F(y,x))}{Z_w(x)}\\&= \arg\max_y\exp(w\bullet F(y,x))\\&= \arg\max_y(w\bullet F(y,x))\end{aligned}</script><p>于是，条件随机场的预测问题称为求非规范化概率的最优路径问题：</p><script type="math/tex; mode=display">\max_y(w\bullet F(y,x))</script><p>这里，路径表示标记序列。其中</p><script type="math/tex; mode=display">w = (w_1,w_2,\cdots,w_K)^T</script><script type="math/tex; mode=display">F(y,x) = (f_1(y,x),f_2(y,x),\cdots,f_K(y,x))^T</script><script type="math/tex; mode=display">f_k(y,x) = \sum_{i=1}^nf_k(y_{i-1},y_i,x,i),\quad k=1,2,\cdots,K</script><p>为了求解最优路径，我们求解下式：</p><script type="math/tex; mode=display">\max_y\sum_{i=1}^nw\bullet F_i(y_{i-1},y_i,x)</script><p>其中</p><script type="math/tex; mode=display">F_i(y_{i-1},y_i,x) = (f_1(y_{i-1},y_i,x,i),f_2(y_{i-1},y_i,x,i),\cdots,f_K(y_{i-1},y_i,x,i))^T</script><p>是局部特征向量。</p><p>下面我们用维比特算法。首先求出位置$1$的各个标记$j=1,2,\cdots,m$的非规范化概率：</p><script type="math/tex; mode=display">\delta_1(j) = w\bullet F_1(y_0=\operatorname{start},y_1=j,x),\quad j=1,2,\cdots,m</script><p>一般地，由递推公式，求出到位置$i$的各个标记$l=1,2,\cdots,m$的非规范化概率的最大值，同时记录非规范化概率最大值的路径</p><script type="math/tex; mode=display">\delta_i(l) = \max_{1\le j\le m}\{\delta_{i-1}(j)+w\bullet F_i(y_{i-1}=j,y_i=l,x)\},l=1,2,\cdots,m</script><script type="math/tex; mode=display">\Psi_i(l) = \arg\max_{1\le j\le m}\{\delta_{i-1}(j)+w\bullet F_i(y_{i-1}=j,y_i=l,x)\},l=1,2,\cdots,m</script><p>直到$i=n$时终止。这时求得非规范化概率的最大值为</p><script type="math/tex; mode=display">\max_y(w\bullet F(y,x)) = \max_{i\le j\le m}\delta_n(j)</script><p>及最优路径的终点</p><script type="math/tex; mode=display">y_n^\star = \arg\max_{1\le j\le m}\delta_n(j)</script><p>由此最优路径返回</p><script type="math/tex; mode=display">y_i^\star=\Psi_{i+1}(y^\star_{i+1}),\quad i=n-1,n-2,\cdots,1</script><p>求得最优路径$y^\star = (y_1^\star,y_2^\star,\cdots,y^\star_n)^T$。</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;条件随机场感觉比HMM要难，虽然我看完了一遍，但是有些地方感觉还是云里雾里的，之后可能还得更详细的阅读。另外条件随机场需要由标注的数据，另外其参数设置也较复杂，实际应用中可能比较难。&lt;/p&gt;</summary>
    
    
    
    <category term="算法" scheme="https://www.hfcouc.work/categories/%E7%AE%97%E6%B3%95/"/>
    
    
    <category term="机器学习" scheme="https://www.hfcouc.work/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>连续观测数据下的HMM</title>
    <link href="https://www.hfcouc.work/2021/12/30/%E8%BF%9E%E7%BB%AD%E8%A7%82%E6%B5%8B%E6%95%B0%E6%8D%AE%E4%B8%8B%E7%9A%84HMM/"/>
    <id>https://www.hfcouc.work/2021/12/30/%E8%BF%9E%E7%BB%AD%E8%A7%82%E6%B5%8B%E6%95%B0%E6%8D%AE%E4%B8%8B%E7%9A%84HMM/</id>
    <published>2021-12-30T13:23:41.000Z</published>
    <updated>2022-01-02T09:51:00.447Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="\assets\js\Meting.min.js"></script><p>之前的HMM都是在离散观测的条件下，现在我们研究一下观测值连续时的HMM。</p><span id="more"></span><h2 id="连续观测下的HMM"><a href="#连续观测下的HMM" class="headerlink" title="连续观测下的HMM"></a>连续观测下的HMM</h2><p>在连续观测下，观测$o_t$是联系变量，矩阵$B$被概率密度函数代替。在给定状态$s_j$下的观测$o_t$的概率密度函数为：</p><script type="math/tex; mode=display">b_j(o_t) = p_j(o_t|\theta_j)</script><p>现在HMM的参数$\lambda=(a_{ij},\theta_j,\pi_j)$被称为连续观测HMM。</p><p>现在在连续的情况下，我们的前向概率变为：</p><script type="math/tex; mode=display">\alpha_1(i) = \pi_ip_i(o_1|\theta_i)</script><script type="math/tex; mode=display">\alpha_{t+1}(j) = \left(\sum_{i=1}^N a_{ij}\alpha_t(i)\right)p_j(o_{t+1}|\theta_j)</script><p>我们的后向概率为：</p><script type="math/tex; mode=display">\beta_T(i)=1\quad i=1,\cdots,N</script><script type="math/tex; mode=display">\beta_t(i) = \sum_{j=1}^Na_{ij}p_j(o_{t+1}|\theta_j)\beta_{t+1}(j)</script><p>我们有：</p><script type="math/tex; mode=display">P(O|\lambda) = \sum_{i}^N\alpha_t(i)\beta_t(i)</script><p>但是它的学习算法发生了改变。我们现在是要估计$\hat{\lambda} = (\hat{\alpha}_{ij},\hat{\theta}_j,\hat{\pi}_j)$，计算$\hat{\alpha}_{ij}$和$\hat{\pi}_j$的公式不变，为：</p><script type="math/tex; mode=display">\hat{a}_{ij} = \frac{\sum_{t=1}^{T-1}\alpha_t(i)\beta_{t+1}(j)a_{ij}p_j(o_{t+1}|\theta_j)}{\sum_{t=1}^{T-1}\alpha_t(i)\beta_t(i)}</script><script type="math/tex; mode=display">\hat{\pi}_i = \frac{\alpha_1(i)\beta_1(i)}{\sum_{i}^N\alpha_1(i)\beta_1(i)}</script><p>但是$o_t$为连续变量，所以对于确定的$o_t$其概率为$0$。所以我们必须在其$\epsilon$邻域内计算其概率，$\epsilon$事先指定为一个很小的数值。如果$p_j(o_t|\theta_j)$为正态分布，那么计算其$\epsilon$邻域内的概率将非常容易：</p><script type="math/tex; mode=display">\int_{o_t-\epsilon}^{o_t+\epsilon}p_j(o|\theta_j)do = \Phi\left(\frac{o_t+\epsilon-m_j}{\sigma_j}\right) - \Phi\left(\frac{o_t-\epsilon-m_j}{\sigma_j}\right)</script><p>下面我们就需要求解$\hat{\theta}$，我们有：</p><script type="math/tex; mode=display">\hat{\theta}=\arg\max_{\theta}\sum_{t=1}^T\sum_{i=1}^N\log p_i(o_t|\theta_i)\alpha_t(i)\beta_t(i)</script><p>对于一个确定的$\theta_j$，上式变为：</p><script type="math/tex; mode=display">\hat{\theta}_j = \arg\max_{\theta_j}\sum_{t=1}^T\log p_j(o_t|\theta_j)\alpha_t(j)\beta_t(j)</script><p>如果是正态分布，我们有：</p><script type="math/tex; mode=display">p_j(o_t|\theta_j) = \frac{1}{\sqrt{2\pi\sigma_j^2}}\exp\left(-\frac{1}{2}\frac{(o_t-m_j)^2}{\sigma_j^2}\right)</script><p>对$m_j$求导使导数为$0$我们有：</p><script type="math/tex; mode=display">\hat{m}_j = \frac{\sum_{t=1}^T\alpha_t(j)\beta_t(j)o_t}{\sum_{t=1}^T\alpha_t(j)\beta_t(j)}</script><p>同时对$\sigma_j^2$求导令其导数为零我们有：</p><script type="math/tex; mode=display">\hat{\sigma}_j^2 =\frac{\sum_{t=1}^T\alpha_t(j)\beta_t(j)(o_t-\hat{m}_j)^2}{\sum_{t=1}^T\alpha_t(j)\beta_t(j)}</script><h2 id="混合HMM"><a href="#混合HMM" class="headerlink" title="混合HMM"></a>混合HMM</h2><p>假设我们观测数据的概率密度函数为$K$个概率密度函数的混合，即：</p><script type="math/tex; mode=display">b_j(o_t) = p_j(o_t|\theta_j) = \sum_{k=1}^Kc_j^{(k)}p_j^{(k)}(o_t|\theta_j^{(k)})</script><p>其中$c_j^{(k)}$为非负归一化权重，$0\le c_j^{(k)}\le 1$使得</p><script type="math/tex; mode=display">\sum_{k=1}^Kc_j^{(k)}=1</script><p>这里优化的策略采用的是将混合分布拆成多个分布，每个对于一个优化的HMM对象(看了好久终于看懂了，这是因为当$x_1,x_2\in [0,1]$时，我们有$\log(x_1+x_2)\ge \log(x_1x_2)$)。</p><p>相较于之前，我们有：</p><script type="math/tex; mode=display">\alpha_{t+1}(j,k) = \left(\sum_{i=1}^n\alpha_t(i,k)a_{ij}\right)\int_{o_t+1-\epsilon}^{o_t+1+\epsilon}c_j^{(k)}p_j^{(k)}(o|\theta_j^{(k)})do</script><script type="math/tex; mode=display">\beta_t(i,k)=\sum_{j=1}^na_{ij}\left(\int_{o_t+1-\epsilon}^{o_t+1+\epsilon}c_j^{(k)}p_j^{(k)}(o|\theta_j^{(k)})do\right)\beta_{t+1}(j,k)</script><p>优化过程中$a,\pi$不变：</p><script type="math/tex; mode=display">\hat{\alpha}_{ij} = \frac{\sum_{t=1}^{T-1}\alpha_t(i)\beta_{t+1}(j)a_{ij}p_j(o_{t+1}|\theta_j)}{\sum_{t=1}^{T-1}\alpha_t(i)\beta_t(i)}</script><script type="math/tex; mode=display">\hat{\pi}_i = \frac{\alpha_1(i)\beta_1(i)}{\sum_{i}^N\alpha_1(i)\beta_1(i)}</script><p>优化$c$得：</p><script type="math/tex; mode=display">\hat{c}_j^{(k)} = \frac{\sum_{t=1}^T\alpha_t(j,k)\beta_t(j,k)}{\sum_{l=1}^K\sum_{t=1}^T\alpha_t(j,l)\beta_t(j,l)}</script><p>$\hat{m}_j^{(k)},\sigma^{2^{(k)}}_j$与之前相似，为：</p><script type="math/tex; mode=display">\hat{m}_j^{(k)} = \frac{\sum_{t=1}^T\alpha_t(j,k)\beta_t(j,k)o_t}{\sum_{t=1}^T\alpha_t(j,k)\beta_t(j,k)}</script><script type="math/tex; mode=display">\hat{\sigma}_j^{2^{(k)}} =\frac{\sum_{t=1}^T\alpha_t(j,k)\beta_t(j,k)(o_t-\hat{m}_j^{(k)})^2}{\sum_{t=1}^T\alpha_t(j,k)\beta_t(j,k)}</script><h2 id="多变量HMM"><a href="#多变量HMM" class="headerlink" title="多变量HMM"></a>多变量HMM</h2><p>假设我们的概率密度函数是多元正态分布：</p><script type="math/tex; mode=display">f(\mathrm{X}) = \frac{1}{\sqrt{(2\pi)^k}|\Sigma|^{\frac{1}{2}}}\exp\left(-\frac{1}{2}(\mathrm{x-\mu})^T\Sigma^{-1}(\mathrm{x-\mu})\right)</script><p>那么我们的概率计算也会发生相应的改变。</p><p>在优化过程中$a$和$\pi$仍然不变：</p><script type="math/tex; mode=display">\hat{a}_{ij} = \frac{\sum_{t=1}^{T-1}\alpha_t(i)\beta_{t+1}(j)a_{ij}p_j(o_{t+1}|\theta_j)}{\sum_{t=1}^{T-1}\alpha_t(i)\beta_t(i)}</script><script type="math/tex; mode=display">\hat{\pi}_i = \frac{\alpha_1(i)\beta_1(i)}{\sum_{i}^N\alpha_1(i)\beta_1(i)}</script><p>$\hat{m}$也大致相同，但是$o_t$变成了向量：</p><script type="math/tex; mode=display">\hat{m}_j = \frac{\sum_{t=1}^T\alpha_t(j)\beta_t(j)o_t}{\sum_{t=1}^T\alpha_t(j)\beta_t(j)}</script><p>$\hat{\Sigma}$也大致相同：</p><script type="math/tex; mode=display">\hat{\Sigma}_j=\frac{\sum_{t=1}^T\alpha_t(j)\beta_t(j)(o_t-\hat{m}_j)(o_t-\hat{m})^T}{\sum_{t=1}^T\alpha_t(j)\beta_t(j)}</script><p>如果是混合多变量HMM，那么有上面的混合HMM相似。</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;之前的HMM都是在离散观测的条件下，现在我们研究一下观测值连续时的HMM。&lt;/p&gt;</summary>
    
    
    
    <category term="算法" scheme="https://www.hfcouc.work/categories/%E7%AE%97%E6%B3%95/"/>
    
    
    <category term="机器学习" scheme="https://www.hfcouc.work/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>神经网络</title>
    <link href="https://www.hfcouc.work/2021/12/23/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    <id>https://www.hfcouc.work/2021/12/23/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/</id>
    <published>2021-12-23T12:44:07.000Z</published>
    <updated>2021-12-23T12:57:10.987Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="\assets\js\Meting.min.js"></script><p>参考自西瓜书、未更新完。还有一天就考研了，想好好休息一下准备考研了，考完研后再更新。</p><span id="more"></span><h2 id="神经网络"><a href="#神经网络" class="headerlink" title="神经网络"></a>神经网络</h2><h3 id="神经元模型"><a href="#神经元模型" class="headerlink" title="神经元模型"></a>神经元模型</h3><p>下图为M-P神经元模型。在这个模型中，神经元收到来自于$n$个其他神经元传递过来的输入信号，这些输入信号通过带权重的连接(connection)进行传递，神经元接收到的总输入值将与神经元的阈值进行比较，然后通过”激活函数”处理以产生神经元的输出。</p><p><img src="https://static01.imgkr.com/temp/b064a4e75dff4a56adc5a5300f3057fe.png" alt=""></p><p>理想的激活函数是下图所示的阶跃函数，它将输入值映射为输出值$0$或$1$，显然$1$对应于神经元兴奋，$0$对应于神经元抑制。然而，阶跃函数具有不连续、不光滑等不太好的性质，因此实际上常用Sigmoid函数作为激活函数。</p><p><img src="https://static01.imgkr.com/temp/eb8af338b48c43adb4f1185f4b041239.png" alt=""></p><p>把许多这样的神经元按一定层次结构连接起来，就得到了神经网络。</p><h3 id="感知机与多层网络"><a href="#感知机与多层网络" class="headerlink" title="感知机与多层网络"></a>感知机与多层网络</h3><p>感知机由两层神经元组成，如下图所示：</p><p><img src="https://static01.imgkr.com/temp/bc0f6f44f7614d91bbb10944597ac6b1.png" alt=""></p><p>但是感知机只适用于线性可分数据集，对于线性不可分数据集无法收敛。于是就有了多层神经网络。</p><h3 id="误差传播算法"><a href="#误差传播算法" class="headerlink" title="误差传播算法"></a>误差传播算法</h3><p>误差传播算法(BP算法)可以用来学习多层神经网络。</p><p>下面我们来看看BP算法究竟是什么样的。给定数据集$D=\{(x_1,y_1),(x_2,y_2),\cdots,(x_m,y_m)\},x_i\in \mathbb{R}^d,y_i\in \mathbb{R}^l$。设只有一个隐藏层，隐藏层具有$q$个神经元，其中输出层第$j$个神经元用阈值用$\theta_j$表示，隐含层第$h$个神经元的阈值用$\gamma_h$表示。输入层第$i$个神经元与隐含层第$h$个神经元之间的连接权为$v_{ih}$，隐含层第$h$个神经元与输出层第$j$个神经元之间的连接权重为$w_{hj}$。如下图所示：</p><p><img src="https://static01.imgkr.com/temp/78d1c347bbba40ffb71dddc518b0e877.png" alt=""></p><p>对训练例$(x_k,y_k)$，假定神经网络的输出为$\hat{y}_k = (\hat{y}_1^k,\hat{y}_2^k,\cdots,\hat{y}_l^k)$，即</p><script type="math/tex; mode=display">\hat{y}_j^k = f(\beta_j-\theta_j)</script><p>则网络在$(x_k,y_k)$上的均方误差为：</p><script type="math/tex; mode=display">E_k = \frac{1}{2}\sum_{j=1}^l(\hat{y}_j^k-y^k_j)^2</script>]]></content>
    
    
    <summary type="html">&lt;p&gt;参考自西瓜书、未更新完。还有一天就考研了，想好好休息一下准备考研了，考完研后再更新。&lt;/p&gt;</summary>
    
    
    
    <category term="算法" scheme="https://www.hfcouc.work/categories/%E7%AE%97%E6%B3%95/"/>
    
    
    <category term="机器学习" scheme="https://www.hfcouc.work/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>决策树</title>
    <link href="https://www.hfcouc.work/2021/12/21/%E5%86%B3%E7%AD%96%E6%A0%91/"/>
    <id>https://www.hfcouc.work/2021/12/21/%E5%86%B3%E7%AD%96%E6%A0%91/</id>
    <published>2021-12-21T14:12:55.000Z</published>
    <updated>2022-01-12T10:38:16.014Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="\assets\js\Meting.min.js"></script><p>以下主要参考李航老师《统计学习方法》。已更新决策树和回归树代码，CART算法代码待补全。</p><span id="more"></span><h2 id="决策树"><a href="#决策树" class="headerlink" title="决策树"></a>决策树</h2><p>决策树学习的三个阶段</p><ol><li>特征选择</li><li>决策树的生成</li><li>决策树的修剪</li></ol><h3 id="特征选择"><a href="#特征选择" class="headerlink" title="特征选择"></a>特征选择</h3><p>特征选择的准则时信息增益或信息增益比。</p><h4 id="信息增益"><a href="#信息增益" class="headerlink" title="信息增益"></a>信息增益</h4><p>我们先介绍熵的定义。设$X$是一个取有限个值的离散随机变量，其概率分布为：</p><script type="math/tex; mode=display">P(X=x_i)=p_i,i=1,2,\cdots,n</script><p>则随机变量$X$的熵的定义为</p><script type="math/tex; mode=display">H(X) = -\sum_{i=1}^np_i\log p_i</script><p>若$p_i=0$，我们定义$0\log0=0$。对数若以$2$为底，单位为比特，以$e$为底，单位为纳特。熵只与$X$的分布有关而与$X$的取值无关，因此我们将$X$的熵记作$H(p)$，即</p><script type="math/tex; mode=display">H(p) =-\sum_{i=1}^np_i\log p_i</script><p>熵越大，随机变量的不确定性就越大。从定义可验证：</p><script type="math/tex; mode=display">0\le H(p)\le \log n</script><p>设有随机变量$(X,Y)$，其联合概率密度分布为：</p><script type="math/tex; mode=display">P(X=x_i,Y=y_j) = p_{ij},\quad i=1,2,\cdots,n;\quad j=1,2,\cdots,m</script><p>条件熵$H(Y|X)$表示在已知随机变量$X$的条件下随机变量$Y$的不确定性。随机变量$X$给定的条件下随机变量$Y$的条件熵$H(Y|X)$，定义为$X$给定条件下$Y$的条件概率分布的熵对$X$的数学期望</p><script type="math/tex; mode=display">H(Y|X) = \sum_{i=1}^n p_iH(Y|X=x_i)</script><p>这里，$p_i = P(X=x_i),i=1,2,\cdots,n$。</p><p>信息增益表示得知特征$X$的信息而使得类$Y$的信息不确定性减少的程度。</p><p>定义(信息增益)：特征$A$对训练数据$D$的信息增益$g(D,A)$，定义为集合$D$的经验熵$H(D)$与特征$A$给定条件下$D$的经验条件熵$H(D|A)$之差，即</p><script type="math/tex; mode=display">g(D,A) = H(D)-H(D|A)</script><p>一般地，熵$H(Y)$与条件熵$H(Y|X)$之差称为互信息。决策树学习中的信息增益等价于训练数据集中类与特征的互信息。</p><p>设训练数据集为$D$，$|D|$表示其样本容量，即样本个数。设有$K$个类$C_k,k=1,2,\cdots,K$，$|C_k|$为属于类$C_k$的样本数。设特征$A$有$n$个不同的取值$\{a_1,a_2,\cdots,a_n\}$，根据特征$A$的取值将$D$划分为$n$个子集$D_1,D_2,\cdots,D_n$，$|D_i|$表示$D_i$的样本数。记子集$D_i$中属于类$C_k$的样本集合为$D_{ik}$，即$D_{ik}=D_i\cap C_k$。</p><p>信息增益的算法：</p><p>输入：训练数据集$D$和特征$A$</p><p>输出：特征$A$对训练数据集$D$的信息增益$g(D,A)$</p><ol><li>计算数据集的经验熵$H(D)$：<script type="math/tex">H(D)=-\sum_{k=1}^K\frac{|C_k|}{|D|}\log_2\frac{|C_k|}{|D|}</script></li><li>计算特征$A$对数据集$D$的经验条件熵$H(D|A)$：<script type="math/tex">H(D|A)=\sum_{i=1}^n\frac{|D_i|}{D}H(D_i)=-\sum_{i=1}^n\frac{|D_i|}{|D|}\sum_{k=1}^K\frac{|D_{ik}|}{|D_i|}\log2\frac{|D_{ik}|}{|D_i|}</script></li><li>计算信息增益：<script type="math/tex">g(D,A)=H(D)-H(D|A)</script></li></ol><h4 id="信息增益比"><a href="#信息增益比" class="headerlink" title="信息增益比"></a>信息增益比</h4><p>以信息增益作为划分训练数据集的特征，存在偏向于选择取值的较多的特征的问题，使用信息增益比可以对这一问题进行校正。</p><p>定义(信息增益)：特征$A$对训练数据集$D$信息增益比$g_R(D,A)$定义为其信息增益$g(D,A)$与训练数据集$D$关于特征$A$的值的熵$H_A(D)$之比，即</p><script type="math/tex; mode=display">g_R(D,A) = \frac{g(D,A)}{H_A(D)}</script><p>。</p><h3 id="决策树的生成"><a href="#决策树的生成" class="headerlink" title="决策树的生成"></a>决策树的生成</h3><h4 id="ID3算法"><a href="#ID3算法" class="headerlink" title="ID3算法"></a>ID3算法</h4><p>输入：训练数据集$D$，特征值阈值$\epsilon$</p><p>输出：决策树$T$</p><ol><li>若$D$中所有实例属于同一类$C_k$，则$T$为单节点树，并将类$C_k$作为该结点的类标记，返回$T$；</li><li>若$A=\varnothing$，则$T$为单结点树，并将$D$中实例树最大的类$C_k$作为该结点的类标记，返回$T$；</li><li>否则，计算$A$中各特征值对$D$的信息增益，选择信息增益最大的特征$A_g$；</li><li>如果$A_g$的信息增益小于阈值$\epsilon$，则置$T$为单节点树，并将$D$中实例数最大的类$C_k$作为该结点的类标记，返回$T$</li><li>否则，对$A_g$的每一可能值$a_i$，依$A_g=a_i$将$D$分割为若干非空子集$D_i$，将$D_i$中实例数最大的类作为标记，构建子结点，由结点及其子节点构成数$T$，返回$T$；</li><li>对第$i$个子结点，以$D_i$为训练集，以$A-\{A_g\}$为特征集，递归调用步骤1到5，得到子树$T_i$，返回$T_i$。</li></ol><p>ID3算法只有数的生成，所以该算法生成的树容易产生过拟合。</p><h4 id="C4-5的生成算法"><a href="#C4-5的生成算法" class="headerlink" title="C4.5的生成算法"></a>C4.5的生成算法</h4><p>与ID3算法的差别仅在于使用信息增益比来选择特征。</p><ol><li>若$D$中所有实例属于同一类$C_k$，则$T$为单节点树，并将类$C_k$作为该结点的类标记，返回$T$；</li><li>若$A=\varnothing$，则$T$为单结点树，并将$D$中实例树最大的类$C_k$作为该结点的类标记，返回$T$；</li><li>否则，计算$A$中各特征值对$D$的信息增益比，选择信息增益比最大的特征$A_g$；</li><li>如果$A_g$的信息增益比小于阈值$\epsilon$，则置$T$为单节点树，并将$D$中实例数最大的类$C_k$作为该结点的类标记，返回$T$</li><li>否则，对$A_g$的每一可能值$a_i$，依$A_g=a_i$将$D$分割为若干非空子集$D_i$，将$D_i$中实例数最大的类作为标记，构建子结点，由结点及其子节点构成数$T$，返回$T$；</li><li>对第$i$个子结点，以$D_i$为训练集，以$A-\{A_g\}$为特征集，递归调用步骤1到5，得到子树$T_i$，返回$T_i$。</li></ol><h3 id="决策树的剪枝"><a href="#决策树的剪枝" class="headerlink" title="决策树的剪枝"></a>决策树的剪枝</h3><p>决策树的剪枝往往通过极小化决策树整体的损失函数来实现。设数$T$的叶结点个数为$|T|$，$t$是数$T$的叶结点，该结点有$N_t$个样本点，其中$k$类的样本点有$N_{tk}$个，$H_t(T)$为叶结点$t$上的经验熵，$\alpha\ge0$为超参数，则决策树学习的损失函数可以定义为</p><script type="math/tex; mode=display">C_{\alpha}(T) = \sum_{t=1}^{|T|}N_tH_t(T)+\alpha|T|</script><p>其中经验熵为：</p><script type="math/tex; mode=display">H_t(T) = -\sum_k\frac{N_{tk}}{N_t}\log\frac{N_{tk}}{N_t}</script><p>在损失函数中，常将右端的第一项记作：</p><script type="math/tex; mode=display">C(T) =\sum_{t=1}^{|T|}N_tH_t(T) = -\sum_{t=1}^{|T|}\sum_{k=1}^K\frac{N_{tk}}{N_t}\log\frac{N_{tk}}{N_t}</script><p>这时有：</p><script type="math/tex; mode=display">C_{\alpha}(T) = C(T)+\alpha|T|</script><p>算法(数的剪枝算法)</p><p>输入：生成算法产生的整个数$T$，参数$\alpha$</p><p>输出：修剪后的子树$T_{\alpha}$</p><ol><li>计算每个结点的经验熵。</li><li>递归地从数地叶结点向上回缩。设一组叶结点回缩到其父结点之前与之后的整体数分别为$T_B$和$T_A$，其对应的损失函数分别是$C_{\alpha}(T_B)$与$C_{\alpha}(T_A)$，其对应的损失函数值分别是$C_{\alpha}(T_B)$与$C_{\alpha}(T_A)$，如果<script type="math/tex">C_{\alpha}(T_A)\le C_{\alpha}(T_B)</script>，则进行剪枝，即将父结点变为新的叶结点。</li><li>返回2，直至不能继续为止，得到损失函数最小的子树$T_{\alpha}$。</li></ol><h3 id="CART算法"><a href="#CART算法" class="headerlink" title="CART算法"></a>CART算法</h3><p>分类与回归树(classfication and regression tree, CART)。</p><p>决策树就是递归地构建二叉决策树的过程。对回归树用平方误差最小化准则，对分类树用基尼系数最小化准则，进行特征选择，生成二叉树。</p><h4 id="回归树的生成"><a href="#回归树的生成" class="headerlink" title="回归树的生成"></a>回归树的生成</h4><p>假设$X$和$Y$分别为输入和输出变量，并且$Y$是连续变量，给定训练数据集</p><script type="math/tex; mode=display">D= \{(x_1,y_1),(x_2,y_2),\cdots,(x_N,y_N)\}</script><p>考虑如何生成回归树。</p><p>假设树将输入空间划分为$M$个单元$R_1,R_2,\cdots,R_M$，并且在每个单元$R_m$上有一个固定的输出值$c_m$，于是回归树模型可表示为：</p><script type="math/tex; mode=display">f(x) = \sum_{m=1}^Mc_mI(x\in R_m)</script><p>当输入空间的划分确定时，可以用平方误差$\sum_{x_i\in R_m}(y_i-f(x_i))^2$来表示回归树对于训练数据的预测误差，求解每个单元上的最优输出值。易知：</p><script type="math/tex; mode=display">\hat{c}_m = \operatorname{ave}(y_i|x_i\in R_m)</script><p>问题是怎么对输入空间进行划分。我们采用启发式的方法，选择第$j$个变量$x^{(j)}$和它的取值$s$，作为切分变量和切分点，并定义两个区域：</p><script type="math/tex; mode=display">R_1(j,s) = \{x|x^{(j)}\le s\}\quad\text{和}R_2(j,s) = \{x|x^{(j)}>s\}</script><p>然后寻找最优切分点和最优切分变量：</p><script type="math/tex; mode=display">\min_{j,s}\left[\min_{c_1}\sum_{x_i\in R_1(j,s)}(y_i-c_1)^2+\min_{c_2}\sum_{x_i\in R_2(j,s)}(y_i-c_2)^2\right]</script><p>对于固定输入变量$j$可以找到最优切分点$s$</p><script type="math/tex; mode=display">\hat{c}_1 = \operatorname{ave}(y_i|x_i\in R_1(j,s))\quad\text{和}\quad \hat{c}_2 = \operatorname{ave}(y_i|x_i\in R_2(j,s))\quad</script><p>算法为：</p><p>输入：训练数据集$D$</p><p>输出：回归树$f(x)$</p><p>在训练数据集中所在的输入空间中，递归地将每个区域划分为两个子区域并决定每个子区域上的输出值，构建二叉决策树：</p><ol><li><p>选择最优切分变量$j$和切分点$s$，求解：</p><script type="math/tex; mode=display">\min_{j,s}\left[\min_{c_1}\sum_{x_i\in R_1(j,s)}(y_i-c_1)^2+\min_{c_2}\sum_{x_i\in R_2(j,s)}(y_i-c_2)^2\right]</script><p>遍历变量$j$，对固定的切分变量$j$扫描切分点$s$，选择使上式达到最小值的对$(j,s)$。</p></li><li><p>对选定的对$(j,s)$划分区域并决定相应的输出值</p><script type="math/tex; mode=display">\begin{gathered}R_{1}(j, s)=\left\{x \mid x^{(j)} \leqslant s\right\}, \quad R_{2}(j, s)=\left\{x \mid x^{(j)}>s\right\} \\\hat{c}_{m}=\frac{1}{N_{m}} \sum_{x_{i} \in R_{m}(j, s)} y_{i}, \quad x \in R_{m}, \quad m=1,2\end{gathered}</script></li><li><p>继续对两个子区域调用步骤1，2，直到满足停止条件。</p></li><li><p>将输入空间划分为$M$个区域$R_1,R_2,\cdots,R_M$，生成决策树：</p><script type="math/tex; mode=display">f(x) = \sum_{m=1}^M\hat{c}_mI(x\in R_m)</script></li></ol><h4 id="分类树的生成"><a href="#分类树的生成" class="headerlink" title="分类树的生成"></a>分类树的生成</h4><p>分类树用基尼指数选择最优特征，同时决定该特征的最优二值切分点。</p><p>定义(基尼系数)：分类问题中，假设有$K$个类，样本点属于第$k$类的概率为$p_k$，则概率分布的基尼指数定义为：</p><script type="math/tex; mode=display">\operatorname{Gini}(p) = \sum_{k=1}^Kp_k(1-p_k) = 1-\sum_{k=1}^Kp_k^2</script><p>对于二分类问题，若样本点属于第$1$个类的概率为$p$，则概率分布的基尼指数为：</p><script type="math/tex; mode=display">\operatorname{Gini}(p) = 2p(1-p)</script><p>对于给定样本集合$D$，其基尼指数为：</p><script type="math/tex; mode=display">\operatorname{Gini}(D) = 1-\sum_{k=1}^K\left(\frac{|C_k|}{|D|}\right)^2</script><p>基尼系数越大，样本集合的不确定性越大。</p><p>如果样本集合$D$根据特征$A$是否取某一可能值$a$被分割成$D_1$和$D_2$两部分，即</p><script type="math/tex; mode=display">D_1=\{(x,y)\in D|A(x)=a\},\quad D_2=D-D_1</script><p>则在特征$A$的条件下，集合$D$的基尼指数定义为</p><script type="math/tex; mode=display">\operatorname{Gini}(D,A) = \frac{|D_1|}{|D|}\operatorname{Gini}(D_1)+\frac{|D_2|}{|D|}\operatorname{Gini}(D_2)</script><p>算法(CART生成算法)</p><p>输入：训练数据集$D$，停止计算的条件</p><p>输出：CART决策树</p><ol><li>设结点的训练数据集为$D$，计算现有特征对该数据集的基尼系数。此时，对于每一个特征$A$，对其可能取的每个值$a$，根据样本点$A=a$的测试为”是”或”否”将$D$分割成$D_1$和$D_2$两个部分，计算$A=a$时的基尼指数。</li><li>在所有可能的特征$A$以及它们所有可能的切分点$a$中，选择基尼指数最小的特征及其对应的切分点作为最优特征与最优切分点。依最优特征与最优切分点，从现结点生成两个子结点，将训练数据集依特征分配到两个子结点中去。</li><li>对两个子结点递归地调用1，2，直到满足终止条件。</li><li>生成CART决策树。</li></ol><h4 id="CART剪枝"><a href="#CART剪枝" class="headerlink" title="CART剪枝"></a>CART剪枝</h4><p>CART剪枝算法由两步组成：首先从生成算法产生的决策树$T_0$低端开始不断剪枝，直到$T_0$的根结点，形成一个子树序列$\{T_0,T_1,\cdots,T_n\}$；然后通过交叉验证法在独立的验证数据集上对子树序列进行测试，从中选择最优子树。</p><p>可以利用递归的方法对数进行剪枝。将$\alpha$从小增大，$0=\alpha_0&lt;\alpha_1&lt;\cdots&lt;\alpha_n&lt;+\infty$，产生一系列的区间$[\alpha_i,\alpha_{i+1}),i=0,1,\cdots,n$的最优子树序列$\{T_0,T_1,\cdots,T_n\}$，序列中的子树是嵌套的。</p><p>具体地，从整体树$T_0$开始剪枝。对$T_0$的任意内部结点$t$，以$t$为单结点数的损失函数是：</p><script type="math/tex; mode=display">C_{\alpha}(t) = C(t) + \alpha</script><p>以$t$为根结点的子树$T_t$的损失函数是：</p><script type="math/tex; mode=display">C_{\alpha}(T_t) = C(T_t)+\alpha|T_t|</script><p>当$\alpha=0$及$\alpha$充分小时，有不等式</p><script type="math/tex; mode=display">C_{\alpha}(T_t)<C_{\alpha}(t)</script><p>当$\alpha$增大时，不等式反向。只要$\alpha=\frac{C(t)-C(T_t)}{|T_t|-1}$，$T_t$与$t$有相同的损失函数值，而$t$的结点少，因此$t$比$T_t$更可取，对$T_t$进行剪枝。</p><p>为此，对$T_0$中每一内部结点$t$，计算</p><script type="math/tex; mode=display">g(t) = \frac{C(t)-C(T_t)}{|T_t|-1}</script><p>它表示剪枝后整体损失函数减少的程度。在$T_0$中剪去$g(t)$最小的$T_t$，将得到的子树作为$T_1$，同时将最小的$g(t)$设为$\alpha_1$。==$T_1$为区间$[\alpha_1,\alpha_2)$的最优子树。==</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 决策树生成</span></span><br><span class="line"><span class="comment"># 定义节点类 二叉树</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Node</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, root=<span class="literal">True</span>, label=<span class="literal">None</span>, feature_name=<span class="literal">None</span>, feature=<span class="literal">None</span></span>):</span></span><br><span class="line">        self.root = root</span><br><span class="line">        self.label = label</span><br><span class="line">        self.feature_name = feature_name</span><br><span class="line">        self.feature = feature</span><br><span class="line">        self.tree = &#123;&#125;</span><br><span class="line">        self.result = &#123;</span><br><span class="line">            <span class="string">&#x27;label:&#x27;</span>: self.label,</span><br><span class="line">            <span class="string">&#x27;feature:&#x27;</span>: self.feature,</span><br><span class="line">            <span class="string">&#x27;tree:&#x27;</span>: self.tree</span><br><span class="line">        &#125;</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__repr__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="keyword">return</span> <span class="string">&#x27;&#123;&#125;&#x27;</span>.<span class="built_in">format</span>(self.result)</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">add_node</span>(<span class="params">self, val, node</span>):</span></span><br><span class="line">        self.tree[val] = node</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">predict</span>(<span class="params">self, features</span>):</span></span><br><span class="line">        <span class="keyword">if</span> self.root <span class="keyword">is</span> <span class="literal">True</span>:</span><br><span class="line">            <span class="keyword">return</span> self.label</span><br><span class="line">        <span class="keyword">return</span> self.tree[features[self.feature]].predict(features)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DTree</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, epsilon=<span class="number">0.1</span></span>):</span></span><br><span class="line">        self.epsilon = epsilon</span><br><span class="line">        self._tree = &#123;&#125;</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 熵</span></span><br><span class="line"><span class="meta">    @staticmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">calc_ent</span>(<span class="params">datasets</span>):</span></span><br><span class="line">        data_length = <span class="built_in">len</span>(datasets)</span><br><span class="line">        label_count = &#123;&#125;</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(data_length):</span><br><span class="line">            label = datasets[i][-<span class="number">1</span>]</span><br><span class="line">            <span class="keyword">if</span> label <span class="keyword">not</span> <span class="keyword">in</span> label_count:</span><br><span class="line">                label_count[label] = <span class="number">0</span></span><br><span class="line">            label_count[label] += <span class="number">1</span></span><br><span class="line">        ent = -<span class="built_in">sum</span>([(p / data_length) * log(p / data_length, <span class="number">2</span>)</span><br><span class="line">                    <span class="keyword">for</span> p <span class="keyword">in</span> label_count.values()])</span><br><span class="line">        <span class="keyword">return</span> ent</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 经验条件熵</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">cond_ent</span>(<span class="params">self, datasets, axis=<span class="number">0</span></span>):</span></span><br><span class="line">        data_length = <span class="built_in">len</span>(datasets)</span><br><span class="line">        feature_sets = &#123;&#125;</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(data_length):</span><br><span class="line">            feature = datasets[i][axis]</span><br><span class="line">            <span class="keyword">if</span> feature <span class="keyword">not</span> <span class="keyword">in</span> feature_sets:</span><br><span class="line">                feature_sets[feature] = []</span><br><span class="line">            feature_sets[feature].append(datasets[i])</span><br><span class="line">        cond_ent = <span class="built_in">sum</span>([(<span class="built_in">len</span>(p) / data_length) * self.calc_ent(p)</span><br><span class="line">                        <span class="keyword">for</span> p <span class="keyword">in</span> feature_sets.values()])</span><br><span class="line">        <span class="keyword">return</span> cond_ent</span><br><span class="line">    <span class="comment">#信息增益</span></span><br><span class="line"><span class="meta">    @staticmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">info_gain</span>(<span class="params">ent, cond_ent</span>):</span></span><br><span class="line">        <span class="keyword">return</span> ent - cond_ent</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">info_gain_train</span>(<span class="params">self, datasets</span>):</span></span><br><span class="line">        count = <span class="built_in">len</span>(datasets[<span class="number">0</span>]) - <span class="number">1</span></span><br><span class="line">        ent = self.calc_ent(datasets)</span><br><span class="line">        best_feature = []</span><br><span class="line">        <span class="keyword">for</span> c <span class="keyword">in</span> <span class="built_in">range</span>(count):</span><br><span class="line">            c_info_gain = self.info_gain(ent, self.cond_ent(datasets, axis=c))</span><br><span class="line">            best_feature.append((c, c_info_gain))</span><br><span class="line">        <span class="comment"># 比较大小</span></span><br><span class="line">        best_ = <span class="built_in">max</span>(best_feature, key=<span class="keyword">lambda</span> x: x[-<span class="number">1</span>])</span><br><span class="line">        <span class="keyword">return</span> best_</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">train</span>(<span class="params">self, train_data</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        input:数据集D(DataFrame格式)，特征集A，阈值eta</span></span><br><span class="line"><span class="string">        output:决策树T</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        _, y_train, features = train_data.iloc[:,:-<span class="number">1</span>], train_data.iloc[:,-<span class="number">1</span>], train_data.columns[:-<span class="number">1</span>]</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 若D中实例属于同一类Ck，则T为单结点树，并将类Ck作为结点的类标记，返回T</span></span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">len</span>(y_train.value_counts()) == <span class="number">1</span>:</span><br><span class="line">            <span class="keyword">return</span> Node(root=<span class="literal">True</span>, label=y_train.iloc[<span class="number">0</span>])</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 若A为空，则T为单结点树，将D中实例树最大的类Ck作为该结点的标记，返回T</span></span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">len</span>(features) == <span class="number">0</span>:</span><br><span class="line">            <span class="keyword">return</span> Node(</span><br><span class="line">                root=<span class="literal">True</span>,</span><br><span class="line">                label=y_train.value_counts().sort_values(ascending=<span class="literal">False</span>).index[<span class="number">0</span>]</span><br><span class="line">            )</span><br><span class="line">        <span class="comment"># 计算最大信息增益 Ag为信息增益最大的特征</span></span><br><span class="line">        max_feature, max_info_gain = self.info_gain_train(np.array(train_data))</span><br><span class="line">        max_feature_name = features[max_feature]</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Ag的信息增益小于阈值eta，则置T为单结点树，并将D中实例数最大的类Ck作为该结点的类标记，返回T</span></span><br><span class="line">        <span class="keyword">if</span> max_info_gain &lt; self.epsilon:</span><br><span class="line">            <span class="keyword">return</span> Node(</span><br><span class="line">                root=<span class="literal">True</span>,</span><br><span class="line">                label=y_train.value_counts().sort_values(</span><br><span class="line">                ascending=<span class="literal">False</span>).index[<span class="number">0</span>]</span><br><span class="line">            )</span><br><span class="line">        <span class="comment"># 构建Ag子集</span></span><br><span class="line">        node_tree = Node(</span><br><span class="line">            root=<span class="literal">False</span>, feature_name=max_feature_name, feature=max_feature)</span><br><span class="line">        </span><br><span class="line">        feature_list = train_data[max_feature_name].value_counts().index</span><br><span class="line">        <span class="keyword">for</span> f <span class="keyword">in</span> feature_list:</span><br><span class="line">            sub_train_df = train_data.loc[train_data[max_feature_name]==f].drop([max_feature_name],axis=<span class="number">1</span>)</span><br><span class="line">            </span><br><span class="line">            <span class="comment"># 递归生成树</span></span><br><span class="line">            sub_tree = self.train(sub_train_df)</span><br><span class="line">            node_tree.add_node(f, sub_tree)</span><br><span class="line">        <span class="keyword">return</span> node_tree</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">fit</span>(<span class="params">self, train_data</span>):</span></span><br><span class="line">        self._tree = self.train(train_data)</span><br><span class="line">        <span class="keyword">return</span> self._tree</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">predict</span>(<span class="params">self, X_test</span>):</span></span><br><span class="line">        <span class="keyword">return</span> self._tree.predict(X_test)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 回归树</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">LeastSqRTree</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, train_X, y, epsilon</span>):</span></span><br><span class="line">        <span class="comment"># 训练集特征值</span></span><br><span class="line">        self.x = train_X</span><br><span class="line">        <span class="comment"># 类别</span></span><br><span class="line">        self.y = y</span><br><span class="line">        <span class="comment"># 特征总数</span></span><br><span class="line">        self.feature_count = train_X.shape[<span class="number">1</span>]</span><br><span class="line">        <span class="comment"># 损失阈值</span></span><br><span class="line">        self.epsilon = epsilon</span><br><span class="line">        <span class="comment"># 回归树</span></span><br><span class="line">        self.tree = <span class="literal">None</span></span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_fit</span>(<span class="params">self, x, y, feature_count, epsilon</span>):</span></span><br><span class="line">        <span class="comment"># 选择最优切分变量j和切分点s</span></span><br><span class="line">        (j, s, minval, c1, c2) = self._divide(x, y, feature_count)</span><br><span class="line">        <span class="comment"># 初始化树</span></span><br><span class="line">        tree = &#123;<span class="string">&#x27;feature&#x27;</span>:j, <span class="string">&quot;value&quot;</span>: x[s, j], <span class="string">&#x27;left&#x27;</span>:<span class="literal">None</span>, <span class="string">&#x27;right&#x27;</span>:<span class="literal">None</span>&#125;</span><br><span class="line">        <span class="keyword">if</span> minval &lt; self.epsilon <span class="keyword">or</span> <span class="built_in">len</span>(y[np.where(x[:,j]&lt;=x[s,j])]) &lt;=<span class="number">1</span>:</span><br><span class="line">            tree[<span class="string">&quot;left&quot;</span>] =c1</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            tree[<span class="string">&quot;left&quot;</span>] = self._fit(x[np.where(x[:,j]&lt;=x[s,j])],</span><br><span class="line">                                     y[np.where(x[:,j]&lt;=x[s,j])],</span><br><span class="line">                                    self.feature_count, self.epsilon)</span><br><span class="line">        <span class="keyword">if</span> minval &lt; self.epsilon <span class="keyword">or</span> <span class="built_in">len</span>(y[np.where(x[:, j] &gt; s)]) &lt;= <span class="number">1</span>:</span><br><span class="line">            tree[<span class="string">&quot;right&quot;</span>] = c2</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            tree[<span class="string">&quot;right&quot;</span>] = self._fit(x[np.where(x[:, j] &gt; x[s, j])],</span><br><span class="line">                                      y[np.where(x[:, j] &gt; x[s, j])],</span><br><span class="line">                                      self.feature_count, self.epsilon)</span><br><span class="line">        <span class="keyword">return</span> tree</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">fit</span>(<span class="params">self</span>):</span></span><br><span class="line">        self.tree = self._fit(self.x, self.y, self.feature_count, self.epsilon)</span><br><span class="line">    </span><br><span class="line"><span class="meta">    @staticmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_divide</span>(<span class="params">x, y, feature_count</span>):</span></span><br><span class="line">        <span class="comment"># 初始化损失误差</span></span><br><span class="line">        cost = np.zeros((feature_count, <span class="built_in">len</span>(x)))</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(feature_count):</span><br><span class="line">            <span class="keyword">for</span> k <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(x)):</span><br><span class="line">                value = x[k,i]</span><br><span class="line">                y1 = y[np.where(x[:,i]&lt;=value)]</span><br><span class="line">                c1 = np.mean(y1)</span><br><span class="line">                y2 = y[np.where(x[:,i]&gt;value)]</span><br><span class="line">                c2 = np.mean(y2)</span><br><span class="line">                y1[:] = y1[:] - c1</span><br><span class="line">                y2[:] = y2[:] - c2</span><br><span class="line">                cost[i,k] = np.<span class="built_in">sum</span>(y1 * y1) + np.<span class="built_in">sum</span>(y2 * y2)</span><br><span class="line">        <span class="comment"># 选取最优损失误差点</span></span><br><span class="line">        cost_index = np.where(cost==np.<span class="built_in">min</span>(cost))</span><br><span class="line">        <span class="comment"># 选取第几个特征值</span></span><br><span class="line">        j = cost_index[<span class="number">0</span>][<span class="number">0</span>]</span><br><span class="line">        <span class="comment"># 选取特征值的切分点</span></span><br><span class="line">        s = cost_index[<span class="number">1</span>][<span class="number">0</span>]</span><br><span class="line">        <span class="comment"># 求两个区域的均值c1,c2</span></span><br><span class="line">        c1 = np.mean(y[np.where(x[:, j] &lt;= x[s, j])])</span><br><span class="line">        c2 = np.mean(y[np.where(x[:, j] &gt; x[s, j])])</span><br><span class="line">        <span class="keyword">return</span> j, s, cost[cost_index], c1, c2</span><br></pre></td></tr></table></figure>]]></content>
    
    
    <summary type="html">&lt;p&gt;以下主要参考李航老师《统计学习方法》。已更新决策树和回归树代码，CART算法代码待补全。&lt;/p&gt;</summary>
    
    
    
    <category term="算法" scheme="https://www.hfcouc.work/categories/%E7%AE%97%E6%B3%95/"/>
    
    
    <category term="机器学习" scheme="https://www.hfcouc.work/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>极限</title>
    <link href="https://www.hfcouc.work/2021/12/19/%E6%9E%81%E9%99%90/"/>
    <id>https://www.hfcouc.work/2021/12/19/%E6%9E%81%E9%99%90/</id>
    <published>2021-12-19T14:22:41.000Z</published>
    <updated>2021-12-22T07:21:39.867Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="\assets\js\Meting.min.js"></script><p>梅加强老师《数学分析》</p><span id="more"></span><h3 id="数列极限"><a href="#数列极限" class="headerlink" title="数列极限"></a>数列极限</h3><h4 id="数列极限的定义"><a href="#数列极限的定义" class="headerlink" title="数列极限的定义"></a>数列极限的定义</h4><p>定义(数列极限)。设$\{a_n\}$为数列，$A\in \mathbb{R}$.如果任给$\epsilon&gt;0$,都存在正整数$N(\epsilon)$，使得当$n&gt;N$时，有</p><script type="math/tex; mode=display">    |a_n-A|<\epsilon</script><p>则称$\{a_n\}$以$A$为极限，或称$\{a_n\}$收敛于$A$，记为</p><script type="math/tex; mode=display">    \lim_{n\rightarrow\infty}a_n=A\text{ 或 }a_n\rightarrow A(n\rightarrow \infty)</script><p>当然我们也可以用$\epsilon-N$语言给出数列$\{a_n\}$不以$A$为极限的定义：如果存在$\epsilon_0&gt;0$，使得任给正数$N$，均存在$n_0&gt;N$满足不等式$|a_{n_0}-A|\ge\epsilon_0$，则$\{a_n\}$不以$A$为极限。</p><p>命题：如果数列$\{a_n\}$有极限，则其极限是唯一的。</p><p>定理(夹逼定理).设$\{a_n\},\{b_n\},\{c_n\}$均为数列，且</p><script type="math/tex; mode=display">    a_n\le b_n\le c_n,\forall n\ge N_0</script><p>其中$N_0$为一整数，如果</p><script type="math/tex; mode=display">    \lim_{n\rightarrow\infty}a_n=A=\lim_{n\rightarrow\infty}c_n</script><p>则$\lim_{n\rightarrow\infty}b_n=A$。</p><p>例题：<br>考虑无限循环小数$A=0.99999\cdots$，问：$A$是否小于$1$？<br>解：我们可以将$A$视为一列有限小数$\{a_n\}$的极限，其中$a_n = 0.99\cdots9(n\text{个}9)$。由于：</p><script type="math/tex; mode=display">    |a_n-1| = 10^{-n}</script><p>根据夹逼定理</p><script type="math/tex; mode=display">a_n\le A\le 1</script><p>而</p><script type="math/tex; mode=display">\lim_{n\rightarrow\infty}a_n=1</script><p>所以</p><script type="math/tex; mode=display">A=1</script><p>例：<br>设$0&lt;\alpha&lt;1$，证明$\lim_{n\rightarrow\infty}[(n+1)^{\alpha}-n^{\alpha}]=0$<br>证明：当$n\ge1$时，有</p><script type="math/tex; mode=display">    \begin{aligned}        0<(n+1)^{\alpha}-n^{\alpha} &= n^{\alpha}[(1+\frac{1}{n})^{\alpha}-1]\\        &\le n^{\alpha}[(1+\frac{1}{n})-1]=\frac{1}{n^{1-\alpha}}    \end{aligned}</script><p>根据夹逼定理我们有$\lim_{n\rightarrow\infty}[(n+1)^{\alpha}-n^{\alpha}]=0$。</p><p>例：设$\alpha&gt;0,a&gt;1$，则$\lim_{n\rightarrow\infty}\frac{n^{\alpha}}{a^n}=0$</p><p>思路：由于分子分母同时含有$n$，因此我们很难进行判断，我们想要做的是根据放缩消去一个$n$，而留下的$n$很容易处理，因此我们对$a^n$进行放缩处理。<br>我们记$a^{\frac{1}{\alpha}}=1+\beta,\beta&gt;0$。由于$n&gt;1$，有</p><script type="math/tex; mode=display">(1+\beta)^n = 1 + n\beta+\frac{1}{2}n(n-1)\beta^2+\cdots+\beta^n>\frac{1}{2}n(n-1)\beta^2</script><p>故</p><script type="math/tex; mode=display">0<\frac{n^{\alpha}}{a^n} = \left[\frac{n}{(1+\beta)^n}\right]^{\alpha} < \left[\frac{2}{(n-1)\beta^2}\right]^\alpha</script><p>由夹逼原理可知$\lim_{n\rightarrow\infty}\frac{n^{\alpha}}{a^n}=0$。</p><p>例：证明$\lim_{n\rightarrow\infty}\frac{1}{\sqrt[n]{n!}}=0$</p><p>注意到当$1\le k\le n$时$(k-1)(n-k)\ge0$，从而$k(n-k+1)\ge n$，我们就有：</p><script type="math/tex; mode=display">    (n!)^2 = (1\cdot n)(2(n-1))\cdots(k(n-k+1))\cdots(n\cdot1)\ge n^n,\forall n\ge1</script><p>因此</p><script type="math/tex; mode=display">    0<\frac{1}{\sqrt[n]{n!}}\le\frac{1}{\sqrt{n}},\forall n\ge1</script><p>由夹逼原理可得：$\lim_{n\rightarrow\infty}\frac{1}{\sqrt[n]{n!}}=0$</p><p>例：证明$\lim_{n\rightarrow\infty}\sqrt[n]{n}=1$<br>证明：记$\sqrt[n]{n}=1+\alpha_n$，当$n&gt;1$时，</p><script type="math/tex; mode=display">    n = (1+\alpha_n)^n=1+n\alpha_n+\frac{1}{2}n(n-1)\alpha_n^2+\cdots+\alpha_n^n > \frac{1}{2}n(n-1)\alpha_n^2</script><p>从而有估计</p><script type="math/tex; mode=display">0<\alpha_n<\sqrt{\frac{2}{n-1}}</script><p>因此，当$n&gt;1$时，有</p><script type="math/tex; mode=display">1<\sqrt[n]{n} = 1+\alpha_n<1+\sqrt{\frac{2}{n-1}}</script><p>由夹逼原理即得：$\lim_{n\rightarrow\infty}\sqrt[n]{n}=1$。</p><p>下面两个为比较重要的例题：</p><p>设$\lim_{n\rightarrow\infty}a_n=A$，证明$\lim_{n\rightarrow\infty}\frac{a_1+a_2+\cdots+a_n}{n}=A$。<br>证明：任给$\epsilon&gt;0$，因为$\lim_{n\rightarrow\infty}a_n=A$，故存在$N_0$，使得当$n&gt;N_0$时，有</p><script type="math/tex; mode=display">    |a_n-A|<\frac{\epsilon}{2}</script><p>令</p><script type="math/tex; mode=display">N>\max\{N_0,2\epsilon^{-1}|a_1+\cdots+a_{N_0}-N_0A|\}</script><p>则当$n&gt;N$时，有</p><script type="math/tex; mode=display">\begin{aligned}&\left|\frac{a_{1}+\cdots+a_{n}}{n}-A\right|=\left|\frac{a_{1}+\cdots+a_{N_{0}}-N_{0} A}{n}+\frac{\left(a_{N_{0}+1}-A\right)+\cdots+\left(a_{n}-A\right)}{n}\right| \\&\leqslant \frac{\left|a_{1}+\cdots+a_{N_{0}}-N_{0} A\right|}{n}+\frac{\left|a_{N_{0}+1}-A\right|+\cdots+\left|a_{n}-A\right|}{n} \\&\leqslant \frac{\left|a_{1}+\cdots+a_{N_{0}}-N_{0} A\right|}{n}+\frac{n-N_{0}}{n} \frac{\varepsilon}{2} \\&<\frac{\varepsilon}{2}+\frac{\varepsilon}{2}=\varepsilon\end{aligned}</script><p>这说明：$\lim_{n\rightarrow\infty}\frac{a_1+a_2+\cdots+a_n}{n}=A$。</p><p>我们再来证明一个跟这个差不多的例题：</p><p>$\lim_{n\rightarrow\infty}a_n=A$，则$\sqrt[n]{a_1\cdots a_n}=A$。</p><p>证明：</p><p>我们可以取对数利用上面那个题的结论即可证明。</p><p>例：<strong>任何实数都是某个有理数列的极限</strong>。<br>证明：设$A$为实数。如果$A$为有理数，则令$a_n=A(n\ge1)$即可。如果$A$为无理数，令</p><script type="math/tex; mode=display">a_n = \frac{[nA]}{n},\forall n\ge1</script><p>其中$[x]$表示不超过$x$的最大整数，因此$a_n$都是有理数。因为$A$不是有理数，故：</p><script type="math/tex; mode=display">nA-1<[nA]<nA,\forall n\ge1</script><p>即</p><script type="math/tex; mode=display">A-\frac{1}{n}<a_n=\frac{[nA]}{n}<A,\forall n\ge1</script><p>由夹逼定律可知$\lim_{n\rightarrow\infty} a_n=A$</p><h4 id="数列极限的基本性质"><a href="#数列极限的基本性质" class="headerlink" title="数列极限的基本性质"></a>数列极限的基本性质</h4><p>命题(有界性)：设数列$\{a_n\}$收敛，则$\{a_n\}$有界<br>由此命题立知，无界数列必定发散。如果$\{a_n\}$发散到$+\infty$，则称$\{a_n\}$发散到$\infty$，记为</p><script type="math/tex; mode=display">    \lim_{n\rightarrow\infty}a_n=\infty,\text{ 或}a_n\rightarrow\infty(n\rightarrow\infty)</script><p>命题(绝对值性质)。设数列$\{a_n\}$收敛到$A$，则$\{|a_n|\}$收敛到$|A|$。</p><p>推论：数列$\{a_n\}$收敛到$0$当且仅当$|a_n|$收敛到$0$；数列$\{a_n\}$收敛到$A$当且仅当$|a_n-A|$收敛到$0$。</p><p>命题(保序性质)。设数列$\{a_n\}$收敛到$A$，$\{b_n\}$收敛到$B$，则有</p><ol><li>如果存在$N_0$，当$n&gt;N_0$时$a_n\ge b_n$，则$A\ge B$。</li><li>反之，如果$A&gt;B$，则存在$N$，使得当$n&gt;N$时$a_n&gt;b_n$。</li></ol><p>推论：设$\lim_{n\rightarrow\infty}a_n=A$，如果$A\neq0$，则存在$N$，使得当$n&gt;N$时，有</p><script type="math/tex; mode=display">    \frac{1}{2}|A|<|a_n|<\frac{3}{2}|A|</script><p>例：设$q&gt;1$，则$\lim_{n\rightarrow\infty}\frac{\log_qn}{n}=0$<br>解：任给$\epsilon&gt;0$，利用之前的结论，有</p><script type="math/tex; mode=display">\lim_{n\rightarrow\infty}\sqrt[n]{n}=1<q^{\epsilon}</script><p>由极限的保序性质，存在$N$，当$n&gt;N$时，有</p><script type="math/tex; mode=display">    \sqrt[n]{n}<q^{\epsilon}</script><p>即</p><script type="math/tex; mode=display">    \frac{\log_qn}{n}<\epsilon,\forall n>N</script><p>这说明：$\lim_{n\rightarrow\infty}a_n=A$.</p><p>命题(四则运算)。设数列$\{a_n\}$收敛到$A$，$\{b_n\}$收敛到$B$，则有：</p><ol><li>$\{\alpha a_n+\beta b_n\}$收敛到$\alpha A+\beta B$，其中$\alpha,\beta$为常数</li><li>$\{a_nb_n\}$收敛到$AB$</li><li>当$B\neq0$时，$\{a_n/b_n\}$收敛到$A/B$</li></ol><p>下面我们引入数列的子列的概念，并研究数列的极限和其子列的极限之间的关系，设</p><script type="math/tex; mode=display">    a_1,a_2,\cdots,a_n,\cdots</script><p>是数列，如果</p><script type="math/tex; mode=display">    1\le n_1<n_2<\cdots<n_k<\cdots</script><p>是一列严格递增的正整数，则称数列</p><script type="math/tex; mode=display">    a_{n_1},a_{n_2},\cdots,a_{n_k},\cdots</script><p>为原数列$\{a_n\}$的子列，记为$\{a_{n_k}\}$。两个特殊的子列$\{a_{2k}\}$和$\{a_{2k-1}\}$分别为偶子列和奇子列。</p><p>命题</p><ol><li>设$\{a_n\}$收敛到$A$，则它的任何子列$\{a_{n_k}\}$也收敛到$A$</li><li>如果$\{a_n\}$的偶子列与奇子列收敛到$A$，则$\{a_n\}$也收敛到$A$</li></ol><h4 id="例题"><a href="#例题" class="headerlink" title="例题"></a>例题</h4><p>设$\lim_{n\rightarrow \infty}(a_{n+1}-a_n)=A$，则$\lim_{n\rightarrow\infty}\frac{a_n}{n}=A$</p><script type="math/tex; mode=display">\frac{a_n}{n} = \frac{a_1+(a_2-a_1)+(a_3-a_2)+\cdots+(a_n-a_{n-1})}{n}</script><p>因为$\lim_{n\rightarrow \infty}(a_{n+1}-a_n)=A$，利用已知例题的结论即可证明得到。</p><p>设$\lim_{n\rightarrow\infty}a_n=A$，证明：</p><script type="math/tex; mode=display">\lim_{n\rightarrow\infty}\frac{1}{n^2}(a_1+2a_2+\cdots+na_n)=\frac{1}{2}A</script><p>对上式变换得到</p><script type="math/tex; mode=display">\frac{1}{n^2}(a_1+2a_2+\cdots+na_n) = \sum_{i=1}^n\frac{i(a_i-A)}{n^2}+\frac{n(n+1)}{2n^2}A</script><p>这样就好证明了：</p><p>因为：</p><script type="math/tex; mode=display">\lim_{n\rightarrow\infty}\frac{i}{n}=0\quad\lim_{n\rightarrow\infty}(a_i-A)=0\Rightarrow\lim_{n\rightarrow\infty}\frac{i(a_i-A)}{n}=0</script><p>所以：</p><script type="math/tex; mode=display">\lim_{n\rightarrow0}\sum_{i=1}^n\frac{i(a_i-A)}{n^2}=0</script><p>有因为：</p><script type="math/tex; mode=display">\lim_{n\rightarrow0}\frac{n(n+1)}{2n^2}A=\frac{1}{2}A</script><p>得证。</p><h4 id="单调数列的极限"><a href="#单调数列的极限" class="headerlink" title="单调数列的极限"></a>单调数列的极限</h4><p>确定原理：非空数集如果有上界则必有上确界，如果有下界则必有下确界。</p><p>设$\{a_n\}$为实数列，如果</p><script type="math/tex; mode=display">a_1\le a_2\le\cdots\le a_n\le \cdots</script><p>则称$\{a_n\}$是单调递增序列，当上式中的$\le$号换成$&lt;$时称$\{a_n\}$是严格单调递增的。</p><p>定理(单调数列的极限)：设$\{a_n\}$为单调数列</p><ol><li>如果$\{a_n\}$为单调递增的数列，则$\lim_{n\rightarrow\infty}a_n=\sup\{a_k|k\ge1\}$</li><li>如果$\{a_n\}$为单调递减序列，则$\lim_{n\rightarrow\infty}a_n=\inf\{a_k|k\ge1\}$</li></ol><p>证明：记$M=\sup\{a_k|k\ge1\}$，先考虑$M$有限的情形。任给$\epsilon&gt;0$，存在$a_N$，使得</p><script type="math/tex; mode=display">M-\epsilon<a_N\le M</script><p>因为$\{a_n\}$是单调递增序列，故当$n&gt;N$时</p><script type="math/tex; mode=display">M-\epsilon<a_N\le a_n\le M<M+\epsilon</script><p>由数列的极限定义可知：</p><script type="math/tex; mode=display">\lim_{n\rightarrow\infty}a_n=M=\sup\{a_k|k\ge1\}</script><p>如果$M=+\infty$，则任给$A&gt;0$，由上确界的定义，存在$a_N$，使得$a_N&gt;A$。由于$\{a_n\}$是单调递增序列，故当$n&gt;N$时有$a_n\ge a_N&gt;A$，从而</p><script type="math/tex; mode=display">\lim_{n\rightarrow\infty}a_n=+\infty=\sup\{a_k|k\ge1\}</script><p>由上面的推理我们也可以得到：单调有界的数列必有极限。</p><p>任何收敛数列都有单调的收敛子列。</p><p>例：设$a_1&gt;0,a_{n+1}=\frac{1}{2}(a_n+\frac{1}{a_n}),n\ge1$。研究数列$\{a_n\}$的极限。</p><p>对于$a_n&gt;0,\forall n\ge1$。我们有：</p><script type="math/tex; mode=display">a_{n+1} = \frac{1}{2}(a_n+\frac{1}{a_n})\ge\frac{1}{2}\cdot2(a_n\cdot\frac{1}{a_n})=1</script><p>故我们有：</p><script type="math/tex; mode=display">a_{n+1} = \frac{1}{2}(a_n+\frac{1}{a_n})\le\frac{1}{2}(a_n+a_n)=a_n</script><p>所以单调递减，而又有下界。因此收敛，记其极限为$A$，则$A\ge1$。另一方面：</p><script type="math/tex; mode=display">A = \lim_{n\rightarrow\infty}a_{n+1}=\lim_{n\rightarrow\infty}\frac{1}{2}(a_n+\frac{1}{a_n}) = \frac{1}{2}(A+\frac{1}{A})</script><p>故$A=1$.</p><p>我们现在讨论几个重要的极限：</p><script type="math/tex; mode=display">a_n = (1+\frac{1}{n})^n, b_n = (1+\frac{1}{n})^{n+1},n\ge1</script><blockquote><p>详细证明过程见49页</p></blockquote><p>我们可以证明$\{a_n\}$单调递增，$\{b_n\}$单调递减。两者均收敛于$e$。</p><script type="math/tex; mode=display">\lim_{n\rightarrow\infty}b_n = \lim_{n\rightarrow\infty}a_n(1+\frac{1}{n})=\lim_{n\rightarrow\infty}a_n=e</script><p>我们可以得到下述重要的不等式：</p><script type="math/tex; mode=display">\left(1+\frac{1}{n}\right)^{n}<\left(1+\frac{1}{n+1}\right)^{n+1}<e<\left(1+\frac{1}{n+1}\right)^{n+2}<\left(1+\frac{1}{n}\right)^{n+1}, \quad \forall n \geqslant 1</script><p>我们令$c_n=1+\frac{1}{2}+\cdots+\frac{1}{n}-\ln n$，可以证明$\{c_n\}$收敛，其极限为$\gamma$，称为Euler常数，计算表明</p><script type="math/tex; mode=display">\gamma = 0.5772156649\cdots</script><p>下面，我们利用单调数列来研究一般的有界数列。设$\{a_n\}$为有界数列，我们要研究它的收敛性。我们不知道$a_n$是否逐渐趋于某个数，一个好的想法就是取考虑$n$很大时$\{a_n\}$中最大的最小的项，看看它们是否相近。当然，最大和最小项不一定存在，但是我们可以利用上确界和下确界来分别代替它们。为此，令：</p><script type="math/tex; mode=display">\underline{a}_{n}=\inf \left\{a_{k} \mid k \geqslant n\right\}, \quad \bar{a}_{n}=\sup \left\{a_{k} \mid k \geqslant n\right\}</script><p>$\{\underline{a}_n\}$和$\{\bar{a}_n\}$分别是单调递增和单调递减的序列，且：</p><script type="math/tex; mode=display">\underline{a}_n\le a_n\le \bar{a}_n</script><p>单调数列$\{\underline{a}_n\}$和$\{\bar{a}_n\}$的极限分别称为$\{a_n\}$的<strong>下极限</strong>和<strong>上极限</strong>，记为：</p><script type="math/tex; mode=display">\varliminf_{n \rightarrow \infty} a_{n}=\lim _{n \rightarrow \infty} \underline{a}_{n}, \quad \varlimsup_{n \rightarrow \infty} a_{n}=\lim _{n \rightarrow \infty} \bar{a}_{n}</script><p>定理：设$\{a_n\}$为有界数列，则下列命题等价：</p><ol><li>$\{a_n\}$收敛</li><li>$\{a_n\}$的上极限和下极限相等</li><li>$\lim_{n\rightarrow\infty}(\bar{a}_n-\underline{a}_n)=0$</li></ol><p>命题：设$\{a_n\},\{b_n\}$为有界数列</p><ol><li>如果存在$N_0$，当$n&gt;N_0$时$a_n\ge b_n$，则<script type="math/tex">\varliminf_{n \rightarrow \infty} a_{n} \geqslant \varliminf_{n \rightarrow \infty} b_{n}, \quad \varlimsup_{n \rightarrow \infty} a_{n} \geqslant \varlimsup_{n \rightarrow \infty} b_{n}</script></li><li><script type="math/tex; mode=display">\overline{\lim}_{n\rightarrow\infty}(a_n+b_n)\le\bar{\lim}_{n\rightarrow\infty}a_n+\bar{\lim}_{n\rightarrow\infty}b_n</script></li></ol><p>设$a_n&gt;0,a_n\rightarrow A(n\rightarrow \infty)$。记$b_n = \sqrt[n]{a_1a_2\cdots a_n}$，则</p><script type="math/tex; mode=display">\lim_{n\rightarrow\infty}b_n=A</script><p>由已知条件，任取$\epsilon&gt;0$，则存在$N$，当$n&gt;N$时，</p><script type="math/tex; mode=display">0<a_n<A+\epsilon</script><p>于是当$n&gt;N$时，有</p><script type="math/tex; mode=display">b_n\le\sqrt[n]{a_1a_2\cdots a_N}(A+\epsilon)^{\frac{n-N}{n}} = \sqrt[n]{a_1a_2\cdots a_N(A+\epsilon)^{-N}}(A+\epsilon)</script><p>令$n\rightarrow\infty$，得</p><script type="math/tex; mode=display">\overline{\lim}_{n\rightarrow\infty}\le A+\epsilon</script><p>同理可证</p><script type="math/tex; mode=display">\underline{\lim}_{n\rightarrow\infty}b_n\ge A-\epsilon</script><p>因为$\epsilon$是任取的，从而必有</p><script type="math/tex; mode=display">\varlimsup_{n \rightarrow \infty} b_{n}=A=\varliminf_{n \rightarrow \infty} b_{n}</script><p>这说明$\{b_n\}$收敛到$A$。</p><h4 id="Cauchy准则"><a href="#Cauchy准则" class="headerlink" title="Cauchy准则"></a>Cauchy准则</h4><p>定义：设$\{a_n\}$为数列，如果任给$\epsilon&gt;0$，均存在$N=N(\epsilon)$，当$m,n&gt;N$时，有</p><script type="math/tex; mode=display">|a_m-a_n|<\epsilon</script><p>则$\{a_n\}$为Cauchy数列或基本列。</p><p>例：对于$n\ge1$，定义</p><script type="math/tex; mode=display">a_n =1+\frac{1}{2}+\cdots+\frac{1}{n}</script><p>则$\{a_n\}$不是Cauchy列。</p><p>证明：对于$n\ge1$，我们有：</p><script type="math/tex; mode=display">\begin{aligned}a_{2n}-a_n &= \frac{1}{n+1}+\cdots+\frac{1}{2n}\\&\ge\frac{1}{2n}+\cdots+\frac{1}{2n}=n\frac{1}{2n}=\frac{1}{2} \end{aligned}</script><p>由此定义即知$\{a_n\}$不是Cauchy数列。</p><p>命题：Cauchy数列必是有界数列。</p><p>证明：按定义，取$\epsilon=1$，则存在$N$，当$m,n&gt;N$时，有</p><script type="math/tex; mode=display">|a_m-a_n|<1</script><p>令$M=\max\{|a_k|+1|1\le k\le N+1\}$，则当$n\le N$时显然$|a_n|\le M$；而当$n&gt;N$时，有：</p><script type="math/tex; mode=display">|a_n|\le |a_n-a_{N+1}|+|a_{N+1}|<1+|a_{N+1}|\le M</script><p>说明$\{a_n\}$是有界数列。</p><p>定理(Cauchy准则)：$\{a_n\}$为Cauchy数列当且仅当它是收敛的。</p><p>证明：</p><p>充分性：设$\{a_n\}$收敛到$A$。则任给$\epsilon&gt;0$，存在$N$，当$n&gt;N$时，有：</p><script type="math/tex; mode=display">|a_n-A|<\frac{1}{2}\epsilon</script><p>因此，当$m,n&gt;N$时，有</p><script type="math/tex; mode=display">|a_m-a_n|\le |a_m-A|+|A-a_n|<\frac{1}{2}\epsilon+\frac{1}{2}\epsilon = \epsilon</script><p>这说明$\{a_n\}$为Cauchy数列。</p><p>必要性：设$\{a_n\}$为Cauchy数列，由上面的命题可知$\{a_n\}$是有界数列，记$A$为其上极限。我们来说明$\{a_n\}$收敛到$A$。</p><p>事实上，由于$\{a_n\}$为Cauchy数列，任给$\epsilon&gt;0$，存在$N$，当$m,n&gt;N$时，有</p><script type="math/tex; mode=display">|a_m-a_n|<\frac{1}{2}\epsilon</script><p>即</p><script type="math/tex; mode=display">-\frac{1}{2}\epsilon<a_m-a_n<\frac{1}{2}\epsilon,\forall m,n>N</script><p>在上式中令$m\rightarrow\infty$，利用上极限的保序性，得</p><script type="math/tex; mode=display">-\frac{1}{2}\epsilon\le \overline{\lim_{m\rightarrow\infty}}a_m-a_n\le\frac{1}{2}\epsilon,\forall n>N</script><p>即</p><script type="math/tex; mode=display">|A-a_n|\le\frac{1}{2}\epsilon<\epsilon,\forall n>N</script><p>这说明，$\{a_n\}$收敛到$A$。</p><p>例：</p><p>设数列$\{a_n\}$满足，存在正数$M$，对于一个$n$都有：</p><script type="math/tex; mode=display">A_n = |a_2-a_1|+|a_3-a_2|+\cdots+|a_{n}-a_{n-1}|\le M</script><p>证明数列$\{a_n\}$是Cauchy数列，从而是收敛的。</p><p>证明：</p><script type="math/tex; mode=display">A_{n+1}-A_n=|a_{n+1}-a_{n}|\ge0</script><p>所以数列$A_n$为递增数列，又因为$0\le A_n \le M$，故$A_n$单调有界，所以数列$A_n$一定有极限(收敛)。因为$A_n$是收敛的，那么它一定是cauthy数列，根据cauthy收敛准则：$\forall \epsilon&gt;0,\exists N$当$m,n&gt;N$时，有：</p><script type="math/tex; mode=display">|A_{m}-A_{n}|<\epsilon</script><p>因为</p><script type="math/tex; mode=display">\epsilon>|A_m-A_n| = |a_{n+1}-a_n|+\cdots + |a_m-a_{m-1}|\ge |a_{m}-a_{n}|</script><p>故$a_n$为Cauchy序列，故$a_n$收敛。</p><h4 id="Stolz公式"><a href="#Stolz公式" class="headerlink" title="Stolz公式"></a>Stolz公式</h4><p>引理：设$b_k&gt;0(1\le k\le n)$，且</p><script type="math/tex; mode=display">m\le \frac{a_k}{b_k}\le M,\forall 1\le k\le n</script><p>则有</p><script type="math/tex; mode=display">m\le \frac{a_1+a_2+\cdots+a_n}{b_1+b_2+\cdots+b_n}\le M</script><p>定理(Stolz公式之一)：设$\{x_n\},\{y_n\}$为数列，且$\{y_n\}$严格单调地趋于$+\infty$，如果</p><script type="math/tex; mode=display">\lim_{n\rightarrow\infty}\frac{x_n-x_{n-1}}{y_n-y_{n-1}}=A</script><p>则</p><script type="math/tex; mode=display">\lim_{n\rightarrow\infty}\frac{x_n}{y_n}=A</script><blockquote><p>证明见课本60页</p></blockquote><p>定理(Stolz公式之二)：设数列$\{y_n\}$严格单调递减趋于$0$，数列$\{x_n\}$也收敛到$0$，如果：</p><script type="math/tex; mode=display">\lim_{n\rightarrow\infty}\frac{x_n-x_{n-1}}{y_n-y_{n-1}}=A</script><p>则</p><script type="math/tex; mode=display">\lim_{n\rightarrow\infty}\frac{x_n}{y_n}=A</script>]]></content>
    
    
    <summary type="html">&lt;p&gt;梅加强老师《数学分析》&lt;/p&gt;</summary>
    
    
    
    <category term="数学" scheme="https://www.hfcouc.work/categories/%E6%95%B0%E5%AD%A6/"/>
    
    
    <category term="数学分析" scheme="https://www.hfcouc.work/tags/%E6%95%B0%E5%AD%A6%E5%88%86%E6%9E%90/"/>
    
  </entry>
  
  <entry>
    <title>线性模型</title>
    <link href="https://www.hfcouc.work/2021/12/18/%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/"/>
    <id>https://www.hfcouc.work/2021/12/18/%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/</id>
    <published>2021-12-18T15:01:52.000Z</published>
    <updated>2021-12-22T07:25:50.765Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="\assets\js\Meting.min.js"></script><p>最小二乘法永不为奴！！！</p><span id="more"></span><h2 id="线性模型"><a href="#线性模型" class="headerlink" title="线性模型"></a>线性模型</h2><h3 id="基本形式"><a href="#基本形式" class="headerlink" title="基本形式"></a>基本形式</h3><script type="math/tex; mode=display">f(x) = w^Tx+b</script><h3 id="线性回归"><a href="#线性回归" class="headerlink" title="线性回归"></a>线性回归</h3><p>对于一元变量：<br>线性回归试图学得</p><script type="math/tex; mode=display">f\left(x_{i}\right)=w x_{i}+b, \text { 使得 } f\left(x_{i}\right) \simeq y_{i}</script><p>对于多元变量：<br>我们令$\hat{w}=(w;b)$，相应地，把数据集$D$表示为一个$m\times(d+1)$大小的矩阵$\mathrm{X}$，即</p><script type="math/tex; mode=display">\mathbf{X}=\left(\begin{array}{ccccc}x_{11} & x_{12} & \ldots & x_{1 d} & 1 \\x_{21} & x_{22} & \ldots & x_{2 d} & 1 \\\vdots & \vdots & \ddots & \vdots & \vdots \\x_{m 1} & x_{m 2} & \ldots & x_{m d} & 1\end{array}\right)=\left(\begin{array}{cc}x_{1}^{T} & 1 \\x_{2}^{T} & 1 \\\vdots & \vdots \\x_{m}^{T} & 1\end{array}\right)</script><p>我们有</p><script type="math/tex; mode=display">\hat{w}^\star = \arg_{\hat{w}}\min(y-\mathrm{X}\hat{w})^T(y-\mathrm{X}\hat{w})</script><p>令$E_{\hat{w}}=(y-\mathrm{X}\hat{w})^T(y-\mathrm{X}\hat{w})$，对$\hat{w}$求导得到：</p><script type="math/tex; mode=display">\frac{\partial E_{\hat{w}}}{\partial\hat{w}} = 2\mathrm{X}^T(\mathrm{X}\hat{w}-y)</script><p>当$\mathrm{X}^T\mathrm{X}$为满秩矩阵或正定矩阵时，我们有</p><script type="math/tex; mode=display">\hat{w}^\star = (\mathrm{X}^T\mathrm{X})^{-1}\mathrm{X}^Ty</script><p>更一般地，考虑单调可微函数$g(\cdot)$，令</p><script type="math/tex; mode=display">y = g^{-1}(w^Tx+b)</script><p>这样的模型称为”广义线性模型”。</p><h3 id="对数几率回归"><a href="#对数几率回归" class="headerlink" title="对数几率回归"></a>对数几率回归</h3><script type="math/tex; mode=display">y = \frac{1}{1+e^{-(w^Tx+b)}}</script><p>变化得</p><script type="math/tex; mode=display">\ln\frac{y}{1-y} = w^Tx+b</script><p>若将$y$视为样本$x$作为正例的可能性，则$1-y$是其反例的可能性，两者的比值：</p><script type="math/tex; mode=display">\frac{y}{1-y}</script><p>称为几率，反映了$x$作为正例的相对可能性。</p><p>我们令</p><script type="math/tex; mode=display">\begin{aligned}p(y=1|x) &= \frac{e^{w^Tx+b}}{1+e^{w^Tx+b}}\\p(y=0|x)&=\frac{1}{1+e^{w^Tx+b}}\end{aligned}</script><p>我们采用最大似然法来估计$w$和$b$：</p><script type="math/tex; mode=display">\ell(w,b) = \sum_{i=1}^m\ln p(y_i|x_i;w,b)</script><p>我们令$\beta=(w;b),\hat{x} = (x;1)$，即$w^Tx+b=\beta^T\hat{x}$，令$p_1(\hat{x};\beta)=p(y=1|\hat{x};\beta),p_0(\hat{x};\beta)=p(y=0|\hat{x};\beta)=1-p_1(\hat{x};\beta)$<br>我们将似然项写为</p><script type="math/tex; mode=display">p(y_i|x_i;w,b) = p_1(\hat{x}_i;\beta)^{y_i}p_0(\hat{x}_i;\beta)^{1-y_i}</script><p>代入似然函数，我们得</p><script type="math/tex; mode=display">\ell(\boldsymbol{\beta})=\sum_{i=1}^{m}\left(-y_{i} \boldsymbol{\beta}^{\mathrm{T}} \hat{\boldsymbol{x}}_{i}+\ln \left(1+e^{\boldsymbol{\beta}^{\mathrm{T}} \hat{\boldsymbol{\alpha}}_{i}}\right)\right)</script><p>最小化似然函数可以用<strong>梯度下降法</strong>和<strong>牛顿法</strong>。</p><p>牛顿法<br>根据二阶泰勒展开我们有：</p><script type="math/tex; mode=display">f(x+\Delta x) = f(x) - \Delta x^T\nabla f + \Delta x^T\nabla^2f\Delta x</script><p>对其进行求导，</p><script type="math/tex; mode=display">-\nabla f + 2\Delta x\nabla^2 f=0</script><p>使导数为零得</p><script type="math/tex; mode=display">\Delta x = \nabla^2f^{-1}\nabla f</script><p>所以以牛顿法为例，其第$t+1$轮迭代解的更新公式为：</p><script type="math/tex; mode=display">\beta^{t+1}=\beta^{t}-\left(\frac{\partial^{2} \ell(\beta)}{\partial \beta \partial \beta^{\mathrm{T}}}\right)^{-1} \frac{\partial \ell(\boldsymbol{\beta})}{\partial \boldsymbol{\beta}}</script><h3 id="LDA判别分析"><a href="#LDA判别分析" class="headerlink" title="LDA判别分析"></a>LDA判别分析</h3><p>LDA的思想非常朴素：给定训练例集，设法将样例投影到一条直线，使得同类样例的投影点尽可能接近、异类样例的投影点尽可能远离。</p><p><img src="https://static01.imgkr.com/temp/4b179e7140244df5ad861570daec95ae.png" alt=""></p><p>给定数据集$D=\{(x_i,y_i)\}_{i=1}^m,y_i\in \{0,1\}$。令$X_i,\mu_i,\Sigma_i$分别表示第$i\in \{0,1\}$类示例的集合、均值向量、协方差矩阵。若将数据投影到直线$w$上，则两类样本的中心再直线上的投影分别为$w^T\mu_0$和$w^T\mu_1$，若将所有的点都投影到直线上，则两类样本的协方差分别为$w^T\Sigma_0w$和$w^T\Sigma_1 w$。</p><p>欲使同类样例的投影点尽可能接近，可以让$w^T\Sigma_0w+w^T\Sigma_1w$尽可能小；而欲使异类样例的投影点尽可能原理，可以使$||w^T\mu_0-w^T\mu_1||_2^2$尽可能大，同时考虑两者得：</p><script type="math/tex; mode=display">\begin{aligned}J &= \frac{||w^T\mu_0-w^T\mu_1||_2^2}{w^T\Sigma_0w+w^T\Sigma_1w}\\&= \frac{w^T(\mu_0-\mu_1)(\mu_0-\mu_1)^Tw}{w^T(\Sigma_0+\Sigma_1)w}\end{aligned}</script><p>定义”类内散度矩阵”</p><script type="math/tex; mode=display">\begin{aligned}S_w &= \Sigma_0+\Sigma_1\\&= \sum_{x\in X_0}(x-\mu_0)(x-\mu_0)^T+\sum_{x\in X_1}(x-\mu_1)(x-\mu_1)^T\end{aligned}</script><p>以及类间散度矩阵：</p><script type="math/tex; mode=display">S_b = (\mu_0-\mu_1)(\mu_0-\mu_1)^T</script><p>则可以重写为：</p><script type="math/tex; mode=display">J = \frac{w^TS_bw}{w^TS_ww}</script><p>这就是LDA欲最大化得目标，即$S_b$与$S_w$的广义瑞利熵。</p><p>如何求解$w$呢？我们发现，如果将$w$乘以常数$\alpha$，则分子分母上的$\alpha$约去，所以最优解与$\alpha$的长度无关而与其方向有关。所以我们令$w^TS_ww=1$($S_w$为常数)。则上式等价于：</p><script type="math/tex; mode=display">\begin{aligned}\min_w&\quad -w^TS_bw\\\operatorname{s.t.}&\quad w^TS_ww=1\end{aligned}</script><p>根据拉格朗日乘子法，我们有：</p><script type="math/tex; mode=display">L(w,\lambda) = -w^TS_bw+\lambda(w^TS_ww-1)</script><p>对其求导得：</p><script type="math/tex; mode=display">-2S_bw+2\lambda S_ww=0</script><p>即</p><script type="math/tex; mode=display">S_bw = \lambda S_ww</script><p>由于我们想要求解的只有$w$，而$\lambda$这个拉格朗日乘子具体取多少值都无所谓，于是我们任意设定$\lambda$来配合我们求解$w$。我们注意到：</p><script type="math/tex; mode=display">S_bw = (\mu_0-\mu_1)(\mu_0-\mu_1)^Tw</script><p>如果我们令$\lambda$恒等于$(\mu_0-\mu_1)^Tw$，那么上式即可改写为：</p><script type="math/tex; mode=display">S_bw = \lambda(\mu_0-\mu_1)</script><p>代入得：</p><script type="math/tex; mode=display">w = S_w^{-1}(\mu_0-\mu_1)</script><p>考虑到数值稳定性，在实践中通常是对$S_w$进行奇异值分解，即$S_w^{-1}=U\Sigma V^T$，然后得$S_w^{-1}=V\Sigma^{-1}U^T$。</p><p>可以将LDA推广到多分类任务中。假定存在$N$个类，且第$i$类示例数为$m_i$，总样本数为$m$。我们先定义”全局散度矩阵”：</p><script type="math/tex; mode=display">\begin{aligned}S_t &= S_b+S_w\\&= \sum_{i=1}^m(x_i-\mu)(x_i-\mu)^T\end{aligned}</script><p>其中$\mu$是所有示例的均值向量。将类内散度矩阵$S_w$定义为每个类别的散度矩阵之和，即</p><script type="math/tex; mode=display">S_w=\sum_{i=1}^NS_{w_i}</script><p>其中</p><script type="math/tex; mode=display">S_{w_i} = \sum_{x\in X_i}(x-\mu_i)(x-\mu_i)^T</script><p>我们可以推得：</p><script type="math/tex; mode=display">\begin{aligned}\mathbf{S}_{b} &=\mathbf{S}_{t}-\mathbf{S}_{w} \\&=\sum_{i=1}^{m}\left(\boldsymbol{x}_{i}-\boldsymbol{\mu}\right)\left(\boldsymbol{x}_{i}-\boldsymbol{\mu}\right)^{\mathrm{T}}-\sum_{i=1}^{N} \sum_{\boldsymbol{x} \in X_{i}}\left(\boldsymbol{x}-\boldsymbol{\mu}_{i}\right)\left(\boldsymbol{x}-\boldsymbol{\mu}_{i}\right)^{\mathrm{T}} \\&=\sum_{i=1}^{N}\left(\sum_{\boldsymbol{x} \in X_{i}}\left((\boldsymbol{x}-\boldsymbol{\mu})(\boldsymbol{x}-\boldsymbol{\mu})^{\mathrm{T}}-\left(\boldsymbol{x}-\boldsymbol{\mu}_{i}\right)\left(\boldsymbol{x}-\boldsymbol{\mu}_{i}\right)^{\mathrm{T}}\right)\right) \\&=\sum_{i=1}^{N}\left(\sum_{\boldsymbol{x} \in X_{i}}\left((\boldsymbol{x}-\boldsymbol{\mu})\left(\boldsymbol{x}^{\mathrm{T}}-\boldsymbol{\mu}^{\mathrm{T}}\right)-\left(\boldsymbol{x}-\boldsymbol{\mu}_{i}\right)\left(\boldsymbol{x}^{\mathrm{T}}-\boldsymbol{\mu}_{i}^{\mathrm{T}}\right)\right)\right) \\&=\sum_{i=1}^{N}\left(\sum_{\boldsymbol{x} \in X_{i}}\left(\boldsymbol{x} \boldsymbol{x}^{\mathrm{T}}-\boldsymbol{x} \boldsymbol{\mu}^{\mathrm{T}}-\boldsymbol{\mu} \boldsymbol{x}^{\mathrm{T}}+\boldsymbol{\mu} \boldsymbol{\mu}^{\mathrm{T}}-\boldsymbol{x} \boldsymbol{x}^{\mathrm{T}}+\boldsymbol{x} \boldsymbol{\mu}_{i}^{\mathrm{T}}+\boldsymbol{\mu}_{i} \boldsymbol{x}^{\mathrm{T}}-\boldsymbol{\mu}_{i} \boldsymbol{\mu}_{i}^{\mathrm{T}}\right)\right) \\&=\sum_{i=1}^{N}\left(\sum_{\boldsymbol{x} \in X_{i}}\left(-\boldsymbol{x} \boldsymbol{\mu}^{\mathrm{T}}-\boldsymbol{\mu} \boldsymbol{x}^{\mathrm{T}}+\boldsymbol{\mu} \boldsymbol{\mu}^{\mathrm{T}}+\boldsymbol{x} \boldsymbol{\mu}_{i}^{\mathrm{T}}+\boldsymbol{\mu}_{i} \boldsymbol{x}^{\mathrm{T}}-\boldsymbol{\mu}_{i} \boldsymbol{\mu}_{i}^{\mathrm{T}}\right)\right) \\&=\sum_{i=1}^{N}\left(-\sum_{\boldsymbol{x} \in X_{i}} \boldsymbol{x} \boldsymbol{\mu}^{\mathrm{T}}-\sum_{\boldsymbol{x} \in X_{i}} \boldsymbol{\mu} \boldsymbol{x}^{\mathrm{T}}+\sum_{\boldsymbol{x} \in X_{i}} \boldsymbol{\mu} \boldsymbol{\mu}^{\mathrm{T}}+\sum_{\boldsymbol{x} \in X_{i}} \boldsymbol{x} \boldsymbol{\mu}_{i}^{\mathrm{T}}+\sum_{\boldsymbol{x} \in X_{i}} \boldsymbol{\mu}_{i} \boldsymbol{x}^{\mathrm{T}}-\sum_{\boldsymbol{x} \in X_{i}} \boldsymbol{\mu}_{i} \boldsymbol{\mu}_{i}^{\mathrm{T}}\right) \\&=\sum_{i=1}^{N}\left(-m_{i} \boldsymbol{\mu}_{i} \boldsymbol{\mu}^{\mathrm{T}}-m_{i} \boldsymbol{\mu} \boldsymbol{\mu}_{i}^{\mathrm{T}}+m_{i} \boldsymbol{\mu} \boldsymbol{\mu}^{\mathrm{T}}+m_{i} \boldsymbol{\mu}_{i} \boldsymbol{\mu}_{i}^{\mathrm{T}}+m_{i} \boldsymbol{\mu}_{i} \boldsymbol{\mu}_{i}^{\mathrm{T}}-m_{i} \boldsymbol{\mu}_{i} \boldsymbol{\mu}_{i}^{\mathrm{T}}\right) \\&=\sum_{i=1}^{N}\left(-m_{i} \boldsymbol{\mu}_{i} \boldsymbol{\mu}^{\mathrm{T}}-m_{i} \boldsymbol{\mu} \boldsymbol{\mu}_{i}^{\mathrm{T}}+m_{i} \boldsymbol{\mu} \boldsymbol{\mu}^{\mathrm{T}}+m_{i} \boldsymbol{\mu}_{i} \boldsymbol{\mu}_{i}^{\mathrm{T}}\right) \\&=\sum_{i=1}^{N} m_{i}\left(-\boldsymbol{\mu}_{i} \boldsymbol{\mu}^{\mathrm{T}}-\boldsymbol{\mu} \boldsymbol{\mu}_{i}^{\mathrm{T}}+\boldsymbol{\mu} \boldsymbol{\mu}^{\mathrm{T}}+\boldsymbol{\mu}_{i} \boldsymbol{\mu}_{i}^{\mathrm{T}}\right) \\&=\sum_{i=1}^{N} m_{i}\left(\boldsymbol{\mu}_{i}-\boldsymbol{\mu}\right)\left(\boldsymbol{\mu}_{i}-\boldsymbol{\mu}\right)^{\mathrm{T}}\end{aligned}</script><p>即</p><script type="math/tex; mode=display">S_b=\sum_{i=1}^{N} m_{i}\left(\boldsymbol{\mu}_{i}-\boldsymbol{\mu}\right)\left(\boldsymbol{\mu}_{i}-\boldsymbol{\mu}\right)^{\mathrm{T}}</script><p>我们常用的是实现目标：</p><script type="math/tex; mode=display">\max_W\frac{\operatorname{tr}(W^TS_bW)}{\operatorname{tr}(W^TS_wW)}</script><p>此公式是二维情况下的推广形式，证明如下：设$W = (w_1,w_2,\cdots,w_i,\cdots,w_{N-1})\in \mathbb{R}^{d\times(N-1)}$，其中$w_i\in \mathbb{R}^{d\times 1}$为$d$行$1$列的列向量，则</p><script type="math/tex; mode=display">\begin{cases}\operatorname{tr}(W^TS_bW) = \sum_{i=1}^{N-1}w_i^TS_bw_i\\\operatorname{tr}(W^TS_wW)=\sum_{i=1}^{N-1}w_i^TS_wW_i\end{cases}</script><p>所以上述实现目标可以变形为：</p><script type="math/tex; mode=display">\max_W\frac{\sum_{i=1}^{N-1}w_i^TS_bw_i}{\sum_{i=1}^{N-1}w_i^TS_wW_i}</script><p>可以看出是对二分类结果的推广。</p><p>上式可以通过如下方式求解：</p><script type="math/tex; mode=display">S_bW = \lambda S_wW</script><p>这个问题与上面二维的时候大致一样，我们也固定分母为$1$，那么优化问题就等价于：</p><script type="math/tex; mode=display">\begin{aligned}\min_W\quad&-\operatorname{tr}(W^TS_bW)\\\operatorname{s.t.}\quad&\operatorname{tr}(W^TS_wW)=1\end{aligned}</script><p>应用拉格朗日乘子法，上述优化问题的拉格朗日函数为：</p><script type="math/tex; mode=display">L(W,\lambda) =-\operatorname{tr}(W^TS_bW)+\lambda(\operatorname{tr}(W^TS_wW)-1)</script><p>根据矩阵求导公式：</p><script type="math/tex; mode=display">\frac{\partial}{\partial X}\operatorname{tr}(X^TBX) = (B+B^T)X</script><p>对上式求导令导数等于$0$，得：</p><script type="math/tex; mode=display">S_b W = \lambda S_wW</script><p>那么$W$为$S_w^{-1}S_b$的$N-1$个最大广义特征值所对应的特征向量组成的矩阵。</p><p>如果将$W$视为一个投影矩阵，则多分类LDA将样本投影到$N-1$维空间，$N-1$通常远小于数据原有的属性数。并且在投影的过程中使用了类别的信息，故LDA也常用来降维。</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;最小二乘法永不为奴！！！&lt;/p&gt;</summary>
    
    
    
    <category term="西瓜书" scheme="https://www.hfcouc.work/categories/%E8%A5%BF%E7%93%9C%E4%B9%A6/"/>
    
    
    <category term="机器学习" scheme="https://www.hfcouc.work/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>实数与极限</title>
    <link href="https://www.hfcouc.work/2021/12/18/%E6%95%B0%E5%AD%A6%E5%88%86%E6%9E%90/"/>
    <id>https://www.hfcouc.work/2021/12/18/%E6%95%B0%E5%AD%A6%E5%88%86%E6%9E%90/</id>
    <published>2021-12-18T15:00:20.000Z</published>
    <updated>2021-12-22T07:22:25.414Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="\assets\js\Meting.min.js"></script><p>卓里奇永远的神！！！</p><span id="more"></span><h2 id="实数与极限"><a href="#实数与极限" class="headerlink" title="实数与极限"></a>实数与极限</h2><p>我们先复习一下函数(映射)的概念，假设有两个集合$X$和$Y$，我们定义一个对应法则$f$，记为$X\xrightarrow{f}Y$，使得对于$\forall x\in X,x\xrightarrow{f}y\in Y$。则$X$为定义域，$Y$为到达域。<br>下面我们引入一个记号$f(X):=\{y\in Y|\exists x(\left(x\in X)\wedge(y=f(x))\right)\}$，称为值域。<br>我们微积分研究的对象：$X\xrightarrow{f}Y\subset \mathbb{R}^n$，其中$f$为可微函数。</p><h3 id="实数的基本公理"><a href="#实数的基本公理" class="headerlink" title="实数的基本公理"></a>实数的基本公理</h3><p>加法公理：</p><ul><li>$\exists$零元$0$，使得$0+x=x+0=x$</li><li>$\exists$负元$-x$，使得$x+(-x)=(-x)+x=0$</li><li>结合律：$\forall x,y,z\in \mathbb{R}$，使得$(x+y)+z=x+(y+z)$</li></ul><p>集合$G$上存在加法运算，且满足上述三条加法公理，则说明$G$为一个群。</p><ul><li>交换律：对于$\forall x,y\in R,x+y=y+x$</li></ul><p>如果运算还满足交换律，则群成为交换群(阿贝尔群)。</p><p>乘法公理：</p><ul><li>单位元(中性元)：$1\in \mathbb{R}-0$，$\forall x\in \mathbb{R},x\cdot1=1\cdot x=x$</li><li>$\forall x\in\mathbb{R}-0,\exists x^{-1}\in\mathbb{R},x^{-1}\cdot x=x\cdot x^{-1}=1$</li><li>$\forall x,y,z\in\mathbb{R},(x\cdot y)\cdot z = x\cdot(y\cdot z)$</li><li>$\forall x,y\in\mathbb{R},x\cdot y = y\cdot x$</li></ul><p>则$\mathbb{R}-0$关于乘法构成一个群。</p><p>有了加法和乘法，我们就可以定义加法和乘法的附加公理(分配律)：</p><script type="math/tex; mode=display">\forall x,y,z\in \mathbb{R},z\cdot(x+y) = (x+y)\cdot z = x\cdot z + y\cdot z</script><p>域<br>如果集合上定义了满足上述公理的$+$和$\cdot$运算以及结合律，则该集合为域(代数域)。</p><p>序公理：我们在集合上定义一个不等关系$\le$，使得$\forall x,y\in\mathbb{R},(x\le y)\vee(y\le x)$</p><ul><li>$\forall x\in \mathbb{R},x\le x$</li><li>$(x\le y)\wedge(y\le x)\Rightarrow(x=y)$</li><li>$(x\le y)\wedge(y\le z)\Rightarrow (x\le z)$</li><li>$(x\le y)\vee (y\le x)$</li></ul><p>加法和序也存在附加公理：</p><script type="math/tex; mode=display">(x\le y)\Rightarrow (x+z)\le (y+z)</script><p>乘法与序也存在附加公理：</p><script type="math/tex; mode=display">(0\le x)\wedge(0\le y)\Rightarrow (0\le xy)</script><p>如果集合$X$上存在不等关系$\le$，满足上述公理的前三条，我们称$X$为一个偏序集，如果还满足第四条，我们称他为线性的序集。</p><p>完备性公理(连续性公理)：给定集合$X,Y\subset \mathbb{R}$，使得$\forall x\in X,\forall y \in Y,x\le y$，则$\exists c\in \mathbb{R}$满足$x\le c\le y,\forall x\in X,\forall y\in Y$。</p><p>给定一个集合，满足加法公理、乘法公理，定义了一个满足序公理的序关系，满足完备性公理，则成为是实数集的一个具体实现。</p><p>如十进制的小数是$\mathbb{R}$的一个实现，还有数轴。</p><h3 id="实数运算的代数性质"><a href="#实数运算的代数性质" class="headerlink" title="实数运算的代数性质"></a>实数运算的代数性质</h3><p>我们首先研究加法。<br>首先我们证明$\mathbb{R}$上存在唯一的加法零元。<br>证明：假设存在两个加法零元$0_1,0_2$，则</p><script type="math/tex; mode=display">0_1 = 0_1+0_2 = 0_2+0_1=0_2</script><p>所以$0_1=0_2$，所以存在唯一的加法零元。</p><p>下面证明$\forall x\in \mathbb{R},\exists$唯一的负元。<br>证明：假设存在两个负元$x_1,x_2$，则</p><script type="math/tex; mode=display">x_1 = x_1 + 0 = x_1 + (x+x_2) = (x_1+x)+x_2 = 0+x_2 = x_2</script><p>所以存在唯一的负元。</p><p>紧接着证明方程$a+x=b$有唯一解$x = b + (-a) = b-a$<br>证明：因为$a\in \mathbb{R}$，所以$\exists!$唯一负元$-a$。</p><script type="math/tex; mode=display">(a+x=b)\Rightarrow((-a)+a+x=(-a)+b) \Rightarrow x = b + (-a) = b-a</script><p>下面我们研究乘法。<br>首先我们需要证明$\mathbb{R}$上存在唯一的$1$<br>证明：假设存在两个单位元$1_1,1_2$，则</p><script type="math/tex; mode=display">1_1  = 1_1\cdot 1_2 = 1_2\cdot 1_1 = 1_2</script><p>下面我们证明$\forall x\in\mathbb{R}-0,\exists!$逆元$x^{-1}$<br>证明：假设存在两个逆元$x_1,x_2$，则</p><script type="math/tex; mode=display">x_1 = x_1\cdot 1 = x_1\cdot(x\cdot x_2) = (x_1\cdot x)\cdot x_2 = x_2</script><p>所以$x_1=x_2$</p><p>紧接着我们证明$\forall a\in \mathbb{R}-0,a\cdot x=b$存在唯一的解$x = b\cdot a^{-1} = a^{-1}\cdot b$。<br>证明：因为$a\in \mathbb{R}-0$，所以存在唯一的逆元$a^{-1}$，所以</p><script type="math/tex; mode=display">(a\cdot x = b)\Rightarrow (a^{-1}\cdot(a\cdot x) = a^{-1}\cdot b)\Rightarrow (x = a^{-1}\cdot b)</script><p>下面我们证明加法与乘法相联系的公理的推论<br>$\forall x\in \mathbb{R},x\cdot 0 = 0$<br>证明：<br>$x\cdot 0 =x\cdot(0+0) = x\cdot 0+x\cdot 0$，两边同时加上逆元$-x\cdot 0$，得$0 = x\cdot 0$。</p><p>$x\in\mathbb{R}-0\Rightarrow x^{-1}\neq0$<br>证明：若$x^{-1}=0\Rightarrow x\cdot x^{-1} = x\cdot 0 = 0$，得$1=0$，矛盾。</p><p>$x\cdot y=0\Rightarrow(x=0)\vee(y=0)$<br>证明：设$y\neq 0$，所以$y^{-1}\neq 0$，所以</p><script type="math/tex; mode=display">(x\cdot y = 0)\Rightarrow(x\cdot y\cdot y^{-1} = 0\cdot y^{-1}=0)\Rightarrow x = 0</script><p>同理可证当$x\neq0$时$y=0$。</p><p>$-x = -1\cdot x$<br>证明：<br>$x + (-1\cdot x) = 1\cdot x + -1\cdot x = (1+(-1))\cdot x = 0\cdot x = 0$<br>所以$-1\cdot x$为$x$的逆元，所以$-x = -1\cdot x$。</p><p>$(-1)\cdot (-x)=x$<br>证明：我们只需证明$(-1)\cdot(-x)$为$-x$的逆元即可。</p><script type="math/tex; mode=display">(-1)\cdot(-x) + (-x) = ((-1)+1)\cdot(-x) = 0\cdot (-x) = 0</script><p>得证。</p><p>$(-x)\cdot(-x)=x\cdot x$<br>证明：</p><script type="math/tex; mode=display">(-x)\cdot (-x) = ((-1)\cdot x)\cdot(-x) = (x\cdot (-1))\cdot(-x) = x\cdot((-1)\cdot(-x)) = x\cdot x</script><p>下面我们研究序公理的推论</p><p>$x\le y$若$x\neq y$，则记作$x&lt;y$，称为严格不等式。<br>那么对于$x,y\in\mathbb{R}$，$x&lt;y,x=y,y&lt;x$只有一个成立。</p><p>下面我们证明：</p><ul><li>$(x&lt;y)\wedge(y\le z)\Rightarrow x&lt;z$</li><li>$(x\le y)\wedge(y&lt;z)\Rightarrow x&lt;z$</li></ul><p>我们证明第一个，第二个与其类似：</p><script type="math/tex; mode=display">(x<y)\wedge(y\le z)\Rightarrow((x\neq y)\wedge(x\le y)\wedge (y\le z))\Rightarrow((x\neq y)\wedge(x\le z))</script><p>若$x=z$，我们有$(x&lt;y)\wedge(y\le x)\Rightarrow(x\neq y)\wedge(x\le y)\wedge(y\le x)\Rightarrow(x\neq y)\wedge(x=y)$，矛盾。所以$x\neq z$，所以$x&lt;z$。</p><h3 id="序公理与加法公理以及乘法公理的推论"><a href="#序公理与加法公理以及乘法公理的推论" class="headerlink" title="序公理与加法公理以及乘法公理的推论"></a>序公理与加法公理以及乘法公理的推论</h3><h4 id="序公理和加法公理"><a href="#序公理和加法公理" class="headerlink" title="序公理和加法公理"></a>序公理和加法公理</h4><p>我们需要证明以下定理：</p><ol><li>$(x&gt;y)\Rightarrow (x+z&gt;y+z)$</li><li>$(x&gt;0)\Rightarrow (-x&lt;0)$</li><li>$(x&gt;y)\wedge(z\ge w)\Rightarrow (x+z)&gt;(y+w)$</li><li>$(x\ge y)\wedge (z&gt;w)\Rightarrow (x+z)&gt;(y+w)$</li></ol><p>下面我们开始证明：</p><p>第一个：</p><script type="math/tex; mode=display">(x>y)\Rightarrow (x\ge y)\Rightarrow (x+z\ge y+z)</script><p>所以我们只需要证明$(x+z\neq y+z)$，即证明$x\neq y$，所以得证。</p><p>第二个：</p><script type="math/tex; mode=display">(x>0)\Rightarrow (x+(-x)>0+(-x))\Rightarrow (0>-x)</script><p>得证。</p><p>第三个和第四个证明一个即可：</p><script type="math/tex; mode=display">((x>y)\wedge(z\ge w))\Rightarrow ((x+z>y+z)\wedge(z+y\ge w+y))\Rightarrow ((x>y)>(y+w))</script><p>得证。</p><h4 id="序公理与乘法公理"><a href="#序公理与乘法公理" class="headerlink" title="序公理与乘法公理"></a>序公理与乘法公理</h4><ol><li>$(x&gt;0)\wedge(y&gt;0)\Rightarrow (xy&gt;0)$</li><li>$(x<0)\wedge(y<0)\Rightarrow (xy>0)$</li><li>$(x&gt;0)\wedge(y&lt;0)\Rightarrow (xy&lt;0)$</li><li>$(x&gt;y)\wedge(z&gt; 0)\Rightarrow(xz&gt;yz)$</li><li>$(x&gt;y)\wedge(z&lt; 0)\Rightarrow(xz&lt;yz)$</li></ol><p>我们先证明第一条：</p><script type="math/tex; mode=display">(x>0)\wedge(y>0)\Rightarrow (x\ge 0)\wedge(y\ge0)\Rightarrow xy\ge 0</script><p>所以我们至于要证明$xy\neq0$，根据之前的定理我们知道$(xy=0)\Rightarrow(x=0)\vee(y=0)$，而我们条件中$x,y$都不等于$0$，所以得证。</p><p>第二条：</p><script type="math/tex; mode=display">((x<0)\wedge(y<0))\Rightarrow((-x>0)\wedge(-y>0))\Rightarrow(-x)\cdot(-y)>0\Rightarrow(xy)>0</script><p>第三条与第二条思路相同，不在赘述。</p><p>下面证明第四条：</p><script type="math/tex; mode=display">((x>y)\wedge(z>0))\Rightarrow((x-y>0)\wedge(z>0))\Rightarrow((x-y)z>0)\Rightarrow(xz-yz)>0\Rightarrow xz>yz</script><p>第五条类似。</p><p>下面我们证明$1&gt;0$：</p><p>我们知道$1&gt;0,1=0,1&lt;0$之中只能有一个成立，我们已经知道$1\neq 0$。所以我们假设$1&lt;0$，那我们有：</p><script type="math/tex; mode=display">(1<0)\wedge(1<0)\Rightarrow(1\cdot1>0)\Rightarrow 1>0</script><p>互相矛盾，所以$1&gt;0$。</p><p>下面我们再证明：</p><ol><li>$(0<x)\Rightarrow(x^{-1}>0)$</li><li>$(0&lt;x&lt;y)\Rightarrow(0&lt;y^{-1}&lt;x^{-1})$</li></ol><p>我们先证明第一条：</p><p>我们假设$x^{-1}&lt;0$，则</p><script type="math/tex; mode=display">(0<x)\wedge(x^{-1}<0)\Rightarrow (x\cdot x^{-1})<0\Rightarrow 1<0</script><p>矛盾，得证。</p><p>第二条：</p><script type="math/tex; mode=display">(0<x<y)\Rightarrow(y^{-1}>0)\wedge(x>0)\Rightarrow(x\cdot y^{-1}>0)\Rightarrow(y^{-1}>x^{-1})</script><p>得证。</p><p>下面我们讨论正数和负数：</p><p>所有大于零的数为正数，如果$x$为正数，则$x^{-1}$也为正数。</p><p>负数：小于零的数为负数。</p><h3 id="完备性公理与数集上下确界的存在性"><a href="#完备性公理与数集上下确界的存在性" class="headerlink" title="完备性公理与数集上下确界的存在性"></a>完备性公理与数集上下确界的存在性</h3><p>定义：$X\subset \mathbb{R}$，若$\exists c\in \mathbb{R},\forall x\in X,x\le c$，则称$X$上有界集合，称$c$为$X$的上界；若$\forall x\in X,x\ge c$，则称$X$下有界集合，称$c$为$X$的下界。</p><p>定义(有界集)：既上有界也下有界。即$\exists c_1,c_2\in \mathbb{R}$使得$\forall x\in X,c_1&lt;x&lt;c_2$。</p><p>定义(最大元、极大元)：集合$X\subset\mathbb{R}$，若$\exists a\in X$使得$\forall x\in X,x\le a$。称$a$为$X$的最大元。即：</p><script type="math/tex; mode=display">\max X := (a\in X)\wedge(\forall x\in X,x\le a)</script><p>定义(最小元、极小元)：</p><script type="math/tex; mode=display">\min X:= ((a\in X)\wedge (\forall x\in X,x\ge a))</script><p>定义(上确界)：上界当中的最小元，记为$\sup X$</p><script type="math/tex; mode=display">(\sup X=s):= (\forall x\in X,x\le s)\wedge (\forall s^{\prime}<s,\exists x^{\prime}\in X,x^{\prime}>s^{\prime})</script><p>定义(下确界)：下界当中的最大元，记为$\inf X$。</p><script type="math/tex; mode=display">(\inf X=s):= (\forall x\in X,x\ge s)\wedge (\forall s^{\prime}>s,\exists x^{\prime}\in X,x^{\prime}<s^{\prime})</script><p>例：$[0,1)$的的上确界为$1$。</p><p>因为$\forall x\in [0,1),x \le1$，并且$\forall a<1$，我们有$\frac{1+a}{2}\in [0,1)$，但是$\frac{1+a}{2}>a$。</p><p>则如果一个集合存在一个最大元，那么它必为上确界；存在最小元，那么它必为下确界。</p><p>那么什么时候存在上确界和下确界呢？</p><p>上确界引理：若$X$为有上确界非空集合，则$X$有唯一的上确界。</p><p>证明：</p><p>我们关于上确界还有一个定义：</p><script type="math/tex; mode=display">\sup X := \min \{c|\forall x\in X,x\le c\}</script><p>由此可以看出上确界就是上界集合中的最小元。我们首先证明唯一性，假设有两个上确界$c_1,c_2$，因为上确界是上界集合中的最小元，所以我们有$c_1\le c_2,c_2\le c_1$，所以$c_1=c_2$。所以如果存在上确界的话，上确界就是唯一的。</p><p>下面我们证明存在上确界。</p><p>我们设$Y=\{y:\forall x\in X,x\le y\}$，由此可以看出$y$为$X$的上界集合，因为$X$有上界，所以$Y$非空。所以我们就有$\forall x\in X,\forall y\in Y,x\le y$。根据完备性公理，$\exists x\in \mathbb{R}$，使得$\forall x\in X,\forall y\in Y,x\le c\le y$，由此我们可以推出$c\in Y$，并且$c$为$Y$的最小元，所以上确界存在。</p><p>得证。</p><p>下确界的证明类似。</p><h3 id="完备性公理相关的基本引理"><a href="#完备性公理相关的基本引理" class="headerlink" title="完备性公理相关的基本引理"></a>完备性公理相关的基本引理</h3><p>定义(序列)：如果函数$f:\mathbb{N}\rightarrow X$，则称$f(n)$为序列。记作：</p><script type="math/tex; mode=display">x_n:= f(n)</script><p>定义(集列套)：对于$X_i\subset \mathbb{R}$，如果$\forall n\in \mathbb{N},X_{i}\supset X_{i+1}$，则称其为集列套。</p><p>闭区间套引理：若闭区间套$I_1\supset I_1\supset I_3\supset \cdots$，存在$c\in \mathbb{R}$使得$c\in I_i,\forall i\in \mathbb{N}$。如果对于$\forall \epsilon &gt;0, \exists I_n$使得$|I_n|&lt;\epsilon$，则$c$唯一。</p><p>证明：</p><p>记$I_n=[a_i,b_i]$，$\forall I_n=[a_n,b_n],I_m=[a_m,b_m]$，我们都有$a_n\le b_m$。我们令$X=\{a_n\},Y=\{b_n\}$。所以对于$\forall a_n\in X,\forall b_m\in Y$，我们都有$a_n\le b_m$，根据完备性定理：$\exists c\in \mathbb{R}$，使得$\forall a_n\in X,\forall b_m\in Y$，都有$a_n\le c\le b_m$，我们取$m=n$，则$a_n\le c\le b_n$，即$c\in I_n$。则存在性就证明完了。</p><p>若$c_1&lt;c_2\in I_n$，则$a_n\le c_1&lt;c_2\le b_n$，则$c_2-c_1&lt; b_n-a_n=|I_n|$，我们取$\epsilon = \frac{c_2-c_1}{2}$，则不存在这样的$I_n$。得证。</p><p>定义：如果$S=\{X\}$，即$S$为集合的集合，令$Y\subset \bigcup_{X\in S}X$，则称$S$为$Y$的一个覆盖。这就说明：$\forall y\in Y,\exists X\in S$使得$y\in X$。</p><p>有限覆盖引理：如果$I=[a,b],I\subset \bigcup U_n$，其中$U_n=[\alpha_n,\beta_n)$，则$\exists U_1\cdots U_k$使得$I\subset \bigcup_{i=1}^k U_i$。</p><p>证明感觉没怎么看懂，等之后再补上。主要是运用反证法。</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;卓里奇永远的神！！！&lt;/p&gt;</summary>
    
    
    
    <category term="数学" scheme="https://www.hfcouc.work/categories/%E6%95%B0%E5%AD%A6/"/>
    
    
    <category term="数学分析" scheme="https://www.hfcouc.work/tags/%E6%95%B0%E5%AD%A6%E5%88%86%E6%9E%90/"/>
    
  </entry>
  
</feed>
