[{"title":"残差神经网络","url":"/2022/09/25/ResNet/","content":"\n## Deep Residual Learning for Image Recognition\n\n<p align=\"center\">\n    <img src=\"https://gimg2.baidu.com/image_search/src=http%3A%2F%2Fi0.hdslb.com%2Fbfs%2Farticle%2Ff38ee6de5f67324a08e2c48c9a58a88196576907.jpg&refer=http%3A%2F%2Fi0.hdslb.com&app=2002&size=f9999,10000&q=a80&n=0&g=0n&fmt=auto?sec=1666673229&t=6a653f4d00066dfeb71a8b120e41ffba\" style=\"zoom: 100%;\" />\n</p>\n\n> 文章地址：https://openaccess.thecvf.com/content_cvpr_2016/html/He_Deep_Residual_Learning_CVPR_2016_paper.html\n\n### Introduction\n\n在早期的神经网络的研究中，人们发现随着神经网络的层数的增加，训练效果会越来越好。在理论上缺失如此，但是事实真是如此吗？随着神经网络的深度的增加，梯度爆炸和梯度消失的问题层出不穷。并且人们发现，随着神经网络深度的增加，训练误差会先降低后增加，着并不是因为过拟合造成的，因为是在训练集上的误差增加，如下图所示。\n\n![](https://raw.githubusercontent.com/HFC666/image/master/img/RN1.jpg)\n\n这可能是因为较深的网络不太容易优化的缘故。\n\n> 由于神经网络在训练的时候采用的是梯度下降的方法，通过后向传播的方式对梯度进行相乘，所以会发成梯度消失和梯度爆炸等方面的问题。\n>\n> + 梯度消失：如果相乘的大部分梯度因子都小于$1$，那么到最后得到的梯度就会接近于$0$。\n> + 梯度爆炸：如果相乘的大部分梯度因子都大于$1$，那么到最后得到的梯度就会非常大。\n\n我们想到的一种方法，如果我们将训练好的浅层神经网络作为前几层，然后增加恒等变换的层，那么这个组合起来的较深的神经网络最差的结果也不会比较浅的神经网络差。\n\n在本文中，我们提出了深度残差网络(deep residual learning)。我们将我们想要的映射表示为$\\mathcal{H}(\\mathrm{x})$，并且让堆叠的非线性层拟合(原神经网络层)映射$\\mathcal{F} := \\mathcal{H}(\\mathrm{x})- \\mathrm{x}$。则原始的映射变成了$\\mathcal{F}(\\mathrm{x})+\\mathrm{x}$。\n\n![](https://raw.githubusercontent.com/HFC666/image/master/img/RN2.jpg)\n\n> 这里我们相当于跳过了一层，将一层的结果通过`short-cut`传到了它的下一层，其中$\\mathcal{F}(\\mathrm{x})$相当于按照之前的神经网络传递得到的值(理解为残差)，则应用激活函数前加上前二层(只有前第二层)的输入。\n\n### Deep Residual Learning\n\n#### Identity Mapping by Shortcuts\n\n我们将残差网络应用到一部分堆叠的层中，就如上图所展示的那样。在本文中我们将残差层定义为：\n$$\n\\mathrm{y} = \\mathcal{F}(\\mathrm{x},\\{W_i\\}) + \\mathrm{x}\n$$\n这里的$\\mathrm{x},\\mathrm{y}$分别指我们考虑的层数的输入和输出。$\\mathcal{F}$表示我们要学习的残r差函数，在上图中，我们有：\n$$\n\\mathcal{F} = W_2\\sigma(W_1\\mathrm{x})\n$$\n其中$\\sigma$表示激活函数。在这里我们并没有增加任何额外的参数和计算复杂度。\n\n$\\mathcal{F}$和$\\mathrm{x}$的维度必须相同，如果不相同的，可以添加一个线性映射$W_s$：\n$$\n\\mathrm{y}  = \\mathcal{F}(\\mathrm{x},\\{W_i\\}) + W_s \\mathrm{x}\n$$\n","tags":["机器学习","深度学习"],"categories":["文献阅读"]},{"title":"Bayesian Approximations to Hidden Semi-Markov Models","url":"/2022/09/13/BHSMM/","content":"\n## Bayesian Approximations to Hidden Semi-Markov Models\n\n<p align=\"center\">\n    <img src=\"https://gimg2.baidu.com/image_search/src=http%3A%2F%2Fc-ssl.duitang.com%2Fuploads%2Fitem%2F201901%2F28%2F20190128035909_ylzge.thumb.1000_0.jpg&refer=http%3A%2F%2Fc-ssl.duitang.com&app=2002&size=f9999,10000&q=a80&n=0&g=0n&fmt=auto?sec=1665652235&t=45a1ebd0446dc8c63c38ef2d01fb2f7f\" style=\"zoom: 100%;\" />\n</p>\n\n> 参考文献：\n>\n> 1. Bayesian Approximations to Hidden Semi-Markov Models for Telemetric Monitoring of Physical Activity\n> 2. Hidden Markov models with arbitrary state dwell-time distributions\n\n<!--more-->\n\n### Modeling Approach\n\n#### Overview of Hidden Markov and Semi-Markov Models\n\n对于马尔可夫模型，我们有\n$$\n\\begin{aligned}\n&s_t\\mid s_{t-1}\\sim \\gamma_{s_{t-1}}\\\\\n&y_t\\mid s_t \\sim f(\\theta_{s_t})\\quad t=1,\\cdots,T\n\\end{aligned}\n$$\n其中$\\gamma_{j}=(\\gamma_{j1},\\cdots,\\gamma_{jK})$表示由状态$j$转移到其他状态的概率。根据马尔可夫链的性质我们很容易得到在状态$j$下停留时间$d$的概率为$p_j(d) = (1-\\gamma_{jj})\\gamma_{jj}^{d-1}$。\n\n更为灵活的框架为HSMM，他在HMM的基础上引入了一个持续时间变量。\n\n![](https://raw.githubusercontent.com/HFC666/image/master/img/BHSMM1.jpg)\n\n如图所示，每个超状态$z = (z_1,\\cdots,z_S)$都与持续时间$d_s$和观测序列$y_s = (y_{t_s}^1,\\cdots,y_{t_s}^2)$相关联，其中$t_s^1 = 1+\\sum_{r<s}d_r$并且$t_s^2 = t_s^1+d_s-1$为片段$s$的第一个和最后一个索引，并且$S$为片段的数量。这里，$d_s$表示$z_s$的长度。HSMM的生成机理可以被总结为：\n$$\n\\begin{aligned}\n&z_s\\mid z_{s-1} \\sim \\pi_{z_{s-1}}\\\\\n&d_s\\mid z_s \\sim g(\\lambda_{z_s})\\\\\n&y_s\\mid z_s \\sim f(\\theta_{z_s}) \\quad s = 1,\\cdots,S\n\\end{aligned}\n$$\n其中$\\pi_j = (\\pi_{j1},\\cdots,\\pi_{jK})$为状态转移概率，其中$\\pi_{jk} = p(z_t=k\\mid z_{t-1}=j,z_t=j)$。注意$\\pi_{jj}=0$，因为自我转移是禁止的。这里$g$表示以$\\lambda_j$为参数的停留时间的分布族。但是这样导致了计算似然的负担，HSMM的计算复杂度为$\\mathcal{O}(T^2K+TK^2)$，而HMM只有$\\mathcal{O}(TK^2)$。\n\n#### Approximations to Hidden Semi-Markov Models\n\n由于本文章中对此的描述不太详细，不容易理解，故找到本文章引用的源文章**Hidden Markov models with arbitrary state dwell-time distributions**来进行说明。\n\n**我们要做的是用HMM来近似HSMM**，这样我们就可以利用HMM的技术来学习HSMM的参数，减少时间复杂度。\n\n下面我们规定一下符号：\n\n1. 原来的HSMM过程\n\n   1. 观察序列：$(X_t)_{t=1,\\cdots,T}$，状态序列$(S_t)_{t=1,\\cdots,T}$。\n   2. $p_k$表示在状态$k(k\\in \\{1,\\cdots,N\\})$下的持续时间的概率密度函数，$F_k$表示其累积概率密度函数。\n   3. 定义状态转移矩阵$\\mathrm{A}=\\{a_{ij}\\}$，其中$a_{ij}=\\Pr(S_{t+1}=j\\mid S_t=i,S_{t+1}\\neq i),i,j=1,\\cdots,N$，其中$\\sum_ja_{ij}=1,a_{ii}=0$。\n\n2. 用来近似HSMM的HMM：\n\n   1. 观测序列：$(X_t^\\star)_{t=1,\\cdots,T}$，状态序列$(S_t^\\star)_{t=1,\\cdots,T} \\in \\{1,2,\\cdots,\\sum_{i=1}^Nm_i\\}$，其中$m_1,m_2,\\cdots,m_N\\in \\mathbb{N},m_0 := 0$。\n   2. 状态聚合：$I_k = \\{n\\mid \\sum_{i=0}^{k-1} m_i < n \\le \\sum_{i=0}^k m_i\\},k=1,\\cdots,N$，并且定义$i_k^- := \\min(I_k)$和$i_k^+ := \\max(I_k)$，我们假设属于$I_k$的每个状态的取值的发射概率密度函数相同，并且与状态为$k$的HSMM的发射概率密度函数相同，即$\\Pr^{X_t^\\star\\mid S_t^\\star \\in I_k} = \\Pr^{X_t\\mid S_t=k},t=1,\\cdots,T,k=1,\\cdots,N$。\n\n我们定义$(S_t^\\star)_{t=1,\\cdots,T}$的状态转移矩阵为$\\mathrm{B}=\\{b_{ij}\\}$，其中$b_{ij}=\\Pr(S_{t+1}^\\star\\mid S_t^\\star=i),i,j=1,\\cdots,\\sum_{i=1}^N m_i$，$\\mathrm{B}$的结构为：\n$$\n   \\mathbf{B}=\\left(\\begin{array}{ccc}\n   \\mathbf{B}_{11} & \\cdots & \\mathbf{B}_{1 N} \\\\\n   \\vdots & \\ddots & \\vdots \\\\\n   \\mathbf{B}_{N 1} & \\cdots & \\mathbf{B}_{N N}\n   \\end{array}\\right)\n$$\n而$m_i\\times m_i$的对角矩阵$B_{ii}(i=1,\\cdots,N)$，对于$m_i\\ge 2$，为：\n$$\n   \\mathbf{B}_{i i}:=\\left(\\begin{array}{c|cccc}\n   0 & 1-c_i(1) & 0 & \\ldots & 0 \\\\\n   \\vdots & 0 & \\ddots & & \\vdots \\\\\n   & \\vdots & & & 0 \\\\\n   0 & 0 & \\ldots & 0 & 1-c_i\\left(m_i-1\\right) \\\\\n   \\hline 0 & 0 & \\ldots & 0 & 1-c_i\\left(m_i\\right)\n   \\end{array}\\right)\n$$\n对于$m_i=1$，我们定义$\\mathrm{B_{ii}}:=1-c_i(1)$，并且$m_i\\times m_j$的矩阵$\\mathrm{B_{ij}}(i,j=1,\\cdots,N,i\\neq j)$为：\n$$\n   \\mathbf{B}_{i j}:=\\left(\\begin{array}{cccc}\n   a_{i j} c_i(1) & 0 & \\ldots & 0 \\\\\n   a_{i j} c_i(2) & 0 & \\cdots & 0 \\\\\n   \\vdots & & & \\\\\n   a_{i j} c_i\\left(m_i\\right) & 0 & \\ldots & 0\n   \\end{array}\\right)\n$$\n这里，对于$r=1,2,3,\\cdots$，我们有：\n$$\nc_k(r):= \\begin{cases}\\frac{p_k(r)}{1-F_k(r-1)} & \\text { for } F_k(r-1)<1 \\\\ 1 & \\text { for } F_k(r-1)=1\\end{cases}\n$$\n函数$c_k$称为停留时间分布的`hazard rates`。注意矩阵$\\mathrm{B}$确实为一个合格的状态转移矩阵，因为他的元素的取值范围为$[0,1]$，并且每一行的和为$1$。同时我们也有如下的性质成立：\n\n1. 对于$i\\neq j ,1\\le i,j\\le N$，$a_{ij}^\\star = a_{ij}$。\n2. 对于任何$k\\in \\{1,\\cdots,N\\}$，我们有：$p_k^{\\star}(r)= \\begin{cases}p_k(r) & \\text { for } r \\leq m_k \\\\ p_k\\left(m_k\\right)\\left(1-c_k\\left(m_k\\right)\\right)^{r-m_k} & \\text { for } r>m_k\\end{cases}$，虽然当$r>m_k$的时候$p^\\star_k(r)\\neq p_k(r)$，但是我们可以选择大的$m_k$使得两者近似相等。\n\n### Bayesian Inference\n\n我们对参数$\\eta = \\{(\\pi_j,\\lambda_j,\\theta_j)\\}_{j=1}^K$添加先验，先验为：\n$$\n\\begin{aligned}\n&\\pi_j\\sim \\text{Dir}(\\alpha_0), (\\theta_j,\\lambda_j)\\sim H\\times G,\\quad j=1,\\cdots,K\\\\\n&z_t^\\star\\mid z_{t-1}^\\star\\sim \\phi_{z_{t-1}^\\star}\\\\\n&y_t^\\star\\mid z_t^\\star \\in A_j\\sim f(\\theta_j),\\quad t=1,\\cdots,T\n\\end{aligned}\n$$\n其图模型为：\n\n![](https://raw.githubusercontent.com/HFC666/image/master/img/BHSMM2.jpg)\n\n> 状态转移矩阵$\\phi_j$完全由$\\pi_j$和$p(d_j=r\\mid \\lambda_j)$确定，因此他们不再是随机变量。\n\n$\\eta$的后验分布有如下的形式：\n$$\np(\\eta\\mid y)\\propto \\mathscr{L}(y\\mid \\eta) \\times \\left[\\prod_{j=1}^K p(\\pi_j)\\times p(\\lambda_j)\\times p(\\theta_j)\\right]\n$$\n其中$\\mathscr{L}(\\cdot)$表示模型的似然概率。因为我们已经将其构建为了HMM，所以我们就可以用HMM的方法来计算似然函数：\n$$\n\\mathscr{L}(y\\mid \\eta) = \\pi_0^{\\star'}P(y_1)\\Phi P(y_2)\\Phi\\cdots\\Phi P(y_{T-1})\\Phi P(y_T)\\mathrm{1}\n$$\n其中$\\bar{A}\\times \\bar{A}$的对角矩阵$\\mathrm{P}(y)$定义为：\n$$\n\\boldsymbol{P}(y)=\\operatorname{diag}\\{\\underbrace{p\\left(y \\mid \\theta_1\\right), \\ldots, p\\left(y \\mid \\theta_1\\right)}_{a_1 \\text { times }}, \\ldots, \\underbrace{p\\left(y \\mid \\theta_K\\right) \\ldots p\\left(y \\mid \\theta_K\\right)}_{a_K \\text { times }}\\}\n$$\n\n\n#### Hamiltonian Monte Carlo, No-U-Turn Sampler and Stan Modelling Language\n\n模型建立完成下面要采样了，由于我们的先验分布与似然分布不是共轭的，得到的后验不具有封闭形式，因此我们采用MCMC的方法对后验分布进行采样。\n\nHMC通过增加动量变量，根据汉密尔顿动力学在高维空间上进行高效的采样，但是步长的选择会影响HMC采样的结果。NUTS绕过了这个麻烦，它使用汉密尔顿动力学来构建采样轨迹，这些轨迹从采样器的当前值移开，直到产生`U-Turn`开始返回，从而最大程度地提高了轨迹的距离。\n\n`stan`建模语言提供了一个概率编程的环境来实现NUTS采样。用户只需要提供模型的三个组成：\n\n1. 采样器的输入：数据和先验超参数\n2. 输出：需要的参数\n3. 计算非归一化后验的步骤\n\n#### Bridge Sampling Estimation of the Marginal Likelihood\n\n`bridge sampling`通过从两个不同分布的MC估计的比值来估计边缘分布，一个是后验分布另一个是合适的提议分布$q(\\eta)$。边缘分布`bridge sampling`的估计为：\n$$\np(\\boldsymbol{y})=\\frac{\\mathbb{E}_{q(\\boldsymbol{\\eta})}[h(\\boldsymbol{\\eta}) \\mathscr{L}(\\boldsymbol{y} \\mid \\boldsymbol{\\eta}) p(\\boldsymbol{\\eta})]}{\\mathbb{E}_{p(\\eta \\mid y)}[h(\\boldsymbol{\\eta}) q(\\boldsymbol{\\eta})]} \\approx \\frac{\\frac{1}{n_2} \\sum_{j=1}^{n_2} h\\left(\\tilde{\\boldsymbol{\\eta}}^{(j)}\\right) \\mathscr{L}\\left(\\boldsymbol{y} \\mid \\tilde{\\boldsymbol{\\eta}}^{(j)}\\right) p\\left(\\tilde{\\boldsymbol{\\eta}}^{(j)}\\right)}{\\frac{1}{n_1} \\sum_{i=1}^{n_1} h\\left(\\overline{\\boldsymbol{\\eta}}^{(i)}\\right) q\\left(\\overline{\\boldsymbol{\\eta}}^{(i)}\\right)}\n$$\n其中$h(\\eta)$为一个选择合理的`bridge`函数，而$p(\\eta)$表示先验分布的联合分布。\n\n#### Comparable Dwell Priors\n\n根据边缘分布来选择模型对先验分布非常敏感，因此我们需要选择好的先验分布。比如我们要从几何分布、负二项分布或者泊松分布中选择合适的分布来对持续时间进行建模。偏移的泊松分布为严格正的，均值为$\\lambda_i+1$，方差为$\\lambda_i$。在泊松分布分布和负二项分布中，参数$\\lambda_j$的先验一般都是$\\lambda_j\\sim \\text{Gamma}(a_{0j},b_{0j})$，其中$\\mathbb{E}[\\lambda_j]=a_{0j}/b_{0j}$并且方差$\\text{Var}[\\lambda_j]=a_{0j}/b_{0j}^2$。为了让所有模型都是可以比较的，我们将几何分布表示为持续时间为$\\tau_j=1(1-\\gamma_{jj})$，其中$\\gamma_{jj}$为自我转移的概率。并且先验为$\\gamma_j=(\\gamma_{j1},\\cdots,\\gamma_{jK})\\sim \\text{Dirichlet}(v_j)$，其中$v_j=(v_{j1},\\cdots,v_{jK})$并且$\\beta_j=\\sum_{i\\neq j}v_{ij}$，并且有：\n$$\n\\mathbb{E}\\left[\\tau_j\\right]=\\frac{v_{j j}+\\beta_j-1}{\\beta_j-1} \\text { and } \\operatorname{Var}\\left[\\tau_j\\right]=\\frac{\\left(v_{j j}+\\beta_j-1\\right)\\left(v_{j j}+\\beta_j-2\\right)}{\\left(\\beta_j-1\\right)\\left(\\beta_j-2\\right)}-\\left(\\frac{v_{j j}+\\beta_j-1}{\\beta_j-1}\\right)^2\n$$\n所以我们认为可比较的先验需要满足$\\mathbb{E}[\\tau_j]=\\mathbb{E}[\\lambda_j+1]$并且$\\text{Var}[\\tau_j] = \\text{Var}[\\lambda_j+1]$。\n\n\n\n\n\n","tags":["贝叶斯","算法"],"categories":["文献阅读"]},{"title":"A Complete Guide to Efficient Transformations of data frames","url":"/2022/09/06/dataframe-julia/","content":"\n## A Complete Guide to Efficient Transformations of data frames\n\n<p align=\"center\">\n    <img src=\"https://gimg2.baidu.com/image_search/src=http%3A%2F%2Fimglf.nosdn.127.net%2Fimg%2FbzV1ZDh6L0F6bkw1U29aaWdrbS9mNUdRUUhObHBlQ2hiMG92NmJPbUhlbm5OclF6TnBqZ21BPT0.png%3FimageView%26thumbnail%3D1680x0%26quality%3D96%26stripmeta%3D0%26type%3Djpg&refer=http%3A%2F%2Fimglf.nosdn.127.net&app=2002&size=f9999,10000&q=a80&n=0&g=0n&fmt=auto?sec=1665015297&t=bb05c7f4d4c434a69eec37f3aff70472\" style=\"zoom: 100%;\" />\n</p>\n\n> 课程地址：https://www.bilibili.com/video/BV1ud4y1d7SL?spm_id_from=333.999.0.0&vd_source=6177c61c946280bb88c727585de76bc8\n\n<!--more-->\n\n### Getting started\n\n首先导入需要的包：\n\n~~~julia\nusing DataFramesMeta   # 此包里面包含了许多宏，同时也导入了DataFrames包\nusing Parquet2         # 导入数据源\nusing Statistics       # 导入一些基本的统计函数\nusing CategoricalArrays# 方便切片操作\nusing GLM              # 用来线性回归\nusing Plots            # 绘图\nusing HypothesisTests  # 假设检验\n~~~\n\n~~~julia\n# 我们可以改变展示的df的大小\nENV[\"COLUMNS\"] = 1000\nENV[\"LINES\"] = 12\n~~~\n\n导入数据：\n\n~~~julia\nds = Parquet2.Dataset(\"puzzles.parquet\")\n~~~\n\n`parquet`格式为`Hadoop`的一种存储格式，我们可以将其转换为数据框格式：\n\n~~~julia\ndf = DataFrame(ds)\n~~~\n\n### Walk before you run: column selection\n\n我们展示`DataFrames.jl`格式和`DataFramesMeta`格式的。\n\n选择一列：\n\n~~~julia\nselect(df, :Themes)\n@select(df, :Themes)\n~~~\n\n选择多列：\n\n~~~julia\nselect(df, \"Themes\", 4, [:Rating, :Moves])\n@select(df, :Themes, :NbPlays, :Rating, :Moves)\n~~~\n\n通常`@select`只接受`Symbol`类型的参数，我们可以使用`$`符号将参数转换为`Symbol`类型：\n\n~~~julia\n@select(df, $\"Themes\", $4, $[:Rating, :Moves])\n~~~\n\n\n\n>+ `DataFramesMate`格式的一般用于静态代码，其中列的名字是固定且已知的并且很容易学习。\n>+ `DataFrames.jl`格式的用于动态代码很方便，其中列的名字是动态生成的。\n\n### Special columns selectors\n\n删除列：\n\n~~~julia\nselect(df, Not(:Themes))\n~~~\n\n得到某一范围内的列：\n\n~~~julia\nselect(df, Between(\"Rating\", \"NbPlays\"))\n~~~\n\n也可以使用正则表达式：\n\n~~~julia\nselect(df, r\"P\")\n~~~\n\n选择满足某个称谓的列：\n\n~~~julia\nselect(df, Cols(endswith(\"s\")))\n~~~\n\n如果删除数据框中不存在的列怎么办？只使用`Not`会失败：\n\n~~~julia\nselect(df, Not(:X))  # 出错\n~~~\n\n但是我们可以将其与`Cols`结合起来：\n\n~~~julia\nselect(df, Not(Cols(==(\"X\"))))\n~~~\n\n> `Cols`函数接收一个返回`ture`或`False`的函数(表达式)\n\n### Column renaming\n\n如果只是想对列名字进行变换，使用`rename`或者`rename!`。\n\n但是有时候我们想要单独操作这个列：\n\n~~~julia\nselect(df, :Themes => :Theme)\n~~~\n\n我们也可以使用函数来对多个列进行变换：\n\n~~~julia\nselect(df, Cols(endswith(\"s\")) .=> identity .=> uppercase)\n~~~\n\n> 我们将上述操作进行分析：对数据框的操作语法为：`[input columns]  => [transform functions] => [output columns]`：\n>\n> + 我们首先选出满足条件的列，之后通过广播对列的元素进行操作，在这里`identity`表示保持原样\n> + 而`[output columns]`也可以是函数来对输入列的名称进行变换，但是这样变换函数只能是`identity`(在换名的前提下)。\n\n### Commonly used selection patterns\n\n将某列移动到第一位：\n\n~~~julia\nselect(df, :OpeningFamily, :)\n~~~\n\n将某列移动到最后一位：\n\n~~~julia\nselect(df, Not(:Moves), :Moves)\n~~~\n\n两者一起：\n\n~~~julia\nselect(df, :OpeningFamily, Not(:Moves), :Moves)\n~~~\n\n当你选择多个列时，列的重复是允许的，但是选择单个列就不允许：\n\n~~~julia\nselect(df, Between(3, 5), Between(1, 3))\nselect(df, :Themes, :Themes) # ArgumentError: duplicate output column name: :Themes\nselect(df, :Themes => :x, :Moves => :x) # ArgumentError: duplicate output column name: :x\n~~~\n\n### Jogging: operations on a single column of a data frame\n\n我们执行以下操作，计算一列的均值：\n\n~~~julia\ncombine(df, :Rating => mean => :Rating_mean)\n~~~\n\n我们也可以把换名字删除，这样就会自动生成名字：\n\n~~~julia\ncombine(df, :Rating => mean)\n~~~\n\n也可以使用一个函数来自动生成列的名字：\n\n~~~julia\ncombine(df, :Rating => mean => x -> x * \"_mean\")\n~~~\n\n使用`DataFramesMeta.jl`：\n\n~~~julia\n@combine(df, :Rating_mean = mean(:Rating))\n~~~\n\n`combine`将很多行转换为一行，但是用`select`函数会将得到的结果变为跟数据框一样的行数：\n\n~~~julia\nselect(df, :Rating => mean)   # 结果拓展为跟数据框一样的行数\n@select(df, :Rating_mean = mean(:Rating))   # 结果与上面相同\n~~~\n\n我们使用`transform`函数保存已有的列并在最后添加一列，这一个为`Moves`中空格的个数：\n\n~~~julia\ntransform(df, :Moves => ByRow(x -> 1 + count(==(' '), x)) => :MoveNo)\n~~~\n\n在`DataFramesMeta`中，我们使用前缀`r`来表示对行操作：\n\n~~~julia\n@rtransform(df, :MoveNo = 1+count(==(' '), :Moves))\n~~~\n\n现在我们开始以`GroupDataFrame`为对象，类似于`Python`中`pandas`中`groupby`后得到的对象。\n\n在这里我们使用`@chain`宏，他进行一连串的操作将上一个操作的输出作为下一个操作的输入。\n\n我们用到`nrow`：\n\n+ `nrow`返回数据框的行数或者每一组的行数\n+ `nrow => :custom_column_name`做同样的事情但是允许你为为列取名字。\n\n~~~julia\n@chain df begin\n    @rtransform(:MoveNo = 1 + count(==(' '), :Moves))\n    groupby(:MoveNo, sort=true)\n    @combine($nrow,\n            :Rating_mean = mean(:Rating),\n            :Popularity_mean = mean(:Popularity))\nend\n~~~\n\n使用`DataFrames`的格式更加紧凑：\n\n~~~julia\n@chain df begin\n    transform(:Moves => ByRow(x -> 1 + count(==(' '), x)) => :MoveNo)\n    groupby(:MoveNo, sort=true)\n    combine(nrow, [:Rating, :Popularity] .=> mean)\nend\n~~~\n\n下面我们研究下`groupby`：\n\n~~~julia\ngdf = groupby(df, :OpeningFamily, sort=true)\ncombine(gdf, :NbPlays => mean)\n@combine(gdf, :Nb_Plays_mean = mean(:NbPlays))\n~~~\n\n排序的话得到的结果更漂亮：\n\n~~~julia\n@chain gdf begin\n    @combine(:Nb_Plays_mean = mean(:NbPlays))\n    sort(:Nb_Plays_mean, rev=true)\nend\n~~~\n\n如果我们想对分箱数据进行分类，那么`CategoricalArrays`的`cut`函数很有用：\n\n~~~julia\n@chain df begin\n    transform(:Popularity => (x -> cut(x, 10)) => :Popularity_10)\n    groupby(:Popularity_10)\n    combine(:Rating => mean)\nend\n\n@chain df begin\n    @transform(:Popularity_10 = cut(:Popularity, 10))\n    groupby(:Popularity_10)\n    @combine(:Rating_mean = mean(:Rating))\nend\n~~~\n\n那么如果我们需要`rating`的分位数怎么办？\n\n~~~julia\n@chain df begin\n    transform(:Popularity => (x -> cut(x, 10)) => :Popularity_10)\n    groupby(:Popularity_10)\n    combine(:Rating .=> [x -> quantile(x, q) for q in [0.25, 0.5, 0.75]] .=> string.(\"Rating\", 25:25:75))\n\t# 这里之所以使用 .=> 是因为[x -> quantile(x, q) for q in [0.25, 0.5, 0.75]]有三个元素，每个元素对应于不同的列名称，\n    # 另一种写法，同样(x -> quantile(x, [0.25, 0.5, 0.75]))返回的也为三个元素的列表\n    combine(:Rating .=> (x -> quantile(x, [0.25, 0.5, 0.75])) .=> string.(\"Rating\", 25:25:75))\nend\n\n@chain df begin\n    @transform(:Popularity_10 = cut(:Popularity, 10))\n    groupby(:Popularity_10)\n    @combine(:Rating25 = quantile(:Rating, 0.25),\n             :Rating50 = quantile(:Rating, 0.5),\n             :Rating75 = quantile(:Rating, 0.75))\nend\n~~~\n\n`quantiles`函数可以接收一系列分位数并且返回一个列表，这样我们可以：\n\n~~~julia\n@chain df begin\n    transform(:Popularity => (x -> cut(x, 10)) => :Popularity_10)\n    groupby(:Popularity_10)\n    combine(:Rating => x -> quantile(x, [0.25, 0.5, 0.75]))\nend\n~~~\n\n但是这样存在问题，每一组变为了三行(将列表中的三个元素变为了三行)，这并不是我们想要的。有很多方法来解决这个问题，第一个方法为`Ref`，其将第二列的元素变为列表：\n\n~~~julia\n@chain df begin\n    transform(:Popularity => (x -> cut(x, 10)) => :Popularity_10)\n    groupby(:Popularity_10)\n    combine(:Rating => (x -> Ref(quantile(x, [0.25, 0.5, 0.75]))))\n    select(:Popularity_10, :Rating_function => string.(\"Rating\", 25:25:75))\nend\n~~~\n\n使用`AsTable`不会自动产生列名：\n\n~~~julia\n@chain df begin\n    transform(:Popularity => (x -> cut(x, 10)) => :Popularity_10)\n    groupby(:Popularity_10)\n    combine(:Rating => (x -> Ref(quantile(x, [0.25, 0.5, 0.75]))))\n    select(:Popularity_10, :Rating_function => AsTable)\nend\n~~~\n\n不适用`Ref`我们可以：\n\n~~~julia\n@chain df begin\n    transform(:Popularity => (x -> cut(x, 10)) => :Popularity_10)\n    groupby(:Popularity_10)\n    combine(:Rating => (x -> [quantile(x, [0.25, 0.5, 0.75])]) => string.(\"Rating\", 25:25:75))\n    # 这样就相当于产生了含有一个元素的列表(双重列表)\nend\n~~~\n\n我们也可以使用`NamedTuple`的方法，且`AsTable`可以使用`NamedTuple`中的列名称\n\n~~~julia\n@chain df begin\n    transform(:Popularity => (x -> cut(x, 10)) => :Popularity_10)\n    groupby(:Popularity_10)\n    combine(:Rating => (x -> (; (Symbol.(\"Rating\", 25:25:75) .=> quantile(x, [0.25, 0.5, 0.75]))...)) => AsTable)\nend\n~~~\n\n第二种方法首先我们先每一组生成三列，然后`unstack`：\n\n~~~julia\n@chain df begin\n    transform(:Popularity => (x -> cut(x, 10)) => :Popularity_10)\n    groupby(:Popularity_10)\n    combine(:Rating => (x -> (name=string.(\"Rating\", 25:25:75), value=quantile(x, [0.25, 0.5, 0.75]))) => AsTable)\n    unstack(:Popularity_10, :name, :value)\nend\n# unstack函数参数(dataframe, rowkeys, colkeys, values)\n~~~\n\n### Let us start running: advanced techniques\n\n如果我们想要对`Themes`字段进行`one-hot`编码怎么办？\n\n~~~julia\nhemes_list = split.(df.Themes)\n\nall_themes = @chain themes_list begin\n    reduce(vcat, _)  # _表示参数填充到这个位置\n    unique\n    sort\nend\n~~~\n\n~~~julia\ndf2 = @chain df begin\n    select(:Themes, :Themes => ByRow(split) => :ThemesSplit)\n    select(:ThemesSplit, [:ThemesSplit => ByRow(x -> t in x) => t for t in all_themes])\nend\n~~~\n\n我们可以计算每类的个数，通过两种方法：\n\n~~~julia\nselect(df2,\n    :ThemesSplit => ByRow(length) => :NbThemes1,\n    AsTable(Not(:ThemesSplit)) => ByRow(sum) => :NbThemes2)\n~~~\n\n我们可以通过两种方式来将多个列传入一个函数：\n\n+ 作为一个`collection`：在这种情况下他们变为位置参数\n+ 使用`AsTable`：在这种情况下他们被作为`NamedTuple`\n\n一般来说`AsTable`的方法会产生一个`NamedTuple`，编译非常耗时。\n\n下面一个是使用位置参数的例子：\n\n~~~julia\n@chain df begin\n    transform(:Popularity => (x -> cut(x, 5)) => :Popularity_5)\n    groupby(:Popularity_5)\n    combine([:Rating, :NbPlays] => cor)\nend\n~~~\n\n也可以将其转换为`NamedTuple`在进行操作：\n\n~~~julia\n@chain df begin\n    transform(:Popularity => (x -> cut(x, 5)) => :Popularity_5)\n    groupby(:Popularity_5)\n    combine(AsTable([:Rating, :NbPlays]) => x -> cor(x.Rating, x.NbPlays))\nend\n~~~\n\n我们看另一种`one-hot`编码的方式(这种方式有点慢)：\n\n~~~julia\n@chain df begin\n    [split(x) .=> true for x in _.Themes]  # 将每一行的Themes分解得到的此变为 词=>true\n    DataFrame.(_)  # 将每一行变为一个数据框\n    reduce(vcat, _, cols=:union)  # 合并所有数据框，列取并集，缺失值为missing\n    coalesce.(false)    # 将缺失值变为false\n    select(_, sort(names(_)))\nend\n~~~\n\n我们使用`unstack`函数来达到加速的目的：\n\n~~~julia\n@chain df begin\n    DataFrame(themes=split.(_.Themes), id=eachindex(_.Themes), fixed=true)\n    flatten(:themes)  # 将themes展开\n    unstack(:id, :themes, :fixed, fill=false)  # 将确实部位填充为false\nend\n~~~\n\n为了进行更为高级的转换，我们使用下列数据来建立模型：\n\n~~~julia\ndf3 = @chain df begin\n    transform(:Themes => ByRow(split) => :ThemesSplit)\n    select(:Popularity,\n           :Rating => (x -> cut(x, 5)) => :Rating_5,\n           [:ThemesSplit => ByRow(x -> t in x) => t for t in all_themes])\nend\n~~~\n\n我们以`popularity`为目标变量构建线性模型：\n\n~~~julia\nmodel_formula = Term(:Popularity) ~ sum(Term.(Symbol.(names(df3, r\"mateIn\"))))\nmodel = lm(model_formula, df3)\n~~~\n\n我们可以将结果转换为数据框：\n\n~~~julia\nDataFrame(coeftable(model))\n~~~\n\n我们学习`combine`的另一种语法，你可以将一个函数作为参数传给他，以方便使用`do-end`语法。\n\n~~~julia\ncombine(df3) do sdf  # sdf表示函数的参数，do后面的相当于一个函数\n    model_formula = Term(:Popularity) ~ sum(Term.(Symbol.(names(df3, r\"mateIn\"))))\n    model = lm(model_formula, df3)\n    return DataFrame(coeftable(model))\nend\n~~~\n\n这种方法对分组数据很有用：\n\n~~~julia\ncombine(groupby(df3, :Rating_5)) do sdf\n    model_formula = Term(:Popularity) ~ sum(Term.(Symbol.(names(sdf, r\"mateIn\"))))\n    model = lm(model_formula, sdf)\n    return DataFrame(coeftable(model))\nend\n~~~\n\n得到的结果很长，我们可以使用`unstack`来使结果变得简单：\n\n~~~julia\n@chain df3 begin\n    groupby(:Rating_5)\n    combine(_) do sdf\n        model_formula = Term(:Popularity) ~ sum(Term.(Symbol.(names(sdf, r\"mateIn\"))))\n        model = lm(model_formula, sdf)\n        return DataFrame(coeftable(model))\n    end\n    unstack(\"Rating_5\", \"Name\", \"Coef.\")\nend\n~~~\n\n我们看有多少的值为$1$。我们用两种方法实现，同时学习`@chain`的`@aside`选项：\n\n~~~julia\n@chain df3 begin\n    select(Cols(startswith(\"mateIn\")) => (+) => :NoTag)\n    @aside @show mean(_.NoTag)  # @aside表示上面的结果不为他的输入，他也不为下一行的输入，即chain跳过他\n    groupby(:NoTag)\n    combine(nrow)\nend\n~~~\n\n下面我们展示如何将多列变为一列：\n\n~~~julia\nselect(df3, AsTable(Cols(startswith(\"mateIn\"))) => ByRow(identity))\n~~~\n\n我们再使用`AsTable`就会复原：\n\n~~~julia\nselect(df3, AsTable(Cols(startswith(\"mateIn\"))) => ByRow(identity) => AsTable)\n~~~\n\n下面是一个例子，从列的集合中选出满足某个条件的列的名称：\n\n~~~julia\nselect(df3, AsTable(Cols(startswith(\"mateIn\"))) => ByRow(findfirst) => :mate)\n~~~\n\n我们看一下他们的分布：\n\n~~~julia\n@chain df3 begin\n    select(AsTable(Cols(startswith(\"mateIn\"))) => ByRow(findfirst) => :mate)\n    groupby(:mate)\n    combine(nrow)\nend\n~~~\n\n我们把`mate`列转换为字符串类型：\n\n~~~julia\n@chain df3 begin\n    select(AsTable(Cols(startswith(\"mateIn\"))) => ByRow(string∘findfirst) => :mate)\n    groupby(:mate, sort=true)\n    combine(nrow)\nend\n~~~\n\n### Subsetting rows\n\n`subset`可以用于选择行：\n\n~~~julia\nsubset(df, :OpeningFamily => ByRow(==(\"Caro-Kann_Defense\")))\n~~~\n\n`DataFrameMeta`的方法更漂亮：\n\n~~~julia\n@rsubset(df, :OpeningFamily == \"Caro-Kann_Defense\")\n~~~\n\n也可以满足多个条件：\n\n~~~julia\nsubset(df, :OpeningFamily => ByRow(==(\"Caro-Kann_Defense\")), :Popularity => x-> x.>mean(x))\n@subset(df, :OpeningFamily .== \"Caro-Kann_Defense\", :Popularity .> mean(:Popularity))\n~~~\n\n~~~julia\ndf4 = @chain df begin\n    @select(:OpeningFamily, :HighPopularity = :Popularity .> mean(:Popularity))\n    groupby(:OpeningFamily)\n    @combine($nrow, :MeanHighPopularity = mean(:HighPopularity))\n    sort(:MeanHighPopularity, rev=true)\nend\n# 绘图\nscatter(df4.nrow, df4.MeanHighPopularity,\n        xlabel=\"number of puzzles\",\n        ylabel=\"probability of above average popularity\",\n        label=nothing,\n        xaxis=:log)\n~~~\n\n","tags":["Julia","编程"],"categories":["课程笔记"]},{"title":"Statistical Rethinking:Chapter6","url":"/2022/07/26/rt6/","content":"\n## Overfitting, Regularization, and Information Criteria\n\n![](https://raw.githubusercontent.com/HFC666/image/master/img/INT1.png)\n\n<!--more-->\n\n我们要应对两个问题：过拟合和欠拟合，过拟合一般采用正则化的方式进行解决，而欠拟合一般采用信息准则的方式。\n\n### The problem with parameters\n\n我们前面提到增加相互关联的预测变量会对预测产生不好的影响，那添加不相关的预测会怎么样呢。添加不相互关联的预测也会对我们的模型造成影响，会使我们的模型对数据更敏感，从而造成过拟合。\n\n#### More parameters always improve fit\n\n我们考虑如下数据：\n\n~~~R\nsppnames <- c(\"afarensis\", \"africanus\", \"habilis\", \"boisei\", \"rudolfensis\",\n              \"ergaster\", \"sapiens\")\nbrainvolcc <- c(438, 452, 612, 521, 752, 871, 1350)\nmasskg <- c(37.0, 35.5, 34.5, 41.5, 55.5, 61.0, 53.5)\nd <- data.frame(species=sppnames, brain=brainvolcc, mass=masskg)\n~~~\n\n我们按照模型复杂度增加对其进行建模：\n\n~~~R\nm6.1 <- lm(brain~mass, data = d)\nm6.2 <- lm(brain~mass+I(mass^2), data = d)\nm6.3 <- lm( brain ~ mass + I(mass^2) + I(mass^3) , data=d )\nm6.4 <- lm( brain ~ mass + I(mass^2) + I(mass^3) + I(mass^4) ,\n            data=d )\nm6.5 <- lm( brain ~ mass + I(mass^2) + I(mass^3) + I(mass^4) +\n              I(mass^5) , data=d )\nm6.6 <- lm( brain ~ mass + I(mass^2) + I(mass^3) + I(mass^4) +\n              I(mass^5) + I(mass^6) , data=d )\n~~~\n\n![](https://raw.githubusercontent.com/HFC666/image/master/img/INT2.jpg)\n\n> 可以看到第6个模型可以精准拟合各个数据，但是却出现了脑容量为负数的情况，这肯定是错误的。\n\n#### Too few parameters hurts, too\n\n但是参数数量过少也是不行的，如：\n\n~~~R\nm6.7 <- lm( brain ~ 1 , data=d )\n~~~\n\n得到的是一条水平线，预测效果也很差。\n看模型是不是欠拟合也可以通过观察其是否对数据敏感，与过拟合对数据敏感相反，欠拟合对数据不敏感。\n\n### Information theory and model performance\n\n我们要估计的是**样本外偏差**。\n\n#### Information and uncertainty\n\n信息是从学习结果中减少的不确定性。我们现在需要去衡量不确定性，而衡量不确定性的指标需要满足下列三个条件：\n\n1. 需要是连续的\n2. 需要随着可能事件种类的增加而增加\n3. 是可加的\n\n满足上述三个条件我们使用熵来衡量不确定性：\n$$\nH(p) = -\\sum_i p_i\\log(p_i) \n$$\n\n#### From entropy to accuracy\n\n我们利用KL散度来衡量两个概率分布之间的距离：\n$$\nD_{\\text{KL}}(p,q) = \\sum_i p_i(\\log(p_i)-\\log(q_i)) = \\sum_i p_i\\log\\left(\\frac{p_i}{q_i}\\right)\n$$\n相当于用$q$近似$p$的熵进去$q$本身的熵，为用$q$近似$p$引进的不确定性。\n\n#### From divergence to deviance\n\n在计算KL散度时我们假设真实的概率分布我们是知道的，但是这与现实不符，如果我们知道真实的概率分布就没有必要进行下面的工作了。\n但是我们没有必要知道真实的概率分布，对于候选分布我们只需要知道其$\\mathbb{E}[\\log(q)]$并进行比较即可，而将每个观察到的情况的对数概率相加就可以提供$\\mathbb{E}[\\log(q)]$的近似值，因此我们定义模型的偏差：\n$$\nD(q) = -2\\sum_i \\log(q_i)\n$$\n我们可以计算任何模型的偏差，通过最大后验估计得到的概率分布来计算每一个观测的概率，之后计算偏差。我们可以使用R语言的`logLik`函数：\n\n~~~R\nm6.1 <- lm(brain~mass, d)\n(-2) * logLik(m6.1)\n~~~\n\n#### From deviance to out-of-sample\n\n散度也只是对样本内数据进行估计，随着模型复杂度的增加，样本内散度也会逐渐降低，这时候我们就需要引进训练集和测试集的概念：\n\n1. 假设有一个大小为$N$的训练集\n2. 在训练集上拟合模型，计算散度记为$D_{\\text{train}}$\n3. 假设另一组大小为$N$的样本通过同一种方法产生，称为测试数据。\n4. 计算测试数据上的散度，记为$D_{\\text{test}}$。\n\n### Regularization\n\n过拟合是因为我们的模型倾向于从训练数据中学习更多的东西，当我们的先验为平缓或趋于平缓的时候，模型认为任何的参数都是同等可能的，那么模型将会从数据中学习更多的数据。\n\n一种方法是添加怀疑的先验(skeptical prior)，它能降低从数据中学习的速率，最常见的先验是正则化先验。如下面的模型：\n$$\n\\begin{aligned}\n\ny_{i} & \\sim \\operatorname{Normal}\\left(\\mu_{i}, \\sigma\\right) \\\\\n\n\\mu_{i} &=\\alpha+\\beta x_{i} \\\\\n\n\\alpha & \\sim \\operatorname{Normal}(0,100) \\\\\n\n\\beta & \\sim \\operatorname{Normal}(0,1) \\\\\n\n\\sigma & \\sim \\operatorname{Uniform}(0,10)\n\n\\end{aligned}\n$$\n对于上述模型，我们对于$\\alpha$的先验是很平坦的，但是对于$\\beta$的先验是很倾斜的，将$\\beta$限制在某个范围。但是过于倾斜的正则先验也会对模型产生不好的效果，过于倾斜的先验会影响从数据中学到信息，从而导致欠拟合，为了防止此类问题发生，我们可以采用交叉验证的方式选取合适的先验。\n\n### Information criteria\n\n赤池信息准则(Akaike information criterion)提供了一个非常简单的平均样本外偏差估计值：\n$$\n\\text{AIC} = D_{\\text{train}}+2p\n$$\n其中$p$为参数的数目，这个定义反映了训练和测试偏差之间的关系。\n但是AIC的应用存在如下限制：\n\n1. 先验必须是平坦的或者被似然的分量压制\n2. 后验分布为近似多元分布\n3. 样本大小$N$远大于参数数目$k$\n   因为平坦的先验一般是不好的先验，所以我们需要更一般的指标，DIC(Deviance Information Criterion)偏差信息准则允许信息丰富的先验，但是仍然假设后验分布为多元高斯分布并且$N\\gg k$，WAIC(Widely Applicable Information Criterion)对后验分布的形状不存在任何假设。\n\n#### DIC\n\nDIC从训练偏差的后验分布中计算得出。因为我们的参数具有后验分布，所以通过参数计算出的偏差也存在后验分布。因此现在定义$D$为偏差的后验分布，令$\\bar{D}$表示$D$的均值，定义$\\hat{D}$为在后验均值处计算得到的偏差，也就是说我们计算参数的后验均值的，将得到的后验均值代入得到偏差$\\hat{D}$。一旦我们计算完成，DIC定义为：\n$$\n\\text{DIC} = \\bar{D}+(\\bar{D}-\\hat{D}) = \\bar{D}+p_D\n$$\n$p_D$类似于AIC中的参数数量，它是衡量模型在拟合训练样本方面的灵活性的“有效”参数数量。\n\n#### WAIC\n\nWAIC的特点是他是逐点的，预测的不确定性在数据中被逐点考虑。定义$\\Pr(y_i)$为训练数据$i$的平均似然，我们所有点的平均似然的对数相加：\n$$\n\\text{lppd} = \\sum_{i=1}^N \\log \\Pr(y_i)\n$$\nWAIC的第二个部分为有效参数数量$p_{\\text{WAIC}}$。定义$V(y_i)$为在训练样本$i$的对数似然的方差，$p_{\\text{WAIC}}$定义为：\n$$\np_{\\text{WAIC}} = \\sum_{i=1}^N V(y_i)\n$$\n则WAIC定义为：\n$$\n\\text{WAIC} = -2(\\text{lppd}-p_{\\text{WAIC}})\n$$\n因为WAIC需要将数据分割成独立的观测$i=1,\\cdots,N$，在某些情况下很难定义，如在时间序列中。\n\n### Using information criteria\n\n#### Model comparison\n\n~~~R\nlibrary(rethinking)\ndata(milk)\nd <- milk[complete.cases(milk),]\nd$neocortex <- d$neocortex.perc / 100\n~~~\n\n这就做好的模型比较的第一步，保证各个模型使用的数据相同。\n\n~~~R\na.start <- mean(d$kcal.per.g)\nsigma.start <- log(sd(d$kcal.per.g))\n\nm6.11 <- map(\n  alist(\n    kcal.per.g ~ dnorm(a, exp(log.sigma))\n  ),\n  data = d, start=list(a=a.start, log.sigma=sigma.start)\n)\n\nm6.12 <- map(\n  alist(\n    kcal.per.g ~ dnorm(mu, exp(log.sigma)),\n    mu <- a + bn*neocortex\n  ),\n  data = d, start=list(a=a.start, bn=0, log.sigma=sigma.start)\n)\n\nm6.13 <- map(\n  alist(\n    kcal.per.g ~ dnorm(mu, exp(log.sigma)),\n    mu <- a + bm*log(mass)\n  ),\n  data=d, start=list(a=a.start,bm=0,log.sigma=sigma.start)\n)\nm6.14 <- map(\n  alist(\n    kcal.per.g ~ dnorm(mu, exp(log.sigma)),\n    mu <- a + bn*neocortex + bm*log(mass)\n  ),\n  data=d, start = list(a=a.start, bn=0, bm=0, log.sigma=sigma.start)\n)\n~~~\n\n我们将集合WAIC和参数估计来对模型进行评估。\n\n##### Comparing WAIC values\n\n~~~R\nWAIC(m6.14)\n       WAIC     lppd  penalty  std_err\n1 -15.06584 12.35506 4.822137 7.514096\n~~~\n\n我们也可以通过WAIC来对不同的模型进行比较：\n\n~~~R\nmilk.models <- compare(m6.11, m6.12, m6.13, m6.14)\n\n       WAIC   SE dWAIC  dSE pWAIC weight\nm6.14 -15.0 7.57   0.0   NA   4.8   0.93\nm6.11  -8.3 4.66   6.7 7.44   1.8   0.03\nm6.13  -7.4 5.82   7.5 5.54   3.3   0.02\nm6.12  -6.1 4.35   8.9 7.69   3.0   0.01\n~~~\n\n+ WAIC指的是每个模型的WAIC值，越小越好\n+ pWAIC指的是参数的有效个数，这为每个模型在拟合样本方面的灵活性提供了线索。\n+ dWAIC是每个WAIC与最低WAIC之间的差值。\n+ weight 是每个模型的 Akaike 权重。这些值是转换后的信息标准值。\n+ SE为WAIC估计的标准误差\n+ dSE是每个模型与排名靠前的模型之间WAIC 差异的标准误差，所以第一个是缺失的\n\nAkaike权重通过重新缩放来提供帮助，其公式为：\n$$\nw_i = \\frac{\\exp(-\\frac{1}{2}\\text{dWAIC}_i)}{\\sum_{j=1}^m\\exp(-\\frac{1}{2}\\text{dWAIC}_j)}\n$$\ndWAIC与得出的表的dWAIC意义相同。模型的权重是对模型对新数据做出最佳预测的概率的估计，以所考虑的模型集为条件。\n\n##### Comparing estimates\n\n比较估计值至少在两个主要方面有帮助。首先，对了解为什么一个或多个特定模型具有较低的 WAIC 值是有用的，其次，不管 WAIC 值如何，我们经常想知道某个参数的后验分布在模型中是否稳定。\n\n~~~R\ncoeftab(m6.11,m6.12,m6.13,m6.14)\n\n          m6.11   m6.12   m6.13   m6.14  \na            0.66    0.35    0.71   -1.09\nlog.sigma   -1.79   -1.80   -1.85   -2.16\nbn             NA    0.45      NA    2.79\nbm             NA      NA   -0.03   -0.10\nnobs           17      17      17      17\n~~~\n\n#### Model averaging\n\n我们之前已经知道当从模型中模拟预测时怎么保持参数的不确定性，现在我们有同样相似的问题，怎么保持模型的不确定性。为了回顾，我们生成并绘制具有最小WAIC的模型的反事实预测(counterfactual predictions )。\n\n~~~R\nnc.seq <- seq(from=0.5,to=0.8,length.out=30) \nd.predict <- list(\nkcal.per.g = rep(0,30), # empty outcome \nneocortex = nc.seq, # sequence of neocortex \nmass = rep(4.5,30) # average mass\n)\npred.m6.14 <- link( m6.14 , data=d.predict )\nmu <- apply( pred.m6.14 , 2 , mean )\nmu.PI <- apply( pred.m6.14 , 2 , PI )\n# plot it all\nplot( kcal.per.g ~ neocortex , d , col=rangi2 ) \nlines( nc.seq , mu , lty=2 )\nlines( nc.seq , mu.PI[1,] , lty=2 ) \nlines( nc.seq , mu.PI[2,] , lty=2 )\n~~~\n\n下面我们计算添加模型平均后验预测，下面时步骤：\n\n1. 计算每个模型的WAIC\n2. 计算每个模型的权重\n3. 计算每个模型的预测结果\n4. 使用模型权重作为比例，将这些值组合成一个预测集合。\n\n~~~R\nmilk.ensemble <- ensemble( m6.11 , m6.12 , m6.13 , m6.14 , data=d.predict )\nmu <- apply( milk.ensemble$link , 2 , mean )\nmu.PI <- apply( milk.ensemble$link , 2 , PI ) \nlines( nc.seq , mu )\nshade( mu.PI , nc.seq )\n~~~\n\n","tags":["概率编程","贝叶斯统计","Statistical Rethinking"],"categories":["书籍阅读"]},{"title":"算法导论课程第二周","url":"/2022/07/26/alo2/","content":"\n<p align=\"center\">\n    <img src=\"https://gimg2.baidu.com/image_search/src=http%3A%2F%2Fp2.itc.cn%2Fimages01%2F20200915%2Fcd3178c68249454aa8e8206ed7626cb6.jpeg&refer=http%3A%2F%2Fp2.itc.cn&app=2002&size=f9999,10000&q=a80&n=0&g=0n&fmt=auto?sec=1661380513&t=cc08b97050020b95b97750370a407272\" style=\"zoom: 100%;\" />\n</p>\n\n> 课程链接：https://ocw.mit.edu/6-006S20\n\n<!--more-->\n\n## Sets and Sorting\n\n### Set interface\n\n+ 包含具有唯一键的元素\n+ 一般我们令键为元素本身，但是也可能存储更多的信息\n+ 支持的操作\n  + Container\n    + `build(X)`：给定可迭代的$X$，用$X$中的元素构建序列\n    + `len()`：返回排列好的元素的个数\n  + Dynamic：\n    + `insert(x)`：将$x$添加到集合(如果`x.key`已经存在则替换即可)\n    + `delete(k)`：删除并返回键为`k`的元素\n  + Order\n    + `iter_ord()`：以键的顺序返回排列好的元素\n    + `find_min()`：找到键最小的元素\n    + `find_max()`：找到键最大的元素\n    + `find_next(k)`：找到键比`k`大的最小的键对应的元素\n    + `find_prev(k)`：找到键比`k`小的最大的键对应的元素\n+ 特殊的接口：字典\n+ 将元素以任意的顺序排列可以实现一个集合(不是很高效)\n+ 将元素以键升序来排列\n  + 更快找到`min/max`\n  + 更快通过二分法找到元素$\\mathcal{O}(\\log n)$\n    ![](https://raw.githubusercontent.com/HFC666/image/master/img/set1.jpg)\n\n> 构建的复杂度为$n\\log n$是因为我们在构建的过程中需要排序，对一个元素排序的复杂度为$\\log n$，$n$个为$n\\log n$。寻找元素即采用二分法，时间复杂度为$\\log n$，插入删除元素既涉及寻找元素有涉及元素插入或删除后的移位，复杂为$\\log n+n=n$，找到下一个和上一个元素的复杂度与找到某个元素相同，为$\\log n$。\n\n那么下面我们关心的就是如何进行排序了：\n排序\n\n+ 输入：具有$n$个元素/键的数组(array)$A$\n+ 输出：对$A$进行排序后得到的数组$B$\n+ 排序为destructive的如果它重写$A$，既并没有创建新的数组，$B$只是$A$的排序版本\n+ 排序为in place的如果它利用$\\mathcal{O}(1)$额外的空间(如创建循环时的临时变量)\n\n~~~Python\nclass Sorted_Array_Set:\n\n    def __init__(self):\n\n        self.A = Array_Seq()\n\n    def __len__(self):\n\n        return len(self.A)\n\n    def __iter__(self):\n\n        yield from self.A\n\n    def iter_order(self):\n\n        yield from self\n\n    def build(self, X):\n\n        self.A.build(X)\n\n        self._sort()\n\n  \n\n    def _sort(self):\n\n        pass\n\n  \n\n    def _binary_search(self, k, i, j):\n\n        if i >= j:\n\n            return i\n\n        m = (i + j) // 2\n\n        x = self.A.get_at(m)\n\n        if x.key > k: return self._binary_search(k ,i ,m-1)\n\n        if x.key < k: return self._binary_search(k, m+1, j)\n\n        return m\n\n    def find_min(self):\n\n        if len(self) > 0: return self.A.get_at(0)\n\n        else: return None\n\n    def find_max(self):\n\n        if len(self) > 0: return self.A.get_at(len(self)-1)\n\n        else: return None\n\n    def find(self, k):\n\n        if len(self) ==0: return None\n\n        i = self._binary_search(k, 0, len(self)-1)\n\n        x = self.A.get_at(i)\n\n        if x.key == k: return x\n\n        else: return None\n\n    def find_next(self, k):\n\n        if len(self) == 0: return None\n\n        i = self._binary_search(k, 0, len(self)-1)\n\n        x = self.A.get_at(i)\n\n        if x.key > k:\n\n            return x\n\n        if i + 1 < len(self): return self.A.get_at(i+1)\n\n        else: return None\n\n  \n\n    def find_prev(self, k):\n\n        if len(self) == 0: return None\n\n        i = self._binary_search(k, 0, len(self)-1)\n\n        x = self.A.get_at(i)\n\n        if x.key < k:\n\n            return x\n\n        if i + 1 < len(self): return self.A.get_at(i-1)\n\n        else: return None\n\n    def insert(self, x):\n\n        if len(self.A) == 0:\n\n            self.A.insert_first(x)\n\n        else:\n\n            i = self._binary_search(x.key, 0, len(self.A)-1)\n\n            k = self.A.get_at(i).key\n\n            if k == x.key:\n\n                self.A.set_at(i, x)\n\n                return False\n\n            if k > x.key: self.A.insert_at(i, x)\n\n            else: self.A.insert_at(i+1, x)\n\n        return True\n\n    def delete(self, k):\n\n        i = self._binary_search(k, 0, len(self.A) - 1)\n\n        assert self.A.get_at(i).key == k\n\n        return self.A.delete_at(i)\n~~~\n\n### Permutation Sort\n\n+ 存在$A$的$n!$种排列，其中一个是排列好的顺序\n+ 对于每一个排列，查看其是否排列好的复杂度为$\\Theta(n)$\n\n~~~Python\ndef permutation_sort(A):\n\tfor B in permutations(A):\n\t\tif is_sorted(B):\n\t\t\treturn B\n~~~\n\n时间复杂为$\\Omega(n!\\cdot n)$，为指数级的。\n\n### Selection Sort\n\n+ 找到A[:i+1]中最大的数并将其放到A[i]中\n+ 递归地对A[:i]进行排序\n\n~~~Python\ndef selection_sort(A, i=None)   # T(i)\n\tif i is None: i = len(A) - 1 # O(1)\n\tif i > 0:                    # O(1)\n\t\tj = prefix_max(A, i)  # S(i)\n\t\tA[i], A[j] = A[j], A[i]\n\t\tselection_sort(A, i-1)   # T(i-1)\n\ndef prefix_max(A, i):            # S(i)\n\tif i > 0:                    # O(1)\n\t\tj = prefix_max(A, i-1)   # S(i-1)\n\t\tif A[i] < A[j]:          # O(1)\n\t\t\treturn j             # O(1)\n\treturn i                     # O(1)\n\t\t\n~~~\n\n下面我们来分析一下`prefix_max`函数：\n\n+ Base case：对于$i=0$，数组有一个元素，所以最大的索引为$i$\n+ 归纳：假设对于$i$是正确的，最大的要么是A[:i]中的要么是A[i]，返回最大的索引\n+ $S(1)=\\Theta(1), S(n) = S(n-1)+\\Theta(1)$\n  + 替换：$S(n)=\\Theta(n), cn=\\Theta(1)+c(n-1)\\Rightarrow 1 = \\Theta(1)$\n    再分析一下`selection_sort`函数：\n+ Base case：对于$i=0$，数组含有一个元素，因此是排列好的\n+ 归纳：假设对于$i$是正确的，排列好的输出的最后一个元素为数组的最大的元素，并且算法将其放在那里；则A[:i]是排列好的\n+ $T(1)=\\Theta(1), T(n) = T(n-1)+\\Theta(n)$\n  + 替换：$T(n)=\\Theta(n^2),cn^2=\\Theta(n)+c(n-1)^2\\Rightarrow c(2n-1)=\\Theta(n)$\n\n> 步骤为先把全部$n$个元素的最大值挑出来放在最后面，之后将剩余的$n-1$个元素的最大值挑出来放在$n-1$位置，递归下去\n\n### Insertion Sort\n\n+ 递归地排列A[:i]\n+ 排列A[:i+1]假设A[:i]已经通过重复地交换排列好了\n\n~~~Python\ndef insertion_sort(A, i = None):       # T(i)\n\tif i is None: i = len(A) - 1       # O(1)\n\tif i > 0:                          # O(1)\n\t\tinsertion_sort(A, i-1)         # T(i-1)\n\t\tinsert_last(A, i)              # S(i)\n\ndef insert_last(A, i):                 # S(i)\n\tif i > 0 and A[i] < A[i-1]:        # O(1)\n\t\tA[i], A[i-1]  = A[i-1], A[i]   # O(1)\n\t\tinsert_last(A, i-1)            # S(i-1)\n~~~\n\n先分析一下`insert_last`函数：\n\n+ Base case：对于$i=0$，数组只有一个元素所以是排列好的\n+ 归纳：假设对于$i$是正确的，如果A[i] >= A[i-1]，数组是排列好的；否则交换后两个元素\n+ $S(1) = \\Theta(1), S(n)=S(n-1)+\\Theta(1)\\Rightarrow S(n)=\\Theta(n)$\n  之后分析一下`insertion_sort`函数：\n+ Base case：对于$i=0$，数组只有一个元素因此是排列好的\n+ 归纳：假设对于$i$时正确，算法通过归纳排列A[:i]，之后`insert_last`正确地排列剩余的部分\n+ $T(1)=\\Theta(1), T(n) = T(n-1)+\\Theta(n)\\Rightarrow T(n) = \\Theta(n^2)$\n\n> 总的思路就是先排列好前$i$个数据，之后通过逐次比较将下一个元素插入正确的位置来排列第$i+1$个元素。\n\n\n选择排序和插入排序都是`in place`排序，意味着它们需要至多常数复杂度的额外空间。而插入排序是稳定的，即再处理具有相同值的排序时相同值的原有顺序不变，而选择排序是不稳定的。如在排列$[2,1,1^\\prime]$，插入排序的结果为$[1,1^\\prime,2]$，而选择排序的结果为$[1^\\prime,1,2]$。\n\n### Merge Sort\n\n+ 递归地排列前一半和后一半\n+ 将排列好的两半混合成一个排列好的列表\n\n~~~Python\ndef merge_sort(A, a = 0, b = None):                  # T(b - a = n)\n\tif  b in None: b = len(A)                        # O(1)\n \tif 1 < b - a :                                   # O(1)\n\t\tc = (a + b + 1) // 2                         # O(1)\n\t\tmerge_sort(A, a, c)                          # T(n/2)\n\t\tmerge_sort(A, c, b)                          # T(n/2)\n\t\tL, R = A[a:c], A[c:b]                        # O(n)\n\t\tmerge(L, R, A, len(L), len(R), a, b)         # S(n)\n\ndef merge(L, R, A, i, j, a, b):                            # S(b - a = n)\n\tif a < b:                                              # O(1)\n\t\tif (j <= 0) or (i > 0 and L[i - 1] > R[j - 1]):    # O(1)\n\t\t\tA[b - 1] = L[i - 1]                            # O(1)\n\t\t\ti = i - 1                                      # O(1)\n\t\telse:                                              # O(1)\n\t\t\tA[b - 1] = R[j - 1]                            # O(1)\n\t\t\tj = j - 1                                      # O(1)\n\t\tmerge(L, R, A, i, j, a, b-1)                       # S(n-1)\n~~~\n\n首先先分析一下merge函数：\n\n+ Base case：对于$n=0$，数组为空，因此排序正确\n+ 归纳：假设对于$i$是正确的，元素A[r]必须是L和R中最大的元素，并且因为它们是排列好的，取最大的元素递归地进行排序\n+ $S(0) = \\Theta(1),S(n)=S(n-1)+\\Theta(1)\\Rightarrow S(n)=\\Theta(n)$\n  下面是`merge_sort`函数：\n+ Base case：对于$n=1$，数组含有一个元素因此是排列好的\n+ 归纳：假设对于$k<n$是正确的，算法通过归纳排列两个一半，之后将其混合成一个排列好的数组\n+ $T(1) = \\Theta(1), T(n)=2T(n/2)+\\Theta(n)$\n+ 替换：假设$T(n)=\\Theta(n\\log n), cn\\log n = \\Theta(n) + 2c(n/2)\\log(n/2)\\Rightarrow cn\\log(2) = \\Theta(n)$\n  归并排序使用线性的额外空间，因此不是`in place`算法。\n\n### Recurrences\n\n有三种方法来解决Recurrences(递归)问题：\n\n+ 替换：猜测一个结果来替换证明递归成立\n+ Recursion Tree：绘制树来表示递归将节点处的计算数求和\n+ Master定理：一个解决大多数递归问题的通用公式。\n\n#### Master Theorem\n\n给定下列形式的递归关系式：$T(n) = aT(n/b)+f(n)$并且$T(1)=\\Theta(1)$，分支因子$a\\ge1$，问题规模缩减因子$b>1$并且渐进非负函数$f(n)$，Master Theorem通过比较$f(n)$和递归树底部的叶子数量$a^{\\log_b^n}=n^{\\log_b a}$来给出答案。当$f(n)$渐进增长速度大于$n^{\\log_ba}$，在每一层做的工作呈几何下降因此根节点占主要地位；否则当$f(n)$增长缓慢，在每一个节点的工作量成几何增加因此每一层的工作占主导。\n![](https://raw.githubusercontent.com/HFC666/image/master/img/set2.jpg)\n$$\n\\begin{array}{c|l|l}\n\n\\text { case } & \\text { solution } & \\text { conditions } \\\\\n\n\\hline 1 & T(n)=\\Theta\\left(n^{\\log _{b} a}\\right) & f(n)=O\\left(n^{\\log _{b} a-\\varepsilon}\\right) \\text { for some constant } \\varepsilon>0 \\\\\n\n\\hline 2 & T(n)=\\Theta\\left(n^{\\log _{b} a} \\log ^{k+1} n\\right) & f(n)=\\Theta\\left(n^{\\log _{b} a} \\log ^{k} n\\right) \\text { for some constant } k \\geq 0 \\\\\n\n\\hline 3 & T(n)=\\Theta(f(n)) & f(n)=\\Omega\\left(n^{\\log _{b} a+\\varepsilon}\\right) \\text { for some constant } \\varepsilon>0 \\\\\n\n& & \\text { and } a f(n / b)<c f(n) \\text { for some constant } 0<c<1\n\n\\end{array}\n$$\nMaster Theorem当$f(n)$为多项式时可以简化，也就是递归的形式为$T(n)=aT(n/b)+\\Theta(n^c),c\\ge0$。\n$$\n\\begin{array}{c|l|l|l}\n\n\\text { case } & \\text { solution } & \\text { conditions } & \\text { intuition } \\\\\n\n\\hline 1 & T(n)=\\Theta\\left(n^{\\log _{b} a}\\right) & c<\\log _{b} a & \\text { Work done at leaves dominates } \\\\\n\n\\hline 2 & T(n)=\\Theta\\left(n^{c} \\log n\\right) & c=\\log _{b} a & \\text { Work balanced across the tree } \\\\\n\n\\hline 3 & T(n)=\\Theta\\left(n^{c}\\right) & c>\\log _{b} a & \\text { Work done at root dominates }\n\n\\end{array}\n$$\n练习：\n![](https://raw.githubusercontent.com/HFC666/image/master/img/set3.jpg)\n\n## Hashing\n\nComparison Model：\n\n+ 在这个模型中，假设算法只能通过比较来区分元素\n+ Comparable items：只支持元素对之间比较的黑盒\n+ 比较为$<,\\le,>,\\ge,=,\\neq$，输出是二元的：正确或错误\n+ 目标：存储$n$个可比较的元素，支持`find(k)`操作\n+ 运行时间以比较的表现为下界，因此数比较的次数\n\nDecision Tree：\n\n+ 任何算法都可以被看作是一个决策树\n+ 内部的一个节点可以被看作一个二元比较，分支为True或False\n+ 对于一个比较算法，决策树是二元的\n+ 一个叶子表示算法终止，产生算法的输出\n+ 一个根节点到叶子节点的路径表示在某些输入上算法的执行\n+ 对每一个算法输出至少有一个节点，因此需要至少$\\ge n+1$叶子节点(包含不存在)\n\nComparison Search Lower Bonud\n\n+ 比较查询算法(comparison search algorithm)最差情况下的运行时间是什么？\n+ 运行时间$\\ge$比较$\\ge$任何从根节点到叶子节点路径最大的长度$\\ge$树的长度\n+ 任何节点数$\\ge n$的二元树的最小高度是多少？\n+ 长度$\\ge [\\log (n+1)]-1=\\Omega(\\log n)$，因此任何比较排序的运行时间为$\\Omega(\\log n)$。\n+ 排序数组可以达到这个界限。\n+ 更一般地，叶子数为$\\Theta(n)$和最大分支因子为$b$的树的长度为$\\Omega(\\log_b n)$。\n+ 为了更快，我们需要一个允许super-constant $w(1)$分支因子的操作，不过要怎么办？\n\nDirect Access Array\n\n+ 利用Word-RAM $\\mathcal{O}(1)$时间复杂度获取索引，线性的分支因子。\n+ 给定元素唯一整数键$k\\in \\{0,\\cdots,u-1\\}$，将其存储在一个数组中，索引为$k$\n+ 如果键可以存储在一个机器字节中，即$u\\le 2^w$，最差$\\mathcal{O}(1)$获取元素\n+ 但是空间$\\mathcal{O}(u)$，当$n\\ll u$时会很糟糕\n+ 例如如果键位十个字母的名字，如果一个比特一个名字，需要$26^{10}\\approx 17.6$ TB空间\n+ 如何使用更少的空间呢？\n\n~~~Python\nclass DirectAccessArray:\n\n    def __init__(self, u):\n\n        self.A = [None] * u\n\n  \n\n    def find(self, k):\n\n        return self.A[k]\n\n  \n\n    def insert(self, x):\n\n        self.A[x.key] = x\n\n  \n\n    def delete(self, k):\n\n        self.A[k] = None\n\n  \n\n    def find_next(self, k):\n\n       for i in range(k, len(self.A)):\n\n        if self.A[i] is not None:\n\n            return self.A[i]\n\n    def find_max(self):\n\n        for i in range(len(self.A)-1, -1, -1):\n\n            if self.A[i] is not None:\n\n                return self.A[i]\n\n  \n\n    def delete_max(self):\n\n        for i in range(len(self.A)):\n\n            x = self.A[i]\n\n            if x is not None:\n\n                self.A[i] = None\n\n                return x\n~~~\n\nHashing\n\n+ 如果$n\\ll u$，将键映射到小范围$m=\\Theta(n)$并且使用小的direct access array\n+ 哈希函数：$h(k):\\{0,\\cdots, u-1\\rightarrow \\{0,\\cdots,m-1\\}$(也被称为哈希映射)\n+ Direct access array 称为哈希表，$h(k)$被称为键$k$的哈希\n+ 如果$m\\ll u$，根据鸠巢原理，没有哈希函数是单射的\n+ 总是存在键$a\\neq b$使得$h(a)=h(b)$：冲突了(Collision)\n+ 不能将不同的元素存储在相同的索引处，所以存储在哪呢？要么：\n  + 存储在数组中的其他位置(open addressing)：分析比较复杂，但是普遍和实用\n  + 存储在支持动态集合接口的其他数据结构中(chaining)\n\nChaining\n\n+ 将冲突存储在另一个数据结构中(a chain)\n+ 如果键大致均匀地分布在索引上，链的大小为$n/m=n/\\Omega(n)=\\mathcal{O}(1)$\n+ 如果链的大小为$\\mathcal{O}(1)$，则所有的操作需要$\\mathcal{O}(1)$时间！\n+ 如果不是，许多元素映射到同一个位置，即$h(k)=\\text{constant}$，链大小为$\\Theta(n)$\n+ 需要好的哈希函数，但是什么是好的哈希函数呢？\n\nHash Functions\n\n+ Division(bad)：$h(k) = (k \\text{ mod }m)$\n  + 当键是均匀分布的话很好\n  + $m$对于存储的键应该避免对称\n  + 大于$2,10$的幂的大素数可以\n  + Python实用加有一个混合的Division的一个版本\n  + 如果$u\\gg n$，每一个哈希函数将有相同的产生$\\mathcal{n}$长度链的输入\n  + 不要用固定的哈希函数。随机抽选一些(但是要小心)\n+ Universal(good, theoretically)：$h_{ab}(k)=(((ak+b)\\text{ mod }p)\\text{ mod }m)$\n  + 哈希族$\\mathcal{H}(p,m) = \\{h_{ab}\\mid a,b\\in \\{0,\\cdots,p-1\\} \\text{ and }a\\neq 0\\}$\n  + 通过一个固定的素数$p>u$进行参数化，其中$a,b$选自$\\{0,1,\\cdots,p-1\\}$\n  + $\\mathcal{H}$为Universal family：$\\Pr_{h\\in\\mathcal{H}}\\{h(k_i)=h(k_j)\\}\\le 1/m\\quad \\forall k_i\\neq k_j\\in \\{0,\\cdots,u-1\\}$\n  + 为什么universality是有用的呢？它暗示了短的链的长度(期望)\n  + $X_{ij}$指示随机变量在$h\\in\\mathcal{H}: X_{ij}=1\\text{ if }h(k_i)=h(k_j),X_{ij}=0\\text{ otherwise}$\n  + 在索引$h(k_i)$处的链的长度为随机变量$X_i=\\sim_j X_{ij}$\n  + 在索引$h(k_i)$处链的期望长度：\n\n$$\n\\begin{aligned}\n\n\\underset{h \\in \\mathcal{H}}{\\mathbb{E}}\\left\\{X_{i}\\right\\}=\\underset{h \\in \\mathcal{H}}{\\mathbb{E}}\\left\\{\\sum_{j} X_{i j}\\right\\}=\\sum_{j} \\underset{h \\in \\mathcal{H}}{\\mathbb{E}}\\left\\{X_{i j}\\right\\} &=1+\\sum_{j \\neq i} \\underset{h \\in \\mathcal{H}}{\\mathbb{E}}\\left\\{X_{i j}\\right\\} \\\\\n\n&=1+\\sum_{j \\neq i}(1) \\operatorname{Pr}_{h \\in \\mathcal{H}}\\left\\{h\\left(k_{i}\\right)=h\\left(k_{j}\\right)\\right\\}+(0) \\underset{h \\in \\mathcal{H}}{\\operatorname{Pr}}\\left\\{h\\left(k_{i}\\right) \\neq h\\left(k_{j}\\right)\\right\\} \\\\\n\n& \\leq 1+\\sum_{j \\neq i} 1 / m=1+(n-1) / m\n\n\\end{aligned}\n$$\n\n\n + 因为$m = \\Omega(n)$，所以上面得到的结果为$1+(n-1)/m=\\mathcal{O}(1)$，所以期望为$\\mathcal{O}(1)$\n\nDynamic\n\n+ 如果$n/m$远大于$1$，用随机选择的新的哈希函数重新构建新的大小$m$\n+ 与动态数组的分析方法一致，cost可以在很多动态操作上分摊(amortized)\n+ 因此一个哈希表可以以期望$\\mathcal{O}(1)$的分摊时间来实现动态集合操作\n\n![](https://raw.githubusercontent.com/HFC666/image/master/img/set4.jpg)","tags":["算法","编程"],"categories":["课程笔记"]},{"title":"算法导论课程第一周","url":"/2022/07/19/alo1/","content":"\n<p align=\"center\">\n    <img src=\"https://img2.baidu.com/it/u=3208607819,4065748703&fm=253&fmt=auto&app=138&f=PNG?w=670&h=500\" style=\"zoom: 100%;\" />\n</p>\n\n> 课程链接：https://ocw.mit.edu/6-006S20\n\n<!--more-->\n\n## Introduction\n\n本课程的目的是教你去解决计算问题，同时证明的解决方案是正确和搞笑的。\n问题：\n\n+ 从问题输入到正确输出的二元关系\n+ 提供正确输出必须满足的条件\n+ 非一般情况：\n  + 例子：在这个房间里，有生日一样的同学吗\n+ 一般情况：\n  + 例子：给定任意$n$个学生，有没有生日相同的同学\n    算法\n+ 将每个输入映射到单个(确定)输出的过程\n+ 算法解决了问题当对于每一个问题输入他都能返回正确的输出\n+ 例子：一个解决生日匹配的算法\n  + 保存生日和姓名的记录(初始为空)\n  + 以一种顺序采访每一位同学\n    + 如果生日存在在记录中，返回找到了匹配队\n    + 否则将生日和姓名添加到记录中\n  + 返回空如果最后一个通过也没有匹配成功\n\n正确性：\n\n+ 程序/算法有固定的大小，所以如何证明其正确性\n+ 对于小的输入，可以分析例子\n+ 对于任意大的输入，算法必须是可以递归或循环的\n+ 必须使用归纳(induction)\n+ 例子：证明生日匹配算法的正确性\n  + 在$k$上归纳：记录的同学的人数\n  + 假设(Hypothesis)：如果前$k$个存在匹配，在采访第$k+1$个学生之前返回匹配\n  + Base case：$k=0$时，$k$没有包含匹配\n  + 假设归纳假设对于$k=k^\\prime$成立，考虑$k^\\prime+1$\n  + 如果前$k^\\prime$个存在一个匹配，那么在归纳之前就已经返回了\n  + 如果前$k^\\prime$没有匹配，因此如果前$k^\\prime+1$个存在匹配，匹配包括$k^\\prime+1$\n  + 算法直接检查学生$k^\\prime+1$在是否在前$k^\\prime$出现\n\n效率(Efficiency)：\n\n+ 一个算法产生一个正确输出有多快\n  + 可以衡量时间，但是时间与机器的性能有关\n  + 可以衡量具有固定时间操作的个数\n  + 与输入大小有关系：越大说明需要更长时间\n  + 是高效的如果是输入大小的多项式函数\n  + 有些时候对于某个问题不存在高效的算法\n+ 渐进符号：忽略常数项和低阶项\n  + 上界$\\mathcal{O}$，下界$\\Omega$，严格界限$\\Theta$\n  + 以下时间估计基于`1GHz`单核机器上每个周期的一次操作\n\n$$\n\\begin{array}{|c|c|c|c|c|c|c|c|}\n\n\\hline \\text { input } & \\text { constant } & \\text { logarithmic } & \\text { linear } & \\text { log-linear } & \\text { quadratic } & \\text { polynomial } & \\text { exponential } \\\\\n\n\\hline n & \\Theta(1) & \\Theta(\\log n) & \\Theta(n) & \\Theta(n \\log n) & \\Theta\\left(n^{2}\\right) & \\Theta\\left(n^{c}\\right) & 2^{\\Theta\\left(n^{c}\\right)} \\\\\n\n\\hline 1000 & 1 & \\approx 10 & 1000 & \\approx 10,000 & 1,000,000 & 1000^{c} & 2^{1000} \\approx 10^{301} \\\\\n\n\\hline \\text { Time } & 1 n s & 10 n s & 1 \\mu s & 10 \\mu s & 1 m s & 10^{3 c-9} s & 10^{281} \\text { millenia } \\\\\n\n\\hline\n\n\\end{array}\n$$\n\n计算模型：\n\n+ 说明可以在$\\mathcal{O}(1)$时间内在机器上执行哪些操作\n+ 这类模型被称为`Word-RAM`\n+ Machine word：$w$位的一个块($w$为一个$w$-bit Word-RAM的字长)\n+ Memory：可以寻址的字长序列\n+ 处理器支持很多常数时间的操作，在$\\mathcal{O}(1)$字数\n  + 正数算数：$+,-,*,//,\\%*$\n  + 逻辑运算：``&&，||，!，==，<,>,<=,=>`\n  + 比特算数：`&,|,<<,>>`\n  + 给定word$a$，可以在读取地址$a$的字，写数据到地址$a$\n+ 内存地址必须可以访问在内存中的任何位置\n  + 需要$w\\ge$表示最大内存地址的位数\n  + $32$位的最大为$4$GB内存，$64$位的最大为$16$EB\n\n数据结构\n\n+ 数据结构是存储无限值数据的一种方式，并支持一系列操作\n+ 一系列操作的集合被称为接口(interface)\n  + 序列\n  + 集合\n+ 数据结构可能使用不同的方法实现相同的接口\n+ 例子：静态列表：固定长度，静态序列接口\n  + StaticArray(n)：分配大小为$n$的静态数组以$\\Theta(n)$的时间\n  + StaticArray.get_at(i)：返回索引为$i$的数据以$\\Theta(1)$的时间\n  + StaticArray.set_at(i,x)：将索引为$i$的位置写入数据$x$以$\\Theta(1)$的时间\n\n~~~Python\ndef birthday_match(students):\n\t'''\n\tFind a pair of students with the same birthday\n\tInput: tuple of student(name, bday) tuples\n\tOutput: tuple of student names or None\n\t'''\n\tn = len(students)         #O(1)\n\trecord = StaticArray(n)   # O(n)\n\tfor k in range(n):        # n\n\t\t(name1, bay1) = students[k] # O(1)\n\t\tfor i in range(k):    # k\n\t\t\t(name2, bday2) = record.get_at(i)   #O(1)\n\t\t\tif bday1 == bday2:       # O(1)\n\t\t\t\treturn (name1, name2)\n\t\t\trecord.set_at(k ,(name1, bday1))    # O(1)\n\treturn None\n~~~\n\n时间复杂度为：\n$$\nO(n) + \\sum_{k=0}^{n-1}(O(1)+k\\cdot O(1)) = O(n^2)\n$$\n为$n$的多项式函数，所以该算法是高效的。\n\n### Asymptotic Notation\n\n下面我们详细地说明一下渐进符号：\n$O(f(n))$表示定义域为自然数满足下列性质的函数的集合：\n$O$符号：非负函数$g(n)$属于$O(f(n))$当且仅当存在正实数$c$和正整数$n_0$使得对于所有的$n\\ge n_0$都有$g(n)\\le c\\cdot f(n)$。\n$\\Omega$符号：非负函数$g(n)$属于$\\Omega(f(n))$当且仅当存在正实数$c$和正整数$n_0$使得对于所有的$n\\ge n_0$都有$c\\cdot f(n)\\le g(n)$。\n$\\Theta$符号：非复函数$g(n)$属于$\\Theta(f(n))$当且仅当$g(n)\\in O(f(n))\\cap \\Omega(f(n))$。\n\n## DATA STRUCTURES AND DYNAMIC ARRAYS\n\n首先我们先区分一下接口(Interface(API/ADT))和数据结构(Data Structure)。\n接口相当于是规范，他告诉我们可以存储什么样的数据，而数据结构会给你一个实际的表示(representation)来告诉你如何去存储数据。接口告诉我们支持什么样的操作，数据结构给我们实现这些操作的算法。你可以把结构理解为问题，而数据结构则是解决方案。\n\n我们将会讨论两类重要的接口：\n\n+ 集合(set)\n+ 序列(sequence)\n  我们也将会讨论两个主要的数据结构的工具：\n+ 数组(arrays)\n+ 基于指针的(pointer based)\n\n### Static sequence interface\n\nstatic sequence interface：存储序列$x_0,x_1,\\cdots,x_{n-1}$，并且支持以下操作：\n\n+ build(X)：对$X$中的元素创建一个新的数据结构\n+ len(X)：返回$X$的长度$n$\n+ iter_seq()：以序列的顺序输出$x_0,x_1,\\cdots,x_{n-1}$\n+ get_at(i)：或者$X$中索引为$i$的$x_i$\n+ set_at(i,X)：将$x_i$设置为$x$\n  解决这一问题的数据结构我们称为static array：数组中的数据在内存中的存储是连续的。这就意味着当我们访问数组中的某个索引的元素就等价于访问内存中数据的初始位置加上元素在数组中内存的索引的位置。同时我们假设对数组数据访问花费的时间是常数（这要求机器的字节数(现在一般为$64$位，可以一次读取$2^{64}$字节的数据)$w\\ge \\log n$）。也就意味着我们在`get_at`、`len`和`set_at`操作花费的时间是$\\mathcal{O}(1)$，而在`build`和`iter_seq`操作花费时间为$\\mathcal{O}(n)$。\n  我们是如何创建静态数组的呢？有很多的内存分配模型(Memory allocation model)：在这里我们假设分配一个长度为$n$的数组耗费的时间为$\\mathcal{O}(n)$，这样还可以得到我们消耗的空间为$\\mathcal{O}(\\text{time})$。\n\n### Dynamic sequence interface\n\n他是静态数组加上下面几个操作：\n\n+ insert_at(i,x)：在索引$i$处插入$x$，后面的以此后移\n+ deleta_at(i)：删除索引处$i$的数据，后面的依次前移\n+ insert_first(x)：将x插入第一个位置\n+ delete_first(x)：删除并返回第一个元素\n+ insert_last(x)：将x添加到最后一个位置\n+ delete_last(x)：删除并返回最后一个元素\n\n链表指的是前一个对象除了存储自己的数值外，还存储下一个对象的地址。静态数组指的是长度不会改变的数组。下面我们比较一下两者。\n当我们在数组的第一个位置插入或删除元素时，静态列表需要所有的元素都向后或向前移动，耗费的时间复杂度为$\\mathcal{O}(n)$，而由于静态数组的长度是不能变换的，所以在末尾添加元素需要我们将原有的数组重新复制一遍，耗费的时间复杂度为$\\mathcal{O}(n)$；而链表在第一个位置插入元素只需要将头指针指向要插入的元素，将要插入的元素储存的地址指向原来的第一个元素的地址，删除第一个元素的话只需要将头指针指向第二个元素的地址，时间复杂度为$\\mathcal{O}(1)$，但是当需要访问数据时，我们需要这个寻址，耗费的时间复杂度为$\\mathcal{O}(n)$。\n可以看出静态数组在非动态操作(查询元素等)的时间复杂度要比链表低，而链表的动态操作(插入、删除元素)的时间复杂度比较低，现在我们将两者擅长的操作结合在一起，即得到动态数组。\n\n### Dynamic arrays\n\n动态数组在Python中即为列表(list)。\n动态数组放松了数组大小必须为$n$的约束，我们令数组的大小为$\\Theta(n)\\ge n$。\n动态数组的头包含了数组的首地址、存储的数据的数量和数组的大小，当我们在数组最后插入元素时，如果数组的大小还够，直接在后面插入即可，但是如果此时数组的大小正好等于数组中元素的大小，我们需要重新分配一个数组，其大小为原来的$2$倍或者其他，也可以为原来的大小$+5$，但是当我们这样做时，我们需要每$5$次都重新分配一次内存，但是相比较于静态数组每一次都要重新分配一下内存还是好的。\n假设我们有一个空的动态列表，我们每一次往里面加入元素，第一次加入一个我们将去大小扩充为$1$，第二次加入元素，我们将其大小扩充为$2$，第三次加入元素我们将其扩充为$4$，第四次加入不扩充，第五次加入元素扩充为$8$，这样以此类推，那么对于长度的$n$的数组我们花费的时间复杂度为：\n$$\n\\sum_{i=1}^{\\log_2 n}2^i = \\mathcal{O}(n)\n$$\n可以看出其时间复杂度为线性的。我们将这种现象称为Amortization：如果任何$K$个操作耗费的时间复杂度$\\le K\\cdot T(n)$，那么每个操作的时间复杂度为$T(n)$。所以每次在最后面插入一个元素的时间复杂度为$\\mathcal{O}(1)$。\n\n所以我们最终的结果为：\n![](https://raw.githubusercontent.com/HFC666/image/master/img/alo1.jpg)","tags":["算法","编程"],"categories":["课程笔记"]},{"title":"B站课程Mathematica剩余部分","url":"/2022/07/18/math2/","content":"\n<p align=\"center\">\n    <img src=\"https://gimg2.baidu.com/image_search/src=http%3A%2F%2Finews.gtimg.com%2Fnewsapp_bt%2F0%2F13376661231%2F641&refer=http%3A%2F%2Finews.gtimg.com&app=2002&size=f9999,10000&q=a80&n=0&g=0n&fmt=auto?sec=1660719664&t=7a06182fcd2599240cf9b86397be591b\" style=\"zoom: 100%;\" />\n</p>\n\n\n>  B站课程链接：https://www.bilibili.com/video/BV1av411N7Xi?spm_id_from=333.999.0.0&vd_source=6177c61c946280bb88c727585de76bc8\n\n<!--more-->\n\n## 第四周\n\n### 重写规则\n\n`Mathematica`的第二原理：计算即重写。所谓重写，即将匹配成功的模式改为其他形式。Mathematica就是由一大堆重写规则组成的。\n\n重写规则的一般形式是：\n\n`(模式)\t(重写符号)\t(重写结果)`\n\n其中重写符号有四种：`->、:>、=、:=`，分别叫做规则、延迟规则、赋值、延迟赋值。重写结果则是一个表达式，它可以依赖重写符号左边被命名的子模式。\n\n规则与延迟规则是一种较为安全的重写机制，因为只有在你应用它们的时候它们才起作用。而赋值和延迟赋值则是添加到系统内部的一种重写规则，一旦添加进去就立刻起作用，并永远存在，除非你手动移除它们(`=.、Clear、Remove`)。\n\n最常见的规则出现在`Solve`类函数的返回值中：\n\n~~~julia\nresult = Solve[x^3 - 3 x + 2 == 0, x]\n{{x -> -2}, {x -> 1}, {x -> 1}}\n~~~\n\n`Solve`的结果是不唯一的，`Mathematica`并不知道我们打算用哪一个，所以只好将它们存储为规则，我们可以从中挑选出我们想要的，然后将它应用在其他地方。\n\n~~~mathematica\nExpand[x^4 + x^2 + 1 /. result[[1]]]\n> 21\n~~~\n\n我们也可以不挑选，这样得到一个由不同规则到来的结果构成的表。\n\n~~~julia\nExpand[x^4 + x^2 + 1 /. result]\n{21, 3, 3}\n~~~\n\n注意在上述操作时，`x`并未发生改变。\n\n规则比赋值灵活，因为赋值是定义全局变量。全局变量一旦重命名就很容易造成程序错误。这是`Mathematica`中最常见的一类错误。为了避免这种错误才有了这种模块化的机制。\n\n规则的左边可以是模式：\n\n~~~mathematica\nrule = (f[x_] -> x^2);\nf[5] + g[2] /. rule\n> 25 + g[2]\n~~~\n\n得到的结果与我们的期望一致，但是\n\n~~~julia\nx = 1;\nrule = (f[x_] -> x^2);\nf[5] + g[2] /. rule\n> 1 + g[2]\n~~~\n\n就会出现问题，这是因为规则直接将全局变量带入了，变成了：\n\n~~~mathematica\nrule\n> f[x_] -> 1\n~~~\n\n但是如果使用延迟规则：\n\n~~~julia\nx = 1;\nrule = f[x_] :> x^2;\nf[5] + g[2] /. rule\n> 25 + g[2]\n~~~\n\n规则和延迟规则(以及赋值和延迟赋值)的区别在于：规则指向的表达式会被`Mathematica`立刻取值，以后应用这条规则时将直接引用这个值；而延迟规则指向的表达式会被`Mathematica`认为是一个程序的入口，之后每次应用这条规则时，`Mathematica`都会重新计算一边这个表达式。\n\n当规则不止一条时，我们把所有规则组成一个表，然后应用它。\n\n~~~mathematica\n(a^b)^c /. {(x_^y_)^z_ :> x^(y z), a -> 2, b -> 3, c -> 4}\n> a^(b c)\n~~~\n\n我们发现只应用了第一个规则，其他规则都没用。这是因为匹配到规则表中的某个规则后，这个规则将会被应用一次，他之后的规则将被忽略。如果向反复匹配、应用，直到找不出可进一步化简的模式，可以用`//.`。\n\n~~~mathematica\nrule = {Log[x_ y_] :> Log[x] + Log[y]};\nLog[a b c d] /. rule\nLog[a b c d] /. rule /. rule\nLog[a b c d] //. rule\nFixedPoint[# /. rule &, Log[a b c d]]\n\n> Log[a] + Log[b c d]\n> Log[a] + Log[b] + Log[c d]\n> Log[a] + Log[b] + Log[c] + Log[d]\n> Log[a] + Log[b] + Log[c] + Log[d]\n~~~\n\n`FixedPoint`函数将一个函数应用到某个对象，直到这个对象不变为止，`.//`就是这样实现的。\n\n运算符`/.`和`//.`的全名分别是ReplaceAll和ReplaceRepeated，它们的一个共同特点是它们会对它们左边表达式的每个子表达式进行匹配和替换。有些时候我们希望控制这种搜索的深度，这时候就需要用Replace这个函数。\n\n~~~mathematica\nReplace[x^2, x^2 -> a^2]\nReplace[x^2, x -> a]\nReplace[x^2, x -> a, 2]\n\n> a^2\n> x^2\n> a^2\n~~~\n\nReplace只返回第一个匹配成功的重写结果，有时候我们希望考察所有可能的重写结果，这时候可以用ReplaceList这个函数。\n\n~~~mathematica\nReplace[a + b + c, x_ + y_ :> f[x, y]]\nReplaceList[a + b + c, x_ + y_ :> f[x, y]]\n\n> f[a, b + c]\n> {f[a, b + c], f[b, a + c], f[c, a + b], f[a + b, c], f[a + c, b], \n f[b + c, a]}\n~~~\n\n例：打印乘法表\n\n~~~mathematica\ntimes[x_, y_] := \n  ToString[x] <> \"x\" <> ToString[y] <> \"=\" <> ToString[x y];\n\n{\"1x1=1\", \"1x2=2\", \"1x3=3\", \"1x4=4\", \"1x5=5\", \"1x6=6\", \"1x7=7\", \\\n\"1x8=8\", \"1x9=9\", \"2x2=4\", \"2x3=6\", \"2x4=8\", \"2x5=10\", \"2x6=12\", \\\n\"2x7=14\", \"2x8=16\", \"2x9=18\", \"3x3=9\", \"3x4=12\", \"3x5=15\", \"3x6=18\", \\\n\"3x7=21\", \"3x8=24\", \"3x9=27\", \"4x4=16\", \"4x5=20\", \"4x6=24\", \"4x7=28\", \\\n\"4x8=32\", \"4x9=36\", \"5x5=25\", \"5x6=30\", \"5x7=35\", \"5x8=40\", \"5x9=45\", \\\n\"6x6=36\", \"6x7=42\", \"6x8=48\", \"6x9=54\", \"7x7=49\", \"7x8=56\", \"7x9=63\", \\\n\"8x8=64\", \"8x9=72\", \"9x9=81\"}timesTable[n_] := \n  times @@@ \n   Sort[ReplaceList[\n     Range[n], {{___, x_, ___} :> {x, x}, {___, x_, ___, \n        y_, ___} :> {x, y}}]];\ntimesTable[9]\n\n{\"1x1=1\", \"1x2=2\", \"1x3=3\", \"1x4=4\", \"1x5=5\", \"1x6=6\", \"1x7=7\", \\\n\"1x8=8\", \"1x9=9\", \"2x2=4\", \"2x3=6\", \"2x4=8\", \"2x5=10\", \"2x6=12\", \\\n\"2x7=14\", \"2x8=16\", \"2x9=18\", \"3x3=9\", \"3x4=12\", \"3x5=15\", \"3x6=18\", \\\n\"3x7=21\", \"3x8=24\", \"3x9=27\", \"4x4=16\", \"4x5=20\", \"4x6=24\", \"4x7=28\", \\\n\"4x8=32\", \"4x9=36\", \"5x5=25\", \"5x6=30\", \"5x7=35\", \"5x8=40\", \"5x9=45\", \\\n\"6x6=36\", \"6x7=42\", \"6x8=48\", \"6x9=54\", \"7x7=49\", \"7x8=56\", \"7x9=63\", \\\n\"8x8=64\", \"8x9=72\", \"9x9=81\"}\n~~~\n\n之前我们提到过`@@`可以换头，而`@@@`可以换内部的头：\n\n~~~mathematica\nhead @@@ f[g[x], h[x, y], k[a, b, c]]\n> f[head[x], head[x, y], head[a, b, c]]\n~~~\n\n规则(Rule、Delayed)和替换(Replace、ReplaceAll、ReplaceRepeated、ReplaceList)是比较安全的重写机制，下面我们看不安全的重写、即赋值。\n\n赋值与规则的一大不同是：赋值操作所定义的重写方法则总是从属于某个符号的，而规则是独立存在的。\n\n为什么会如此？因为如果不将全局规则(赋值)分派给某个符号，而是作为独立规则存在，那么在做任何计算的时候，都要检查以下待计算的表达式中是否有能够匹配全局规则的子表达式存在，这是极大的浪费。如果全局规则从属于某个符号，那么只有在遇到这个符号的时候我们才需要去尝试匹配，这就比较有效率。\n\n运算符`=.`每次清掉一条赋值，如果想要清空所有赋值，可以用`Clear`函数：\n\n~~~mathematica\nf[1] = 1;\nf[n_] := n f[n - 1];\nf[1] =. (*只清除f[1]*)\nClear[f] (*清空所有f*)\nClear[\"Global`*\"](*清空所有变量*)\n~~~\n\nClear的作用是清掉规则，之后的符号还存在于系统中，还占着内存。如果想完全移除这个变量，将它占用的内存释放掉，则应该用Remove。\n\n赋值行为也可以以其他的形式出现：\n\n~~~mathematica\ni=1;\nx=1;\ni++\ni--\n++i\n--i\ni += di\ni -= di\nx *= c\nx /= c\n\n> 1\n> 2\n> 2\n> 1\n> 1+ di\n> 1\n> c\n> 1\n~~~\n\n赋值还可以同时进行\n\n~~~mathematica\nx = y = value\n{x, y} = {value1, value2}\n{x, y} = {y, x}\n~~~\n\n有一些函数可以改变其参数的值：\n\n~~~mathematica\nlist = {a, b, c};\nAppendTo[list ,d]\nPrependTo[list, z]\n~~~\n\n再次强调一下，AppendTo和PrependTo这两个函数是比较慢的。对于比较长的表，尽量不要反复用它们。\n\n当习惯于C语言编程的人编写Mathematica程序的时候，总是喜欢用表来存储数组。\n\n事实上在Mathematica中，更好的做法是把数组视为从自然数到其他集合的函数：\n\n~~~mathematica\na[1] = 1;\na[n_] := n/2 /; EvenQ[n];\nArray[a, 10]\n> {1, 1, a[3], 2, a[5], 3, a[7], 4, a[9], 5}\n~~~\n\n这样的好处是，函数对象不需要初始化，也不需要指定尺寸。\n\n另外，函数的自变量可以是任何东西，所以可以实现一些更有趣的结构。\n\n~~~mathematica\n张三[身高] = 1.75 m;\n张三[体重] = 70 kg;\n谁_[BMI] ^:= 谁[体重] / 谁[身高]^2;\n张三[BMI]\n> (22.8571 kg)/m^2\n~~~\n\n这种写法实际上体现了一种面向对象编程的想法，下面是另一种更花哨的写法：\n\n~~~mathematica\n张三 /: 张三.身高 = 1.75 m;\n张三 /: 张三.体重 = 70 kg;\nBMI /: (谁_).BMI := (谁.体重)/(谁.身高)^2\n张三.BMI\n> (22.8571 kg)/m^2\n~~~\n\n新出现的两个运算符`^:=`和`/:`我们稍后再解释。\n\n赋值和规则的另一个重大不同是：多次赋值产出的规则表是按照从特殊到一般的顺序来应用的，而且与它们出现的顺序无关；规则表则不同，应用顺序等于它们在表中的排列顺序。\n\n~~~mathematica\nf[1] = 1;\nf[n_] := n + 1;\n{f[1], f[2]}\n> {1, 3}\n\ng[n_] := n + 1;\ng[1] = 1;\n{g[1], g[2]}\n> {1, 3}\n~~~\n\n在这里`f[1]=1`和`g[n]=1`为特殊规则，故不管顺序如何，结果都是`{1,3}`。\n\n~~~mathematica\n{h[1], h[2]} /. {h[1] -> 1, h[n_] :> n + 1}\n{h[1], h[2]} /. {h[n_] :> n + 1, h[1] -> 1}\n> {1, 3}\n> {2, 3}\n~~~\n\n而规则则与顺序有关。\n\n如果要使用很长的规则表`{p1->r1,p2->r2,......}`，最好用`Mathematica`内部函数`Dispatch`压缩一下，这样可以提高替换的效率。\n\n~~~mathematica\nrules = Table[x[i] -> RandomInteger[{1, i}], {i, 10000}];\ndispatch = Dispatch[rules];\nTiming[Table[x[i] /. rules, {i, 10000}];]\nTiming[Table[x[i] /. dispatch, {i, 10000}];]\n\n> {2.125, Null}\n> {0.015625, Null}\n~~~\n\n`Mathematica`中同时赋值、延迟赋值和全局规则等原理可以帮助我们实现一种非常优雅的机制。\n\n~~~mathematica\nf[0] = 1;\nf[1] = 1;\nf[n_] := f[n - 1] + f[n - 2];\nTiming[f[20]]\n> {0.015625, 10946}\n~~~\n\n~~~mathematica\ng[0] = 1;\ng[1] = 1;\ng[n_] := g[n] = g[n - 1] + g[n - 2];\nTiming[g[20]]\n> {0., 10946}\n~~~\n\n可以看到比之前快了很多。在后一种实现中，因为多了一个同时赋值，所以之前计算过的值都被自动存储下来，这对于递归函数来说非常重要，因为可以减少大量的重复计算。\n\n我们之前说过，赋值这个规则总是从属于某个符号的。一个重要的问题是，到底从属于哪个符号？\n\n对于下面的语句\n\n~~~mathematica\nF[X1,X2,...,Xn]=value\n~~~\n\nMathematica默认这样一条规则是从属于符号F。\n\n但这未必是我们想要的，例如前面张三的BMI这个例子中，左边表达式的头\"谁_\"是一个模式，而不是固定的符号，这时候就无法分配这条规则。另一种情况是：\n\n~~~mathematica\nlog[x_] + log[y_] := log[x y]\n~~~\n\n因为左边表达式的头为`Plus`，我们无法改变其规则。尽管我们可以通过解除保护的方法强行对函数Plus增加一条规则，但这不是正确的做法。因为这样做的后果是，系统每次进行加法运算都不得不检查待计算的表达式是否符合我们增加的这条规则，这是极大的资源浪费。\n\n对于这两种情况，我们都需要一种机制，来指定规则所从属的对象。例如在第一个例子中，我们可以将这条规则指定给BMI这个符号，在第二个例子中，我们可以将规则给自定义函数log。\n\n~~~mathematica\nlog[x_] + log[y_] ^:= log[x y];\n?log\n>  UpValue Definitions\tlog[x_]+log[y_]^:=log[x y]\n~~~\n\n可以看到这条规则附属于log。\n\n一般来说，运算符`^:=`会将这条规则指定给下层的所有不受保护的函数，此时赋值号右边的值叫做这些函数的`upvalue`。\n\n~~~mathematica\nf[g[x_], h[y_]] ^:= x^2 + Sin[y];\n~~~\n\n此时规则挂在`f`下的函数`g`和`h`上。\n\n如果我们想要更明确地指派规则，可以用：\n\n~~~mathematica\ng /: f[g[x_], h[y_]] := x^2 + Sin[y];\n~~~\n\n这样规则就只定义在`g`上。\n\n但是需要注意的是，这种规则指派只能改动一层，如果再深一层就不被允许了。\n\n~~~mathematica\ng/:f[h[g[x_]]]:= x+1;(*出错*)\n~~~\n\n## 第五周\n\n### 函数与泛函编程\n\n前缀、中缀和后缀：\n\n~~~mathematica\nf[x]\nf@x\n> f[x]\n> f[x]\n\nf[x, y]\nx~f~y\n> f[x,y]\n> f[x,y]\n\nf[g[x]]\nf@g[x]\nf@g@x\n> f[g[x]]\n> f[g[x]]\n> f[g[x]]\n\nx // g // f\n> f[g[x]]\n~~~\n\n#### 函数的属性\n\n`Orderless`：交换性\n\n~~~mathematica\nSetAttributes[f, Orderless]; f[x, y, z] == f[y, z, x]\n> True\n~~~\n\n`Flat`：结合性\n\n~~~mathematica\nSetAttributes[f, Flat]; f[x, f[y, z]] = f[f[x, y], z] == f[x, y, z]\n> True\n~~~\n\n`OneIdentity`\n\n~~~mathematica\nSetAttributes[f, OneIdentity]; MatchQ[a, f[x_: 0, y_]]\n> True\n~~~\n\n`Listable`\n\n~~~mathematica\nSetAttributes[f, Listable]; f[{1, 2, 3}]\n> {f[1], f[2], f[3]}\n~~~\n\n#### 匿名函数\n\n自定义筛选函数\n\n~~~mathematica\nS = Normal@Series[Cos[x]/(x^4 Tan[x]), {x, 0, 5}]\n> 1/x^5 - 5/(6 x^3) + 67/(360 x) - (19 x)/3024 + (247 x^3)/604800 + (\n 89 x^5)/4790016\n \nPlus @@ Cases[S, a_. x^d_ /; d < 0]\nSelect[S, (# /. {a_. x^d_ :> d}) < 0 &]\nSelect[S, MatchQ[#, a_. x^d_ /; d < 0] &]\n~~~\n\n#### 带下标的函数\n\n~~~mathematica\np = Plus @@ ((g @@ #))[u[0]] Times @@ u /@ # & /@ Partitions[5]\n> {u[0] u[5], u[0] u[1] u[4], u[0] u[2] u[3], u[0] u[1]^2 u[3], \n u[0] u[1] u[2]^2, u[0] u[1]^3 u[2], u[0] u[1]^5}\n~~~\n\n#### 函数的函数\n\n~~~mathematica\nf = Function[x, Function[y, x + y]];\nf[2]\nf[2][2]\n> Function[y$, 2 + y$]\n> 4\n~~~\n\n这里`y$`表示一个临时变量。\n\n#### 泛函操作\n\n~~~mathematica\nMap[f, {a, b, c}]\nf /@ {a, b, c}\n\n> {f[a], f[b], f[c]}\n> {f[a], f[b], f[c]}\n~~~\n\n关于MapIndexed，不止可以得到映射后的值，还会得到索引。\n\n~~~mathematica\nMapIndexed[f, {a, b, c}]\n> {{f[1, a]}, {f[2, b]}, {f[3, c]}}\n(*还可以指定深度*)\nMapIndexed[f, {{a, b, c}, {e, f}}, {2}]\n> {{{f[1, a], f[1, a]}, {f[1, b], f[2, b]}, {f[1, c], \n   f[3, c]}}, {{f[2, e], f[1, e]}, {f[2, f], f[2, f]}}}\n~~~\n\n例子\n\n~~~mathematica\nL = {a, b, c, d};\nMapIndexed[\n Print[\"The position of \", #1, \" in the List \", \" is \", #2[[1]], \n   \".\"] &, L\n ]\n > The position of a in the List  is 1.\n > The position of b in the List  is 2.\n > The position of c in the List  is 3.\n > The position of d in the List  is 4.\n~~~\n\n其中`#1`表示函数的第一个参数，`#2`表示第二个参数。\n\n`MapThread`函数：\n\n~~~mathematica\nMapThread[f, {{a, b, c}, {p, q, r}, {x, y, z}}]\n(*注意每一个列表必须同样大小*)\n> {f[a, p, x], f[b, q, y], f[c, r, z]}\n~~~\n我们也可以用已有的函数实现：\n~~~mathematica\nf @@@ Transpose[{{a, b, c}, {p, q, r}, {u, v, w}}]\n> {f[a, p, u], f[b, q, v], f[c, r, w]}\n~~~\nScan函数也有类似于Map函数的用法，但是存在一点不同：\n~~~mathematica\nScan[Print[\"Hey! I'm \", #] &, {a, b, c}]\n\n> Hey! I'm a\n> Hey! I'm b\n> Hey! I'm c\n\nMap[Print[\"Hey! I'm \", #] &, {a, b, c}]\n> Hey! I'm a\n> Hey! I'm b\n> Hey! I'm c\n> {Null, Null, Null}\n~~~\nScan不关心中间的返回值。\nThrough函数：\n~~~mathematica\nThrough[(f + g + h)[x, y]]\nThrough[f[g, h][x, y]]\n> f[x, y] + g[x, y] + h[x, y]\n> f[g[x, y], h[x, y]]\n~~~\n#### 函数迭代\n~~~mathematica\nNest[f, x, 3]\nNestList[f, x, 3]\n> f[f[f[x]]]\n> {x, f[x], f[f[x]], f[f[f[x]]]}\n~~~\n#### 函数不动点\n~~~mathematica\nf[x_] := N[(x + 2/x)/2, 20]\nFixedPoint[f, 1]\n> 1.4142135623730950488\nFixedPointList[f, 1]\n> {1, 1.5000000000000000000, 1.4166666666666666667, \\\n1.4142156862745098039, 1.4142135623746899106, 1.4142135623730950488, \\\n1.4142135623730950488}\n~~~\n#### 条件迭代\n~~~mathematica\nNestWhile[#^2 &, 2, (# < 10^10) &]\nNestWhileList[#^2 &, 2, (# < 10^10) &]\n> 18446744073709551616\n> {2, 4, 16, 256, 65536, 4294967296, 18446744073709551616}\n~~~\n条件迭代也可以用于求解不动点\n~~~mathematica\nf[x_] := N[(x + 3/x)/2, 20]\nNestWhile[f, 1, Unequal, 2]\n> 1.7320508075688772935\n\nNestWhileList[Mod[3 #, 57] &, 1, Unequal, All]\n> {1, 3, 9, 27, 24, 15, 45, 21, 6, 18, 54, 48, 30, 33, 42, 12, 36, 51, \\\n39, 3}\n~~~\n#### 折叠运算\n~~~mathematica\nFold[f, x, {a, b, c}]\nFoldList[f, x, {a, b, c}]\n\n> f[f[f[x, a], b], c]\n> x, f[x, a], f[f[x, a], b], f[f[f[x, a], b], c]}\n~~~\n## 第六周\n\n### 模块化、语境和包\n在Mathematica中，变量默认为全局变量。这是因为大多数Mathematica程序都比较短小，因此只要靠用户自己维护就可以避免全局变量重名带来的各种问题。但是有些时候人们也需要编制大型的Mathematica程序，这时候就需要将一些全局变量隔离、打包的机制，这就是今天要将的模块化、语境和包等概念。\n模块化：Module\n~~~mathematica\ny = 3;\nf[x_] := Module[{y}, y = x + 1; x/y];\nf[10]\ny\n\n10/11\n3\n~~~\nModule的作用是定义一个封闭的环境。其中存在一些局部变量(由第一个参数(必须是一个表)的元素组成)。当Module表达式求值完毕之后，这个环境就不存在了，其中的局部变量自然也会消失。\n~~~mathematica\nprint := Module[{y}, y]\nprint\n> y$14115\n~~~\n在Module中，局部变量`y`被替换为带有内部编号的一种变量`y$num`，其中`num`是一个自然数，取自Mathematica内部的一个计数变量`$ModuleNumber`。每次调用Module或者具有类似功能的结构时都会增加`$ModuleNumber`，于是`y$`后面的数字永远不相同，所以也就不会有重名问题。\n我们可以用`Unique`生成计数器：\n~~~mathematica\ny = Unique[x]\ny = Unique[x]\ny = Unique[x]\n\nx$14690\nx$14691\nx$14692\n~~~\n注意，带`$`和编号的变量具有Temporary属性：\n~~~mathematica\nModule[{x}, Print[x, \" has attributes \", Attributes[x]]]\n> x$15746 has attributes {Temporary}\n~~~\n我们可以使用`?x*`来查看`x`的临时属性。\n带有临时属性的变量在生存期结束时就会被系统`Remove`，除非它们生存期内被显式地返回给外部的全局环境。\n我们下面考察以下另一种结构`with`来与`Module`做一下对比：\n~~~mathematica\nWith[{n}, Print[n]]\nWith[{n = 5}, Print[n]]\nWith[{n = a}, Print[n]]\n\n> With[{n}, Print[n]]\n> 5\n> a\n~~~\n~~~mathematica\nWith[{n = 5}, n = 4; Print[n]]\n(*打印5，警告，n不能再被赋值了*)\n\n{n = 4; Print[n]} /. {n -> 5}\n> 4\n(* 因为是先打印，再替换，所以打印出的是4 *)\n\nReleaseHold[Hold[(n = 4; Print[n])] /. {n -> 5}]\n> 5\n(*首先先将(n = 4; Print[n]) hold住，使其变为一个表达式而不执行，替换完成后释放hold执行*)\n~~~\n\n~~~mathematica\nWith[{n = 5}, Print[n]]\nPrint[n]\n\n> 5\n> n\n~~~\nWith的作用是定义一个封闭的环境，其中存在一些局部变量。在With的第二个参数表达式中所有这些常量都会直接替换成第一个参数中赋值语句右边的值。在这个环境中，局部常量的值是不变的。\n~~~mathematica\na = x;\nWith[{n = a}, n = b; Print[n]]\na\nx\nb\n> b\n> b\n> b\n> b\n~~~\n注意，在这个例子中，`a`是一个外部环境的变量，Mathematica的变量实际上都是指针。所以，在With内部，n是与a相等的指针，当我们写n=b时，实际上是在写a=b，它的作用是把a的值换成b的值，n和a始终始终是指向同一个地方，所以\"n\"仍是常量。\n但是如果变成`n=1;n=b`就会警告，因为如果`n=1`，那么我们就无法将常量`1`赋值为其他的值。\nModule和With的内部环境叫做相应的局部变量或局部变量的作用域。下面我们看几种隐式出现的作用域：\n~~~mathematica\nf = Function[x, Function[y, x + y]];\nf[y]\n> Function[y$, y + y$]\n~~~\n在这个例子中，`f`的函数体部分因为某种外部原因，这是Mathematica就会把内层的变量`y`自动命名为`y$`，这是一个临时变量，他会在函数定义结束后被销毁。另一个自动改名的例子：\n~~~mathematica\nWith[{w = x}, h[x_] :> w + x]\n> h[x$_] :> x + x$\n~~~\nFunction定义的是纯函数，或者叫匿名函数、$\\lambda-$表达式，它们的参数是所谓的哑变量，可以随意换名字而不影响定义。Mathematica在这里做的正是这样一件事，其原因是为了避免与可能存在的全局变量重名。\n除了Module、With、Function以外，各种赋值，规则中的模式名称(x_、x__、x:pattern)也具有自动改名的功能，这些赋值、规则语句就成为这些改名后的临时变量的作用域。注意，常量是不能自动改名的。\n~~~mathematica\nx =  1; y = a; f[x_] := (x = 2); g[y_] := (y = 2);\n{f[x], g[y], x, y}\n> {2,2,1,2}\n(*第一个居于报错，因为我们想赋值1=2*)\n~~~\n再看一些哑变量的例子：\n~~~mathematica\ni = 50; x = a;(*此例子中x必须是变量，不能是常数，否则不能做积分变量*)\n{Sum[i, {i, 1, 100}], Integrate[x, {x, 0, 1}]}\n{i, x}\n> {5050, 1/2}\n> {50, a}\n~~~\n~~~mathematica\np[n_] := Integrate[f[x] x^n, {x, 0, 1}]\np[x](*局部变量与全局变量混淆*)\nq[n_] := Module[{x}, Integrate[f[x] x^n, {x, 0, 1}]]\nq[x]\n~~~\n\nModule和With都是通过改名的办法将作用域内的变量变为与外部变量不同的局部变量。有的时候我们需要另外一种隔离手段，我们想要构造一个作用域，其中局部变量与全局变量具有相同的\"名字\"和不同的\"值\"，这就要用Block来做隔离。\n~~~mathematica\nModule[{x = a + 1}, x^2 + 3]\nBlock[{x = a + 1}, x^2 + 3]\n> 3 + (1 + a)^2\n> 3 + (1 + a)^2\n~~~\n~~~mathematica\ny = x^2 + 3;\nModule[{x = a + 1}, y]\nBlock[{x = a + 1}, y]\n\n> 3 + x^2\n> 3 + (1 + a)^2\n~~~\n我们可以换一种角度来解释两者的不同。在上面的例子中，Module里的局部变量只生存在Module内，它不会跟踪y的定义跑出到Module外面去；另一方面，Block里的局部变量x会跟踪出去，找出y中出现的所有x，然后替换，所以它的作用域可以从Block延伸出去，侵入到y的作用域中。上面提到的积分变量必须是变量是因为积分是用`Block`实现的。\n\n利用这些域规则我们可以创造出很对有趣的函数。\n闭包的例子：计数器产生器\n~~~mathematica\nCounterCreator[first_: 1, delta_: 1] := \n Module[{i = first - delta}, {i += delta} &]\ncounter1 = CounterCreator[];\ncounter2 = CounterCreator[];\ncounter3 = CounterCreator[19, 2];\n~~~\n这样就创建了三个计数器，各个计数器之间互不干扰。\n另一个闭包的例子，状态机：\n~~~mathematica\nStateMachine = \n  Module[{i}, \n   Function[func, i = 0; \n    Switch[func, 0, i &, 1, ++i &, 2, --i &, 3, (i = 0) &, _, \n     Print[\"Illegal function!\"]]]];\n \nread = StateMachine[0];\nup = StateMachine[1];\ndown = StateMachine[2];\nreset = StateMachine[3];\n~~~\n这四个相互影响。\n在各种程序语言中，还有一种显式指定作用域的方法。典型的例子就是C++语言中的命名空间(namespace)。在Mathematica中也有一种类似的机制，叫做语境(Context)。\nGlobal\\`是Mathematica的默认语境，所有用户自己定义的符号、函数都存在于这个语境中。用户也可以自己定义语境：\n~~~mathematica\nfoo`x = 2;\n(*语境也可以嵌套*)\nfoo`bar`x = 2\n~~~\n语境就像是文件系统中的文件夹，语境下面的嵌套语境就像文件夹下的子文件夹。语境中的符号、函数则像文件夹中的各种文件。无论如何，我们在Mathematica中总存在于一个语境中，它可以通过变量`$Context`访问。\n如果Mathematica遇到一个符号，他首先会在当前语境中查找这个符号的意义。如果找不到，则会在`$ContextPath`所指定语境路径按顺序查找这个符号。\n我们可以通过赋值改变当前语境，也可以向语境路径中添加我们的语境：\n~~~mathematica\n$Context = \"foo`\"\n$ContextPath = PrependTo[$ContextPath, \"bar\"]\n~~~\n函数`Context[]`可以查找当前语境或者某个符号所属的语境：\n~~~mathematica\nContext[]\nContext[x]\n~~~\n函数`Contexts`可以列出所有的语境，或者按通配符列出相应的语境：\n~~~mathematica\nContexts[\"System`**\"]\n~~~\n语境可以通过`Begin[\"Content\"]`和`End[]`进出：\n~~~mathematica\nBegin[\"新语境\"]\nPrint[x=1];\nEnd[]\n~~~\n我们后面介绍包(package)的概念、包都自带一个语境。如果导入了某个程序包，那么它的语境会自动添加为语境路径的第一个语境。\n~~~mathematica\nNeeds[\"Quaternions\"]\n$ContextPath\n~~~\n## 第七周\n### 优化技术\n要谈优化，首先要有测量：\n~~~mathematica\nTiming[FactorInteger[2^(2^8) + 1]]\nAbsoluteTiming[FactorInteger[2^(2^8) + 1]]\n\n{0.359375, {{1238926361552897, \n   1}, {93461639715357977769163558199606896584051237541638188580280321\\\n, 1}}}\n\n{0.365291, {{1238926361552897, \n   1}, {93461639715357977769163558199606896584051237541638188580280321\\\n, 1}}}\n~~~\n还有空间的度量\n~~~mathematica\npts = RandomReal[1, {10000, 2}];\ngr = Graphics[Point /@ pts];\ngrMulti = Graphics[Point[pts]];\n\nByteCount /@ {gr, grMulti}\n\n{1760128, 160248}\n~~~\n除此之外，还有`MemoryInUse`等。\n我们主要关心对时间的优化，有以下几条大的原则：\n1. 不要重复造轮子，尽量用Mathematica自己的函数。\n2. 不要自己写循环：要尽可能地将循环都转为表处理。\n3. 要善于用数学。\n4. 知道得越多，算得越快。\n5. 简单比复杂好\n6. 纯函数比模式匹配快\n### 字符串处理\n`StringLength`获取字符串长度、`StringJoin`连接字符串。`<>`字符串连接。\n`CharacterRange[\"a\",\"z\"]`：生成从\"a\"到\"z\"的ASSIC码。\n~~~mathematica\nalphabet = StringJoin[CharacterRange[\"a\", \"z\"]]\nStringTake[alphabet, 12]\nStringDrop[alphabet, 12]\nStringPart[alphabet, 12]\nStringTake[alphabet, {12, 16}]\nStringTake[alphabet, {12 ;; 16}]\n~~~\n字符串模式：\n~~~mathematica\nMatchQ[{a, b, c, d}, {___, x_, x_, ___}]\nMatchQ[{a, b, c, c, d}, {___, x_, x_, ___}]\n~~~\n\n~~~mathematica\nStringMatchQ[\"abcd\", ___ ~~ x_ ~~ x_ ~~ ___]\nStringMatchQ[\"abccd\", ___ ~~ x_ ~~ x_ ~~ ___]\n\nStringFreeQ[\"aabbccdd\", \"bc\" ~~ x_ ~~ \"d\"]\nStringCases[\"aabbccdd\", x_ ~~ x_]\nStringCases[\"aabbccdd\", x_ ~~ x_ :> x <> x <> x]\n\nStringPosition[\"aabbccdd\", x_ ~~ x_ ~~ y_ ~~ y_]\nStringCount[\"abcadcadcbqwertaac\", \"a\"]\nStringCount[\"abcadcadcbqwertaac\", \"a\" ~~ _ ~~ \"c\"]\n\nStringReplace[\"abbaabbaa\", \"ab\" -> \"X\"]\nStringReplace[\"ababbabbaaababa\", \"ab\" .. -> \"X\"]\n\nStringReplaceList[\"ccccc\", \"c\" -> \"XYX\"]\nStringReplaceList[\"abcdeabacde\", {\"abc\" -> \"X\", \"cde\" -> \"Y\"}]\nStringReplace[\"the cat in the hat\",  \n Except[Characters[\"aeiou\"]] -> \"\"]\n\nStringSplit[\"a bbb cccc aa d\"]\nStringSplit[\"a----bbb---ccc--dddd\", \"--\"]\nStringSplit[\"a----bbb---ccc--dddd\", \"-\" ..]\nStringSplit[\"a--.-bbb- -|-ccc--ddd\", (\"-\" | \".\" | \"|\" | \" \") ..]\n~~~\n字符串的完整形式：\n~~~mathematica\nFullForm[\"a\" ~~ _ ~~ \"b\"]\nFullForm[\"a\" ~~ __]\nFullForm[\"a\" ~~ ___]\n~~~\n\n~~~mathematica\nFullForm[\"a\" ..]\nFullForm[\"a\" ...]\n\nRepeated[\"a\"]\nRepeatedNull[\"a\"]\n~~~\n其他字符串模式：\n~~~mathematica\nStringSplit[\"a bbb \\t cc \\rcc aa \\n d\", Whitespace]\nStringSplit[\"a384bcibeyu198e7wr\", CharacterRange[\"0\", \"9\"]]\nStringSplit[\"a384bcibeyu198e7wr\", LetterCharacter]\nStringMatchQ[\"abaababb\", StartOfString ~~ \"a\" ~~ __]\nStringMatchQ[\"abaababb\", __ ~~ \"a\" ~~ EndOfString]\n~~~\n将一般的表达式转换为字符串搜索会提速：\n~~~mathematica\ntest = Range[1000];\ntest[[{50, 75}]] = 5;\nPosition[test, 5]\nMatchQ[test, {___, x_, ___, x_, ___, x_, ___}] // Timing\n\n> {{5}, {50}, {75}}\n> {3.71875, True}\n\nteststr = FromCharacterCode[test];\nStringPosition[teststr, FromCharacterCode[5]]\nStringMatchQ[teststr, \n  StringExpression[___, x_, ___, x_, ___, x_, ___]] // Timing\n\n> {{5, 5}, {50, 50}, {75, 75}}\n> {0.015625, True}\n~~~","tags":["编程","Mathematica"],"categories":["课程笔记"]},{"title":"Statistical Rethinking:Chapter5","url":"/2022/07/16/rt5/","content":"\n## Multivariate Linear Models\n\n![](https://raw.githubusercontent.com/HFC666/image/master/img/MLN1.png)\n\n<!--more-->\n\n美国平均每人多少华夫饼店与离婚率存在着相关关系，一般某个州平均每人每个华夫饼店的数量越多，离婚率越高，但是两者之间并没有明显的因果关系，主要是南部的州含有较多的华夫饼店并且不允许未婚同居。所以相关性在自然界非常常见，但是相关并不意味着因果关系，我们需要工具来区分单纯的关联和因果关系。这就是为什么如此多的统计工作致力于多元回归，使用多个预测变量来模拟输出变量。\n\n### Spurious association\n\n假设我们用结婚率和结婚时的年龄两个变量来预测离婚率，我们的模型为：\n$$\n\\begin{aligned}\n\nD_{i} & \\sim \\operatorname{Normal}\\left(\\mu_{i}, \\sigma\\right) \\\\\n\n\\mu_{i} &=\\alpha+\\beta_{R} R_{i}+\\beta_{A} A_{i} \\\\\n\n\\alpha & \\sim \\operatorname{Normal}(10,10) \\\\\n\n\\beta_{R} & \\sim \\operatorname{Normal}(0,1) \\\\\n\n\\beta_{A} & \\sim \\operatorname{Normal}(0,1) \\\\\n\n\\sigma & \\sim \\operatorname{Uniform}(0,10)\n\n\\end{aligned}\n$$\n下面我们拟合模型：\n\n~~~R\n# 归一化数据\nd$MedianAgeMarriage.s <- (d$MedianAgeMarriage-mean(d$MedianAgeMarriage))/\n  sd(d$MedianAgeMarriage)\n\nd$Marriage.s <- (d$Marriage - mean(d$Marriage))/sd(d$Marriage)\n\nm5.3 <- map(\n  alist(\n    Divorce ~ dnorm(mu, sigma),\n    mu <- a + bR*Marriage.s + bA*MedianAgeMarriage.s,\n    a ~ dnorm(10, 10),\n    bR ~ dnorm(0, 1),\n    bA ~ dnorm(0, 1),\n    sigma ~ dunif(0, 10)\n  ),\n  data = d\n)\nprecis(m5.3)\n\n       mean   sd  5.5% 94.5%\na      9.69 0.20  9.36 10.01\nbR    -0.13 0.28 -0.58  0.31\nbA    -1.13 0.28 -1.58 -0.69\nsigma  1.44 0.14  1.21  1.67\n~~~\n\n可以看到bR非常接近于零，而bA的值离零很远。我们可以说，一旦我们知道了一个州的结婚年龄的中位数，就几乎没有或没有额外的预测能力来了解该州的结婚率。\n\n我们也可以绘制多元后验，包括：\n\n+ Predictor residual plots：这些图展示输出值与预测残差值的关系\n+ Counterfactual plots\n+ Posterior prediction plots\n\n下面先看Predictor residual plots：\n在我们关于离婚率的多变量模型中，我们有两个预测变量(结婚率和结婚年龄的中位数)。为了计算每一个的预测残差，我们需要利用另一个预测变量对其进行建模，因此对于结婚率，下列模型是我们需要的：\n$$\n\\begin{aligned}\n\nR_{i} & \\sim \\operatorname{Normal}\\left(\\mu_{i}, \\sigma\\right) \\\\\n\n\\mu_{i} &=\\alpha+\\beta A_{i} \\\\\n\n\\alpha & \\sim \\operatorname{Normal}(0,10) \\\\\n\n\\beta & \\sim \\operatorname{Normal}(0,1) \\\\\n\n\\sigma & \\sim \\operatorname{Uniform}(0,10)\n\n\\end{aligned}\n$$\n\n~~~R\nm5.4 <- map(\n  alist(\n    Marriage.s ~ dnorm(mu, sigma),\n    mu <- a + b*MedianAgeMarriage.s,\n    a ~ dnorm(0, 10),\n    b ~ dnorm(0, 1),\n    sigma ~ dunif(0, 10)\n  ),\n  data = d\n)\nmu <- coef(m5.4)['a'] + coef(m5.4)['b']*d$MedianAgeMarriage.s\nm.resid <- d$Marriage.s - mu\n~~~\n\n得到残差，我们可以以它为横坐标，以离婚率为纵坐标绘制如下图所示的图：\n![](https://raw.githubusercontent.com/HFC666/image/master/img/MLN2.jpg)\n\n> 左边的图，垂直的虚线表示结婚率等于预测值(残差为零)，左边的区域表示结婚率低于期望值，而右边的区域表示结婚率高于期望值。左右两边的离婚率大小大致相等，所以说明结婚率和离婚率的关系较小。也可以将残差理解为结婚率在去除结婚年龄的中位数的影响后得到的东西。\n> 右边的图类似。\n\n\nCounterfactual plots：固定一个变量不变，其他变量对于输出变量的影响。\n\n~~~R\nA.avg <- mean(d$MedianAgeMarriage.s)\nR.seq <- seq(from=-3, to=3, length.out=30)\npred.data <- data.frame(\n  Marriage.s = R.seq,\n  MedianAgeMarriage.s=A.avg\n)\n\nmu <- link(m5.3, data = pred.data)\nmu.mean <- apply(mu, 2, mean)\nmu.PI <- apply(mu, 2, PI)\n\nR.sim <- sim(m5.3, data = pred.data, n=1e4)\nR.PI <- apply(R.sim, 2, PI)\n\nplot(Divorce ~ Marriage.s, data=d, type=\"n\")\nmtext(\"MedianAgeMarriage.s=0\")\nlines(R.seq, mu.mean)\nshade(mu.PI, R.seq)\nshade(R.PI, R.seq)\n\n~~~\n\n我们也可以绘制另一个变量的结果：\n\n~~~R\nR.avg <- mean( d$Marriage.s )\nA.seq <- seq( from=-3 , to=3.5 , length.out=30 )\npred.data2 <- data.frame(\nMarriage.s=R.avg, \nMedianAgeMarriage.s=A.seq\n)\nmu <- link( m5.3 , data=pred.data2 )\nmu.mean <- apply( mu , 2 , mean )\nmu.PI <- apply( mu , 2 , PI )\nA.sim <- sim( m5.3 , data=pred.data2 , n=1e4 )\nA.PI <- apply( A.sim , 2 , PI )\nplot( Divorce ~ MedianAgeMarriage.s , data=d , type=\"n\" ) \nmtext( \"Marriage.s = 0\" )\nlines( A.seq , mu.mean ) \nshade( mu.PI , A.seq ) \nshade( A.PI , A.seq )\n~~~\n\n结果为\n![](https://raw.githubusercontent.com/HFC666/image/master/img/MLN3.jpg)\n\n> 由此可以看出结婚率与离婚率几乎没有什么关系。\n\nPosterior prediction plots：\n\n~~~R\nmu <- link(m5.3)\nmu.mean <- apply(mu, 2, mean)\nmu.PI <- apply(mu, 2, PI)\n\ndivorce.sim <- sim(m5.3, n=1e4)\ndivorce.PI <- apply(divorce.sim, 2, PI)\n\nplot(mu.mean~d$Divorce, col=rangi2, ylim=range(mu.PI), xlab=\"Observed divorce\", ylab=\"Predicted divorce\")\nabline(a=0, b=1, lty=2)\nfor (i in 1:nrow(d)) {\n  lines(rep(d$Divorce[i],2), c(mu.PI[1,i], mu.PI[2,i]),col=rangi2)\n}\n~~~\n\n也可以用另一种方法绘制：\n\n~~~R\ndivorce.resid <- d$Divorce - mu.mean\no <- order(divorce.resid)\ndotchart(divorce.resid[o], labels = d$Loc[o], xlim=c(-6,5),cex=0.6)\nabline(v=0, col=col.alpha(\"black\",0.2))\nfor (i in 1:nrow(d)) {\n  j <- o[i]\n  lines(d$Divorce[j]-c(mu.PI[1,j], mu.PI[2,j]), rep(i,2))\n  points(d$Divorce[j]-c(divorce.PI[1,j], divorce.PI[2,j]), rep(i,2),\n         pch=3, cex=0.6, col=\"gray\")\n}\n~~~\n\n### Masked relationship\n\n下面我们将使用一个新的数据集：\n\n~~~R\ndata(\"milk\")\nd <- milk\n~~~\n\n我们考虑数据集中的如下三个变量：\n\n+ `kcal.per.g`：每克牛奶的卡路里热量\n+ `mass `：雌性平均体重，单位千克\n+ `neocortex.perc`：新皮质质量占大脑总质量的百分比\n  这里的问题是牛奶的能量含量（这里用千卡来衡量）与大脑新皮层的百分比有关。我们最终也需要雌性体重，以查看隐藏变量之间关系的掩蔽。\n  我们首先需要删除含有缺失值的行：\n\n~~~R\ndcc <- d[complete.cases(d),]\n~~~\n\n下面我们构建模型：\n\n~~~R\nm5.5 <- map(\n  alist(\n    kcal.per.g ~ dnorm(mu, sigma),\n    mu <- a + bn*neocortex.perc,\n    a ~ dnorm(0, 100),\n    bn ~ dnorm(0, 1),\n    sigma ~ dunif(0,1)\n  ),\n  data = dcc\n)\nprecis(m5.5)\n\n> precis(m5.5, digits = 3)\n       mean    sd   5.5% 94.5%\na     0.353 0.471 -0.399 1.106\nbn    0.005 0.007 -0.007 0.016\nsigma 0.166 0.028  0.120 0.211\n~~~\n\n可以用`neocortex.perc`来拟合模型得到的结果并不好，系数接近于0。下面我们用变量`mass`的对数试一下：\n\n~~~R\ndcc$log.mass <- log(dcc$mass)\nm5.6 <- map(\n  alist(\n    kcal.per.g ~ dnorm( mu , sigma ) ,\n    mu <- a + bm*log.mass ,\n    a ~ dnorm ( 0 , 100 ) ,\n    bm ~ dnorm ( 0 , 1 ) ,\n    sigma ~ dunif( 0 , 1 )\n  ) ,\n  data=dcc )\nprecis(m5.6)\n\n\n       mean   sd  5.5% 94.5%\na      0.71 0.05  0.63  0.78\nbm    -0.03 0.02 -0.06  0.00\nsigma  0.16 0.03  0.11  0.20\n~~~\n\n可以看到结果也不是很好，那么我们试试将两个变量同时加入：\n\n~~~R\nm5.7 <- map(\n  alist(\n    kcal.per.g ~ dnorm( mu , sigma ) ,\n    mu <- a + bn*neocortex.perc + bm*log.mass ,\n    a ~ dnorm ( 0 , 100 ) ,\n    bn ~ dnorm ( 0 , 1 ) ,\n    bm ~ dnorm ( 0 , 1 ) ,\n    sigma ~ dunif( 0 , 1 )\n  ) ,\n  data=dcc)\nprecis(m5.7)\n\n> precis(m5.7)\n       mean   sd  5.5% 94.5%\na     -1.08 0.47 -1.83 -0.34\nbn     0.03 0.01  0.02  0.04\nbm    -0.10 0.02 -0.13 -0.06\nsigma  0.11 0.02  0.08  0.15\n~~~\n\n可以看到当两个变量结合后其与预测变量的相关性都得到了提升。\n下面我们绘制在`log(mass)`固定的情况下其余两个变量之间的关系：\n\n~~~R\nmean.log.mass <- mean( log(dcc$mass) ) \nnp.seq <- 0:100\npred.data <- data.frame(\n  neocortex.perc=np.seq, \n  log.mass=mean.log.mass\n)\nmu <- link( m5.7 , data=pred.data , n=1e4 )\nmu.mean <- apply( mu , 2 , mean )\nmu.PI <- apply( mu , 2 , PI )\nplot( kcal.per.g ~ neocortex.perc , data=dcc , type=\"n\" ) \nlines( np.seq , mu.mean )\nlines( np.seq , mu.PI[1,] , lty=2 ) \nlines( np.seq , mu.PI[2,] , lty=2 )\n~~~\n\n![](https://raw.githubusercontent.com/HFC666/image/master/img/MLN4.jpeg)\n可以看到`neocortex.perc`与`kcal.per.g`的相关性得到了增强。为什么将新皮质和体重添加到同一模型中会导致两者的估计效应更大？在这种情况下，有两个变量与结果相关，但一个与结果正相关，另一个与结果负相关。此外，两个解释变量彼此呈正相关。结果，它们倾向于相互抵消。\n也就是说当两个变量单独与结果的关系较弱，但是两个变量之间存在着正相关，可以相互促进对结果的影响。\n\n### When adding variables hurts\n\n为什么在上面的例子中不使用所有的变量呢？这是因为使用所有的变量会存在以下问题：\n\n1. 多重共线性\n2. post-treatment bias\n3. 过拟合\n   多重共线性会导致与结果没有多少关联性的变量也会得到很大的关联性。\n   下面我们看一个例子，我们知道人的身高与其腿长存在关系，我们按照下述方式生成体长和腿长的数据：\n\n~~~R\nN <- 100 # number of individuals\nheight <- rnorm(N,10,2) # sim total height of each\nleg_prop <- runif(N,0.4,0.5) # leg as proportion of height\nleg_left <- leg_prop*height + # sim left leg as proportion + error\n  rnorm( N , 0 , 0.02 )\nleg_right <- leg_prop*height + # sim right leg as proportion + error\n  rnorm( N , 0 , 0.02 )\n# combine into data frame\nd <- data.frame(height,leg_left,leg_right)\n~~~\n\n在拟合模型之前我们预测体长与腿长的系数应该大约为体长的均值除以腿长比例的均值$10/4.5=2.2$，但是结果却为：\n\n~~~R\nm5.8 <- map(\n  alist(\n    height ~ dnorm( mu , sigma ) ,\n    mu <- a + bl*leg_left + br*leg_right ,\n    a ~ dnorm( 10 , 100 ) ,\n    bl ~ dnorm( 2 , 10 ) ,\n    br ~ dnorm( 2 , 10 ) ,\n    sigma ~ dunif( 0 , 10 )\n  ) , \n  data=d )\nprecis(m5.8)\n\n      mean   sd  5.5% 94.5%\na     1.16 0.32  0.64  1.67\nbl    0.02 2.23 -3.55  3.58\nbr    1.96 2.21 -1.58  5.50\nsigma 0.61 0.04  0.54  0.68\n~~~\n\n与我们的预测相差很大。\n这是因为左右腿的长度具有明显的共线性，我们可以将其看作同一个变量，即：\n$$\n\\begin{aligned}\n\n&y_{i} \\sim \\operatorname{Normal}\\left(\\mu_{i}, \\sigma\\right) \\\\\n\n&\\mu_{i}=\\alpha+\\left(\\beta_{1}+\\beta_{2}\\right) x_{i}\n\n\\end{aligned}\n$$\n这样的话，我们估计出来的$\\beta_1+\\beta_2$应该是准确的，而$\\beta_1,\\beta_2$之中的一个大会导致另一个小。\n\nPost-treatment bias：因为包含其他预测变量而对后验估计产生影响。下面是一个例子：\n\n~~~R\n# number of plants\nN <- 100\n# simulate initial heights \nh0 <- rnorm(N,10,2)\n# assign treatments and simulate fungus and growth \ntreatment <- rep( 0:1 , each=N/2 )\nfungus <- rbinom( N , size=1 , prob=0.5 - treatment*0.4 )\nh1 <- h0 + rnorm(N, 5 - 3*fungus)\n# compose a clean data frame\nd <- data.frame( h0=h0 , h1=h1 , treatment=treatment , fungus=fungus )\n~~~~\n\n~~~R\nm5.13 <- map(\n  alist(\n    h1 ~ dnorm(mu,sigma),\n    mu <- a + bh*h0 + bt*treatment + bf*fungus,\n    a ~ dnorm(0,100),\n    c(bh,bt,bf) ~ dnorm(0,10), \n    sigma ~ dunif(0,10)\n  ),\n  data=d )\nprecis(m5.13)\n\n\n       mean   sd  5.5% 94.5%\na      4.85 0.50  4.06  5.65\nbh     1.01 0.05  0.93  1.08\nbt     0.05 0.21 -0.29  0.38\nbf    -2.57 0.22 -2.92 -2.22\nsigma  0.91 0.06  0.80  1.01\n~~~\n\n我们知道我们的照料(treatment)对植物的照料很重要，但是结果显示其影响很小，这是因为真菌(fungus)是照料的结果，添加post-treatment(fungus)会掩盖treatment的结果，这是因为知道了真菌再知不知道treatment对结果已经不重要了。\n\n### Categorical variables\n\n二分类变量例子：男女身高：\n$$\n\\begin{aligned}\n\nh_{i} & \\sim \\operatorname{Normal}\\left(\\mu_{i}, \\sigma\\right) \\\\\n\n\\mu_{i} &=\\alpha+\\beta_{m} m_{i} \\\\\n\n\\alpha & \\sim \\operatorname{Normal}(178,100) \\\\\n\n\\beta_{m} & \\sim \\operatorname{Normal}(0,10) \\\\\n\n\\sigma & \\sim \\operatorname{Uniform}(0,50)\n\n\\end{aligned}\n$$\n$m_i=1$表示男性，$m_i=0$表示女性。\n多分类变量：对于多分类变量，我们可以这样对其进行编码：\n\n~~~R\ndata(milk) \nd <- milk\nunique(d$clade)\nd$clade.NWM <- ifelse( d$clade==\"New World Monkey\" , 1 , 0 )\nd$clade.OWM <- ifelse( d$clade==\"Old World Monkey\" , 1 , 0 )\nd$clade.S <- ifelse( d$clade==\"Strepsirrhine\" , 1 , 0 )\n~~~\n\n构建模型为：\n$$\n\\begin{aligned}\n\nk_{i} & \\sim \\operatorname{Normal}\\left(\\mu_{i}, \\sigma\\right) \\\\\n\n\\mu_{i} &=\\alpha+\\beta_{\\mathrm{NWM}} \\mathrm{NWM}_{i}+\\beta_{\\mathrm{OWM}} \\mathrm{OWM}_{i}+\\beta_{\\mathrm{S}} \\mathrm{S}_{i} \\\\\n\n\\alpha & \\sim \\operatorname{Normal}(0.6,10) \\\\\n\n\\beta_{\\mathrm{NWM}} & \\sim \\operatorname{Normal}(0,1) \\\\\n\n\\beta_{\\mathrm{OWM}} & \\sim \\operatorname{Normal}(0,1) \\\\\n\n\\beta_{\\mathrm{S}} & \\sim \\operatorname{Normal}(0,1) \\\\\n\n\\sigma & \\sim \\operatorname{Uniform}(0,10)\n\n\\end{aligned}\n$$\n还有另一种构建方法，即采用索引的方法：\n\n~~~R\n# 对不同类别采用不同的索引\n( d$clade_id <- coerce_index(d$clade) )\n\nm5.16_alt <- map(\nalist(\nkcal.per.g ~ dnorm( mu , sigma ) ,\nmu <- a[clade_id] ,\na[clade_id] ~ dnorm( 0.6 , 10 ) , \nsigma ~ dunif( 0 , 10 )\n) , \ndata=d )\nprecis( m5.16_alt , depth=2 )\n~~~\n\n","tags":["概率编程","贝叶斯统计","Statistical Rethinking"],"categories":["书籍阅读"]},{"title":"Statistical Rethinking:Chapter4","url":"/2022/07/14/rt4/","content":"\n## Linear Models\n![](https://raw.githubusercontent.com/HFC666/image/master/img/LM1.png)\n\n<!--more-->\n\n托勒密的地心说在一段时间内曾经占据天文学的统治地位。托勒密用轨道内的轨道即本轮(epicycles)来描述行星复杂的活动，如下图所示。虽然这个模型现在看来是错的，但是却具有很高的预测准确度。\n![](https://raw.githubusercontent.com/HFC666/image/master/img/LM2.jpg)\n线性回归(Linear Regression)为应用统计的地心模型。与地心说一样，线性回归可以有效地描述各种各样的自然现象。但是如果我们用线性回归描述线性内部的原理结果，我们将很容易犯错误。即虽然可能会得到很好的预测，但是其内部的机制不一定正确。\n\n### Why normal distributions are normal\n#### Normal by addition\n任何将来自同一分布的随机值相加的过程都会收敛到正态分布。这可以理解为产生此数据的源分布产生的数据为此数据的均值加上一个随机波动，随着数据数量的增多，随机波动逐渐互相抵消，均值处数据最多，得到正态分布。\n#### Normal by multiplication\n下面是另一种方法来得到高斯分布。假设每年的增长率位于$1.0\\sim 1.1$，经过$12$年，总增长率为：\n~~~R\nprod(1 + runif(12, 0, 0.1))\n~~~\n那么总增长率的分布为什么？我们采$10000$个样绘制其概率分布：\n~~~R\ngrowth <- replicate(10000, prod(1+runif(12,0,0.1)))\ndens(growth, norm.comp=TRUE)\n~~~\n得到的结果如下：\n![](https://raw.githubusercontent.com/HFC666/image/master/img/LM3.jpeg)\n可以看到近似正态分布。我们之前说过正态分布可以通过将随机波动相加得到，但是我们这里是相乘，为什么也会得到正态分布呢？\n这是因为我们每次相乘的生长率很小，可以近似于加法，如：\n$$\n1.1\\times1.1=1.21=(1+0.1)(1+0.1)=1+0.2+0.01\\approx 1.2\n$$\n#### Normal by log-multiplication\n相乘大的增长率不会产生高斯分布，但是它们的对数会产生高斯分布：\n~~~R\nlog.big <- replicate(10000, log(prod(1+runif(12, 0, 0.5))))\ndens(log.big, norm.comp=TRUE)\n~~~\n![](https://raw.githubusercontent.com/HFC666/image/master/img/LM4.jpeg)\n#### Using Gaussian distributions\n使用高斯分布主要有两个原因：一个是因为其在自然界中分布广泛，另一个原因是[[EXP]]，根据指数分布的性质，高斯分布作为指数分布的特例，具有最大熵(包含最少信息)。\n### A language for describing models\n统计模型一般采用下述方法表示：\n$$\n\\begin{aligned}\n\n\\text { outcome }_{i} & \\sim \\operatorname{Normal}\\left(\\mu_{i}, \\sigma\\right) \\\\\n\n\\mu_{i} &=\\beta \\times \\operatorname{predictor}{ }_{i} \\\\\n\n\\beta & \\sim \\operatorname{Normal}(0,10) \\\\\n\n\\sigma & \\sim \\operatorname{HalfCauchy}(0,1)\n\n\\end{aligned}\n$$\n### A Gaussian model of height\n#### The data\n~~~R\ndata(\"Howell1\")\nd <- Howell1\nd2 <- d[d$age >= 18,]\n~~~\n#### The model\n假设我们的模型为：\n$$\n\\begin{aligned}\nh_i &\\sim \\text{Normal}(\\mu,\\sigma)\\\\\n\\mu&\\sim \\text{Normal}(178, 20)\\\\\n\\sigma&\\sim \\text{Uniform}(0,50)\n\\end{aligned}\n$$\n#### Grid approximation of the posterior distribution\n~~~R\nmu.list <- seq(from=140, to=160, length.out=200)\nsigma.list <- seq(from=4, to=9, length.out=200)\npost <- expand.grid(mu=mu.list, sigma=sigma.list)\npost$LL <- sapply(1:nrow(post), function(i) sum(\n  dnorm(d2$height, mean = post$mu[i], sd=post$sigma[i], log=TRUE)\n))\npost$prod <- post$LL + dnorm(post$mu, 178, 20, TRUE) + dunif(post$sigma, 0, 50, TRUE)\npost$prob <- exp(post$prod - max(post$prod))\n~~~\n你可以对结果进行绘制：\n~~~R\ncontour_xyz(post$mu, post$sigma, post$prob)\n~~~\n#### Sampling from the posterior\n~~~R\nsample.rows <- sample(1:nrow(post), size = 1e4, replace = TRUE, prob = post$prob)\nsample.mu <- post$mu[sample.rows]\nsample.sigma <- post$sigma[sample.rows]\n~~~\n绘制结果\n~~~R\nplot(sample.mu, sample.sigma, cex=0.5, pch=16, col=col.alpha(rangi2,0.1))\n~~~\n#### Fitting the model with `map`\n下面我们利用二次逼近来对后验分布进行估计：\n~~~R\nflist <- alist(\n  height ~ dnorm(mu, sigma),\n  mu ~ dnorm(178, 20),\n  sigma ~ dunif(0, 50)\n)\nm4.1 <- map(flist, data = d2)\nprecis(m4.1)\n\n        mean   sd   5.5%  94.5%\nmu    154.61 0.41 153.95 155.27\nsigma   7.73 0.29   7.27   8.20\n~~~\n`map`通过像爬坡一样的方式来估计后验，它需要从一个初始位置开始。我们可以指定初始位置，一个好的初始位置位于最大后验概率(MAP)处：\n~~~R\nstart <- list(mu=mean(d2$height), sigma = sd(d2$height) )\n~~~\n注意这里是`list`而不是`alist`，它们两者的区别是`list`会计算你放到其内部的代码，而`alist`不会。\n#### Sampling from a `map` fit\n二次逼近不止计算了每个参数的方差，还计算了它们之间的协方差，即协方差矩阵。\n~~~R\nvcov(m4.1)\n\n               mu       sigma\nmu    0.169740079 0.000217271\nsigma 0.000217271 0.084906413\n~~~\n我们可以用`extract.samples`函数进行抽样，其实质为对多元高斯进行抽样：\n~~~R\npost <- extract.samples(m4.1, n=1e4)\n~~~\n$\\sigma$的分布可能不是正态分布，所以我们的二次逼近可能存在问题，但是我们可以假设$\\sigma$的对数为正态分布，那么变为：\n~~~R\nm4.1_logsigma <- map(\n  alist(\n    height ~ dnorm(mu, exp(log_sigma)),\n    mu ~ dnorm(178, 20),\n    log_sigma ~ dnorm(2, 10)\n), data =d2)\n\npost <- extract.samples(m4.1_logsigma)\nsigma <- exp(post$log_sigma)\n~~~\n### Adding a predictor\n#### The linear model strategy\n我们的模型为：\n$$\n\\begin{aligned}\n\nh_{i} & \\sim \\operatorname{Normal}\\left(\\mu_{i}, \\sigma\\right) \\\\\n\n\\mu_{i} &=\\alpha+\\beta x_{i} \\\\\n\n\\alpha & \\sim \\operatorname{Normal}(178,100) \\\\\n\n\\beta & \\sim \\operatorname{Normal}(0,10) \\\\\n\n\\sigma & \\sim \\operatorname{Uniform}(0,50)\n\n\\end{aligned}\n$$\n#### Fitting the model\n~~~R\nm4.3 <- map(\n  alist(\n    height ~ dnorm(mu, sigma),\n    mu <- a + b*weight,\n    a ~ dnorm(156, 100),\n    b ~ dnorm(0, 10),\n    sigma ~ dunif(0,50)\n  ),\n  data=d2\n)\n~~~\n得到的结果如下：\n~~~R\nprecis(m4.3)\n\n        mean   sd   5.5%  94.5%\na     113.89 1.91 110.85 116.94\nb       0.90 0.04   0.84   0.97\nsigma   5.07 0.19   4.77   5.38\n~~~\n我们可以查看各个参数之间的相关系数：\n~~~R\n                  a             b         sigma\na      1.0000000000 -0.9898830268  0.0006161621\nb     -0.9898830268  1.0000000000 -0.0005736179\nsigma  0.0006161621 -0.0005736179  1.0000000000\n~~~\n可以看出$a$与$b$几乎成负相关，这并不是我们想要的，我们可以采用下面的方法：\n~~~R\nd2$weight.c <- d2$weight - mean(d2$weight)\n\nm4.4 <- map(\n  alist(\n    height ~ dnorm(mu, sigma),\n    mu <- a + b*weight.c,\n    a ~ dnorm(178, 100),\n    b ~ dnorm(0, 10),\n    sigma ~ dunif(0, 50)\n  ),\n  data = d2\n)\n\nprecis(m4.4)\n        mean   sd   5.5%  94.5%\na     154.60 0.27 154.17 155.03\nb       0.90 0.04   0.84   0.97\nsigma   5.07 0.19   4.77   5.38\n\ncov2cor(vcov(m4.4))\n                  a             b         sigma\na      1.000000e+00 -1.630888e-11  2.556226e-07\nb     -1.630888e-11  1.000000e+00 -6.380061e-05\nsigma  2.556226e-07 -6.380061e-05  1.000000e+00\n~~~\n可以看到$a$和$b$的相关系数接近于零，这是因为$a$表示当体重为$0$时的身高，而变换后的体重均值为$0$，因为线性回归经过体重的均值、身高的均值这个点，而体重的均值为$0$，所以$a$就表示在体重为体重均值时身高的值，即身高的均值，所以与$b$无关。\n\n我们也可以绘制后验分布得到的结果：\n~~~R\nplot(height~weight, data=d2)\nabline(a=coef(m4.3)[\"a\"], b=coef(m4.3)[\"b\"])\n~~~\n![](https://raw.githubusercontent.com/HFC666/image/master/img/LM5.jpeg)\n上图我们只是绘制了一条直线，对应于后验的均值。为了表达不确定性，我们可以对后验分布进行抽样。\n~~~R\npost <- extract.samples(m4.3, n=20)\n\nplot(d2$weight, d2$height, xlim=range(d2$weight), ylim=range(d2$height), col=rangi2, xlab=\"weight\", ylab=\"height\")\n\nfor (i in 1:20) {\n  abline(a=post$a[i], b=post$b[i], col=col.alpha(\"black\", 0.3))\n}\n~~~\n![](https://raw.githubusercontent.com/HFC666/image/master/img/LM6.jpeg)\n对于每个确定的`weight`，由于$a$和$b$都为高斯分布，所以生成的$\\mu_i = a + b w_i$也服从高斯分布。我们可以使用link函数，link函数默认从后验分布中抽取$1000$个样本，对每个固定的weight根据这$1000$个样本计算其$\\mu$的值。\n\n~~~R\nmu <- link(m4.3)\nstr(mu)\n\n num [1:1000, 1:352] 158 157 157 157 157 ...\n~~~\n生成的`mu`为一个矩阵，行表示从后验分布中抽取的样本，列表示数据的数目。\n这样我们就可以绘制在某个确定weight值上$\\mu$的分布：\n~~~R\nweight.seq <- seq(from=25, to=70, by=1)\nmu <- link(m4.3, data = data.frame(weight=weight.seq))\nplot(height~weight, d2, type=\"n\")\nfor (i in 1:100) {\n  points(weight.seq, mu[i,], pch=16, col=col.alpha(rangi2, 0.1))\n}\n~~~\n![](https://raw.githubusercontent.com/HFC666/image/master/img/LM7.jpeg)\n这样我们也可以计算关于$\\mu$的分布的信息了：\n~~~R\nmu.mean <- apply(mu, 2, mean)\nmu.HPDI <- apply(mu, 2, HPDI, prob=0.89)\n\nplot( height ~ weight , data=d2 , col=col.alpha(rangi2,0.5) )\nlines( weight.seq , mu.mean )\nshade( mu.HPDI , weight.seq )\n~~~\n之前我们只是对平均高度$\\mu$进行采样，现在我们生成真实的高度height。这意味着我们也将$\\sigma$的不确定性结合在一起。\n~~~R\nsim.height <- sim(m4.3, data = list(weight=weight.seq))\nstr(sim.height)\nheight.PI <- apply( sim.height , 2 , PI , prob=0.89 )\n~~~\n我们也可以绘制图：\n~~~R\nplot(height~weight, d2, col=col.alpha(rangi2, 0.5))\nlines(weight.seq, mu.mean)\nshade(mu.HPDI, weight.seq,col=col.alpha(rangi2, 0.5))\nshade(height.PI, weight.seq,col=col.alpha(rangi2, 0.5))\n~~~\n> 不知到为什么shade函数失效","tags":["概率编程","贝叶斯统计","Statistical Rethinking"],"categories":["书籍阅读"]},{"title":"A tutorial on Bayesian nonparametric models","url":"/2022/07/12/BN/","content":"\n## A Tutorial on Bayesian Nonparametric Models\n\n<p align=\"center\">\n    <img src=\"https://gimg2.baidu.com/image_search/src=http%3A%2F%2Fimg.mp.itc.cn%2Fupload%2F20170401%2F66c0602968c0463c805dcd5ab16a87dc_th.jpeg&refer=http%3A%2F%2Fimg.mp.itc.cn&app=2002&size=f9999,10000&q=a80&n=0&g=0n&fmt=auto?sec=1660210656&t=6f0b4c7c45167ff59154a4b5ee75eb30\" style=\"zoom: 100%;\" />\n</p>\n\n\n> 文章地址：https://www.sciencedirect.com/science/article/abs/pii/S002224961100071X\n\n<!--more-->\n\n### Introduction\n\n统计模型的一个核心问题就是模型选择，怎样选择模型复杂度，如在聚类问题中选择聚类的数目等。贝叶斯无参模型通过让数据确定模型复杂度来跳过模型选择的步骤。比如说在传统的机器学习中如聚类中，我们需要事先确定聚类的数目，而贝叶斯无参模型会估计聚类的数目并允许未来的数据来自之前没有的类。\n\n一般我们进行贝叶斯推断时是对参数的后验分布进行推断，无参贝叶斯与其他贝叶斯模型显著的不同在于其隐藏结构是随着数据增长的。它的复杂度，如聚类的数目，是后验分布的一部分。这些参数无需实现给定，而是作为分析数据的一部分来确定。\n\n在本文中我们主要介绍两种模型，混合模型(mixture models)和隐语义模型(latent factor models)。贝叶斯无参混合模型，也被称为中国餐馆过程混合(或者狄利克雷过程混合)，从数据中推断聚类的个数并且随着数据的增长允许聚类数增长。\n\n隐语义模型将观察到的数据分解为潜在因素的线性组合。关于因子分布的不同假设会产生如因子分析、主成分分析、独立成分分析等等。印度自助餐过程隐语义模型通过数据推断因子的数量并且允许因子的数量随着新数据的加入而增加。\n\n### Mixture models and clustering\n\n在一个混合模型中，每一个观测数据都假设属于一个类。\n\n#### Finite mixture modeling\n\n有限混合模型假设有$K$个类，每个类与参数$\\theta_k$有关。每一个观测$y_n$都假设是首先通过$P(c_n)$来选择一个类$c_n$之后通过与参数$\\theta_{c_n}$相对应的分布中产生观测，每一个参数对应一个分布$F(y_n\\mid \\theta_{c_n})$。\n\n贝叶斯混合模型引入了混合分布$P(c)$的先验和聚类参数的先验$\\theta\\sim G_0$。\n\n这个生成过程定义了关于观测、聚类分配(数据属于哪个类)和聚类参数的联合分布：\n$$\nP(\\mathrm{y,c},\\theta) = \\prod_{k=1}^K G_0(\\theta_k)\\prod_{n=1}^NF(Y_n\\mid\\theta_{c_n})P(c_n)\n$$\n其中观测为$\\mathrm{y} = \\{y_1,\\cdots,y_N\\}$，聚类分配为$\\mathrm{c}=\\{c_1,\\cdots,c_N\\}$，聚类参数为$\\theta=\\{\\theta_1,\\cdots,\\theta_K\\}$。\n\n> 这里$N$个样本相乘是因为样本之间是独立同分布的，而$K$个$G_0(\\theta_k)$相乘是因为每个$\\theta_k$也是独立的。\n\n给定数据集，我们经常对聚类分配感兴趣，我们可以用贝叶斯公式计算它们：\n$$\nP(\\mathrm{c}\\mid\\mathrm{y}) = \\frac{P(\\mathrm{y}\\mid \\mathrm{c})P(\\mathrm{c})}{\\sum_{\\mathrm{c}}P(\\mathrm{y}\\mid\\mathrm{c})P(\\mathrm{c})}\n$$\n其中似然函数可以通过对$\\theta$积分得到：\n$$\nP(\\mathrm{y\\mid c}) = \\int_{\\theta}\\left[\\prod_{n=1}^NF(\\mathrm{y}\\mid\\theta_{c_n})\\prod_{k=1}^KG_0(\\theta_k)\\right]d\\theta\n$$\n后验分布是很难处理的，因为计算分母需要将每种分配方式进行求和。\n\n#### The Chinese restaurant process\n\n见我的一篇博客(https://hfcouc.work/2022/07/06/BNP/)\n\n### Latent factor models and dimensionality reduction\n\n混合模型假设每个数据都来自$K$个类中的一个。隐语义模型削弱了这一假设：每个观测受到$K$个分量的不同的影响。\n\n隐语义模型可以用来降维当分量的数目小于数据维度时。每个观察都与一个分量激活向量（潜在因子）相关联，该向量描述了每个分量对其贡献的程度，并且这个向量可以看作是观察本身的低维表示。\n\n非常著名的降维模型有因子分析(FA)，主成分分析(PCA)和独立成分分析(ICA)等，所有的都假设因子的数量$K$已经确定了。无参贝叶斯模型允许分量的数量随着数据的增长而增多。\n\n在经典的降维模型中，我们的观测数据一般为$N$个$M$维向量，$\\mathrm{Y=\\{y_1,\\cdots,y_N\\}}$。因此$\\mathrm{Y}$每行表示一个数据。数据假设是从有噪声的隐语义的加权组合产生的：\n$$\n\\mathrm{y_n = Gx_n+\\epsilon_n}\n$$\n其中$\\mathrm{G}$为$M\\times K$的矩阵表示，表示隐语义$k$怎么影响观测维度$m$，$x_n$为一个$K$维向量表示每个隐语义的影响，其中$\\epsilon_n$维独立高斯噪声向量。我们可以将其拓展为稀疏模型通过将$\\mathrm{G}$进行分解，$\\mathrm{G}_{mk}=z_{mk}w_{mk}$，其中$z_{mk}$是一个二元掩码(mask)变量指示因子$k$是否影响维度$m$，而$w_{ik}$为连续权重变量。这个有时被称为`spike and slab`模型因为$x_{mk}$的期望为一个在隐语义空间上的`slab`$P(w_{mk})$和在零的位置$P(z_{mk}=0)$的`spike`。\n\n我们使用贝叶斯的方法推理隐语义、掩码变量和权重。我们在它们上面定义先验并利用贝叶斯公式计算后验$P(\\mathrm{G,Z,W\\mid Y})$。\n\n利用无参贝叶斯模型，我们令数据自动确定隐语义的数目$K$。$Z$为一个二元矩阵，有有限多的行和无限多的列。\n\n与中国餐馆过程类似，$\\mathrm{Z}$上的无限容量的分布也被赋予了一个有关烹饪的隐喻，被称为印度自助餐过程。一个顾客(维度)进入一个具有无限菜品的自主餐馆。顾客$m$选择菜品$k$的概率正比于他被之前顾客选择的次数$h_k$。当顾客考虑了所有之前采样过的菜肴($h_k>0$)的菜肴时，他会额外采样之前从未采样过的$\\operatorname{Poisson}(\\alpha/N)$菜肴。当所有$M$个顾客都选完菜后，形成的二元矩阵$\\mathrm{Z}$从IBP(Indian buffet process)中采样得到。\n\n与中国餐馆过程不同的是，中国餐馆过程每个观测只能选择一个隐语义，而印度自助餐过程每个观测的每个维度可以选择无数多个隐语义。如下图：\n\n![](https://raw.githubusercontent.com/HFC666/image/master/img/IBP.jpg)\n\n> 印度餐馆过程：IBP的生成过程，其中有序号的菱形代表顾客，与与它们相关的观测连接。大圆形表示菜品，与与它们相关的参数$\\phi$相连。每个顾客选择多个菜品，并且每个顾客的观测为选择的菜品的参数的线性组合。\n\n现在回到我们的后验分布，我们想要对我们的隐语义矩阵进行推理：\n$$\nP(\\mathrm{X,W,Z}\\mid \\mathrm{Y})\\propto P(\\mathrm{Y\\mid X,W,Z})P(\\mathrm{X})P(\\mathrm{W})P(\\mathrm{Z})\n$$\n精确的推理是很困难的因为归一化参数需要对所有可能的二元矩阵进行积分。\n### Inference\n在BNP建模中最基本的计算问题是计算后验概率。下面我们只简单介绍几种计算方法。\n+ MCMC\n+ 变分贝叶斯推断：见我的一篇博客(https://hfcouc.work/2022/07/10/VBI/)\n\n### Limitations and extensions\n我们前面介绍到的两种BNP只是最简单的两种，它们存在着很多限制，为了解决这些限制有很多上述模型的拓展。\n#### Hierarchical structure\n第一个限制是关于分组数据的：我们如何捕捉一个群体中的共性和特性呢？比如说同一种动物既有共性也有自己的特性。解决这个问题的标准贝叶斯方法是基于层次模型，在这个模型中，个体由于来自同一群体的分布而被耦合在一起。在非参数的设定下发展出的模型为分层狄利克雷过程。\n#### Time series models\n第二个限制是关于序列数据的：我们怎样才能捕捉到序列观察结果之间的依赖关系？关于这方面非常著名的是隐马尔可夫模型。无限隐马尔科夫模型提出了相同的序列结构，但采用了无限多的潜在类，无限隐马尔可夫模型为分层狄利克雷过程的一个特殊情况。\n作为`HMM`（隐状态为离散的）的替代品，线性动态系统（也被称为自回归移动平均模型）假定隐状态是连续的，并根据线性高斯马尔可夫过程随时间演变。在一个切换的线性动态系统中，系统可以有许多动态模式；这使得边际转移分布可以是非线性的。有学者探索了切换线性动态系统的非参数变体，其中动态模式的数量是利用HDP先验从数据中推断出来的。\n#### Spatial models\n很多数据集上的依赖是在空间上的。如某种疾病在某个地方出现，在它附近的地方也可能出现。在BNP模型中捕获这种依赖关系的一种方法是使DP的基分布依赖于一个位置变量。","tags":["贝叶斯","算法"],"categories":["文献阅读"]},{"title":"变分贝叶斯推断","url":"/2022/07/10/VBI/","content":"\n## Variational Bayesian inference\n\n<p align=\"center\">\n    <img src=\"https://img0.baidu.com/it/u=2387178916,3720682298&fm=253&fmt=auto?w=1280&h=720\" style=\"zoom: 100%;\" />\n</p>\n\n> 参考文献\n>\n> 1. [徐亦达老师变分推断课件](https://github.com/roboticcam/machine-learning-notes/blob/master/files/variational.pdf)\n> 2. [A tutorial on variational Bayesian inference](https://link.springer.com/article/10.1007/s10462-011-9236-8)\n> 3. [白板推导指数族分布](https://www.bilibili.com/video/BV1QW411y7D3?spm_id_from=333.337.search-card.all.click&vd_source=6177c61c946280bb88c727585de76bc8)\n> 4. [白板推导变分推断](https://www.bilibili.com/video/BV1DW41167vr?spm_id_from=333.337.search-card.all.click&vd_source=6177c61c946280bb88c727585de76bc8)\n\n<!--more-->\n\n### Log-likelihood and Evidence Lower Bound(ELOB)\n\n下列表达式总是成立：\n$$\n\\ln(p(X)) = \\ln(p(X,Z)) - \\ln(P(Z\\mid X))\n$$\n所以下式也成立：\n$$\n\\ln(P(X)) = \\left[\\ln(p(X,Z))-\\ln(q(Z))\\right] - \\left[\\ln(p(Z\\mid X))-\\ln(q(Z))\\right]\n$$\n所以现在我们有\n$$\n\\ln(p(X)) = \\ln\\left(\\frac{p(X,Z)}{q(Z)}\\right) - \\ln\\left(\\frac{p(Z\\mid X)}{q(Z)}\\right)\n$$\n两边同时取期望：\n$$\n\\begin{aligned}\n\\ln (p(X)) &=\\int q(Z) \\ln \\left(\\frac{p(X, Z)}{q(Z)}\\right) \\mathrm{d} Z-\\int q(Z) \\ln \\left(\\frac{p(Z \\mid X)}{q(Z)}\\right) \\mathrm{d} Z \\\\\n&=\\underbrace{\\int q(Z) \\ln (p(X, Z)) \\mathrm{d} Z-\\int q(Z) \\ln (q(Z)) \\mathrm{d} Z}_{\\mathcal{L}(q)}+\\underbrace{\\left(-\\int q(Z) \\ln \\left(\\frac{p(Z \\mid X)}{q(Z)}\\right) \\mathrm{d} Z\\right)}_{\\mathbb{K} \\mathbb{L}(q \\| p)} \\\\\n&=\\mathcal{L}(q)+\\mathbb{K} \\mathbb{L}(q \\| p)\n\\end{aligned}\n$$\nKL散度一般用于度量两个概率分布函数之间的距离，其定义如下：\n$$\n\\mathbb{KL}[p(X)\\mid q(X)] = \\sum_{x\\in X}\\left[p(x)\\log\\frac{p(x)}{q(x)}\\right] = \\mathbb{E}_{x\\sim p(x)}\\left[\\log\\frac{p(x)}{q(x)}\\right]\n$$\n我们要做的就是找到与后验分布$p(Z\\mid X)$最接近的简单分布$p(Z)$。\n\n### Alternative Evidence Lower Bound(ELOB)\n\n我们看另一种推导方法：\n$$\n\\begin{aligned}\n\\ln (p(X)) &=\\log \\int_{Z} p(X, Z) \\mathrm{d} z \\\\\n&=\\log \\int_{Z} p(X, Z) \\frac{q(Z)}{q(Z)} \\mathrm{d} z \\\\\n&=\\log \\left(\\mathbb{E}_{q}\\left[\\frac{p(X, Z)}{q(Z)}\\right]\\right) \\\\\n& \\geq \\mathbb{E}_{q}\\left[\\log \\left(\\frac{p(X, Z)}{q(Z)}\\right)\\right] \\text { using Jensen's inequality } \\\\\n&=\\mathbb{E}_{q}[\\log (p(X, Z))]-\\mathbb{E}_{q}[\\log (q(Z))] \\\\\n& \\triangleq \\mathcal{L}(q)\n\\end{aligned}\n$$\n\n### Maximize Evidence Lower Bound(ELOB)\n\n我们给每个部分一个名字：\n$$\n\\begin{array}{ll}\n\\text {Evidence Lower Bound (ELOB):} & \\mathcal{L}(q)=\\int q(Z) \\ln (p(X, Z)) \\mathrm{d} Z-\\int q(Z) \\ln (q(Z)) \\mathrm{d} Z \\\\\n\\mathrm{KL} \\text { divergence: } & \\mathbb{K} \\mathbb{L}(q \\| p)=-\\int q(Z) \\ln \\left(\\frac{p(Z \\mid X)}{q(Z)}\\right) d Z\n\\end{array}\n$$\n\n+ 注意$p(X)$对于$q(Z)$的选择是固定的。我们想要去选择一个$q(Z)$函数最小化KL散度，因此$q(Z)$变得离$p(Z\\mid X)$越来越近。很容易验证，当$q(Z)=p(Z\\mid X)$时，KL散度为$0$。\n+ 我们知道$\\ln p(X) = \\mathcal{L}(q)+\\mathbb{KL}(q\\| p)$。最小化$\\mathbb{KL}(q\\| p)$等同于最大化$\\mathcal{L}(q)$。\n\n\n\n\n\n我们可以选择$q(Z)$使得\n$$\nq(Z) = \\prod_{i=1}^Mq_i(Z_i)\n$$\n其中$M$为$Z$的维度，也就是说$q(Z)$的各个维度是独立的，这被称为**平均场变分贝叶斯**。\n\n> 注意$q(Z)$对联合概率密度函数$p(Z\\mid X)$是一个很好地近似，但是边缘分布$q(Z_i)$对$p(Z_i\\mid x)$的近似不一定好。\n\n将其带入到$\\mathcal{L}(q)$中：\n$$\n\\begin{aligned}\n\\mathcal{L}(q) &=\\int q(Z) \\ln (p(X, Z)) \\mathrm{d} Z-\\int q(Z) \\ln (q(Z)) \\mathrm{d} Z \\\\\n&=\\underbrace{\\int \\prod_{i=1}^{M} q_{i}\\left(Z_{i}\\right) \\ln (p(X, Z)) \\mathrm{d} Z}_{\\text {part (1) }}-\\underbrace{\\int \\prod_{i=1}^{M} q_{i}\\left(Z_{i}\\right) \\sum_{i=1}^{M} \\ln \\left(q_{i}\\left(Z_{i}\\right)\\right) \\mathrm{d} Z}_{\\text {part (2) }}\n\\end{aligned}\n$$\n我们先看Part1，假设我们只对$Z_i$感兴趣，将其拿出来，变为：\n$$\n(\\operatorname{Part} 1)=\\int_{Z_{j}} q_{j}\\left(Z_{j}\\right)\\left(\\int_{Z_{i \\neq j}} \\ldots \\int \\prod_{i \\neq j}^{M} q_{i}\\left(Z_{i}\\right) \\ln (p(X, Z)) \\prod_{i \\neq j}^{M} d Z_{i}\\right) d Z_{j}\n$$\n或者将其写为更紧凑的形式：\n$$\n(\\operatorname{Part} 1)=\\int_{Z_{j}} q_{j}\\left(Z_{j}\\right)\\left(\\int_{Z_{i \\neq j}} \\cdots \\int \\ln (p(X, Z)) \\prod_{i \\neq j}^{M} q_{i}\\left(Z_{i}\\right) d Z_{i}\\right) d Z_{j}\n$$\n或者，为了让其更具有意义，可以将其放进一个期望函数里：\n$$\n(\\operatorname{Part} 1)=\\int_{Z_{j}} q_{j}\\left(Z_{j}\\right)\\left[\\mathbb{E}_{i \\neq j}[\\ln (p(X, Z))]\\right] d Z_{j}\n$$\n现在再看Part2：\n$$\n(\\text { Part 2) }=\\int \\prod_{i=1}^{M} q_{i}(Z_{i}) \\sum_{i=1}^{M} \\ln \\left(q_{i}(Z_{i}\\right)) d Z\n$$\n将其化简：\n$$\n\\begin{aligned}\n\\operatorname{(Part2)} &= \\int q(Z)\\sum_{i=1}^M\\ln(q_i(Z_i))dZ\\\\\n&=\\sum_{i=1}^M\\int_{Z}q(Z_1,\\cdots,Z_M)\\ln(q_i(Z_i))dZ\\\\\n&=\\sum_{i=1}^M\\int_{Z_i}q_i(Z_i)\\ln(q_i(Z_i))dZ_i\n\\end{aligned}\n$$\n\n\n假设现在我们只对$q_j(Z_j)$感兴趣，则其余部分可以看作常数，因此上式可以进一步写为：\n$$\n(\\text { Part } 2)=\\int_{Z_{j}} q_{j}\\left(Z_{j}\\right) \\ln \\left(q_{j}\\left(Z_{j}\\right)\\right) d Z_{j}+\\text { const}\n$$\n则$\\mathcal{L}(q)$变为：\n$$\n\\mathcal{L}(q)=\\operatorname{Part}(1)-\\operatorname{Part}(2)=\\int_{Z_{j}} q_{j}\\left(Z_{j}\\right) \\mathbb{E}_{i \\neq j}[\\ln (p(X, Z))] \\mathrm{d} Z_{j}-\\int_{Z_{j}} q_{j}\\left(Z_{j}\\right) \\ln \\left(q_{j}\\left(Z_{j}\\right)\\right) \\mathrm{d} Z_{j}+\\operatorname{const}\n$$\n我们定义：\n$$\n\\ln(\\tilde{p}_j(X,Z_j)) = \\mathbb{E}_{i\\neq j}[\\ln(p(X,Z))]\n$$\n或者定价的我们可以将ELOB写为：\n$$\n\\mathcal{L}(q_j) = \\int_{Z_j}q_j(Z_j)\\ln\\left[\\frac{\\tilde{p}_j(X,Z_j)}{q_j(Z_j)}\\right]+\\text{const}\n$$\n这与\n$$\n-\\mathbb{KL}\\left(\\exp(\\mathbb{E}_{i\\neq j}[\\ln(p(X,Z))])\\| q_i(Z_i)\\right)\n$$\n相等。\n\n**所以我们可以最大化ELOB，或者$\\mathcal{L}(q)$，通过最小化这个特殊的KL散度，也就是找到近似和最优$q^\\star_i(Z_i)$，使得**\n$$\n\\ln(q_i^\\star(Z_i)) = \\mathbb{E}_{i\\neq j}[\\ln(p(X,Z))]\n$$\n\n### Example\n\n令数据$\\mathcal{D}=\\{x_1,\\cdots,x_n\\}$，则\n$$\n\\begin{gathered}\np(\\mathcal{D} \\mid \\mu, \\tau)=\\prod_{i=1}^{n}\\left(\\frac{\\tau}{2 \\pi}\\right)^{\\frac{1}{2}} \\exp \\left(\\frac{-\\tau}{2}\\left(x_{i}-\\mu\\right)^{2}\\right) \\\\\n=\\left(\\frac{\\tau}{2 \\pi}\\right)^{\\frac{n}{2}} \\exp \\left(\\frac{-\\tau}{2} \\sum_{i=1}^{n}\\left(x_{i}-\\mu\\right)^{2}\\right) \\\\\np(\\mu \\mid \\tau)=\\mathcal{N}\\left(\\mu_{0},\\left(\\lambda_{0} \\tau\\right)^{-1}\\right) \\propto \\exp \\left(\\frac{-\\lambda_{0} \\tau}{2}\\left(\\mu-\\mu_{0}\\right)^{2}\\right) \\\\\np(\\tau)=\\operatorname{Gamma}\\left(\\tau \\mid a_{0}, b_{0}\\right) \\propto \\tau^{a_{0}-1} \\exp ^{-b_{0} \\tau}\n\\end{gathered}\n$$\n并且\n$$\np(\\mathcal{D},\\mu,\\tau) = p(\\mathcal{D}\\mid \\mu,\\tau)p(\\mu\\mid \\tau)p(\\tau)\n$$\n则：\n$$\np(\\mu,\\tau\\mid d)\\propto p(\\mathcal{D}\\mid \\mu,\\tau)p(\\mu\\mid \\tau)p(\\tau)=\\mathcal{N}(\\mu_n,(\\lambda \\tau)^{-1})\\text{Gamma}(\\tau\\mid a_n,b_n)\n$$\n其中\n$$\n\\begin{aligned}\n\\mu_{n} &=\\frac{\\lambda_{0} \\mu_{0}+n \\bar{x}}{\\lambda_{0}+n} \\\\\n\\lambda_{n} &=\\lambda_{0}+n \\\\\na_{n} &=a_{0}+n / 2 \\\\\nb_{n} &=b_{0}+\\frac{1}{2} \\sum_{i=1}^{n}\\left(x_{i}-\\bar{x}\\right)^{2}+\\frac{\\lambda_{0} n\\left(\\bar{x}-\\mu_{0}\\right)^{2}}{2\\left(\\lambda_{0}+n\\right)}\n\\end{aligned}\n$$\n可以看出是由解析解的，但是为了例子，我们再采用变分贝叶斯的方法，我们假设$q(\\mu,\\tau)$：\n$$\nq(\\mu,\\tau) = q_{\\mu}(\\mu)q_\\tau(\\tau)\n$$\n则：\n$$\n\\begin{aligned}\n\\ln \\left(q_{\\mu}^{*}(\\mu)\\right) &=\\mathbb{E}_{q_{\\tau}}[\\ln (p(\\mu, \\tau \\mid \\mathcal{D}))] \\\\\n&=\\mathbb{E}_{q_{\\tau} \\tau}[\\ln (p(\\mathcal{D} \\mid \\mu, \\tau))+\\ln p(\\mu \\mid \\tau)]+\\text { const. } \\quad \\text { remove terms do NOT contain } \\mu \\\\\n&=\\mathbb{E}_{q_{\\tau}}[\\underbrace{-\\frac{\\tau}{2} \\sum_{i=1}^{n}\\left(x_{i}-\\mu\\right)^{2}+\\underbrace{\\frac{\\lambda_{0} \\tau}{2}\\left(\\mu-\\mu_{0}\\right)^{2}}_{\\ln p(\\mu \\mid \\gamma)}]}_{\\ln (p(\\mathcal{D} \\mid \\mu, \\tau))}+\\text { const. }\\\\\n&=-\\frac{\\mathbb{E}_{q_{\\tau}}[\\tau]}{2} \\underbrace{\\left[\\sum_{i=1}^{n}\\left(x_{i}-\\mu\\right)^{2}+\\lambda_{0}\\left(\\mu-\\mu_{0}\\right)^{2}\\right]}_{\\text {terms contain } \\mu \\text { but does not contain } \\tau}+\\text { const. }\n\\end{aligned}\n$$\n将关于$\\mu$的项展开：\n$$\n\\begin{aligned}\n& \\sum_{i=1}^{n}\\left(x_{i}-\\mu\\right)^{2}+\\lambda_{0}\\left(\\mu-\\mu_{0}\\right)^{2}=n \\mu^{2}-2 n \\mu \\bar{x}+\\lambda_{0} \\mu^{2}-2 \\lambda_{0} \\mu_{0} \\mu+\\text { const. } \\\\\n=&\\left(n+\\lambda_{0}\\right) \\mu^{2}-2 \\mu\\left(n \\bar{x}+\\lambda_{0} \\mu_{0}\\right)=\\left(n+\\lambda_{0}\\right)\\left(\\mu^{2}-\\frac{2 \\mu\\left(n \\bar{x}+\\lambda_{0} \\mu_{0}\\right)}{\\left(n+\\lambda_{0}\\right)}\\right) \\\\\n=&\\left(n+\\lambda_{0}\\right)\\left(\\mu-\\frac{\\left(n \\bar{x}+\\lambda_{0} \\mu_{0}\\right)}{\\left(n+\\lambda_{0}\\right)}\\right)^{2}+\\text { const. }\n\\end{aligned}\n$$\n因此我们有：\n$$\n\\begin{aligned}\n\\ln \\left(q_{\\mu}^{*}(\\mu)\\right) &=-\\frac{\\mathbb{E}_{q_{\\tau}}[\\tau]}{2}\\left[\\sum_{i=1}^{n}\\left(x_{i}-\\mu\\right)^{2}+\\lambda_{0}\\left(\\mu-\\mu_{0}\\right)^{2}\\right]+\\text { const. } \\\\\n&=-\\frac{\\mathbb{E}_{q_{\\tau}}[\\tau]\\left(n+\\lambda_{0}\\right)}{2}\\left(\\mu-\\frac{\\left(n \\bar{x}+\\lambda_{0} \\mu_{0}\\right)}{\\left(n+\\lambda_{0}\\right)}\\right)^{2}+\\text { const. } \\\\\n&=\\mathcal{N}\\left(\\frac{n \\bar{x}+\\lambda_{0} \\mu_{0}}{n+\\lambda_{0}}, \\mathbb{E}_{q_{\\tau}}[\\tau]\\left(n+\\lambda_{0}\\right)\\right)\n\\end{aligned}\n$$\n关于$\\tau$，我们有\n$$\n\\begin{aligned}\n\\ln \\left(q_{\\tau}^{*}(\\tau)\\right) &=\\mathbb{E}_{q_{\\mu}}[\\ln (p(\\mu, \\tau \\mid \\mathcal{D}))] \\\\\n&=\\mathbb{E}_{q_{\\mu}}[\\ln (p(\\mathcal{D} \\mid \\mu, \\tau))+\\ln p(\\mu \\mid \\tau)+\\ln p(\\tau)]+\\text { const. } \\\\\n&=\\mathbb{E}_{q_{\\mu}}[\\underbrace{\\frac{n}{2} \\ln (\\tau)-\\frac{\\tau}{2} \\sum_{i=1}^{n}\\left(x_{i}-\\mu\\right)^{2}}_{\\ln (p(\\mathcal{D} \\mid \\mu, \\tau))} \\underbrace{-\\frac{\\lambda_{0} \\tau}{2}\\left(\\mu-\\mu_{0}\\right)^{2}}_{\\ln p(\\mu \\mid \\gamma)} \\underbrace{+\\left(a_{0}-1\\right) \\ln (\\tau)-b_{0} \\tau}_{\\ln p(\\tau)}]+\\text { const. }\n\\end{aligned}\n$$\n将没有$\\mu$的项拿出积分：\n$$\n\\begin{aligned}\n&=\\frac{n}{2} \\ln (\\tau)+\\left(a_{0}-1\\right) \\ln (\\tau)-b_{0} \\tau-\\frac{\\tau}{2} \\mathbb{E}_{q_{\\mu}(\\mu)}\\left[\\sum_{i=1}^{n}\\left(x_{i}-\\mu\\right)^{2}+\\lambda_{0}\\left(\\mu-\\mu_{0}\\right)^{2}\\right]+\\text { const. } \\\\\n&=(\\underbrace{\\frac{n}{2}+a_{0}}_{a_{n}}-1) \\ln (\\tau)-\\tau(\\underbrace{b_{0}+\\frac{1}{2} \\mathbb{E}_{q_{\\mu}(\\mu)}\\left[\\sum_{i=1}^{n}\\left(x_{i}-\\mu\\right)^{2}+\\lambda_{0}\\left(\\mu-\\mu_{0}\\right)^{2}\\right]}_{b_{n}}+\\text { const. }\n\\end{aligned}\n$$\n重写为：\n$$\n\\begin{aligned}\nb_{n} &=b_{0}+\\frac{1}{2} \\mathbb{E}_{q_{\\mu}}\\left[\\sum_{i=1}^{n}\\left(x_{i}-\\mu\\right)^{2}+\\lambda_{0}\\left(\\mu-\\mu_{0}\\right)^{2}\\right] \\\\\n&=b_{0}+\\frac{1}{2} \\mathbb{E}_{q_{\\mu}}\\left[-2 \\mu n \\bar{x}+n \\mu^{2}+\\lambda_{0} \\mu^{2}-2 \\lambda_{0} \\mu_{0} \\mu\\right]+\\sum_{i=1}^{n}\\left(x_{i}\\right)^{2}+\\lambda_{0} \\mu_{0}^{2} \\\\\n&=b_{0}+\\frac{1}{2}\\left[\\left(n+\\lambda_{0}\\right) \\mathbb{E}_{q_{\\mu}}\\left[\\mu^{2}\\right]-2\\left(n \\bar{x}+\\lambda_{0} \\mu_{0}\\right) \\mathbb{E}_{q_{\\mu}}[\\mu]+\\sum_{i=1}^{n}\\left(x_{i}\\right)^{2}+\\lambda_{0} \\mu_{0}^{2}\\right]\n\\end{aligned}\n$$\n因为$q_{\\mu}(\\mu)$事先定义好了我们可以计算$\\mathbb{E}_{q_\\mu}[\\mu]$和$\\mathbb{E}_{q_\\mu}[\\mu^2]$。\n\n### 随机梯度变分推断\n\n上面提到的基于平均场的变分推断实际上是坐标上升法，其存在一些问题：\n\n+ 假设太强，对复杂模型也许假设不好甚至不成立。\n+ 即使假设是成立的，但是因为其递推式包含很多积分，也可能无法计算。\n\n下面我们采用随机梯度上升的方法来求解变分推断问题：\n\n我们知道目标函数：\n$$\n\\hat{q} = \\arg\\min _q\\mathbb{KL}(q\\| p) = \\arg\\max_q\\mathcal{L}(q)\n$$\n其中$q$是$z$的函数，设其参数为$\\phi$，我们将其记为$q_{\\phi}(z)$，那么我们的目标函数变为：\n$$\n\\hat{q} = \\arg\\max_{\\phi}\\mathcal{L}(\\phi)\n$$\n其中\n$$\n\\mathcal{L}(\\phi) = \\mathbb{E}_{q_\\phi}[\\log P]\n$$\n因为是随机梯度下降，所以我们每次选取一个样本，假设选取的样本为$x_i$，那么目标函数变为：\n$$\n\\mathcal{L}(\\phi) = \\mathbb{E}_{q_\\phi}[\\log P(x_i,z) - \\log q_{\\phi}(z)]\n$$\n对其进行求导，得：\n$$\n\\begin{aligned}\n\\nabla_{\\phi} \\mathcal{L}(\\phi) &=\\nabla_{\\phi} \\mathbb{E}_{q_{\\phi}}\\left[\\log P\\left(x_i, z\\right)-\\log q_{\\phi}\\right] \\\\\n&=\\nabla_{\\phi} \\int q_{\\phi}\\left[\\log P\\left(x_i, z\\right)-\\log q_{\\phi}\\right] d z \\\\\n&=\\int \\nabla_{\\phi}\\left(q_{\\phi}\\left[\\log P\\left(x_i, z\\right)-\\log q_{\\phi}\\right]\\right) d z \\\\\n&=\\underbrace{\\int \\nabla_{\\phi} q_{\\phi} \\cdot\\left[\\log P\\left(x_i, z\\right)-\\log q_{\\phi}\\right] d z}_{\\text{Part1}}+\\underbrace{\\int q_{\\phi} \\cdot \\nabla_{\\phi}\\left[\\log P\\left(x_i, z\\right)-\\log q_{\\phi}\\right] d z}_{\\text{Part2}}\n\\end{aligned}\n$$\n我们首先看Part2：\n$$\n\\begin{aligned}\n\\text { Part2 } &=\\int q_{\\phi} \\cdot \\nabla_{\\phi}\\left[\\log P\\left(x_i, z\\right)-\\log q_{\\phi}\\right] d z \\\\\n&=\\int q_{\\phi} \\cdot\\left(-\\nabla_{\\phi} \\log q_{\\phi}\\right) d z \\\\\n&=\\int q_{\\phi} \\cdot\\left(-\\frac{1}{q_{\\phi}} \\nabla_{\\phi} q_{\\phi}\\right) d z \\\\\n&=-\\int \\nabla_{\\phi} q_{\\phi} d z \\\\\n&=-\\nabla_{\\phi} \\int q_{\\phi} d z \\\\\n&=-\\nabla_{\\phi} 1 \\\\\n&=0\n\\end{aligned}\n$$\n所以\n$$\n\\nabla_\\phi\\mathcal{L}(\\phi) = \\int\\nabla_\\phi q_\\phi\\cdot[\\log P(x_i,z)-\\log q_\\phi]dz\n$$\n如果能写成期望的形式，我们就可以采用蒙特卡洛的方法对其进行采样，因此进行一个小的变换：$\\nabla_\\phi q_\\phi = \\nabla(\\log q_\\phi)q_\\phi$，得到\n$$\n\\begin{aligned}\n\\nabla_\\phi\\mathcal{L}(\\phi) &= \\int \\nabla_\\phi(\\log q_\\phi)\\cdot q_\\phi\\cdot[\\log P(x_i,z)-\\log q_\\phi]dz\\\\\n&= \\mathbb{E}_{q_\\phi}[\\nabla_\\phi(\\log q_\\phi)\\cdot (\\log P(x_i,z)-\\log q_\\phi)]\n\\end{aligned}\n$$\n这样就可以采用蒙特卡洛的方式进行采样后求解期望：\n\n从$q_{\\phi}(z)$中采样$z$，$z_l \\sim q_{\\phi}(z),l=1,2,\\cdots,L$，因此：\n$$\n\\nabla_\\phi\\mathcal{L}(\\phi)\\approx \\frac{1}{L}\\sum_{l=1}^L\\nabla_\\phi\\log q_{\\phi}(z_l)(\\log P(x_i,z_l)-\\log q_\\phi(z_l))\n$$\n但是存在一个问题，因为$q_{\\phi}(z)$为概率密度函数，所以其值位于$[0,1]$，在$[0,1]$内对数函数的变化非常大，这就导致其方差较大，导致此方法很可能无法使用。\n\n因此我们采用了重参数化技巧，假定$z = g_\\phi(\\epsilon,x_i),\\epsilon\\sim P(\\epsilon)$，其中$g_\\phi$为参数变换的函数，相当于$z$把随机性转移到了$\\epsilon$上，根据随机变量变换的性质：\n$$\n|q_\\phi(z)dz| = |P(\\epsilon)d\\epsilon|\n$$\n\n> 即：\n> $$\n> \\frac{q_\\phi(z)}{P(\\epsilon)} = \\left|\\frac{dz}{d\\epsilon}\\right|\n> $$\n\n将上述变换代入梯度，得：\n$$\n\\begin{aligned}\n\\nabla_{\\phi} \\mathcal{L}(\\phi) &=\\nabla_{\\phi} \\int\\left[\\log P\\left(x_i, z\\right)-\\log q_{\\phi}\\right] q_{\\phi} d z \\\\\n&=\\nabla_{\\phi} \\int\\left[\\log P\\left(x_i, z\\right)-\\log q_{\\phi}\\right] \\cdot P(\\epsilon) d \\epsilon \\\\\n&=\\nabla_{\\phi} \\mathbb{E}_{P(\\epsilon)}\\left[\\log P\\left(x_i, z\\right)-\\log q_{\\phi}\\right]\n\\end{aligned}\n$$\n因为$P(\\epsilon)$与$\\phi$的梯度无关，因此可以将其放在期望内部：\n$$\n\\begin{aligned}\n\\nabla_{\\phi} \\mathcal{L}(\\phi) &=E_{P(\\epsilon)}\\left[\\nabla_{\\phi}\\left(\\log P\\left(x_i, z\\right)-\\log q_{\\phi}\\right)\\right] \\\\\n&=E_{P(\\epsilon)}\\left[\\nabla_{z}\\left(\\log P\\left(x_i, z\\right)-\\log q_{\\phi}(z)\\right) \\nabla_{\\phi} z\\right] \\\\\n&=E_{P(\\epsilon)}\\left[\\nabla_{z}\\left(\\log P\\left(x_i, z\\right)-\\log q_{\\phi}(z)\\right) \\nabla_{\\phi} g_{\\phi}\\left(\\epsilon, x_i\\right)\\right]\n\\end{aligned}\n$$\n这样就可以再次采用蒙特卡洛的方法：\n$$\n\\begin{aligned}\n&\\epsilon_l \\sim P(\\epsilon), \\quad l=1,2, \\cdots, L \\\\\n&\\nabla_{\\phi} \\mathcal{L}(\\phi) \\approx \\frac{1}{L} \\sum_{l=1}^{L}\\left[\\nabla_{z}\\left(\\log P\\left(x_i, z\\right)-\\log q_{\\phi}(z)\\right) \\nabla_{\\phi} g_{\\phi}\\left(\\epsilon_l, x_i\\right)\\right]\n\\end{aligned}\n$$\n\n\n### Variational Bayes with message passing\n\n上述的手动推导的方式有些繁琐，但是现在的变分信息传递算法(variational message passing(VMP))可以自动对共轭指数分布族进行推导。对于非共轭指数网络，如果需要以牺牲精度为代价快速逼近，VMP 可能仍然有用。\n\n对于指数族分布，其形式为：\n$$\nP(x\\mid \\eta) = h(x)\\exp\\left(\\eta^T\\phi(x)-A(\\eta)\\right)\n$$\n其中$\\eta$为参数，$\\phi(x)$为充分统计量。\n\n关于指数族分布的标准理论证明了指数分布的充分统计量的性质为：\n$$\n\\langle \\phi(x)\\rangle = \\nabla_{\\eta}A(\\eta)\\mid_\\eta\n$$\n即充分统计量的期望为$A(\\eta)$函数对$\\eta$的导数。\n\n我们用$\\text{pa}(z_i)$表示$z_i$的父结点，$\\text{ch}(z_i)$表示其子结点，$\\text{cop}(z_i;\\text{ch})$表示与$x_i$共同子结点为$\\text{ch}$父结点的集合；$\\text{cop}(z_i)$表示与$z_i$有共同子结点的父结点。我们的更新公式为：\n$$\n\\ln q_i(z_i) = \\langle\\ln P(z_i,\\text{mb}(z_i)),D\\rangle_{q(\\text{mb}(z_i))}\n$$\n其中$\\langle\\rangle_{q(\\text{mb}(z_i))}$相当于对$q(\\text{mb}(z_i))$求期望，而$\\text{mb}$表示马尔可夫毯。\n\n> 马尔可夫毯：在随机变量的全集$U$中，对于给定的变量$\\mathrm{X}\\in \\mathrm{U}$和变量集$\\mathrm{MB}\\subset \\mathrm{U}$，若有：\n> $$\n> \\mathrm{X}\\perp \\{\\mathrm{U-MB-\\{X\\}}\\mid\\mathrm{MB}\\}\n> $$\n> 则称能满足上述条件的最小变量集$\\mathrm{MB}$为$\\mathrm{X}$的马尔可夫毯。\n\n则可以写为：\n$$\n=\\langle\\ln P(\\text{pa}(z_i))+\\ln P(\\text{cop}(z_i))+\\ln P(z_i\\mid\\text{pa}(z_i))+\\ln P(\\text{ch}(z_i)\\mid z_i,\\text{cop}(z_i))\\rangle_{q(\\text{mb}(z_i))}\n$$\n去除和$z_i$无关的常数项：\n$$\n=\\langle \\ln P(z_i\\mid \\text{pa}(z_i))\\rangle_{q(\\text{pa}(z_i))} +\\langle\\ln P(\\text{ch}(z_i)\\mid z_i,\\text{cop}(z_i))\\rangle_{q(\\text{ch}(z_i),\\text{cop}(z_i))}\n$$\n将子结点拆开得：\n$$\n=\\langle \\ln P(z_i\\mid \\text{pa}(z_i))\\rangle_{q(\\text{pa}(z_i))} +\\sum_{\\text{ch}\\in\\text{ch}(z_i)}\\langle\\ln P(\\text{ch}\\mid z_i,\\text{cop}(z_i))\\rangle_{q(\\text{ch},\\text{cop}(z_i))}\n$$\n我们将会将这两部分分开考虑\n\n#### Messages from parents\n\n共轭指数节点$z_i$由自然参数向量$\\phi_i$参数化。通过这些节点的定义：\n$$\n\\begin{aligned}\n\\langle\\ln P(z_i\\mid \\text{pa}(z_i))_{q(\\text{pa}(z_i))} &= \\langle \\phi_i\\mu(z_i) + f_i(z_i)+g_i(\\phi_i)\\rangle_{q(\\text{pa}(z_i))}\\\\\n&=\\langle\\phi_i\\rangle_{q(\\text{pa}(z_i))}\\mu_i(z_i)+f_i(z_i)+\\langle g(\\phi_i)\\rangle_{q(\\text{pa}(z_i))}\n\\end{aligned}\n$$\n由于$\\phi$和$g$是父节点充分统计量的多线性函数（通过构造），并且使用平均场假设，我们可以简单地采用它们的公式（定义为以父节点的单个值为条件）并将期望替换为充分统计，根据需要得到整个表达式的期望值。因此$z_i$的父结点只需要将它们的充分统计期望作为信息传递给$z_i$。\n\n#### Messages to parents\n\n指数族的一个关键性质是我们可以通过将其自然参数相加来得到相似分布的乘积：\n$$\n\\begin{aligned}\n&\\exp\\left[\\phi_1\\mu(z_i)+f(z_i)+g(\\phi_1)\\right]\\cdot\\exp\\left[\\phi_2\\mu(z_i)+f(z_i)+g(\\phi_2)\\right]\\\\\n&= \\exp\\left[(\\phi_1+\\phi_2)\\mu(z_i)+f(z_i)+g(\\phi_1+\\phi_2)\\right]\n\\end{aligned}\n$$\n第二个性质是关于共轭，$\\phi$和$g$在父结点的充分统计量中也是多线性的。因此我们总是可以通过找到函数$\\phi_{ij},f_j,g_{ij}$来重新组合公式是其像是一个父结点$z_j\\in\\text{pa}(z_i)$的函数：\n$$\n\\left\\langle\\ln P\\left(z_{i} \\mid \\text{pa}\\left(z_{i}\\right)\\right)\\right\\rangle_{q\\left(\\text{pa}\\left(z_{i}\\right)\\right)}=\\left\\langle\\phi_{i j} u_{j}\\left(z_{j}\\right)+f_{i j}\\left(z_{j}\\right)+g_{i j}\\left(\\phi_{i j}\\right)\\right\\rangle_{q\\left(\\text{pa}\\left(z_{i}\\right)\\right)}\n$$\n和以前一样，我们可以通过使用多线性属性来处理期望，以将所有期望推到充分统计量附近。因此，从父结点的角度来看，这是根据其子结点和共同父结点的充分统计期望来写的。因此，我们可以传递一个似然信息，包括：\n$$\n\\phi_{ij}\\left(\\langle\\mu(z_i)\\rangle,\\{\\langle\\mu(\\text{cop})\\rangle\\}_{\\text{cop}\\in\\text{cop}(z_j;z_i)}\\right)\n$$\n然后，父结点可以通过第一个属性将这些简单地添加到其先前的参数中。\n\n### Example\n\n![](https://raw.githubusercontent.com/HFC666/image/master/img/VBI1.jpg)\n\n则：\n$$\n\\ln P(\\mu\\mid m,\\beta) = \\begin{bmatrix}m\\beta&-\\beta/2\\end{bmatrix}\\cdot \\begin{bmatrix}\\mu\\\\\\mu^2\\end{bmatrix} - \\frac{1}{2}(-\\ln \\beta + \\beta m^2+\\ln2\\pi)\n$$\n其中$\\phi(x) = \\begin{bmatrix}\\mu\\\\\\mu^2\\end{bmatrix}, \\eta^T = \\begin{bmatrix}m\\beta&-\\beta/2\\end{bmatrix},A(\\eta) = \\frac{1}{2}(-\\ln \\beta + \\beta m^2+\\ln2\\pi)$。\n\n所以很容易得到充分统计量的期望为：\n$$\n\\left\\langle \\begin{bmatrix}\\mu\\\\\\mu^2\\end{bmatrix}\\right\\rangle = \\nabla A(\\eta) = \\begin{bmatrix}\\mu\\\\\\mu^2+\\beta^{-1}\\end{bmatrix}\n$$\n\n\n","tags":["贝叶斯","算法"],"categories":["文献阅读"]},{"title":"指数分布族","url":"/2022/07/10/EXP/","content":"\n## Exponential Family Distribution\n\n![](https://raw.githubusercontent.com/HFC666/image/master/img/EX1.png)\n\n> 课程地址：https://www.bilibili.com/video/BV1QW411y7D3?spm_id_from=333.337.search-card.all.click&vd_source=6177c61c946280bb88c727585de76bc8\n\n<!--more-->\n\n### 背景\n\n指数分布族是可以写为如下形式的分布：\n$$\nP(x\\mid \\eta) = h(x)\\exp\\left(\\eta^T\\phi(x)-A(\\eta)\\right)\n$$\n其中$\\eta$为参数向量，$x\\in \\mathbb{R}^p$，$A(\\eta)$为对数配分函数(log partition function)。\n\n下面我们解释一下**配分函数**，配分函数可以理解为**归一化因子**，例如我们在无向图模型中经常用到的：\n$$\nP(X\\mid \\theta) = \\frac{1}{Z}\\hat{P}(X\\mid \\theta)\n$$\n其中$\\hat{P}(X\\mid \\theta)$是我们构造出来的分布，但是概率分布必须满足和为$1$，所以我们在前面添加归一化因子使得：\n$$\nZ = \\int_x \\hat{P}(x\\mid \\theta)dx\n$$\n可以看出$Z$与$X$无关，那为什么$A(\\eta)$称为对数配分函数呢？这是因为：\n$$\n\\begin{aligned}\nP(x\\mid \\eta) &= h(x)\\exp\\left(\\eta^T\\phi(x)-A(\\eta)\\right)\\\\\n&= \\frac{1}{\\exp(A(\\eta))}h(x)\\exp(\\eta^T\\phi(x))\n\\end{aligned}\n$$\n所以$Z = \\exp(A(\\eta))\\Rightarrow A(\\eta)=\\ln Z$，所以其被称为对数配分函数。\n\n其中$\\phi(x)$为**充分统计量**。充分统计量指的是能够包含样本中所有信息的统计量。\n\n如对于数据$x_1,\\cdots,x_N$，我们假设其服从于高斯分布，那么其充分统计量就为：\n$$\n\\phi(x) = \\begin{bmatrix}\\sum_{i=1}^Nx_i\\\\\\sum_{i=1}^Nx_i^2\\end{bmatrix}\n$$\n因为有了这两个统计量我们就可以求出其**均值**和**方差**。\n\n在贝叶斯推断中我们常常遇到这样的问题：\n$$\nP(Z\\mid X) = \\frac{P(X\\mid Z)P(Z)}{\\int_ZP(X\\mid Z)P(Z)dZ}\n$$\n有时候积分很难算出，即使积分算出了，$P(Z\\mid X)$的形式可能很复杂，我们无法求解其期望和方差，这时候我们可以采用采样的方法(MCMC)或者通过变分推断来寻找接近$P(Z\\mid X)$的概率分布$Q(X)$。\n\n但是指数族分布可以采用共轭的性质。\n\n指数族分布与广义线性模型，广义线性模型的重要组成部分为：\n\n1. 线性组合，如$w^Tx$\n2. link function：为激活函数的逆函数\n3. 指数族分布：$y\\mid x\\sim$指数族分布\n\n概率图模型中非常重要的一组模型为无向图RBF，与指数族分布具有非常重要的关系。\n\n另外当分布为指数族分布时，变分推断可见极大地简化。\n\n### 高斯分布的指数族形式\n\n高斯分布的形式为：\n$$\n\\begin{aligned}\nP(x\\mid\\theta) &= \\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\exp\\left(-\\frac{1}{2\\sigma^2}(x-\\mu)^2\\right)\\quad \\theta = (\\mu,\\sigma^2)\\\\\n&=\\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\exp\\left(-\\frac{1}{2\\sigma^2}(x^2-2\\mu x+\\mu^2)\\right)\\\\\n&=\\exp\\left(\\log(2\\pi\\sigma^2)^{-\\frac{1}{2}}\\right)\\exp\\left(-\\frac{1}{2\\sigma^2}\\begin{pmatrix}-2\\mu&1\\end{pmatrix}\\begin{pmatrix}x\\\\x^2\\end{pmatrix}-\\frac{\\mu^2}{2\\sigma^2}\\right)\\\\\n&=\\exp\\left(\\underbrace{\\begin{pmatrix}\\frac{\\mu}{\\sigma^2}&-\\frac{1}{2\\sigma^2}\\end{pmatrix}}_{\\eta^T}\\cdot\\underbrace{\\begin{pmatrix}x\\\\x^2\\end{pmatrix}}_{\\phi(x)}-\\underbrace{\\left(\\frac{\\mu^2}{2\\sigma^2}+\\frac{1}{2}\\log2\\pi\\sigma^2\\right)}_{A(\\eta)}\\right)\n\\end{aligned}\n$$\n其中我们令$\\eta_1=\\frac{\\mu}{\\sigma^2},\\eta_2=-\\frac{1}{2\\sigma^2}$。则$\\sigma^2=-\\frac{1}{2\\eta_2},\\mu=-\\frac{\\eta_1}{2\\eta_2}$。代入$A(\\eta)$，得：\n$$\nA(\\eta) = -\\frac{\\eta_1^2}{4\\eta_2}+\\frac{1}{2}\\log\\left(-\\frac{\\pi}{\\eta_2}\\right)\n$$\n\n### 对数配分函数与充分统计量的关系\n\n\n\n我们之前提到过：\n$$\n\\exp(A(\\eta)) = \\int_x h(x)\\exp(\\eta^T\\phi(x))dx\n$$\n两边同时对$\\eta$求导，得：\n$$\n\\begin{aligned}\n\\exp(A(\\eta))\\cdot A^\\prime(\\eta) &= \\frac{\\partial}{\\partial \\eta}(\\int h(x)\\exp(\\eta^T\\phi(x))dx)\\\\\n&= \\int_xh(x)\\exp(\\eta^T\\phi(x))\\phi(x)dx\n\\end{aligned}\n$$\n两边同除以$A^\\prime(\\eta)$，得\n$$\n\\begin{aligned}\nA^\\prime(\\eta) &= \\frac{\\int_xh(x)\\exp(\\eta^T\\phi(x))\\phi(x)dx}{\\exp(A(\\eta))}\\\\\n&=\\int_x \\underbrace{h(x)\\exp(\\eta^T\\phi(x)-A(\\eta))}_{P(x\\mid\\eta)}\\phi(x)dx\\\\\n&= \\mathbb{E}_{P(x\\mid\\eta)}[\\phi(x)]\n\\end{aligned}\n$$\n所以$A^\\prime(\\eta) = \\mathbb{E}_{P(x\\mid \\eta)}[\\phi(x)]$。\n\n同样地，我们也可以研究一下二阶导，对式子\n$$\nA^\\prime(\\eta) =\\int_x \\underbrace{h(x)\\exp(\\eta^T\\phi(x)-A(\\eta))}_{P(x\\mid\\eta)}\\phi(x)dx\n$$\n两边同时求导得：\n$$\n\\begin{aligned}\nA^{\\prime\\prime}(\\eta) &= \\int_x \\underbrace{h(x)\\exp(\\eta^T\\phi(x)-A(\\eta))}_{P(x\\mid\\eta)}(\\phi(x)-A^{\\prime}(\\eta))\\phi(x)dx\\\\\n&= \\int_xP(x\\mid\\eta)(\\phi(x)-\\mathbb{E}_{P(x\\mid\\eta)}[\\phi(x)])\\phi(x)dx\\\\\n&= \\int_x P(x\\mid \\eta)\\phi(x)^2dx - \\mathbb{E}_{P(x\\mid\\eta)}[\\phi(x)])\\int_xP(x\\mid \\eta)\\phi(x)dx\\\\\n&= \\mathbb{E}_{P(x\\mid\\eta)}[\\phi(x)^2] - \\left(\\mathbb{E}_{P(x\\mid \\eta)}[\\phi(x)]\\right)^2\\\\\n&=\\operatorname{Var}[\\phi(x)]\n\\end{aligned}\n$$\n\n### 极大似然估计与充分统计量\n\n假设我们的数据为：$D=\\{x_1,x_2,\\cdots,x_N\\}$，所以我们有：\n$$\n\\begin{aligned}\n\\eta_{\\text{mle}} &= \\arg\\max\\log P(D\\mid \\eta)\\\\\n&=\\arg\\max \\log\\prod_{i=1}^N P(x_i\\mid \\eta)\\\\\n&=\\arg\\max \\sum_{i=1}^N\\log P(x_i\\mid \\eta)\\\\\n&=\\arg\\max\\sum_{i=1}^N\\log\\left[h(x_i)\\exp(\\eta^T\\phi(x_i)-A(\\eta))\\right]\\\\\n&= \\arg\\max\\sum_{i=1}^N\\left[\\log h(x_i)+\\eta^T\\phi(x_i)-A(\\eta)\\right]\\\\\n&= \\arg\\max\\sum_{i=1}^N(\\eta^T\\phi(x_i)-A(\\eta))\n\\end{aligned}\n$$\n我们对其求导，得：\n$$\n\\begin{aligned}\n\\frac{\\partial}{\\partial \\eta}\\left(\\sum_{i=1}^N\\eta^T\\phi(x_i)-A(\\eta)\\right)&=\\sum_{i=1}^N\\frac{\\partial}{\\partial \\eta}(\\eta^T\\phi(x_i)-A(\\eta))\\\\\n&=\\sum_{i=1}^N\\phi(x_i)-A^{\\prime}(\\eta)\\\\\n&=\\sum_{i=1}^N\\phi(x_i)-NA^{\\prime}(\\eta)\n\\end{aligned}\n$$\n令导数等于$0$，得\n$$\nA^{\\prime}(\\eta_{\\text{mle}}) = \\frac{1}{N}\\sum_{i=1}^N\\phi(x_i)\n$$\n这样我们就可以求出$\\eta_{\\text{mle}}$，可以看出$\\eta_{\\text{mle}}$仅与$\\phi(x)$有关，即确定了$\\phi(x)$即确定了$\\eta_{\\text{mle}}$，即确定了分布，更加验证了$\\phi(x)$充分统计量的结论。\n\n### 最大熵角度\n\n假设一个事件发生的概率为$p$，其信息量为$-\\log p$。熵的概念就是信息量关于分布本身的期望：\n$$\n\\begin{aligned}\n\\mathbb{E}_{p(x)}[-\\log p] &= -\\int_x p(x)\\log p(x)dx\\\\\n&= -\\sum_x p(x)\\log p(x)\n\\end{aligned}\n$$\n最大熵的思想通俗来说就是等可能的，当我们对一个事件一无所知时，我们一般假设其是等可能的。下面看一个例子：\n\n我们用$H[P]$来表示熵：\n$$\nH[p] = -\\sum_x p(x)\\log p(x)\n$$\n我们假设$x$是离散的，$x$可以取值的个数为$K$，概率分别对应于$p_1,\\cdots,p_K$，并且$\\sum_{i=1}^K p_i=1$。所以其熵为：\n$$\nH[p] = -\\sum_{i=1}^K p_i\\log (p_i)\n$$\n我们令其最大，即变为了优化问题：\n$$\n\\begin{aligned}\n&\\min \\sum_{i=1}^K p_i\\log p_i\\\\\n&\\text{s.t. }\\sum_{i=1}^K p_i=1\n\\end{aligned}\n$$\n我们可以直接用拉格朗日乘子法进行求解，定义：\n$$\n\\mathcal{L}(p,\\lambda) = \\sum_{i=1}^K p_i\\log(p_i)+\\lambda(1-\\sum_{i=1}^Kp_i)\n$$\n对$p_i$求导，得：\n$$\n\\frac{\\partial \\mathcal{L}(p,\\lambda)}{\\partial p_i} = \\log(p_i)+p_i\\cdot\\frac{1}{p_i}-\\lambda\n$$\n所以：\n$$\np_i = \\exp(\\lambda-1)\n$$\n对于每个$p_i$都是如此，所以$p_1=p_2=\\cdots=p_K=\\frac{1}{K}$，所以其为等可能的。\n\n\n\n最大熵原理：在满足既定事实的前提下，具有最大熵的分布即为我们想要的分布。在机器学习中，我们的既定事实即为数据，假设我们的数据为$D=\\{x_1,\\cdots,x_N\\}$。\n\n在这里我们引入经验分布的概念，其是对已知样本的描述，其定义为：\n$$\n\\hat{p}(X=x) = \\frac{\\text{count}(x)}{N}\n$$\n因为分布$\\hat{p}$我们已经求出来了，所以对于$x$的任意函数$f(x)$向量，我们也能求其期望：\n$$\n\\mathbb{E}_{\\hat{p}}[f(x)] = \\Delta(\\text{已知})\n$$\n这个就是我们的**已知约束**。\n\n下面我们求满足上述约束的最大熵的分布，这就变成了优化问题：\n$$\n\\begin{aligned}\n&\\min \\sum_x p(x)\\log p(x)\\\\\n&\\text{s.t. } \\sum_x p(x)=1\\\\\n&\\quad\\quad \\mathbb{E}_{\\hat{p}} [f(x) ]= \\mathbb{E}_p[f(x)] = \\Delta\n\\end{aligned}\n$$\n定义拉格朗日乘子：\n$$\n\\mathcal{L}(p,\\lambda_0,\\lambda) = \\sum_x p(x)\\log p(x) + \\lambda_0(1-\\sum_x p(x))+\\lambda^T(\\Delta-\\mathbb{E}_p[f(x)])\n$$\n对$p(x)$求导得：\n$$\n\\frac{\\mathcal{L}}{\\partial p(x)} =(\\log p(x)+1)-\\lambda_0-\\lambda^T f(x)\n$$\n令其等于$0$，得\n$$\n\\log p(x) = \\lambda^T f(x) + \\lambda_0-1\n$$\n所以\n$$\np(x) = \\exp\\left(\\lambda^T f(x) - (1-\\lambda_0)\\right)\n$$\n为指数族分布。\n\n\n\n","tags":["概率论"],"categories":["课程笔记"]},{"title":"狄利克雷过程","url":"/2022/07/06/BNP/","content":"\n<p align=\"center\">\n    <img src=\"https://img2.baidu.com/it/u=2682475456,642314187&fm=253&fmt=auto&app=138&f=JPEG?w=799&h=500\" style=\"zoom: 100%;\" />\n</p>\n\n> 参看文献：\n>\n> 1. [徐亦达老师课程](https://www.bilibili.com/video/BV1Tp411R7Sf?spm_id_from=333.337.search-card.all.click&vd_source=6177c61c946280bb88c727585de76bc8)\n> 2. [狄利克雷过程文献](http://hil.t.u-tokyo.ac.jp/~kameoka/SAP/papers/Teh2010a.pdf)\n> 3. [A Tutorial on Bayesian Nonparametric Models](https://www.sciencedirect.com/science/article/abs/pii/S002224961100071X)\n\n<!--more-->\n\n## 狄利克雷过程\n\n### Introduction\n\n考虑下列问题，假设我们用高斯混合模型来做聚类。假设我们的数据为$x_1,x_2,\\cdots,x_N$，那我们的似然函数的对数为：\n$$\n\\sum_{i=1}^N\\log\\sum_{l=1}^K\\alpha_i\\mathcal{N}(\\mu_i,\\sigma_i^2)\n$$\n但是我们需要事先确定聚类的个数，但是在很多情况下聚类的个数并不是那么容易确定，我们需要从数据中学习到聚类的个数。一个方法是我们将聚类的数目$K$也作为一个参数，那么我们的参数$\\theta = (K,\\theta_1,\\sigma_1,\\cdots,\\theta_K,\\sigma_K)$。我们的优化问题变为：\n$$\n\\hat{\\theta} = \\arg\\max_{\\theta}\\sum_{i=1}^N\\log\\sum_{l=1}^K\\alpha_i\\mathcal{N}(\\mu_i,\\sigma_i^2)\n$$\n但是我们很容易发现当$K=N$，$\\mu_i$为数据的值，$\\sigma_i=0$的时候似然函数达到最大，即每一个数据都是一个类，这不是我们想要的。\n我们的一个方法是假设每个数据$x_i$都来自于参数为$\\theta_i$的一个分布，而$\\theta_i\\sim H(\\theta)$，其中分布$H(\\theta)$为连续分布，但是这样会参在新的问题：因为$H(\\theta)$为连续分布，所以$P(\\theta_i=\\theta_j)=0,i\\neq j$。所以这样每个数据来自的分布都不一样，又回到了之前提到的$K=N$的问题了。所以我们令$\\theta$来自一个离散分布$G(\\theta)$，而$G\\sim \\text{DP}(\\alpha,H)$，其中DP表示狄利克雷过程，而$H$为之前的连续分布，$\\alpha>0$为常数，其反映$G$的离散程度，越大表示离散程度越小，当$\\alpha\\rightarrow0$时，$G$离散程度最大，为一个点；当$\\alpha\\rightarrow \\infty$时，$G\\approx H$。\n\n> $H$也不一定是连续的，其被称为base measure。\n\n注意这里的$G$为一个random measure，我们每次从$\\text{DP}(\\alpha,H)$中采样得到的不是一个数值，而是一个分布。假设我们采集到的分布为：\n\n![](https://raw.githubusercontent.com/HFC666/image/master/img/DP1.png)\n\n> 其中$G_1,G_2$为两次采样产生的分布，上面的棍子表示概率密度，其和为$1$。我们将其分为$a_1,a_2,\\cdots,a_d$等$d$个区域，其中每个区域的总概率密度符合狄利克雷分布，即\n> $$\n> (G(a_1),G(a_2),\\cdots,G(a_d))\\sim \\text{Dir}(\\alpha H(a_1),\\alpha H(a_2),\\cdots,\\alpha H(a_d))\n> $$\n> 这就是狄利克雷过程的定义。\n\n关于狄利克雷分布：\n$$\nP(x_1,\\cdots,x_i,\\cdots,x_K)\\sim \\text{Dir}(\\alpha_1,\\cdots,\\alpha_i,\\cdots,\\alpha_K)\n$$\n则\n$$\n\\begin{aligned}\n\\mathbb{E}[x_i]  &= \\frac{\\alpha_i}{\\sum_k \\alpha_k}\\\\\n\\text{Var}[x_i]&=\\frac{\\alpha_i(\\sum_k\\alpha_k-\\alpha_i)}{(\\sum_k\\alpha_k)^2(\\sum_{k}\\alpha_k+1)}\n\\end{aligned}\n$$\n将其带入到狄利克雷过程中，得：\n$$\n\\mathbb{E}[G[a_i]] = \\frac{\\alpha H(a_i)}{\\sum_k\\alpha H(a_k)} = H(a_i)\n$$\n\n$$\n\\text{var}[G[a_i]] = \\frac{\\alpha H(a_i)(\\alpha-\\alpha H(a_i))}{\\alpha^2(\\alpha+1)} = \\frac{H(a_i)(1-H(a_i))}{\\alpha+1}\n$$\n\n我们之前说过关于$\\alpha$的性质。现在我们来看一下，均值(期望)与$\\alpha$无关，当$\\alpha\\rightarrow \\infty$是，方差趋近于$0$，这说明不管我们怎么划分，在每个$a_i$处，$G(a_i)=H(a_i)$，说明$G(x)=H(x)$，即$G(x)$是连续的是最不离散的版本；如果$\\alpha=0$，则$\\text{Var}=H(a_i)(1-H(a_i))$，这正是伯努利分布的方差，因此在每个划分上我们都可以用一根棍来表示它们(包括不划分)，这是最离散的版本。\n\n### Construction\n\n那我们如何构建$G$呢？即如何从$H$中采样得到$G$呢？我们采用`stick-breaking construction`的方法来产生$G$。其构建方式为：\n\n1. 从$H$中采样得到$\\theta_1$，即$\\theta_1\\sim H$\n2. 采样$\\beta_1\\sim\\text{Beta}(1,\\alpha)$\n3. 权重$\\pi_1=\\beta_1$\n4. 采样$\\theta_2\\sim H$\n5. 采样$\\beta_2\\sim \\text{Beta}(1,\\alpha)$\n6. 权重$\\pi_2 = (1-\\pi)\\beta_2$\n   以此类推，这样权重$\\pi_2$相当于是从取完$\\pi_1$剩下的权重中取得。\n\n我们在看一下关于$\\alpha$得一些内容，当$\\alpha=0$时，$\\mathbb{E}(\\beta_i)=1,\\text{Var}(\\beta_i)=0$，所以一次就把权重全部采完，即只产生一个样本，对应于最离散的情况，而当$\\alpha=\\infty$时，$\\mathbb{E}(\\beta_i)=0$，相当于连续分布的情况。我们常将此采样方法写为$\\pi \\sim \\text{GEM}(\\alpha)$。\n\n### Property\n\n下面我们再回顾一下。对于狄利克雷过程，有如下性质：\n$$\nG\\sim \\text{DP}(\\alpha,H)\\Leftrightarrow (G(a_1),\\cdots,G(a_n))\\sim \\text{DIR}(\\alpha H(a_1),\\cdots,\\alpha H(a_n)),\\quad \\text{for any partitions }a_1,\\cdots,a_n\n$$\n总结之前讲过的，我们有：\n$$\nG\\sim \\text{DP}(\\alpha,H)\n$$\n\n$$\n\\theta_1,\\cdots,\\theta_N\\sim G\n$$\n\n$$\nX_i\\sim F(\\theta_i)\n$$\n\n其图模型为：\n\n![](https://raw.githubusercontent.com/HFC666/image/master/img/DP2.png)\n\n下面我们研究一下$G$的后验分布$P(G\\mid \\theta_1,\\cdots,\\theta_N)$：\n$$\nP(G\\mid \\theta_1,\\cdots,\\theta_N)\\propto P(\\theta_1,\\cdots,\\theta_N\\mid G)P(G)\n$$\n在研究之前我们先看一个关于狄利克雷分布和多项式分布的例子：\n假设\n$$\n\\begin{aligned}\np_1,\\cdots,p_N&\\sim\\text{DIR}(\\alpha_1,\\cdots,\\alpha_N)\\\\\nn_1,\\cdots,n_N&\\sim\\text{Mult}(p_1,\\cdots,p_N)\n\\end{aligned}\n$$\n那么\n$$\n\\begin{aligned}\nP(p_1,\\cdots,p_N\\mid n_1,\\cdots,n_N)&\\propto\\left(\\frac{\\Gamma(\\sum_{i=1}^N\\alpha_i)}{\\prod_{i=1}^N\\Gamma(\\alpha_i)}\\prod P_i^{\\alpha_i-1}\\right)\\left(\\frac{(\\sum_{i=1}^Nn_i)!}{n_1!\\cdots n_N!}\\prod_{i=1}^NP_i^{n_i}\\right)\\\\&\\propto \\prod_{i=1}^NP_i^{\\alpha_i+n_i-1}\\\\ &= \\text{DIR}(\\alpha_1+n_1,\\cdots,\\alpha_N+n_N)\n\\end{aligned}\n$$\n\n### Posterior\n\n有了前面的指示，下面我们来看一下后验分布：\n对于任何划分\n$$\n\\begin{aligned}\nP(G(a_1),\\cdots,G(a_K)\\mid \\theta_1,\\cdots,\\theta_K)&\\propto P(\\theta_1,\\cdots,\\theta_K\\mid G(a_1),\\cdots,G(a_K))P(G)\\\\\n&=\\text{Mult}(n_1,\\cdots,n_K)\\text{DIR}(\\alpha H(a_1),\\cdots,\\alpha H(a_K))\\\\\n&= \\text{Dir}(n_1+\\alpha H(a_1),\\cdots,n_K+\\alpha H(a_K))\\\\\n&= \\text{DP}\\left(\\alpha+n,\\frac{\\alpha H+\\sum_{i=1}^K\\delta_{\\theta_i}}{\\alpha+n}\\right)\n\\end{aligned}\n$$\n其中$n=\\sum_{i=1}^K n_i，G=\\sum_{i=1}^\\infty\\pi_i\\delta_{\\theta_i}$。最后一步是怎么来的呢？\n所以我们得到的后验分布为：\n$$\nP(G(a_1),\\cdots,G(a_K))\\sim \\text{Dir}(n_1+\\alpha H(a_1),\\cdots,n_K+\\alpha H(a_K))\n$$\n根据之前讲过的狄利克雷过程的性质，狄利克雷过程的第一个参数是对应的狄利克雷分布的测度和，而第二个参数为归一化后的一个概率分布，理解为狄利克雷分布的参数除以归一化系数。所以第一个参数为：$\\sum_{i=1}^K n_i+\\alpha H(a_i) = \\alpha+n$，而第二个参数为$\\frac{\\alpha H+\\sum_{i=1}^K\\delta_{\\theta_i}}{\\alpha+n}$，其中$\\alpha+n$为归一化参数，而分子$\\sum_{i=1}^N\\delta_{\\theta_i}(a_j)$实际上就是表示$n_j$。\n我们再看一下得到的分布：\n$$\n\\frac{\\alpha H+\\sum_{i=1}^N\\delta_{\\theta_i}}{\\alpha+n} = \\frac{\\alpha}{\\alpha+n}H+\\frac{\\sum_{i=1}^N\\delta_{\\theta_i}}{\\alpha+n}\n$$\n为一个连续的分布加上一个离散的分布，这被称为`spike and slab`。\n\n### Predictive distribution\n\n什么是预测分布呢？预测分布为：\n$$\n\\begin{aligned}\nP(X_i\\mid X_{-i}) &= \\int_w P(X_i,w\\mid X_{-i})dw\\\\\n&= \\int_w P(X_i\\mid w,X_{-i})P(w\\mid X_{-i})dw\\\\\n&= \\int_wP(X_i\\mid w)P(w\\mid X_{-i})dw\n\\end{aligned}\n$$\n其中$X_{-i}$表示去除第$i$项后的$X$。\n对于狄利克雷过程，我们想要求得的为：\n$$\nP(\\theta_i\\mid \\theta_{-i}) = \\int_G P(\\theta_i\\mid G)P(G\\mid \\theta_{-i})dG\n$$\n由此可以看出，我们的预测分布可以看作是后验分布$\\theta_i$在后验分布$P(G\\mid \\theta_{-i})$下的期望，根据狄利克雷分布的性质，其期望为\n$$\n\\frac{\\alpha}{\\alpha+n}H+\\frac{\\sum_{i=1}^N\\delta_{\\theta_i}}{\\alpha+n}\n$$\n所以其预测分布等于后验分布。\n\n$\\theta_1,\\theta_2,\\cdots$预测分布的序列被称为`Blackwell-MacQueen urn scheme`。这个名字来源于一个隐喻。特别地，在$\\Theta$中的每一个值都是唯一的颜色，并且抽样$\\theta\\sim G$来给球上色。另外我们有一个盒子来装之前看过的球。起初在盒子里没有球，我们从$H$中取颜色，$\\theta_1\\sim H$，给球上色并将其放在盒子里。在之后的步骤中，如在$n+1$步中，我们要么以$\\frac{\\alpha}{\\alpha+n}$抽取一个新颜色($\\theta_{n+1}\\sim H$)，给球染色并将其放到盒子中，或者以概率$\\frac{n}{\\alpha+n}$从盒子中取出一个球，将新球涂成它的颜色(从经验分布中抽样)并放到盒子里。\n\n`Blackwell-MacQueen urn scheme`可以被用来证明DP的存在。我们可以在序列$\\theta_1,\\theta_2,\\cdots$上构建分布，通过迭代地在给定$\\theta_1,\\cdots,\\theta_{i-1}$的条件下采样$\\theta_i$。对于$n\\ge1$令\n$$\nP(\\theta_1,\\cdots,\\theta_n) = \\prod_{i=1}^nP(\\theta_i\\mid \\theta_1,\\cdots,\\theta_{i-1})\n$$\n可以得到这个随机序列是无限可交换的。也就是说，对于每一个$n$，生成$\\theta_1,\\cdots,\\theta_n$的概率等于以任何顺序采样得到它们的概率。\n\n下面我们来证明一下，令$I_k$表示第$k$类的索引，$N_k$表示第$k$类的样本数，那么在第$k$类的样本的上述关于$\\theta$的概率为：\n$$\n\\frac{\\alpha\\cdot1\\cdot2\\cdots(N_k-1)}{(I_{k,1}-1+\\alpha)(I_{k,2}-1+\\alpha)\\cdots(I_{k,N_k)}-1+\\alpha)}\n$$\n第一项是因为我们第一次到新的类$k$，所以概率为$\\frac{\\alpha}{I_{k,1}-1+\\alpha}$，第二项是因为$k$已经出现了，所以概率为$\\frac{1}{(I_{k,2}-1+\\alpha)}$，以此类推。对于所有的类：\n$$\np(\\theta_{1:N}) = \\prod_{k=1}^K\\frac{\\alpha(N_k-1)!}{(I_{k,1}-1+\\alpha)(I_{k,2}-1+\\alpha)\\cdots(I_{k,N_k}-1+\\alpha)}\n$$\n注意所有$I_k$的并为所有的索引，我们将索引合并，得：\n$$\np(\\theta_{1:N}) = \\frac{\\alpha^K\\prod_{k=1}^K(N_k-1)!}{\\prod_{i=1}^N(i-1+\\alpha)}\n$$\n所以很明显看出来是无限可交换的。\n\n\n\n现在`de Finetti's theorem`说明对于任何无限可交换序列$\\theta_1,\\theta_2,\\cdots$存在一个随机分布$G$使得序列可以被分解为独立同分布地从下列采样：\n$$\nP(\\theta_1,\\cdots,\\theta_n) = \\int\\prod_{i=1}^n G(\\theta_i)dP(G)\n$$\n在我们的设置中，随机分布$P(G)$的先验正是狄利克雷过程$\\text{DP}(\\alpha,H)$，因此DP存在。\n\n### Clustering, Partitions and the Chinese Restaurant Process\n\nDP的离散性质也暗示了聚类的特性。现在我们假设$H$是光滑的，因此所有的重复值都由于DP的离散性质而不是$H$自身。因此采样得到的值有重复的，令$\\theta_1^\\star,\\cdots,\\theta_m^\\star$为$\\theta_1,\\cdots,\\theta_n$去除重复值后的结果并且$n_k$为$\\theta_k^\\star$重复的次数。预测分布可以被等价地写为：\n$$\n\\theta_{n+1}\\mid \\theta_1,\\cdots,\\theta_n\\sim \\frac{1}{\\alpha+n}\\left(\\alpha H+\\sum_{k=1}^m n_k\\delta_{\\theta_k^\\star}\\right)\n$$\n\n\n我们可以通过查看由聚类引起的划分来进一步研究DP的聚类属性。$\\theta_1,\\cdots,\\theta_n$去除重复值后将对集合$[n]=\\{1,\\cdots,n\\}$分区引进了聚类使得在每一个类$k$中，$\\theta_i$取相同的值$\\theta_k^\\star$。\n\n分区的分布被称为中国餐馆过程(CRP)。在这个情境下我们有一个中国餐馆，里面有无穷多个桌子，每个桌子可以坐无穷多个人。第一个人进入餐馆坐在第一个位置，第二个人可以坐在第一个人的位置或者坐在新的位置。一般地，第$n+1$个人要么以正比于$n_k$的概率坐在已经有人的位置$k$，或者以正比于$\\alpha$的概率坐在新的位置。\n\n我们也可以估计聚类数目的期望。假设共有$n$个观测，对于$i\\ge1$，观测$\\theta_i$以$\\frac{\\alpha}{\\alpha+i-1}$的概率取新的值，所以聚类数$m$的期望为：\n$$\n\\mathbb{E}[m\\mid n] = \\sum_{i=1}^n\\frac{\\alpha}{\\alpha+i-1}\\in \\mathcal{O}(\\alpha \\log n)\n$$\n\n\n因为$\\theta$为离散值，具有相同$\\theta$值得数据表示属于同一类，我们可以用$Z$来表示属于哪一类。即计算$P(Z_i\\mid Z_{-i})$。有多少类只与参数$\\alpha$有关，而$\\theta$的位置(值)则与$H$有关。\n\n\n\n我们下面计算：\n$$\nP(Z_i=m\\mid Z_{-i}) = \\frac{P(Z_i=m,Z_{-i})}{P(Z_{-i})}\n$$\n我们如何将其与狄利克雷过程结合起来呢？将其与狄利克雷过程结合起来很难，因为狄利克雷过程有无限多的项，我们可以假设其外$K$项，然后再将$K$取无穷。我们用如下方法：\n$$\n\\begin{aligned}\nP(Z_i=m\\mid Z_{-i}) &= \\frac{P(Z_i=m,Z_{-i})}{P(Z_{-i})}\\\\\n&=\\frac{\\int_{P_1,\\cdots,P_K}P(Z_i=m,Z_{-i}\\mid P_1,\\cdots,P_K)\\text{DIR}(\\alpha/K,\\cdots,\\alpha/K)dP}{\\int_{P_1,\\cdots,P_K}P(Z_{-i}\\mid P_1,\\cdots,P_K)\\text{DIR}(\\alpha/K,\\cdots,\\alpha/K)dP}\n\\end{aligned}\n$$\n关于积分的计算就用到我们之前的多项式分布和狄利克雷分布共轭的知识了：\n$$\n\\begin{aligned}\n&\\int_{p_1,\\cdots,p_K}P(n_1,\\cdots,n_K\\mid p_1,\\cdots,p_K)P(p_1,\\cdots,p_K\\mid \\alpha_1,\\cdots,\\alpha_K)dP\\\\\n&=\\frac{n!}{n_1!\\cdots n_K!}\\frac{\\Gamma(\\sum\\alpha_i)}{\\prod\\Gamma(\\alpha_i)}\\int_{p_1,\\cdots,p_K}\\prod_{i=1}^K p_i^{n_i+\\alpha_i-1}dp\\\\\n&=\\frac{n!}{n_1!\\cdots n_K!}\\frac{\\Gamma(\\sum\\alpha_i)}{\\prod\\Gamma(\\alpha_i)}\\frac{\\prod\\Gamma(\\alpha_i+n_i)}{\\Gamma(\\sum\\alpha_i+n)}\n\\end{aligned}\n$$\n我们将其应用到分子和分母上，首先定义符号：$n_{l,-i}$表示去除第$i$个数据后属于第$l$类的个数。我们知道第二项：\n$$\n\\frac{\\Gamma(\\sum\\alpha_i)}{\\prod\\Gamma(\\alpha_i)}\n$$\n只与先验有关，所以分子分母都一样。\n\n我们先看第一项，因为在我们的情景下，即便类数相等，每个类的个数相等，但是相同个体不属于同一个类，这两种划分方法得到的第一项的值是相同的，但是在我们的情境下是不同的，所以第一项不应该存在。\n\n第三项代入为：\n$$\n\\frac{\\Gamma(\\alpha/K+n_{m,-i}+1)\\prod_{l\\neq i} \\Gamma(\\alpha/K+n_{l,-i})}{\\Gamma(\\alpha+n)}\\cdot \\frac{\\Gamma(\\alpha+n-1)}{\\prod\\Gamma(\\sum_l \\alpha/K+n_{l,-i})}\n$$\n伽马函数具有如下性质：\n$$\n\\Gamma(x) = (x-1)\\Gamma(x-1)\n$$\n上式化简得到：\n$$\n\\frac{n_{m,-i}+\\frac{\\alpha}{K}}{n+\\alpha-1}\n$$\n\n\n当$K\\rightarrow \\infty$为\n$$\n\\frac{n_{m,-i}}{n+\\alpha-1}\n$$\n对$m$进行求和得到：\n$$\n\\sum_m\\frac{n_{m,-i}}{n+\\alpha-1} = \\frac{n-1}{n+\\alpha-1}\\neq1\n$$\n这与概率密度的定义不同，所以有$\\frac{\\alpha}{n+\\alpha-1}$的概率属于新的一个类，这就是**中国餐馆过程**。\n\n### Dirichlet Process Mixture Models\n\n狄利克雷混合模型可以写为：\n$$\n\\begin{aligned}\n&\\pi \\mid \\alpha \\sim \\operatorname{GEM}(\\alpha)\\\\\n&\\theta_{k}^{*} \\mid H \\sim H\\\\\n&z_{i} \\mid \\pi \\sim \\operatorname{Mult}(\\pi)\\\\\n&x_{i} \\mid z_{i},\\left\\{\\theta_{k}^{*}\\right\\} \\sim F\\left(\\theta_{z_{i}}^{*}\\right)\n\\end{aligned}\n$$\n狄利克雷混合模型为无限混合模型，指的是具有无限可数个类的混合模型。与事先确定了类的有限混合模型不同，狄利克雷混合模型会根据数据确定聚类的数目。\n\n> 狄利克雷过程为无参贝叶斯方法的一种，为什么是无参？我们理解是狄利克雷过程是分布的先验，而狄利克雷过程采样得到的分布为我们产生数据的分布，在有参数的模型中此分布有参数(感觉像废话)，而狄利克雷过程产生的分布无法用参数表示故为无参数模型。我们将分布作为概率的一部分，并且存在自己的先验分布，也可以让我们通过数据来自动调节分布的复杂度。","tags":["贝叶斯","算法"],"categories":["文献阅读"]},{"title":"书籍Thinking Julia后几章阅读笔记","url":"/2022/07/03/thinkjulia2/","content":"\n<p align=\"center\">\n    <img src=\"https://gimg2.baidu.com/image_search/src=http%3A%2F%2Fi0.hdslb.com%2Fbfs%2Farticle%2F0641bd5a05ae260e3167b7a6b6f389143abb0416.jpg&refer=http%3A%2F%2Fi0.hdslb.com&app=2002&size=f9999,10000&q=a80&n=0&g=0n&fmt=auto?sec=1659401858&t=1004217d7de1a7d354b99be904052734\" style=\"zoom: 100%;\" />\n</p>\n\n> 书籍网址：https://benlauwens.github.io/ThinkJulia.jl/latest/book.html\n\n<!--more-->\n\n## Dictionaries\n\n### A Dictionary Is a Mapping\n\n~~~julia\njulia> eng2sp = Dict()\nDict{Any, Any}()\n~~~\n\n我们可以使用方括号为字典添加数据：\n\n~~~julia\njulia> eng2sp[\"one\"] = \"uno\"\n\"uno\"\njulia> eng2sp\nDict{Any, Any} with 1 entry:\n  \"one\" => \"uno\"\n~~~\n\n上述输出的格式也可以作为输入的格式：\n\n~~~julia\njulia> eng2sp = Dict(\"one\"=>\"uno\", \"two\"=>\"dos\",\"three\"=>\"tres\")\nDict{String, String} with 3 entries: …\njulia> eng2sp\nDict{String, String} with 3 entries:\n  \"two\"   => \"dos\"\n  \"one\"   => \"uno\"\n  \"three\" => \"tres\"\n~~~\n\n可以看出字典输出的顺序与我们输入的顺序不同，在`Julia`中，字典的输出顺序是不可预测的。\n\n查看长度：\n\n~~~julia\njulia> length(eng2sp)\n3\n~~~\n\n`keys`函数返回字典的键：\n\n~~~julia\njulia> ks = keys(eng2sp);\njulia> print(ks)\n[\"two\", \"one\", \"three\"]\n~~~\n\n也可以用 `∈` 符号查看某个字符串是否在keys里：\n\n~~~julia\njulia> \"one\" ∈ ks\ntrue\n~~~\n\n使用`values`函数获取字典的值：\n\n~~~julia\njulia> vs = values(eng2sp);\n\njulia> \"uno\" ∈ vs\ntrue\n~~~\n\n字典有`get`函数，给定key获取其值，如果key不存在，则返回默认值：\n\n~~~julia\njulia> get(eng2sp, \"one\",1)    \n\"uno\"                          \n                               \njulia> get(eng2sp, \"four\",1)   \n1                              \n~~~\n\n### Reverse Lookup\n\n我们可以根据值找键：\n\n~~~julia\njulia> findall(isequal(\"uno\"), eng2sp) \n1-element Vector{String}:              \n \"one\"                                 \n~~~\n\n### Global Variables\n\n全局变量是位于`Main`中的变量，我们一般将其用为`flag`来表示真假，如：\n\n~~~julia\nverbose = true\n\nfunction example1()\n    if verbose\n        println(\"Running example1\")\n    end\nend\n~~~\n\n但是如果你\n\n~~~julia\nbeen_called = false\n\nfunction example2()\n    been_called = true         # WRONG\nend\n~~~\n\n你会发现`been_called`的值没有改变，这是因为函数中的`been_called`是局部变量，与函数外的变量无关系。\n\n要在函数内部重新分配全局变量，必须在使用它之前声明该变量为全局变量：\n\n~~~julia\nbeen_called = false\n\nfunction example2()\n    global been_called\n    been_called = true\nend\n~~~\n\n如果一个全局变量引用了一个可变值，您可以修改该值，而不需要声明变量为全局变量：\n\n~~~julia\nknown = Dict(0=>0, 1=>1)\n\nfunction example4()\n    known[2] = 1\nend\n~~~\n\n因此，您可以添加、删除和替换全局数组或字典的元素，但如果您想重新分配变量，您必须声明它是全局的：\n\n~~~julia\nknown = Dict(0=>0, 1=>1)\n\nfunction example5()\n    global known\n    known = Dict()\nend\n~~~\n\n## Tuples\n\n### Tuples Are Immutable\n\n元组是不可变的，其元素可以是不同的类型：\n\n~~~julia\njulia> t = 'a', 'b', 'c', 'd', 'e'\n('a', 'b', 'c', 'd', 'e')\n~~~\n\n创建只有一个元素的数组，后面必须加`,`：\n\n~~~julia\njulia> t1 = ('a',)\n('a',)\njulia> typeof(t1)\nTuple{Char}\n~~~\n\n如果提供了多个参数，则结果是一个具有给定参数的元组\n\n~~~julia\njulia> t3 = tuple(1, 'a', pi)\n(1, 'a', π = 3.1415926535897...)\n~~~\n\n可以使用数字对元组进行索引：\n\n~~~julia\njulia> t = ('a', 'b', 'c', 'd', 'e');\n\njulia> t[1]\n'a': ASCII/Unicode U+0061 (category Ll: Letter, lowercase)\njulia> t[2:4]\n('b', 'c', 'd')\n~~~\n\n但是如果你尝试改变元组的值则会报错：\n\n~~~julia\njulia> t[1] = 'A'\nERROR: MethodError: no method matching setindex!(::NTuple{5,Char}, ::Char, ::Int64)\n~~~\n\n元组逐元素比较大小：\n\n~~~julia\njulia> (0, 1, 2) < (0, 3, 4)\ntrue\njulia> (0, 1, 2000000) < (0, 3, 4)\ntrue\n~~~\n\n### Tuple Assignment\n\n交换`a,b`的值我们可以：\n\n~~~julia\na, b = b, a\n~~~\n\n左侧的值的数量要少于右边值的数量：\n\n~~~julia\njulia> (a, b) = (1, 2, 3)\n(1, 2, 3)\njulia> a  \n1         \n          \njulia> b  \n2         \njulia> a, b, c = 1, 2\nERROR: BoundsError: attempt to access (1, 2)\n  at index [3]\n~~~\n\n### Tuples as Return Values\n\n元组也可以作为返回值，如内置函数`diverm`返回整除和余数：\n\n~~~julia\njulia> q, r = divrem(7, 3);\n\njulia> @show q r;\nq = 2\nr = 1\n~~~\n\n### Variable-length Argument Tuples\n\n函数可以接受可变数量的参数。末尾带有`...`的参数将参数聚集成元组：\n\n~~~julia\nfunction printall(args...)\n    println(args)\nend\n\njulia> printall(1, 2.0, '3')\n(1, 2.0, '3')\n~~~\n\n同样的元组后加`...`可以将元组分解作为多个参数传入：\n\n~~~julia\njulia> t = (7, 3);\n\njulia> divrem(t)\nERROR: MethodError: no method matching divrem(::Tuple{Int64,Int64})\njulia> divrem(t...)\n(2, 1)\n~~~\n\n### Arrays and Tuples\n\n内建函数`zip`可以将两个或多个序列转换为元组的集合，每个元组包含每个序列的一个元素。\n\n~~~julia\njulia> s = \"abc\";\n\njulia> t = [1,2,3];\n\njulia> zip(s,t)\nzip(\"abc\", [1, 2, 3])\n~~~\n\n返回一个`zip`对象，`zip`对象是一种迭代器，它是对序列进行迭代的任何对象。迭代器在某些方面类似于数组，但与数组不同的是，不能使用索引从迭代器中选择元素。我们可以使用`for`循环对其进行遍历：\n\n~~~julia\njulia> for pair in zip(s,t)\n       println(pair)\n       end\n('a', 1)\n('b', 2)\n('c', 3)\n~~~\n\n如果你想用数组的方法来操纵`zip`对象，可以将其转换为数组：\n\n~~~julia\njulia> collect(zip(s,t))\n3-element Vector{Tuple{Char, Int64}}:\n ('a', 1)\n ('b', 2)\n ('c', 3)\n~~~\n\n如果序列长度不一样，那么用长度较小的序列：\n\n~~~julia\njulia> collect(zip(\"Anne\", \"Elk\"))\n3-element Array{Tuple{Char,Char},1}:\n ('A', 'E')\n ('n', 'l')\n ('n', 'k')\n~~~\n\n你可以在`for`循环中使用元组赋值来遍历元组数组：\n\n~~~julia\njulia> t = [('a', 1), ('b', 2), ('c', 3)];\n\njulia> for (letter, number) in t\n           println(number, \" \", letter)\n       end\n1 a\n2 b\n3 c\n~~~\n\n使用`enumerate`来获得索引和元素：\n\n~~~julia\njulia> for (index, element) in enumerate(\"abc\")\n           println(index, \" \", element)\n       end\n1 a\n2 b\n3 c\n~~~\n\n### Dictionaries and Tuples\n\n我们可以使用元组来初始化字典：\n\n~~~julia\njulia> t = [('a', 1), ('c', 3), ('b', 2)];\n\njulia> d = Dict(t)\nDict{Char,Int64} with 3 entries:\n  'a' => 1\n  'c' => 3\n  'b' => 2\n~~~\n\n将`zip`和`Dict`结合起来创建字典很简洁：\n\n~~~julia\njulia> d = Dict(zip(\"abc\", 1:3))\nDict{Char,Int64} with 3 entries:\n  'a' => 1\n  'c' => 3\n  'b' => 2\n~~~\n\n## Files\n\n### Reading and Writing\n\n使用`w`模式对文件进行写，如果文件存在则清空以前的内容，不存在则重新创建一个。\n\n~~~julia\njulia> fout = open(\"output.txt\", \"w\")\nIOStream(<file output.txt>)\n~~~\n\n写入\n\n~~~julia\njulia> line1 = \"This here's the wattle,\\n\"\n\"This here's the wattle,\\n\"\n\njulia> write(fout, line1)\n24\n~~~\n\n返回的为写入的字符串长度。\n\n当写入完毕后，要关闭文件：\n\n~~~julia\njulia> close(fout)\n~~~\n\n### Filenames and Paths\n\n`pwd`获取当前路径，`abspath`获取文件所处绝对路径，`ispath`判断是否为路径，`isdir`判断是否为文件夹，`readdir`读取路径下所有文件和路径。\n\n### Catching Exceptions\n\n~~~julia\ntry\n    fin = open(\"bad_file.txt\")\ncatch exc\n    println(\"Something went wrong: $exc\")\nfinally\n    println(\"finally\")\nend\n~~~\n\n### Serialization\n\n函数`serialize`和`deserialize`可以将任何类型的对象转为字节数组。\n\n~~~julia\njulia> using Serialization\n\njulia> io = IOBuffer()\nIOBuffer(data=UInt8[...], readable=true, writable=true, seekable=true, append=false, size=0, maxsize=Inf, ptr=1, mark=-1)\n\njulia> t = [1,2,3];\n\njulia> serialize(io, t)\n24\n\njulia> print(take!(io))\nUInt8[0x37, 0x4a, 0x4c, 0x0f, 0x04, 0x00, 0x00, 0x00, 0x15, 0x00, 0x08, 0xe2, 0x01, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x02, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x03, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00]\n~~~\n\n输出的类型不利于我们阅读，我们可以将其转换为原来的格式：\n\n~~~julia\njulia> io = IOBuffer();\n\njulia> t1 = [1, 2, 3];\n\njulia> serialize(io, t1)\n24\n\njulia> s = take!(io)\n36-element Vector{UInt8}: …\n\njulia> t2 = deserialize(IOBuffer(s));\n\njulia> print(t2)\n[1, 2, 3]\n~~~\n\n尽管新的对象和原对象的值相同，但是它们不是同一个对象：\n\n~~~julia\njulia> t1 == t2          \ntrue                     \n                         \njulia> t1 === t2         \nfalse                    \n~~~\n\n### Modules\n\n如果你有一个文件`wc.jl`，内容为：\n\n~~~julia\nfunction linecount(filename)\n    count = 0\n    for line in eachline(filename)\n        count += 1\n    end\n    count\nend\n\nprint(linecount(\"wc.jl\"))\n~~~\n\n如果你运行此段代码，它读取自己并输出$9$。\n\n~~~julia\njulia> include(\"wc.jl\")\n9\n~~~\n\n`Julia`引入了一些模块(module)来创建单独的变量工作区，即新的全局作用域。`import`允许控制来自其他模块的哪些名称是可见的，`export`确定哪些名称是公共的，即可以在模块外不加模块名称前缀来使用。\n\n~~~julia\nmodule LineCount\n    export linecount\n\n    function linecount(filename)\n        count = 0\n        for line in eachline(filename)\n            count += 1\n        end\n        count\n    end\nend\n~~~\n\n将上述文件保存为`LineCount.jl`，之后运行`push!(LOAD_PATH, \".\")`，将当前目录添加到工作路径，`.`表示当前路径。之后\n\n~~~julia\njulia> using LineCount\n\njulia> linecount(\"wc.jl\")\n11\n~~~\n\n## Structs and Objects\n\n### Composite Types\n\n我们可以通过`struct`来定义复合类型，如定义`Point`类型：\n\n~~~julia\nstruct Point\n    x\n    y\nend\njulia> p = Point(3.0, 4.0)\nPoint(3.0, 4.0)\n~~~\n\n返回值是对`Point`对象的引用，我们将其赋值给`p`。\n\n### Structs are Immutable\n\n你可以使用`.`符号来获取字段的值：\n\n~~~julia\njulia> x = p.x\n3.0\njulia> p.y\n4.0\n~~~\n\n`struct`在默认情况下是不可变的，构造之后字段的值不可改变：\n\n~~~julia\njulia> p.y = 1.0\nERROR: setfield! immutable struct of type Point cannot be changed\n~~~\n\n### Mutable Structs\n\n我们可以使用`mutate`关键字使`struct`可变：\n\n~~~julia\nmutable struct MPoint\n    x\n    y\nend\n\njulia> blank = MPoint(0.0, 0.0)\nMPoint(0.0, 0.0)\njulia> blank.x = 3.0\n3.0\njulia> blank.y = 4.0\n4.0\n~~~\n\n### Rectangles\n\n我们可以创建一个矩形对象，字段为一个端点的坐标和宽度和长度：\n\n~~~julia\n\"\"\"\nRepresents a rectangle.\n\nfields: width, height, corner.\n\"\"\"\nstruct Rectangle\n    width\n    height\n    corner\nend\n\njulia> origin = MPoint(0.0, 0.0)\nMPoint(0.0, 0.0)\njulia> box = Rectangle(100.0, 200.0, origin)\nRectangle(100.0, 200.0, MPoint(0.0, 0.0))\n~~~\n\n\n\n### Instances as Arguments\n\n实例可以作为参数：\n\n~~~julia\nfunction printpoint(p)\n    println(\"($(p.x), $(p.y))\")\nend\n~~~\n\n当把一个实例传入到函数中去，函数会改变实例的字段值：\n\n~~~julia\nfunction movepoint!(p, dx, dy)\n    p.x += dx\n    p.y += dy\n    nothing\nend\n\njulia> origin = MPoint(0.0, 0.0)\nMPoint(0.0, 0.0)\njulia> movepoint!(origin, 1.0, 2.0)\n\njulia> origin\nMPoint(1.0, 2.0)\n~~~\n\n但是，你可以修改不可变对象的可变属性的值，如：\n\n~~~julia\nfunction moverectangle!(rect, dx, dy)\n  movepoint!(rect.corner, dx, dy)\nend\n\njulia> box\nRectangle(100.0, 200.0, MPoint(0.0, 0.0))\njulia> moverectangle!(box, 1.0, 2.0)\n\njulia> box\nRectangle(100.0, 200.0, MPoint(1.0, 2.0))\n~~~\n\n### Instances as Return Values\n\n~~~julia\nfunction findcenter(rect)\n    Point(rect.corner.x + rect.width / 2, rect.corner.y + rect.height / 2)\nend\n~~~\n\n### Copying\n\n`Julia`内建函数`deepcopy`可以复制任何对象：\n\n~~~julia\njulia> p1 = MPoint(3.0, 4.0)\nMPoint(3.0, 4.0)\njulia> p2 = deepcopy(p1)\nMPoint(3.0, 4.0)\njulia> p1 ≡ p2\nfalse\njulia> p1 == p2\nfalse\n~~~\n\n`≡`的结果我们应该能接受，因为是不同的对象，但是`==`的结果我们可能无法接受，因为两个对象的值是相同的，这是因为在`Julia`中可变复合类型`mutate`，`Julia`不知道什么是等价的。\n\n而在不可变复合类型中相反：\n\n~~~julia\njulia> p1 = Point(1,2)\nPoint(1, 2)\n\njulia> p2 = deepcopy(p1)\nPoint(1, 2)\n\njulia> p1==p2\ntrue\n\njulia> p1===p2\ntrue\n~~~\n\n因为我们不会改变`p1`，所以复制也就没意义。\n\n### Debugging\n\n判断`p`是否为`Point`类型：\n\n~~~julia\njulia> p isa Point\ntrue\n~~~\n\n查看`Point`类型的字段：\n\n~~~julia\njulia> fieldnames(Point)\n(:x, :y)\n~~~\n\n或者使用：\n\n~~~julia\njulia> isdefined(p, :x)\ntrue\njulia> isdefined(p, :z)\nfalse\n~~~\n\n## Structs and Functions\n\n### Time\n\n我们构建时间类：\n\n~~~julia\n\"\"\"\nRepresents the time of day.\n\nfields: hour, minute, second\n\"\"\"\nstruct MyTime\n    hour\n    minute\n    second\nend\n~~~\n\n~~~julia\njulia> time = MyTime(11, 59, 30)\nMyTime(11, 59, 30)\n~~~\n\n### Pure Functions\n\n下面是两个时间相加的简单原型：\n\n~~~julia\nfunction addtime(t1, t2)\n    MyTime(t1.hour + t2.hour, t1.minute + t2.minute, t1.second + t2.second)\nend\n~~~\n\n上面函数被称为纯函数，因为它并没有改变传入的两个对象的值。\n\n~~~julia\njulia> start = MyTime(9, 45, 0);\n\njulia> duration = MyTime(1, 35, 0);\n\njulia> done = addtime(start, duration);\n\njulia> printtime(done)\n10:80:00\n~~~\n\n出现了$80$分钟，这与我们的常识不同，我们需要添加新的规则\n\n~~~julia\nfunction addtime(t1, t2)\n    second = t1.second + t2.second\n    minute = t1.minute + t2.minute\n    hour = t1.hour + t2.hour\n    if second >= 60\n        second -= 60\n        minute += 1\n    end\n    if minute >= 60\n        minute -= 60\n        hour += 1\n    end\n    MyTime(hour, minute, second)\nend\n~~~\n\n### Modifiers\n\n~~~julia\nfunction increment!(time, seconds)\n    time.second += seconds\n    if time.second >= 60\n        time.second -= 60\n        time.minute += 1\n    end\n    if time.minute >= 60\n        time.minute -= 60\n        time.hour += 1\n    end\nend\n~~~\n\n上面定义的函数会改变传入的参数值，我们称其为`Modifiers`。\n\n##  Multiple Dispatch\n\n在`Julia`中，你可以写在不同类型上运行的代码，这被称为泛型编程。\n\n### Type Declarations\n\n运算符`::`将类型注释附加到表达式和变量：\n\n~~~julia\njulia> (1 + 2) :: Float64\nERROR: TypeError: in typeassert, expected Float64, got Int64\njulia> (1 + 2) :: Int64\n3\n~~~\n\n这帮助确认你的程序按照你期望的方式进行。\n\n`::`运算符也可以添加到赋值语句的左边，作为声明的一部分：\n\n~~~julia\njulia> function returnfloat()\n           x::Float64 = 100\n           x\n       end\nreturnfloat (generic function with 1 method)\njulia> x = returnfloat()\n100.0\njulia> typeof(x)\nFloat64\n~~~\n\n也可以作为函数定义的一部分：\n\n~~~julia\nfunction sinc(x)::Float64\n    if x == 0\n        return 1\n    end\n    sin(x)/(x)\nend\n~~~\n\n### Methods\n\n方法是具有特定特征的函数：\n\n~~~julia\nusing Printf\n\nstruct MyTime\n    hour :: Int64\n    minute :: Int64\n    second :: Int64\nend\n\nfunction printtime(time::MyTime)\n    @printf(\"%02d:%02d:%02d\", time.hour, time.minute, time.second)\nend\n~~~\n\n上面的函数定义了输入的数据类型，为方法。\n\n### Constructors\n\n构造是用来创建对象的一类特殊的函数。`MyTime`的默认构造函数方法有以下特征：\n\n~~~julia\nMyTime(hour, minute, second) # 默认构造函数\nMyTime(hour::Int64, minute::Int64, second::Int64)\n~~~\n\n我们也可以添加自己的外部构建方法：\n\n~~~julia\nfunction MyTime(time::MyTime)\n    MyTime(time.hour, time.minute, time.second)\nend\n~~~\n\n我们也可以构建内部构造函数：\n\n~~~julia\nstruct MyTime\n    hour :: Int64\n    minute :: Int64\n    second :: Int64\n    function MyTime(hour::Int64=0, minute::Int64=0, second::Int64=0)\n        @assert(0 ≤ minute < 60, \"Minute is not between 0 and 60.\")\n        @assert(0 ≤ second < 60, \"Second is not between 0 and 60.\")\n        new(hour, minute, second)\n    end\nend\n~~~\n\n现在结构`MyTime`有$4$个内部构造方法：\n\n~~~julia\nMyTime()\nMyTime(hour::Int64)\nMyTime(hour::Int64, minute::Int64)\nMyTime(hour::Int64, minute::Int64, second::Int64)\n~~~\n\n> 内部构造函数会覆盖默认构造函数。\n\n内部构造函数总是定义在结构的内部，且能使用一个特殊函数`new`来创建最新定义的类型的对象。\n\n另一种不使用`new`函数的参数的方法为：\n\n~~~julia\nmutable struct MyTime\n    hour :: Int\n    minute :: Int\n    second :: Int\n    function MyTime(hour::Int64=0, minute::Int64=0, second::Int64=0)\n        @assert(0 ≤ minute < 60, \"Minute is between 0 and 60.\")\n        @assert(0 ≤ second < 60, \"Second is between 0 and 60.\")\n        time = new()\n        time.hour = hour\n        time.minute = minute\n        time.second = second\n        time\n    end\nend\n~~~\n\n但是这样数据结构必须是可变的。\n\n### `show`\n\n`show`函数是一个特殊的函数，返回对象的字符串表示。\n\n~~~julia\nusing Printf\n\nfunction Base.show(io::IO, time::MyTime)\n    @printf(io, \"%02d:%02d:%02d\", time.hour, time.minute, time.second)\nend\n~~~\n\n在这里前缀`Base`是需要的因为我们创建了`Base.show`函数的新方法。\n\n~~~julia\njulia> time = MyTime(9, 45)\n09:45:00\n~~~\n\n### Operator Overloading\n\n通过定义符号方法，你可以定义在定义的类型上的符号的行为。例如：\n\n~~~julia\nimport Base.+\n\nfunction +(t1::MyTime, t2::MyTime)\n    seconds = timetoint(t1) + timetoint(t2)\n    inttotime(seconds)\nend\n\njulia> start = MyTime(9, 45)\n09:45:00\njulia> duration = MyTime(1, 35, 0)\n01:35:00\njulia> start + duration\n11:20:00\n~~~\n\n### Multiple Dispatch\n\n在上面我们定义了两个`MyTime`类型相加，但是你可能也想整数和`Mytime`类型相加：\n\n~~~julia\nfunction +(time::MyTime, seconds::Int64)\n    increment(time, seconds)\nend\n\njulia> start = MyTime(9, 45)\n09:45:00\njulia> start + 1337\n10:07:17\n~~~\n\n相加是可交换的，因此我们定义\n\n~~~julia\nfunction +(seconds::Int64, time::MyTime)\n  time + seconds\nend\n\njulia> 1337 + start\n10:07:17\n~~~\n\n当一个函数被调用时决定使用哪个方法的决定被称为分派。\n\n### Generic Programming\n\n多分派在必要时很有用，但(幸运的是)并不总是必要的。通常，您可以通过编写对不同类型的参数正确工作的函数来避免这种情况。\n\n如下面的函数不仅可以对字符串使用，也可以对其他序列类型的数据使用：\n\n~~~julia\nfunction histogram(s)\n    d = Dict()\n    for c in s\n        if c ∉ keys(d)\n            d[c] = 1\n        else\n            d[c] += 1\n        end\n    end\n    d\nend\n\njulia> t = (\"spam\", \"egg\", \"spam\", \"spam\", \"bacon\", \"spam\")\n(\"spam\", \"egg\", \"spam\", \"spam\", \"bacon\", \"spam\")\njulia> histogram(t)\nDict{Any,Any} with 3 entries:\n  \"bacon\" => 1\n  \"spam\"  => 4\n  \"egg\"   => 1\n~~~\n\n使用多种类型的函数称为多态函数。多态可以促进代码重用。\n\n## Subtyping\n\n### Cards\n\n假设我们有一副扑克牌，我们想比较其大小，根据花色和数字，先定义卡牌类：\n\n~~~julia\nstruct Card\n    suit :: Int64\n    rank :: Int64\n    function Card(suit::Int64, rank::Int64)\n        @assert(1 ≤ suit ≤ 4, \"suit is not between 1 and 4\")\n        @assert(1 ≤ rank ≤ 13, \"rank is not between 1 and 13\")\n        new(suit, rank)\n    end\nend\n~~~\n\n### Global Variables\n\n定义对应的全局变量来一一对应花色和数组：\n\n~~~julia\nconst suit_names = [\"♣\", \"♦\", \"♥\", \"♠\"]\nconst rank_names = [\"A\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\", \"10\", \"J\", \"Q\", \"K\"]\n\n\nfunction Base.show(io::IO, card::Card)\n    print(io, rank_names[card.rank], suit_names[card.suit])\nend\n\njulia> Card(3, 11)\nJ♥\n~~~\n\n### Comparing Cards\n\n~~~julia\nimport Base.<\n\nfunction <(c1::Card, c2::Card)\n    (c1.suit, c1.rank) < (c2.suit, c2.rank)\nend\n~~~\n\n### Decks\n\n~~~julia\nstruct Deck\n    cards :: Array{Card, 1}\nend\n\nfunction Deck()\n    deck = Deck(Card[])\n    for suit in 1:4\n        for rank in 1:13\n            push!(deck.cards, Card(suit, rank))\n        end\n    end\n    deck\nend\n\nfunction Base.show(io::IO, deck::Deck)\n    for card in deck.cards\n        print(io, card, \" \")\n    end\n    println()\nend\n\njulia> Deck()\nA♣ 2♣ 3♣ 4♣ 5♣ 6♣ 7♣ 8♣ 9♣ 10♣ J♣ Q♣ K♣ A♦ 2♦ 3♦ 4♦ 5♦ 6♦ 7♦ 8♦ 9♦ 10♦ J♦ Q♦ K♦ A♥ 2♥ 3♥ 4♥ 5♥ 6♥ 7♥ 8♥ 9♥ 10♥ J♥ Q♥ K♥ A♠ 2♠ 3♠ 4♠ 5♠ 6♠ 7♠ 8♠ 9♠ 10♠ J♠ Q♠ K♠\n~~~\n\n### Add, Remove, Shuffle and Sort\n\n~~~julia\nfunction Base.pop!(deck::Deck)\n    pop!(deck.cards)\nend\n\nfunction Base.push!(deck::Deck, card::Card)\n    push!(deck.cards, card)\n    deck\nend\n\nusing Random\n\nfunction Random.shuffle!(deck::Deck)\n    shuffle!(deck.cards)\n    deck\nend\n~~~\n\n\n\n### Abstract Types and Subtyping\n\n现在我们想要定义手(hand)的类型，其与Decks类型大致相同，因此我们先定义抽象类型：\n\n~~~julia\nabstract type CardSet end\n~~~\n\n在类型后可以使用`<:`来指定其为谁的子类型，如果没有则为`Any`的子类型。\n\n我们可以指定`Deck`为`CardSet`的子类\n\n~~~julia\nstruct Deck <: CardSet\n    cards :: Array{Card, 1}\nend\n\nfunction Deck()\n    deck = Deck(Card[])\n    for suit in 1:4\n        for rank in 1:13\n            push!(deck.cards, Card(suit, rank))\n        end\n    end\n    deck\nend\n~~~\n\n~~~julia\nstruct Hand <: CardSet\n    cards :: Array{Card, 1}\n    label :: String\nend\n\nfunction Hand(label::String=\"\")\n    Hand(Card[], label)\nend\n~~~\n\n\n\n~~~julia\njulia> deck = Deck();\n\njulia> deck isa CardSet\ntrue\n~~~\n\n### Abstract Types and Functions\n\n我们现在可以给抽象类型定义函数：\n\n~~~julia\nfunction Base.show(io::IO, cs::CardSet)\n    for card in cs.cards\n        print(io, card, \" \")\n    end\nend\n\nfunction Base.pop!(cs::CardSet)\n    pop!(cs.cards)\nend\n\nfunction Base.push!(cs::CardSet, card::Card)\n    push!(cs.cards, card)\n    nothing\nend\n~~~\n\n\n\n~~~julia\njulia> deck = Deck()\nA♣ 2♣ 3♣ 4♣ 5♣ 6♣ 7♣ 8♣ 9♣ 10♣ J♣ Q♣ K♣ A♦ 2♦ 3♦ 4♦ 5♦ 6♦ 7♦ 8♦ 9♦ 10♦ J♦ Q♦ K♦ A♥ 2♥ 3♥ 4♥ 5♥ 6♥ 7♥ 8♥ 9♥ 10♥ J♥ Q♥ K♥ A♠ 2♠ 3♠ 4♠ 5♠ 6♠ 7♠ 8♠ 9♠ 10♠ J♠ Q♠ K♠\njulia> shuffle!(deck)\nJ♦ 10♣ 8♠ 9♥ 5♠ 7♣ 6♦ A♠ J♣ 7♠ 5♦ 10♥ 3♦ 9♦ 9♣ 4♣ 8♦ 8♥ 5♣ A♥ K♥ K♦ K♠ 4♦ A♦ Q♥ 6♠ 2♦ 6♥ 2♣ 10♠ 3♥ 2♥ J♥ Q♣ 5♥ 2♠ 9♠ 10♦ Q♠ 3♠ 8♣ K♣ 7♥ 3♣ J♠ 4♥ 6♣ 7♦ 4♠ A♣ Q♦\njulia> card = pop!(deck)\nQ♦\njulia> push!(hand, card)\n~~~\n\n## The Goodies: Syntax\n\n### Named Tuples\n\n~~~julia\njulia> x = (a=1, b=1+1)\n(a = 1, b = 2)\njulia> x.a\n1\n~~~\n\n### Functions\n\n可以以紧凑形式定义函数：\n\n~~~julia\njulia> f(x,y) = x + y\nf (generic function with 1 method)\n~~~\n\n#### Anonymous Functions\n\n我们可以定义没有名字的函数：\n\n~~~julia\njulia> x -> x^2 + 2x - 1\n#1 (generic function with 1 method)\njulia> function (x)\n           x^2 + 2x - 1\n       end\n#3 (generic function with 1 method)\n~~~\n\n匿名函数通常是其他函数的参数：\n\n~~~julia\njulia> using Plots\n\njulia> plot(x -> x^2 + 2x - 1, 0, 10, xlabel=\"x\", ylabel=\"y\")\n~~~\n\n#### Keyword Arguments\n\n函数参数可以命名：\n\n~~~julia\njulia> function myplot(x, y; style=\"solid\", width=1, color=\"black\")\n           ###\n       end\nmyplot (generic function with 1 method)\njulia> myplot(0:10, 0:10, style=\"dotted\", color=\"blue\")\n~~~\n\n关键字参数用`;`隔开，但调用的时候可以使用`,`。\n\n#### Closures\n\n`closure`是允许函数获取在函数范围之外的变量的方法：\n\n~~~julia\njulia> foo(x) = ()->x\nfoo (generic function with 1 method)\n\njulia> bar = foo(1)\n#1 (generic function with 1 method)\n\njulia> bar()\n1\n~~~\n\n### Blocks\n\n块是一种将许多语句分组的方法。\n\n~~~julia\n🐢 = Turtle()\n@svg begin\n    forward(🐢, 100)\n    turn(🐢, -90)\n    forward(🐢, 100)\nend\n~~~\n\n#### `let` Blocks\n\n~~~julia\njulia> x, y, z = -1, -1, -1;\n\njulia> let x = 1, z\n           @show x y z;\n       end\nx = 1\ny = -1\nERROR: UndefVarError: z not defined\njulia> @show x y z;\nx = -1\ny = -1\nz = -1\n~~~\n\n在第一个`let`中，`@show`打印局部变量`x,z`，打印全局变量`y`，全局变量`x,y,z`并未受影响。\n\n#### `do` Blocks\n\n~~~julia\njulia> data = \"This here's the wattle,\\nthe emblem of our land.\\n\"\n\"This here's the wattle,\\nthe emblem of our land.\\n\"\njulia> open(\"output.txt\", \"w\") do fout\n           write(fout, data)\n       end\n48\n~~~\n\n这里`fout`为文件流，我们不再需要手动关闭`fout`。\n\n这在函数上等价于\n\n~~~julia\njulia> f = fout -> begin\n           write(fout, data)\n       end\n#3 (generic function with 1 method)\njulia> open(f, \"output.txt\", \"w\")\n48\n~~~\n\n匿名函数常被用为`open`的参数：\n\n~~~julia\nfunction open(f::Function, args...)\n    io = open(args...)\n    try\n        f(io)\n    finally\n        close(io)\n    end\nend\n~~~\n\ndo块可以从它的封闭作用域捕获变量。例如，上面`open…do`例子中的变量`data`是从外部作用域捕获的。\n\n### Control Flow\n\n#### Ternary Operator\n\n~~~julia\njulia> a = 150\n150\njulia> a % 2 == 0 ? println(\"even\") : println(\"odd\")\neven\n~~~\n\n#### Short-Circuit Evaluation\n\n`&&`和`||`是短路运算符，当前一个表达式的值可以确定结果是另一个就不需要计算了。\n\n#### Tasks (aka Coroutines)\n\n任务是一种能够传递协同控制而不返回的控制结构。在Julia中，任务可以作为第一个参数为`Channel`对象的函数来实现。通道用于将值从函数传递给被调用方。\n\n~~~julia\nfunction fib(c::Channel)\n    a = 0\n    b = 1\n    put!(c, a)\n    while true\n        put!(c, b)\n        (a, b) = (b, a+b)\n    end\nend\n~~~\n\n`take!`可以从通道中取值：\n\n~~~julia\njulia> fib_gen = Channel(fib);\n\njulia> take!(fib_gen)\n0\njulia> take!(fib_gen)\n1\njulia> take!(fib_gen)\n1\njulia> take!(fib_gen)\n2\njulia> take!(fib_gen)\n3\n~~~\n\n通道也可以作为迭代对象\n\n~~~julia\njulia> for val in Channel(fib)\n           print(val, \" \")\n           val > 20 && break\n       end\n0 1 1 2 3 5 8 13 21\n~~~\n\n### Types\n\n#### Primitive Types\n\n在`Julia`中，你可以定义自己的原始类型，标准原始类型也用同样的方法定义：\n\n~~~julia\nprimitive type Float64 <: AbstractFloat 64 end\nprimitive type Bool <: Integer 8 end\nprimitive type Char <: AbstractChar 32 end\nprimitive type Int64 <: Signed 64 end\n~~~\n\n创建`Byte`原始类型和其构造函数：\n\n~~~julia\njulia> primitive type Byte 8 end\n\njulia> Byte(val::UInt8) = reinterpret(Byte, val)\nByte\njulia> b = Byte(0x01)\nByte(0x01)\n~~~\n\n`reinterpret`函数用于存储一个8位的无符号整数的位到字节中。\n\n#### Parametric Types\n\n`Julia`的类型是参数的，说明其类型可以带参数。\n\n~~~julia\nstruct Point{T<:Real}\n    x::T\n    y::T\nend\n~~~\n\n#### Type Unions\n\n~~~julia\njulia> IntOrString = Union{Int64, String}\nUnion{Int64, String}\njulia> 150 :: IntOrString\n150\njulia> \"Julia\" :: IntOrString\n\"Julia\"\n~~~\n\n\n\n类型联合在大多数计算机语言中是对类型进行推理的内部构造。然而，`Julia`向用户公开了该特性，因为当类型联合只有少量类型时，可以生成有效的代码。这个特性为`Julia`程序员控制调度提供了极大的灵活性。\n\n### Methods\n\n#### Parametric Methods\n\n方法定义也可以含有参数的类型参数：\n\n~~~julia\njulia> isintpoint(p::Point{T}) where {T} = (T === Int64)\nisintpoint (generic function with 1 method)\njulia> p = Point(1, 2)\nPoint{Int64}(1, 2)\njulia> isintpoint(p)\ntrue\n~~~\n\n#### Function-like Objects\n\n`Julia`中的任何对象都可以被调用，这种可以被调用的对象被称为`functors`：\n\n~~~julia\nstruct Polynomial{R}\n    coeff::Vector{R}\nend\n\nfunction (p::Polynomial)(x)\n    val = p.coeff[end]\n    for coeff in p.coeff[end-1:-1:1]\n        val = val * x + coeff\n    end\n    val\nend\n~~~\n\n去计算多项式，我们简单调用它：\n\n~~~julia\njulia> p = Polynomial([1,10,100])\nPolynomial{Int64}([1, 10, 100])\njulia> p(3)\n931\n~~~\n\n### Constructors\n\n参数类型可以显式或隐式构造\n\n~~~julia\njulia> Point(1,2)         # implicit T\nPoint{Int64}(1, 2)\njulia> Point{Int64}(1, 2) # explicit T\nPoint{Int64}(1, 2)\njulia> Point(1,2.5)       # implicit T\nERROR: MethodError: no method matching Point(::Int64, ::Float64)\n~~~\n\n为每个`T`生成默认的内部和外部构造函数：\n\n~~~julia\nstruct Point{T<:Real}\n    x::T\n    y::T\n    Point{T}(x,y) where {T<:Real} = new(x,y)\nend\n\nPoint(x::T, y::T) where {T<:Real} = Point{T}(x,y);\n~~~\n\n并且每一个`x`和`y`的类型都相同。\n\n当类型不同时可以：\n\n~~~julia\nPoint(x::Real, y::Real) = Point(promote(x,y)...);\n~~~\n\n### Conversion and Promotion\n\n#### Conversion\n\n数据可以从一个类型转为另一个类型：\n\n~~~julia\njulia> x = 12\n12\njulia> typeof(x)\nInt64\njulia> convert(UInt8, x)\n0x0c\njulia> typeof(ans)\nUInt8\n~~~\n\n我们可以增加自己的`convert`方法：\n\n~~~julia\njulia> Base.convert(::Type{Point{T}}, x::Array{T, 1}) where {T<:Real} = Point(x...)\n\njulia> convert(Point{Int64}, [1, 2])\nPoint{Int64}(1, 2)\n~~~\n\n#### Promotion\n\n`Promotion`是将混合类型的值转换为单一公共类型：\n\n~~~julia\njulia> promote(1, 2.5, 3)\n(1.0, 2.5, 3.0)\n~~~\n\n### Metaprogramming\n\n`Julia`代码可以表示为语言本身的数据结构。这允许程序转换和生成自己的代码。\n\n#### Expressions\n\n每一个`Julia`程序开始都为一个字符串：\n\n~~~julia\njulia> prog = \"1 + 2\"\n\"1 + 2\"\n~~~\n\n下一步是将每个字符串解析为一个名为表达式的对象，表达式由`Julia`类型`Expr`表示：\n\n~~~julia\njulia> ex = Meta.parse(prog)\n:(1 + 2)\njulia> typeof(ex)\nExpr\njulia> dump(ex)\nExpr\n  head: Symbol call\n  args: Array{Any}((3,))\n    1: Symbol +\n    2: Int64 1\n    3: Int64 2\n~~~\n\n`dump`函数显示带有注释的`expr`对象。\n\n表达式可以由`:()`或`quote`块构造：\n\n~~~julia\njulia> ex = quote\n           1 + 2\n       end;\n~~~\n\n#### `eval`\n\n可以用`eval`函数计算表达式的值：\n\n~~~julia\njulia> eval(ex)\n3\n~~~\n\n#### Macros\n\n宏可以包括程序中生成的代码。宏将`Expr`对象的元组直接映射到编译后的表达式：\n\n~~~julia\nmacro containervariable(container, element)\n    return esc(:($(Symbol(container,element)) = $container[$element]))\nend\n~~~\n\n#### Generated Functions\n\n宏`@generated`根据参数的类型为方法创建专门的代码：\n\n~~~julia\n@generated function square(x)\n    println(x)\n    :(x * x)\nend\n\njulia> x = square(2); # note: output is from println() statement in the body\nInt64\njulia> x              # now we print x\n4\njulia> y = square(\"spam\");\nString\njulia> y\n\"spamspam\"\n~~~\n\n### Missing Values\n\n缺失的值可以通过`missing`对象表示，该对象是`Missing`类型的单例实例：\n\n~~~julia\njulia> a = [1, missing]\n2-element Array{Union{Missing, Int64},1}:\n 1\n  missing\n\njulia> sum(a)\nmissing\n\njulia> sum(skipmissing([1, missing]))\n1\n~~~\n\n## The Goodies: Base and Standard Library\n\n### Measuring Performance\n\n可以使用宏`@time`来比较程序运行的快慢。\n\n### Collections and Data Structures\n\nJulia提供了另一种内置类型，称为set，它的行为类似于没有值的字典键的集合。集合提供了计算常见集合操作的函数和运算符。\n\n~~~julia\nfunction subtract(d1, d2)\n    res = Dict()\n    for key in keys(d1)\n        if key ∉ keys(d2)\n            res[key] = nothing\n        end\n    end\n    res\nend\n~~~\n\n~~~julia\nfunction subtract(d1, d2)\n    setdiff(d1, d2)\nend\n~~~\n\n~~~julia\nfunction hasduplicates(t)\n    d = Dict()\n    for x in t\n        if x ∈ d\n            return true\n        end\n        d[x] = nothing\n    end\n    false\nend\n~~~\n\n~~~julia\nfunction hasduplicates(t)\n    length(Set(t)) < length(t)\nend\n~~~\n\n~~~julia\nfunction usesonly(word, available)\n    for letter in word\n        if letter ∉ available\n            return false\n        end\n    end\n    true\nend\n~~~\n\n~~~julia\nfunction usesonly(word, available)\n    Set(word) ⊆ Set(available)\nend\n~~~\n\n### Interfaces\n\n`Julia`指定了一些非正式的接口来定义行为，例如，具有特定目标的方法。当您为某个类型扩展这样的方法时，可以使用该类型的对象来构建这些行为。\n\n例如斐波那契：\n\n~~~julia\nstruct Fibonacci{T<:Real} end\nFibonacci(d::DataType) = d<:Real ? Fibonacci{d}() : error(\"No Real type!\")\n\nBase.iterate(::Fibonacci{T}) where {T<:Real} = (zero(T), (one(T), one(T)))\nBase.iterate(::Fibonacci{T}, state::Tuple{T, T}) where {T<:Real} = (state[1], (state[2], state[1] + state[2]))\n~~~\n\n首先定义了结构和构造方法，之后定义了初始循环，返回值第一个为$0$，之后一个元组为`state`用于下面的下一次循环。\n\n~~~julia\njulia> for e in Fibonacci(Int64)\n           e > 100 && break\n           print(e, \" \")\n       end\n0 1 1 2 3 5 8 13 21 34 55 89\n~~~\n\n","tags":["Julia","编程"],"categories":["书籍阅读"]},{"title":"书籍Thinking Julia前十章阅读笔记","url":"/2022/07/01/thinkjulia/","content":"\n<p align=\"center\">\n    <img src=\"https://gimg2.baidu.com/image_search/src=http%3A%2F%2Fimglf6.lf127.net%2Fimg%2FanBSb2NqdEc5dTlseXpNTWxpY0RWQ2NwUlAyVmFEa2lFSVo0TVM4WXNwSndDMUdvN3crMS9nPT0.jpg%3FimageView%26thumbnail%3D2160x0%26quality%3D90%26interlace%3D1%26type%3Djpg&refer=http%3A%2F%2Fimglf6.lf127.net&app=2002&size=f9999,10000&q=a80&n=0&g=0n&fmt=auto?sec=1659263087&t=d2faeee051aa3c102a86e173d74306ad\" style=\"zoom: 100%;\" />\n</p>\n\n> 书籍网址：https://benlauwens.github.io/ThinkJulia.jl/latest/book.html\n\n<!--more-->\n\n## The Way of the program\n\n### What is a Program?\n\n一个程序应该包括以下几部分：\n\n+ 输入：从键盘、文件、网络或者其他设备中获取数据\n+ 输出：将数据展示在屏幕上、或将其存储在文件中、将其发送到网络上等等\n+ 数学：执行基本的数学运算，如加法乘法等\n+ 条件执行：查看特定的条件并且运行适当的代码\n+ 重复：重复地做一些动作，通常有一些变化\n\n### The First Program\n\n~~~julia\njulia> println(\"Hello, World!\")\nHello, World!\n~~~\n\n### Arithmetic Operators\n\n符号`+`，`-`，`*`，`/`，`^`分别表示加法、减法、乘法、除法、幂运算。\n\n~~~julia\njulia> 40 + 2         \n42                    \n                      \njulia> 43 - 1         \n42                    \n                      \njulia> 6 * 7          \n42                    \n                      \njulia> 84 / 2         \n42.0                  \n                      \njulia> 6 ^ 2 + 6      \n42                    \n~~~\n\n> 注意在除法中我们得到是$42.0$而不是$42$，后面我们会解释原因。\n\n### Values and Types\n\n我们可以使用`typeof`函数来获取类型。\n\n~~~julia\njulia> typeof(2)\nInt64\n\njulia> typeof(42.0)\nFloat64\n\njulia> typeof(\"Hello, World!\")\nString\n~~~\n\n## Variables, Expressions and Statements\n\n### Assignment Statements\n\n使用`=`对变量进行赋值。\n\n~~~julia\njulia> n = 17\n17\n~~~\n\n### Variable Names\n\n命名可以采用字母、数字和`_`，但是数字不能在开头，也不能使用关键字。\n\n下面为`Julia`的关键字：\n\n~~~julia\nabstract type    baremodule   begin      break       catch\nconst            continue     do         else        elseif\nend              export       finally    for         function\nglobal           if           import     importall   in\nlet              local        macro      module      mutable struct\nprimitive type   quote        return     try         using\nstruct           where        while\n~~~\n\n### Expressions and Statements\n\n表达式是值（values）、变量（variables）和符号（operators）的组合。\n\n~~~julia\njulia> 42\n42\n\njulia> n\n17\n\njulia> n + 25\n42\n~~~\n\n语句是具有效果的代码单位，例如创建变量或显示值。\n\n~~~julia\njulia> n = 17\n17\n\njulia> println(n)\n17\n~~~\n\n### Operator Precedence\n\n括号优先级最高，其次是幂，之后是乘除法、加减法优先级最低。\n\n### String Operations\n\n`*`表示字符串拼接，`^`表示字符串重复。\n\n~~~julia\njulia> first_str = \"throat\"\n\"throat\"\n\njulia> second_str = \"warbler\"\n\"warbler\"\n\njulia> first_str * second_str\n\"throatwarbler\"\n\njulia> \"Spam\"^3\n\"SpamSpamSpam\"\n~~~\n\n### Comments\n\n注释用`#`。\n\n### Debugging\n\n`Julia`中存在三种错误。\n\n+ 语法错误(Syntax error)：语法指的是程序的结构和关于该结构的规则。如括号少一边。\n+ 运行时错误(Runtime error)：指的是程序运行时出错，也被称为异常。\n+ 语义错误(Semantic error)：这种错误程序会运行，但是得到的结果不是我们想要的。\n\n## Functions\n\n### Function Calls\n\n我们之前已经看到一个函数调用的例子：\n\n~~~julia\njulia> println(\"Hello, World!\")\nHello, World!\n~~~\n\n`Println()`是函数的名字，括号里面是函数的参数。\n\n`Julia`提供一些类型转换的函数：\n\n`parse`函数将字符串类型转为任何数字(number)类型\n\n~~~julia\njulia> parse(Int64, \"32\")\n32\n\njulia> parse(Float64, \"3.14159\")\n3.14159\n\njulia> parse(Int64, \"Hello\")\nERROR: ArgumentError: invalid base 10 digit 'H' in \"Hello\"\n~~~\n\n`trunc`可以将字符串截断为整数：\n\n~~~julia\njulia> trunc(Int64, 3.9999)\n3\n\njulia> trunc(Int64, -2.3)\n-2\n~~~\n\n`float`将整数转为浮点数：\n\n~~~julia\njulia> float(24)\n24.0\n~~~\n\n`string`将参数转换为字符串：\n\n~~~julia\njulia> string(32)\n\"32\"\n\njulia> string(32.2)\n\"32.2\"\n~~~\n\n### Adding New Functions\n\n我们使用`function`定义函数，以`end`结束。\n\n~~~julia\nfunction printlyrics()\n    println(\"I'm a lumberjack, and I'm okay.\")\n    println(\"I sleep all night and I work all day.\")\nend\n~~~\n\n我们也可以在函数中调用函数：\n\n~~~julia\nfunction repeatlyrics()\n    printlyrics()\n    printlyrics()\nend\n~~~\n\n>  函数在运行前必须被定义。\n\n### Parameters and Arguments\n\n下面我们定义一个含有参数的函数：\n\n~~~julia\nfunction printtwice(bruce)\n    println(bruce)\n    println(bruce)\nend\n~~~\n\n传入的参数可以是值(value)、表达式和变量。\n\n### Variables and Parameters Are Local\n\n函数的变量和参数是局部的。\n\n### Fruitful Functions and Void Functions\n\n有返回值的函数称为Fruitful Functions，没有返回值的函数称为Void Functions。没有返回值的函数的返回值为`nothing`，我们只能使用`show`函数打印。\n\n~~~julia\njulia> result = printtwice(\"Bing\")\nBing\nBing\njulia> show(result)\nnothing\n~~~\n\n`nothing`的类型为`Nothing`：\n\n~~~julia\njulia> typeof(nothing)\nNothing\n~~~\n\n## Case Study: Interface Design\n\n### Turtles\n\n我们使用下列命令下载所需要的包：\n\n首先在`REPL`下按`]`键调出包管理：\n\n~~~julia\n(v1.0) pkg> add https://github.com/BenLauwens/ThinkJulia.jl\n~~~\n\n~~~julia\njulia> using ThinkJulia\n\njulia> 🐢 = Turtle()\nLuxor.Turtle(0.0, 0.0, true, 0.0, (0.0, 0.0, 0.0))\n~~~\n\n>  输入：`🐢` (**`\\:turtle: TAB`**)。\n\n一旦创建了一个乌龟，可以调用函数来让他移动。\n\n~~~julia\n@svg begin\n    forward(🐢, 100)\nend\n~~~\n\n`@svg`关键字运行一个绘制SVG图片的宏。宏是Julia的一个重要但高级的特性。\n\n为了绘制直角，修改宏\n\n~~~julia\n🐢 = Turtle()\n@svg begin\n    forward(🐢, 100)\n    turn(🐢, -90)\n    forward(🐢, 100)\nend\n~~~\n\n### Simple Repetition\n\n我们使用`for`循环来绘制一个正方形：\n\n~~~julia\n🐢 = Turtle()\n@svg begin\n    for i in 1:4\n        forward(🐢, 100)\n        turn(🐢, -90)\n    end\nend\n~~~\n\n## Conditionals and Recursion\n\n### Floor Division and Modulus\n\n`floor division`符号`÷` (**`\\div TAB`**)，两个数字相除后得到整数。取余符号`%`得到余数。\n\n~~~julia\njulia> minutes = 105            \n105                             \n                                \njulia> hours = minutes÷60       \n1                               \n                                \njulia> remainder = minutes % 60 \n45                              \n~~~\n\n### Boolean Expressions\n\n布尔表达式为结果为真或假的表达式。\n\n~~~julia\n      x == y               # x is equal to y\n      x != y               # x is not equal to y\n      x ≠ y                # (\\ne TAB)\n      x > y                # x is greater than y\n      x < y                # x is less than y\n      x >= y               # x is greater than or equal to y\n      x ≥ y                # (\\ge TAB)\n      x <= y               # x is less than or equal to y\n      x ≤ y                # (\\le TAB)\n~~~\n\n### Logical Operators\n\n与`&&`\n\n或`||`\n\n非`!`\n\n其中`&&`的优先级高于`||`。\n\n### Conditional Execution\n\n```julia\nif x > 0\n    println(\"x is positive\")\nend\n```\n\n### Alternative Execution\n\n```julia\nif x % 2 == 0\n    println(\"x is even\")\nelse\n    println(\"x is odd\")\nend\n```\n\n### Chained Conditionals\n\n~~~julia\nif x < y\n    println(\"x is less than y\")\nelseif x > y\n    println(\"x is greater than y\")\nelse\n    println(\"x and y are equal\")\nend\n~~~\n\n### Nested Conditionals\n\n~~~julia\nif x == y\n    println(\"x and y are equal\")\nelse\n    if x < y\n        println(\"x is less than y\")\n    else\n        println(\"x is greater than y\")\n    end\nend\n~~~\n\n~~~julia\nif 0 < x < 10\n    println(\"x is a positive single-digit number.\")\nend\n~~~\n\n### Recursion\n\n函数自己调用自己被称为递归。\n\n~~~julia\nfunction printn(s, n)\n    if n ≤ 0\n        return\n    end\n    println(s)\n    printn(s, n-1)\nend\n~~~\n\n### Keyboard Input\n\n`Julia`提供内建函数`readline`来停止程序来等待用户输入一些东西，当用户键入`RETURN`或`ENTERN`时程序继续运行，`readline`以字符串的形式返回用户输入的内容。\n\n~~~julia\njulia> text = readline()\nWhat are you waiting for?\n\"What are you waiting for?\"\n~~~\n\n## Fruitful Functions\n\n### Return Values\n\n函数具有返回值：\n\n~~~julia\nfunction area(radius)\n    a = π * radius^2\n    return a\nend\n~~~\n\n我们也可以省略`return`，默认最后一行为函数的返回值：\n\n~~~julia\nfunction area(radius)\n    π * radius^2\nend\n~~~\n\n### Checking Types\n\n~~~julia\nfunction fact(n)\n    if !(n isa Int64)\n        error(\"Factorial is only defined for integers.\")\n    elseif n < 0\n        error(\"Factorial is not defined for negative integers.\")\n    elseif n == 0\n        return 1\n    else\n        return n * fact(n-1)\n    end\nend\n~~~\n\n## Iteration\n\n### The `while` Statement\n\n~~~julia\nfunction countdown(n)\n    while n > 0\n        print(n, \" \")\n        n = n - 1\n    end\n    println(\"Blastoff!\")\nend\n~~~\n\n#### break\n\n~~~julia\nwhile true\n    print(\"> \")\n    line = readline()\n    if line == \"done\"\n        break\n    end\n    println(line)\nend\nprintln(\"Done!\")\n~~~\n\n#### continue\n\n~~~julia\nfor i in 1:10\n    if i % 3 == 0\n        continue\n    end\n    print(i, \" \")\nend\n~~~\n\n## Strings\n\n### Characters\n\n一个字符用单引号`''`包裹：\n\n~~~julia\njulia> 'x'\n'x': ASCII/Unicode U+0078 (category Ll: Letter, lowercase)\njulia> '🍌'\n'🍌': Unicode U+01f34c (category So: Symbol, other)\njulia> typeof('x')\nChar\n~~~\n\n### A String Is a Sequence\n\n我们可以对字符串进行索引：\n\n~~~julia\njulia> fruit = \"banana\"\n\"banana\"\njulia> letter = fruit[1]\n'b': ASCII/Unicode U+0062 (category Ll: Letter, lowercase)\njulia> fruit[end]\n'a': ASCII/Unicode U+0061 (category Ll: Letter, lowercase)\n~~~\n\n### `length`\n\n`length`返回字符串的长度：\n\n```julia\njulia> fruits = \"🍌 🍎 🍐\"\n\"🍌 🍎 🍐\"\njulia> len = length(fruits)\n5\n```\n\n字符串是使用`UTF-8`进行编码的。`UTF-8`是一个可变宽度的编码，这意味着并非所有字符均以相同数量的字节编码。因此我们索引可能会出错。\n\n函数`sizeof`可以给出字符串的字节数：\n\n~~~julia\njulia> sizeof(\"🍌\")\n4\n~~~\n\n这意味着`UTF-8`编码的字符串中的字节索引并不是并不总是一个字符的合理的索引：\n\n~~~julia\njulia> fruits[2]\nERROR: StringIndexError(\"🍌 🍎 🍐\", 2)\n~~~\n\n### Traversal\n\n~~~julia\nindex = firstindex(fruits)\nwhile index <= sizeof(fruits)\n    letter = fruits[index]\n    println(letter)\n    global index = nextind(fruits, index)\nend\n~~~\n\n`nextind`返回下一个合理的索引；`prevind`查找前一个合理的索引。\n\n### String Slices\n\n~~~julia\njulia> str = \"Julius Caesar\";\n\njulia> str[1:6]\n\"Julius\"\njulia> str[:]\n\"Julius Caesar\"\n~~~\n\n### Strings Are Immutable\n\n~~~julia\njulia> greeting = \"Hello, world!\"\n\"Hello, world!\"\njulia> greeting[1] = 'J'\nERROR: MethodError: no method matching setindex!(::String, ::Char, ::Int64)\n~~~\n\n### String Interpolation\n\n使用符号`$`可以将表达式的值插入到字符串中：\n\n~~~julia\njulia> greet = \"Hello\"  \n\"Hello\"                 \n                        \njulia> whom = \"World\"   \n\"World\"                 \n\njulia> \"$(greet), $(whom)!\"\n\"Hello, World!\"\n~~~\n\n### Searching\n\n~~~julia\nfunction find(word, letter)\n    index = firstindex(word)\n    while index <= sizeof(word)\n        if word[index] == letter\n            return index\n        end\n        index = nextind(word, index)\n    end\n    -1\nend\n~~~\n\n### Looping and Counting\n\n~~~julia\nword = \"banana\"\ncounter = 0\nfor letter in word\n    if letter == 'a'\n        global counter = counter + 1\n    end\nend\nprintln(counter)\n~~~\n\n### String Library\n\n~~~julia\njulia> uppercase(\"Hello, World!\")\n\"HELLO, WORLD!\"\n\njulia> findfirst(\"a\", \"banana\")\n2:2\n\njulia> findfirst(\"na\", \"banana\")\n3:4\n\njulia> findnext(\"na\", \"banana\", 4)\n5:6\n~~~\n\n### The `∈` Operator\n\n~~~julia\njulia> 'a' ∈ \"banana\"    # 'a' in \"banana\"\ntrue\n~~~\n\n### String Comparison\n\n~~~julia\nword = \"Pineapple\"\nif word < \"banana\"\n    println(\"Your word, $(word), comes before banana.\")\nelseif word > \"banana\"\n    println(\"Your word, $(word), comes after banana.\")\nelse\n    println(\"All right, bananas.\")\nend\n> Your word, Pineapple, comes before banana.\n~~~\n\n##  Arrays\n\n`Array`为`Julia`内置的数据结构。\n\n### An Array is a Sequence\n\n数组中元素不一定要是同一种类型，最简单的创建数组的方法是用中括号`[]`：\n\n~~~julia\n[\"spam\", 2.0, 5, [10, 20]]\n~~~\n\n~~~julia\njulia> cheeses = [\"Cheddar\", \"Edam\", \"Gouda\"];\n\njulia> numbers = [42, 123];\n\njulia> empty = [];\n\njulia> print(cheeses, \" \", numbers, \" \", empty)\n[\"Cheddar\", \"Edam\", \"Gouda\"] [42, 123] Any[]\n~~~\n\n我们可以使用`typeof`来查看类型：\n\n~~~julia\njulia> typeof(cheeses)\nArray{String,1}\njulia> typeof(numbers)\nArray{Int64,1}\njulia> typeof(empty)\nArray{Any,1}\n~~~\n\n数组类型包含数组中元素的类型和维度。\n\n### Arrays Are Mutable\n\n数组是可变的：\n\n~~~julia\njulia> numbers[2] = 5\n5\njulia> print(numbers)\n[42, 5]\n~~~\n\n### Traversing an Array\n\n~~~julia\nfor cheese in cheeses\n    println(cheese)\nend\n\nfor x in []\n    println(\"This can never happens.\")\nend\n~~~\n\n### Array Slices\n\n~~~julia\njulia> t = ['a', 'b', 'c', 'd', 'e', 'f'];\n\njulia> print(t[1:3])\n['a', 'b', 'c']\njulia> print(t[3:end])\n['c', 'd', 'e', 'f']\n~~~\n\n`[:]`拷贝数组：\n\n~~~julia\njulia> print(t[:])\n['a', 'b', 'c', 'd', 'e', 'f']\n~~~\n\n### Array Library\n\n`push!`向数组中添加元素：\n\n~~~julia\njulia> t = ['a', 'b', 'c'];\n\njulia> push!(t, 'd');\n\njulia> print(t)\n['a', 'b', 'c', 'd']\n~~~\n\n`append!`将第一个数组的元素放在第二个数组前面：\n\n~~~julia\njulia> t1 = ['a', 'b', 'c'];\n\njulia> t2 = ['d', 'e'];\n\njulia> append!(t1, t2);\n\njulia> print(t1)\n['a', 'b', 'c', 'd', 'e']\n~~~\n\n`sort!`排序，改变原数组，`sort`不改变原数组，带`!`的函数改变原来的数组。\n\n~~~julia\njulia> t = ['d', 'c', 'e', 'b', 'a'];\n\njulia> sort!(t);\n\njulia> print(t)\n['a', 'b', 'c', 'd', 'e']\n\njulia> t1 = ['d', 'c', 'e', 'b', 'a'];\n\njulia> t2 = sort(t1);\n\njulia> print(t1)\n['d', 'c', 'e', 'b', 'a']\njulia> print(t2)\n['a', 'b', 'c', 'd', 'e']\n~~~\n\n\n\n### Dot Syntax\n\n函数前添加`.`是逐元素操作：\n\n~~~julia\njulia> t = uppercase.([\"abc\", \"def\", \"ghi\"]);\n\njulia> print(t)\n[\"ABC\", \"DEF\", \"GHI\"]\n~~~\n\n### Deleting (Inserting) Elements\n\n`splice!`删除返回元素：\n\n~~~julia\njulia> t = ['a', 'b', 'c'];\n\njulia> splice!(t, 2)\n'b': ASCII/Unicode U+0062 (category Ll: Letter, lowercase)\njulia> print(t)\n['a', 'c']\n~~~\n\n`pop!`删除并返回最后一个元素：\n\n~~~julia\njulia> t = ['a', 'b', 'c'];\n\njulia> pop!(t)\n'c': ASCII/Unicode U+0063 (category Ll: Letter, lowercase)\njulia> print(t)\n['a', 'b']\n~~~\n\n`popfirst!`删除返回第一个元素：\n\n~~~julia\njulia> t = ['a', 'b', 'c'];\n\njulia> popfirst!(t)\n'a': ASCII/Unicode U+0061 (category Ll: Letter, lowercase)\njulia> print(t)\n['b', 'c']\n~~~\n\n`pushfirst!` and `push!`分别在第一位和最后一位插入数据。\n\n`deleteat!`删除但不返回元素：\n\n~~~julia\njulia> t = ['a', 'b', 'c'];\n\njulia> print(deleteat!(t, 2))\n['a', 'c']\n~~~\n\n `insert!`在指定位置插入元素：\n\n~~~julia\njulia> t = ['a', 'b', 'c'];\n\njulia> print(insert!(t, 2, 'x'))\n['a', 'x', 'b', 'c']\n~~~\n\n### Arrays and Strings\n\n`collect`可以将字符串转为数组：\n\n~~~julia\njulia> t = collect(\"spam\");\n\njulia> print(t)\n['s', 'p', 'a', 'm']\n~~~\n\n`split`可以将字符串按照分隔符分割为数组：\n\n~~~julia\njulia> t = split(\"spam-spam-spam\", '-');\n\njulia> print(t)\nSubString{String}[\"spam\", \"spam\", \"spam\"]\n~~~\n\n`join`是`split`的逆运算，将数组转为字符串：\n\n~~~julia\njulia> t = [\"pining\", \"for\", \"the\", \"fjords\"];\n\njulia> s = join(t, ' ')\n\"pining for the fjords\"\n~~~\n\n### Objects and Values\n\n一个对象是变量可以引用的东西。\n\n对于字符串：\n\n~~~julia\njulia> a = \"banana\"\n\"banana\"\njulia> b = \"banana\"\n\"banana\"\njulia> a ≡ b\ntrue\n~~~\n\n对于数组：\n\n~~~julia\njulia> a = [1, 2, 3];\n\njulia> b = [1, 2, 3];\n\njulia> a ≡ b\nfalse\n~~~\n\n### Aliasing\n\n~~~julia\njulia> a = [1, 2, 3];\n\njulia> b = a;\n\njulia> b ≡ a\ntrue\n~~~\n\n变量与对象的关联称为引用。在这个例子中，有两个对同一个对象的引用。\n\n### Array Arguments\n\n当把一个数组传给函数时，函数得到数组的引用，函数会改变数组：\n\n~~~julia\nfunction deletehead!(t)\n    popfirst!(t)\nend\n\njulia> letters = ['a', 'b', 'c'];\n\njulia> deletehead!(letters);\n\njulia> print(letters)\n['b', 'c']\n~~~\n\n函数`vcat`创建新的数组，不会改变原数组：\n\n~~~julia\njulia> t3 = vcat(t1, [4]);\n\njulia> print(t1)\n[1, 2, 3]\njulia> print(t3)\n[1, 2, 3, 4]\n~~~\n\n切片也会创建新数组，不会改变原数组：\n\n~~~julia\nfunction baddeletehead(t)\n    t = t[2:end]                # WRONG!\nend\n\njulia> t4 = baddeletehead(t3);\n\njulia> print(t3)\n[1, 2, 3, 4]\njulia> print(t4)\n[2, 3, 4]\n~~~\n\n","tags":["Julia","编程"],"categories":["书籍阅读"]},{"title":"B站课程Mathematica前三周","url":"/2022/07/01/math/","content":"\n<p align=\"center\">\n    <img src=\"https://gimg2.baidu.com/image_search/src=http%3A%2F%2Fi0.hdslb.com%2Fbfs%2Farticle%2F7d065302ef2984abddb60152758c13bd66f74fba.jpg&refer=http%3A%2F%2Fi0.hdslb.com&app=2002&size=f9999,10000&q=a80&n=0&g=0n&fmt=auto?sec=1659263087&t=447b47f50cbb4d4d181ed18e4ac43cb5\" style=\"zoom: 100%;\" />\n</p>\n\n>  B站课程链接：https://www.bilibili.com/video/BV1av411N7Xi?spm_id_from=333.999.0.0&vd_source=6177c61c946280bb88c727585de76bc8\n\n<!--more-->\n\n## 第一周\n\n### 值、变量、类型\n\n以下三种对象成为原子(atom)：\n\n符号(Symbol)：由字母和数字(数字不能在起始位置)构成的有限序列\n\n数字(Number)：整数、有理数、实数、复数\n\n字符串(String)：由双引号`\"\"`括起来的任意字符构成的有限序列\n\n#### 命名\n\n我们在命名时应注意避开系统内建符号，系统内建符号的特点为：\n\n+ 由第一个字母大写的单词组成(Camel命名法)：True、False、FactorInteger、SetAttributes\n+ 用来做判断的函数末尾通常有\"Q\"：EvenQ、PrimeQ、MatchQ\n+ 用人名命名的符号=人名+符号名：EulerGamma、BesselJ、DiracDelta\n\n#### 类型检查\n\n一个编程语言的类型检查越严格，程序员受到的束缚越大。Mathematica程序通常来说都比较短小，靠程序员自觉检查类型基本上就足够了，所以不需要太严格的类型检查机制。一个较弱的类型检查可以让程序员写出更灵活的程序。\n\n~~~mathematica\nSin[\"I'm a string!\"]\n> Sin[\"I'm a string!\"]\nSin[\"I'm a string!\"] /. {\"I'm a string!\" -> Pi/3}\n> Sqrt[3]/2\n~~~\n\n### 条件、循环、子程序\n\n在Mathematica中，条件和循环结构其实用的并不多。这是因为条件结构的功能基本上可以通过核心语言中的模式匹配来完成；而循环结构的功能则可以通过表处理和泛函编程完成。\n\n#### 简单的条件判断\n\n~~~mathematica\nIf[True, Print[\"Then\"]]\n> Then\nIf[False, Print[\"Then\"], Print[\"Else\"]]\n> Else\nIf[a == b, Print[\"Then\"], Print[\"Else\"], Print[\"Unevaluated\"]]\n> Unevaluated\nIf[a === b, Print[\"Then\"], Print[\"Else\"], Print[\"Unevaluated\"]]\n> Else\n~~~\n\n#### 多重条件判断\n\n~~~mathematica\nx = 5; Which[x == 1, 1, x == 2, 2, x == 3, 3, True, Print[\"x!=1,2,3\"]]\n> x!=1,2,3\nPlot[Piecewise[{{1, x == 0}, {Sin[x]/x, x != 0}}], {x, -4 Pi, 4 Pi}]\n~~~\n\n![](https://raw.githubusercontent.com/HFC666/image/master/img/math1.jpg)\n\n~~~mathematica\nSwitch[b, True, 1, False, 0, _, Print[\"b is not a boolean value!\"]]\n> b is not a boolean value!\n~~~\n\n#### 简单循环\n\n~~~mathematica\nDo[Print[\"哟,\"], {3}]; Print[\"切克闹!\"]\nDo[Print[Prime[i]], {i, 2, PrimePi[100]}]\nDo[Print[i, \" is a prime number.\"], {i, {2, 3, 5, 7, 11}}]\n~~~\n\n#### 复杂循环\n\n~~~mathematica\nFor[i = 1; t = i, i <= 10, i++, Print[t *= i]]\nn = 1234567; While[Not[PrimeQ[++n]]]; n\n~~~\n\n### 函数\n\n函数定义方法一：模式匹配+延迟赋值\n\n~~~mathematica\nf[x_] := x^2;\nf[x_, y_] := x y;\n~~~\n\n函数定义方法二：纯函数($\\lambda-$表达式、匿名函数)\n\n~~~mathematica\nf1 = Function[x, x^2];\n(* 简写形式 *)\nf2 = #^2 &;\ng1 = Function[{x, y}, x y];\n(* 简写形式 *)\ng2 = #1 #2 &;\n~~~\n\n例子：求不大于给定正整数$n$的所有素数的和\n\n~~~mathematica\n(* 类C实现 *)\nmyPrimeQ = \n  Function[x, i = 2; max = Floor[Sqrt[x]] + 1; \n   While[Mod[x, i] != 0 && i < max, i++]; Not[i < max]];\nmyPrimeSum = \n  Function[n, sum = 0; Do[If[myPrimeQ[x], sum += x], {x, 2, n}]; sum];\n(*核心语言实现。Mathematica内部存储了前10亿个素数的素数表*)\nmyPrimesum2 = Plus @@ Prime /@ Range @ PrimePi[#] &;\nTiming[#[100000]] & /@ {myPrimeSum, myPrimesum2}\n> {{3.4375, 454396537}, {0., 454396537}}\n~~~\n\n## 第二周\n\n### 核心语言\n\n什么是Mathematica核心语言\n\n我们已经看到，虽然Mathematica提供了一些函数可以让我们像编写C程序一样编写Mathematica程序，但是这样编写出来的程序的效率很成问题，而且程序本身也不易懂。另一方面，我们还展示了如何用所谓Mathematica核心语言编制出更高效、更简明的程序。如：\n\n~~~mathematica\nmyPrimesum2 = Plus @@ Prime /@ Range @ PrimePi[#] &;\n~~~\n\n这里有很多符号(`@@`、`/@`、`#`、`&`)，它们其实只是一些Mathematica内建函数的简写形式，其完整形式为：\n\n~~~mathematica\nmyPrimesum2 = Function[n,\n  Apply[\n   Plus,\n   Map[\n    Prime,\n    Range[\n     PrimePi[n]\n     ]\n    ]\n   ]\n  ] \n~~~\n\n在Mathematica中，我们可以用TreeForm来获得一个表达式的语法树。\n\n~~~mathematica\nTreeForm[(a + b^n)/z == x]\n~~~\n\n![](https://raw.githubusercontent.com/HFC666/image/master/img/math2.jpg)\n\n我们也可以用FullForm来获得一个表达式在Mathematica内部的完整形式：\n\n~~~mathematica\nFullForm[(a + b^n)/z == x]\n> FullForm[(a + b^n)/z == x]\n~~~\n\n在Mathematica中，满足如下条件的对象就叫做表达式\n\n1. 原子对象是表达式\n2. 若$\\mathrm{F,X_1,X_2,\\cdots,X_n}$是表达式，则$\\mathrm{F[X_1,X_2,\\cdots,X_n]}$也是表达式。\n\nMathematica中的一切对象都是表达式，一个Mathematica程序就是一个表达式。\n\n> 这一事实如此地重要，以至于有人将其称为Mathematica的第一原理：万物皆表(达式)。\n\nMathematica的计算：\n\n1. 从待计算对象中识别一些可化简的模式\n2. 将识别出的模式用已知的规则进行化解\n\nMathematica是这样进行计算的，其中第一步叫做模式匹配，第二步叫做规则代入。基于模式和规则的计算模型在数理逻辑或计算机科学中叫做重写系统。\n\nMathematica第二原理：计算即重写。\n\n### 表达式与表\n\n根据定义，一个表达式或者是原子，或者是形如$\\mathrm{F[X_1,X_2,\\cdots,X_n]}$的函数。事实上，原子也可以看成后者的特殊情况，只要我们把函数的自变量个数取成零就行了。所以，以后我们讨论表达式的时候，总把它写成$\\mathrm{F[X_1,X_2,\\cdots,X_n]}$的样子。\n\n给定一个表达式$\\mathrm{F[X_1,X_2,\\cdots,X_n]}$，我们称$\\mathrm{F}$是它的\"头\"。\n\n~~~mathematica\nHead /@ {1, 1/2, True, \"number\", a + b, a - b, a*b, \n  a/b, (f + g)[x1, x2, x3]}\n> {Integer, Rational, Symbol, String, Plus, Plus, Times, Times, f + g}\n~~~\n\n我们发现对于原子表达式：符号的头总是Symbol；数字的头则依赖于它的类型，结果可以是Integer、Rational、Real和Complex；字符串的头总是String；图片的头是Image等等。\n\n利用这个性质，我们可以判断一个表达式是否是原子。\n\n~~~mathematica\nmyAtomQ = \n  Function[ex, \n   MemberQ[{Symbol, Integer, Rational, Reals, Complex, String, \n     Image}, Head[ex]]];\n~~~\n\n除了头以外，我们也常常需要将表达式的参数部分取出来。取出来的东西一些表达式构成的序列，是没有头的。但是在Mathematica里所有的表达式都必须有头，所以，为了处理这种无头表达式，Mathematica引入表(List)这个概念，然后规定所哟无头表达式的头都是List。\n\n~~~mathematica\nex = f[x1, x2, x3];\nList @@ ex\nApply[g, ex]\n(*可以将Apply函数理解为换头*)\n> {x1, x2, x3}\n> g[x1, x2, x3]\n~~~\n\n表这种表达式还有一种变体，叫做序列(Sequence)。序列可以认为是没有两边花括号(\"{}\")的表，或者说，表是用序列的元素做成了一个新的对象，而序列是某种更原始的东西。\n\n~~~mathematica\nex = h[1, 2, 3];\nseq = Sequence @@ ex\nlst = List @@ ex\n> Sequence[1, 2, 3]\n> {1, 2, 3}\n~~~\n\n~~~mathematica\nf[seq]\nf[lst]\nf @@ lst\nf[seq, lst, 4, 5, 6]\n> f[1, 2, 3]\n> f[{1, 2, 3}]\n> f[1, 2, 3]\n> f[1, 2, 3, {1, 2, 3}, 4, 5, 6]\n~~~\n\n上面的例子表明，当我们想把一个表达式的参数传递给另一个函数时，用List换头的结果可能不是我们想要的，因为多了一层花括号。如果不想要这层花括号，就要用Sequence换头。\n\n除了用Head和Apply以外，Mathematica还提供了另一种访问复合表达式内部表达式的方法，即系统内建函数Part，简写形式为`[[...]]`。\n\n~~~mathematica\nex = f[x1, x2, x3];\n{ex[[0]], ex[[1]], ex[[2]], ex[[3]]}\n> {f, x1, x2, x3}\n~~~\n\n对于嵌套的表达式，我们可以多次地取Part：\n\n~~~mathematica\nex = f[a, g[b, c], h[d, k[e, i], j]];\nex[[3]][[2]][[2]]\n> i\n~~~\n\n这个操作可以通过一个Part实现：\n\n~~~mathematica\nex[[3, 2, 2]]\n> i\n~~~\n\n另外Part还有更多的用法：\n\n~~~mathematica\nex[[-1, -2, -1]]\nex[[{2, 3}]]\nex[1 ;; 2]\nex[[1 ;; 3 ;; 2]]\n> i\n> f[g[b, c], h[d, k[e, i], j]]\n> f[a, g[b, c], h[d, k[e, i], j]][1 ;; 2]\n> f[a, h[d, k[e, i], j]]\n~~~\n\n最常用的Part都有属于它们自己的内建函数：\n\n~~~mathematica\nFunction[op, op[f[x1, x2, x3, x4]]] /@ {First, Last, Rest, Most}\n> {x1, x4, f[x2, x3, x4], f[x1, x2, x3]}\nTake[f[x1, x2, x3, x4], {2, 3}]\nDrop[f[x1, x2, x3, x4], {2, 3}]\n> f[x2, x3]\n> f[x1, x4]\n~~~\n\n对于Take函数，我们可以这样替代：\n\n~~~mathematica\nf[x1, x2, x3, x4][[{2, 3}]]\n> f[x2, x3]\n~~~\n\n对于给定的表达式，有两个值很重要，即它的长度和深度：\n\n~~~mathematica\nLength[f[g[x1, h[x2, x3]], x4]]\nDepth[f[g[x1, h[x2, x3]], x4]]\n> 2\n> 4\n~~~\n\n### 表的构造\n\n~~~mathematica\nRange[10]\nRange[2, 10]\nRange[2, 10, 3]\n> {1, 2, 3, 4, 5, 6, 7, 8, 9, 10}\n> {2, 3, 4, 5, 6, 7, 8, 9, 10}\n> {2, 5, 8}\n~~~\n\n~~~mathematica\nTable[i^2 + i + 1, {i, 10}]\nTable[KroneckerDelta[i, j - 1] + t KroneckerDelta[i, j + 4], {i, \n   5}, {j, 5}] // MatrixForm\n> {3, 7, 13, 21, 31, 43, 57, 73, 91, 111}\n~~~\n\n$$\n\\left(\\begin{array}{lllll}\n0 & 1 & 0 & 0 & 0 \\\\\n0 & 0 & 1 & 0 & 0 \\\\\n0 & 0 & 0 & 1 & 0 \\\\\n0 & 0 & 0 & 0 & 1 \\\\\nt & 0 & 0 & 0 & 0\n\\end{array}\\right)\n$$\n\n~~~mathematica\nArray[#^2 + # + 1 &, 10]\n> {3, 7, 13, 21, 31, 43, 57, 73, 91, 111}\n#^2 + # + 1 & /@ Range[10]\n> {3, 7, 13, 21, 31, 43, 57, 73, 91, 111}\n\nArray[f, 5, 2, h](*5指的是长度，2指的是起始位置，h指的是头*)\n> h[f[2], f[3], f[4], f[5], f[6]]\n~~~\n\n~~~mathematica\nTuples[Range[3], 2]\n> {{1, 1}, {1, 2}, {1, 3}, {2, 1}, {2, 2}, {2, 3}, {3, 1}, {3, 2}, {3, \n  3}}\nOuter[f, Range[3], Range[2]]\n> {{f[1, 1], f[1, 2]}, {f[2, 1], f[2, 2]}, {f[3, 1], f[3, 2]}}\n~~~\n\n### 表的查询和搜索\n\n~~~mathematica\nex = f[x1, x2, x3, x4];\nFunction[i, MemberQ[ex, i]] /@ {f, x1, x2, x3, x4, x5, x6}\nFunction[i, FreeQ[ex, i]] /@ {f, x1, x2, x3, x4, x5, x6}\n> {False, True, True, True, True, False, False}\n> {False, False, False, False, False, True, True}\n~~~\n\n可以看出MemberQ和FreeQ结果并不是相反的。\n\nMemberQ函数还有参数：\n\n~~~mathematica\nMemberQ[ex, f, Heads -> True]\n> True\n~~~\n\nFreeQ函数可以指定深度（默认深度是所有深度都查找）：\n\n~~~mathematica\nFreeQ[{{1, 1, 3, 0}, {2, 1, 2, 2}}, 0]\nFreeQ[{{1, 1, 3, 0}, {2, 1, 2, 2}}, 0, 1](*在第一层搜索*)\n> False\n> True\n~~~\n\n还有Count函数(默认只看第一层)：\n\n~~~mathematica\neuler = (a + b^n)/n == x;\n{Count[euler, n], Count[euler, n, Infinity]}\n> {0, 2}\n~~~\n\nPosition可以找到位置：\n\n~~~mathematica\nPosition[euler, n]\nPosition[euler, n, 4]\nPosition[euler, n, {4}](*带{}表示只搜这一层，不带表示最深搜到这层*)\nFunction[level, Position[euler, n, level]] /@ {0, 1, 2, 3, 4}\nFunction[level, Position[euler, n, level]] /@ {3, 4}\nFunction[level, Position[euler, n, level]] /@ {{3}, {4}}\n> {{1, 1, 2, 2}, {1, 2, 1}}\n> {{1, 1, 2, 2}, {1, 2, 1}}\n> {{1, 1, 2, 2}}\n> {{}, {}, {}, {{1, 2, 1}}, {{1, 1, 2, 2}, {1, 2, 1}}}\n> {{{1, 2, 1}}, {{1, 1, 2, 2}, {1, 2, 1}}}\n> {{{1, 2, 1}}, {{1, 1, 2, 2}}}\n~~~\n\nSelect选取满足某个条件的：\n\n~~~mathematica\nSelect[Prime /@ Range[10], OddQ]\nSelect[Prime /@ Range[10], Mod[#, 4] == 1 &]\n> {3, 5, 7, 11, 13, 17, 19, 23, 29}\n> {5, 13, 17, 29}\n~~~\n\n### 表的添加、删除和修改\n\n下面的操作不改变表达式本身：\n\n~~~mathematica\nex = f[a, b, c];\n{Prepend[ex, z], Append[ex, d], Insert[ex, i, 2], Insert[ex, i, -2]}\n> {f[z, a, b, c], f[a, b, c, d], f[a, i, b, c], f[a, b, i, c]}\nex\n> f[a, b, c]\n~~~\n\n下面的操作会改变表达式：\n\n~~~mathematica\n{PrependTo[ex, z], AppendTo[ex, d]}\n> {f[z, a, b, c], f[z, a, b, c, d]}\n~~~\n\nDelete也不改变原表达式：\n\n~~~mathematica\nDelete[ex, 1]\nDelete[ex, {{1}, {-1}}]\n> f[a, b, c, d]\n> f[a, b, c]\n~~~\n\nReplacePart也不改变：\n\n~~~mathematica\n{ReplacePart[ex, 1 -> x], ex}\n> {f[x, a, b, c, d], f[z, a, b, c, d]}\n~~~\n\n但是下面会改变：\n\n~~~mathematica\nex[[1]] = y;\nex\n> f[y, a, b, c, d]\n~~~\n\n下面的是旋转的函数：\n\n~~~mathematica\nReverse[ex]\nRotateLeft[ex, 2]\nRotateRight[ex, -1]\n> f[d, c, b, a, y]\n> f[b, c, d, y, a]\n> f[a, b, c, d, y]\n~~~\n\n### 集合运算\n\n头部一样的表达式之间的集合运算：\n\n~~~mathematica\nJoin[f[x1, x2], f[x1, x3]]\n> f[x1, x2, x1, x3]\nUnion[f[x1, x2, x1, x3]]\n> f[x1, x2, x3]\nIntersection[f[x1, x2, x3], f[x1, x2, x4]]\n> f[x1, x2]\nComplement[f[x1, x2, x3], f[x1, x2, x4]]\n> f[x3]\n~~~\n\n### 排序\n\n~~~mathematica\nlist = Array[RandomInteger[10] &, {20, 2}]\nSort[list]\n> {{8, 0}, {6, 7}, {4, 3}, {2, 10}, {9, 6}, {1, 0}, {5, 10}, {8, \n  10}, {1, 1}, {8, 5}, {5, 8}, {9, 3}, {10, 7}, {4, 3}, {2, 9}, {4, \n  5}, {5, 1}, {1, 0}, {4, 3}, {5, 9}}\n> {{1, 0}, {1, 0}, {1, 1}, {2, 9}, {2, 10}, {4, 3}, {4, 3}, {4, 3}, {4, \n  5}, {5, 1}, {5, 8}, {5, 9}, {5, 10}, {6, 7}, {8, 0}, {8, 5}, {8, \n  10}, {9, 3}, {9, 6}, {10, 7}}\n\nSort[list, Function[{list1, list2}, list1[[1]] < list2[[1]]]]\n> {{1, 0}, {1, 1}, {1, 0}, {2, 9}, {2, 10}, {4, 3}, {4, 5}, {4, 3}, {4, \n  3}, {5, 9}, {5, 1}, {5, 8}, {5, 10}, {6, 7}, {8, 5}, {8, 10}, {8, \n  0}, {9, 3}, {9, 6}, {10, 7}}\nSort[list, (#1[[1]] < #2[[1]]) || (#1[[1]] == #2[[1]] && #1[[2]] > #2[[2]]) &]\n> {{1, 1}, {1, 0}, {1, 0}, {2, 10}, {2, 9}, {4, 5}, {4, 3}, {4, 3}, {4, \n  3}, {5, 10}, {5, 9}, {5, 8}, {5, 1}, {6, 7}, {8, 10}, {8, 5}, {8, \n  0}, {9, 6}, {9, 3}, {10, 7}}\n  \nlist = {2, 3, 5, 1, 4}\nSort[list]\nOrdering[list]\nlist[[Ordering[list]]]\n\n> {2, 3, 5, 1, 4}\n> {1, 2, 3, 4, 5}\n> {4, 1, 2, 5, 3}\n> {1, 2, 3, 4, 5}\n~~~\n\n## 第三周\n\n例：一种常用的提速技巧\n\n`Append`函数很慢，因为在表的最后插入数据，需要遍历整个表。\n\n问题：找到不大于$n$的所有无平方因子的自然数\n\n~~~mathematica\nsolution1 = \n  Function[n, L = {}; \n   Function[i, If[SquareFreeQ[i], AppendTo[L, i]]] /@ Range[n]; L];\nsolution2 = \n  Function[n, L = {}; \n   Function[i, If[SquareFreeQ[i], PrependTo[L, i]]] /@ Range[n]; \n   Reverse[L]];\n(*嵌套表:Flatten*)\nsolution3 = \n  Function[n, L = {}; \n   Function[i, If[SquareFreeQ[i], L = {L, i}]] /@ Range[n]; \n   Flatten[L]];\n(*收获与播种*)\nsolution4 = \n  Function[n, \n   Reap[Function[i, If[SquareFreeQ[i], Sow[i], 0]] /@ Range[n]][[2, \n     1]]];\n\nTiming[#[50000]][[1]] & /@ {solution1, solution2, solution3, \n  solution4}\n> {1.5625, 1.34375, 0.125, 0.109375}\n~~~\n\n但是Flatten函数也存在缺点，就是当我们搜索的结果中含有表时，会破坏我们搜索结果的结构，我们可以使用下面的方法解决：\n\n求解`Pell`方程$x^2-2y^2=1$的满足$1\\le y\\le n$的解。\n\n~~~mathematica\nsolution5 = \n  Function[n, L = {}; \n   Do[If[IntegerQ[x = Sqrt[1 + 2 y^2]], L = {L, list[x, y]}], {y , \n     n}]; Flatten[L] /. {list -> List}];\n~~~\n\n这里是用`list`代替`List`放置被压平，之后又还原。\n\n但收获与播种不存在此类问题：\n\n~~~mathematica\nsolution6 = \n Function[n, \n  Reap[Do[If[IntegerQ[x = Sqrt[1 + 2 y^2]], Sow[{x, y}]], {y, n}]][[2,\n     1]]]\n~~~\n\n收获与播种是很有用的一种构造表的方法，它们还有更高级的用法：播种的时候可以给每个种子加标签，收获时可以按照标签或模式匹配进行收获。\n\n~~~mathematica\nReap[\n Sow[张三, {披萨, 可乐, 鸡翅}];\n Sow[李四, {意面, 可乐, 鸡翅}];\n Sow[王五, {披萨, 雪碧, 薯条}];\n Sow[刘六, {意面, 红茶, 沙拉}];\n Sow[陈七, {披萨, 可乐, 薯条}];\n Sow[杨八, {意面, 橙汁, 沙拉}];,\n 红茶\n ]\n > Null, {{刘六}}}\n \n Reap[\n Sow[张三, {披萨, 可乐, 鸡翅}];\n Sow[李四, {意面, 可乐, 鸡翅}];\n Sow[王五, {披萨, 雪碧, 薯条}];\n Sow[刘六, {意面, 红茶, 沙拉}];\n Sow[陈七, {披萨, 可乐, 薯条}];\n Sow[杨八, {意面, 橙汁, 沙拉}];,\n _, #1 -> #2 &\n ]\n \n > {Null, {披萨 -> {张三, 王五, 陈七}, 可乐 -> {张三, 李四, 陈七}, 鸡翅 -> {张三, 李四}, \n  意面 -> {李四, 刘六, 杨八}, 雪碧 -> {王五}, 薯条 -> {王五, 陈七}, 红茶 -> {刘六}, \n  沙拉 -> {刘六, 杨八}, 橙汁 -> {杨八}}}\n~~~\n\n`,`后的表示模式匹配,`_`表示所有模式都可，而后面的为匿名函数。\n\n### 模式匹配\n\n我们已经提过Mathematica的第二原理：计算即重写。重写分两步、分别是模式匹配和规则代入。我们先讲模式匹配。\n\n所谓模式，是指满足一定条件的表达式构成的集合。而模式匹配，就是从给定的待计算表达式中搜索出符合某种模式（即输入这个集合）的子表达式。模式匹配完成之后，我们就可以对这些匹配出来的子表达式应用计算规则，从而达到计算或化简的目的。有些时候，我们的目标本身就是某种搜索结果，如果善用模式匹配，就可以写出非常高效的程序。\n\n根据定义，单个表达式也可以认为是一种模式，它只包含一个表达式作为其元素，这在模式匹配的时候也是很常见的。不过这种模式太简单了，我们称之为平凡模式，我们后面讨论模式时一般总假设是非平凡的。\n\n最简单的(非平凡)的模式是`_`，全名`Blank[]`，它代表一切表达式。\n\n~~~mathematica\nFullForm /@ {f[_], g[_, _], _[x, y], _[_, _, _]}\n> {f[Blank[]],g[Blank[],Blank[]],Blank[][x,y],Blank[][Blank[],Blank[],Blank[]]}\n\nFullForm /@ {_ + _, _ - _, _*_, _^_}\n> {Times[2,Blank[]],0,Power[Blank[],2],Power[Blank[],Blank[]]}\n~~~\n\n~~~mathematica\nMatchQ[a + b, _ + _]\nMatchQ[a + a, _ + _]\nMatchQ[a - b, _ - _]\nMatchQ[a - a, _ - _]\nMatchQ[a*b, _*_]\nMatchQ[a*a, _*_]\nMatchQ[a/b, _/_]\nMatchQ[a/a, _/_]\nMatchQ[g[a, b], _[_, _]]\n> False\n> True\n> False\n> True\n> False\n> True\n> False\n> True\n> True\n~~~\n\n`+,-,/,*`为具体的二元运算，而`g`为抽象的二元运算，所以结果可能有所不同。\n\n我们可以将匹配好的模式命名，其完整形式为Pattern[name, pattern]，简写形式有两种，分别对应于不同的优先级。\n\n~~~mathematica\nFullForm[x_]\nFullForm[x : _]\nFullForm[x_[_]]\nFullForm[x : _[_]]\n> Pattern[x,Blank[]]\n> Pattern[x,Blank[]]\n> Pattern[x,Blank[]][Blank[]]\n> Pattern[x,Blank[][Blank[]]]\n~~~\n\n如果在一个模式中，同一个命名模式出现了多次，它们会被认为是同样的。\n\n~~~mathematica\nMatchQ[f[a, a], f[x_, x_]]\nMatchQ[f[a, b], f[x_, x_]]\nMatchQ[f[a, b], f[x_, y_]]\n> True\n> False\n> True\n~~~\n\n注意模式匹配是按Mathematica内部的FullForm匹配的，它总是基于结构的，而非基于数学的。例如当我们匹配`x^_`这个模式时，`x`本身并不会匹配到，尽管在数学上$x=x^1$。\n\n~~~mathematica\n{1, x, x^2, x^3} /. {x^n_ :>  p[n]}\n> 1, x, p[2], p[3]}\n~~~\n\n为了解决这个问题，我们可以这样：\n\n~~~mathematica\n{1, x, x^2, x^3} /. {x^n_ :>  p[n], 1 -> p[0], x -> p[1]}\n> {p[0], p[1], p[2], p[3]}\n~~~\n\n但是如果有涉及到带有交换性、结合性的函数，Mathematica也会变得聪明一些。\n\n~~~mathematica\n{a + b, b + c, Plus[a, Plus[b, c]]} /. {b + x_  :> x}\n> {a, c, a + c}\n~~~\n\n这是因为Plus这个函数在Mathematica内部具有Flat和Orderless两种属性，分别对应于结合性和交换性。Mathematica在做模式匹配的时候会考虑这些属性导致的一些等价形式，如Plus[a,b] = Plus[b,a]，Plus[a,Plus[b,c]] = Plus[b, Plus[a,c]]等等。\n\n我们可以用Cases函数来列出所匹配的东西，不指定深度只搜第一层：\n\n~~~mathematica\nCases[1 + x + f[x^2, x^3], x^_]\nCases[1 + x + f[x^2, x^3], x^_, Infinity]\n> {}\n> {x^2, x^3}\n~~~\n\n~~~mathematica\nMax[Cases[a0 + a1 x + a2 x^2 + a3 x^3, x^n_ :> n, Infinity]]\n> 3\n~~~\n\n可以用模式匹配模式：\n\n~~~mathematica\nCases[{a -> b, c -> d}, HoldPattern[a -> _]]\n> {a -> b}\n~~~\n\n还可以用DeleteCases去掉被匹配到的东西。\n\n~~~mathematica\nDeleteCases[f[x] + g[y], f[_]]\n> g[y]\nDeleteCases[CoefficientList[(1 + x)^10 + (1 - x)^10, x], 0]\n> {2, 90, 420, 420, 90, 2}\n(*CoefficientList求多项式系数*)\n~~~\n\n比简单匹配稍微复杂一点的是类型匹配，完整形式为Blank[head]：\n\n~~~mathematica\nCases[{1, 2.5, x, y, f[x]}, _f]\nCases[{1, 2.5, x, y, f[x]}, _Symbol]\nCases[{1, 2.5, x, y, f[x]}, _Integer]\nCases[{1, 2.5, x, y, f[x]}, _Real]\n> {f[x]}\n> {x,y}\n> {1}\n> {2.5}\n~~~\n\n更复杂的是带条件的模式：\n\n~~~mathematica\nCases[{1, 2, 3, 4, 5, 6, x, y}, _?(EvenQ[(# + #^2)/2] &)]\nCases[{1, 2, 3, 4, 5, 6, x, y}, _?(Not@EvenQ[(# + #^2)/2] &)]\nCases[{1, 2, 3, 4, 5, 6, x, y}, Except[_?(EvenQ[(# + #^2)/2] &)]]\nCases[{1, 2, 3, 4, 5, 6, x, y}, \n Except[_?(EvenQ[(# + #^2)/2] &), _?NumberQ]]\n > {3, 4}\n > {1, 2, 5, 6, x, y}\n > {1, 2, 5, 6, x, y}\n > 1, 2, 5, 6}\n~~~\n\n`Except[c,p]`满足`p`但不满足`c`。\n\n与命名类似，条件也有更低优先级的一种简写形式：\n\n~~~mathematica\nCases[{{1, 2}, {2, 3}, {3, 1}}, _?(#[[1]] < #[[2]] &)]\nCases[{{1, 2}, {2, 3}, {3, 1}}, {x_, y_} /; x < y]\n> {{1, 2}, {2, 3}}\n> {{1, 2}, {2, 3}}\n~~~\n\n运算符`/;`经常被用来定义分情况的函数，如著名的$3x+1$问题：\n\n~~~mathematica\nf[n_] := n/2 /; EvenQ[n]\nf[n_] := 3 n + 1 /; OddQ[n]\n~~~\n\n如何定义双线性运算？\n\n~~~mathematica\ninner[x1_ + x2_, x3_] := inner[x1, x3] + inner[x2, x3]\ninner[x1_, x2_ + x3_] := inner[x1, x2] + inner[x1, x3]\ninner[a_?NumberQ x1_, x2_] := a inner[x1, x2]\ninner[x1_, a_?NumberQ x2_] := a inner[x1, x2]\n\ninner[3 x + 2 y, z/2]\n> 3/2 inner[x, z] + inner[y, z]\n~~~\n\n有时候我们需要对好几种情况做同一种规则代入，这时候就需要\"或然匹配\"，其形式为`p1|p2|p3`：\n\n~~~mathematica\n{1, 1/2, 0.25, 3 + 4 I} /. {_Rational -> 0, _Real -> 0}\n{1, 1/2, 0.25, 3 + 4 I} /. {_Rational | _Real -> 0}\nCases[Symbol /@ CharacterRange[\"a\", \"z\"], Except[a | e | i | o | u]]\n> {1, 0, 0, 3 + 4 I}\n> {1, 0, 0, 3 + 4 I}\n> {b, c, d, f, g, h, j, k, l, m, n, p, q, r, s, t, v, w, x, y, z}\n~~~\n\n之前模式匹配都是针对一个表达式的，模式匹配还可以对表达式序列进行。\n\n~~~mathematica\n{f[], f[x], f[x, y]} /. {f[a__] :> {a}}\n{f[], f[x], f[x, y]} /. {f[a___] :> {a}}\n> {f[], {x}, {x, y}}\n> {{}, {x}, {x, y}}\n~~~\n\n两个下划线对于空(0个表达式，对应于f[]无表达式)不匹配，三个下划线匹配空为空。\n\n例子：判断表中元素是否都是素数。\n\n~~~mathematica\nlistPrimeQ[list_] := \n Not@MatchQ[list, {___, _?(Not[PrimeQ[#]] &), ___}]\n (*前后两个___，表示前面有0个或多个表达式，中间的不能为素数*)\nlist = Array[#^2 + # + 41 &, 40, 0]\nlistPrimeQ[list]\n> True\n~~~\n\n用Longest和Shortest可以控制`__`和`___`的匹配长度：\n\n~~~mathematica\n{a, b, c, d, e, f, g} /. {x__, y__, z__} -> {{x}, {y}, {z}}\n{a, b, c, d, e, f, g} /. {x__, Longest[y__], z__} -> {{x}, {y}, {z}}\n> {{a}, {b}, {c, d, e, f, g}}\n> {{a}, {b, c, d, e, f}, {g}}\n~~~\n\n重复模式：\n\n~~~mathematica\nCases[{f[a], f[a, b], f[a, a], f[a, a, a]}, f[a ..]]\nCases[{f[a], f[a, b], f[a, a], f[a, a, a]}, f[Repeated[a]]]\nCases[{f[a], f[a, b], f[a, a], f[a, a, a]}, f[Repeated[a, 2]]]\nCases[{f[a], f[a, b], f[a, a], f[a, a, a]}, f[Repeated[a, {2, 3}]]]\n> {f[a], f[a, a], f[a, a, a]}\n> {f[a], f[a, a], f[a, a, a]}\n> {f[a], f[a, a]}\n> {f[a, a], f[a, a, a]}\n~~~\n\n模式序列：\n\n~~~mathematica\nf[x : PatternSequence[_, _], y___] := p[{x}, {y}]\n{f[1], f[1, 2], f[1, 2, 3, 4, 5]}\n> {f[1], p[{1, 2}, {}], p[{1, 2}, {3, 4, 5}]}\n{a, b, b, a, b, a, b, a, a, \n  b} /. {___, x : Longest[PatternSequence[a, b] ..], ___} :> {x}\n> {a, b, a, b}\n~~~\n\n模式的默认值：\n\n~~~mathematica\nplus[x_: 0, y_: 0] := x + y;\nplus[]\nplus[x]\nplus[x, y]\n> 0\n> x\n> x+y\n~~~\n\n~~~mathematica\n{1, x, x^2, x^3} /. {x^n_ :> n}\n{1, x, x^2, x^3} /. {x^n_. :>  n}(*相当于把Power函数的默认值也考虑进去了*)\n> {1, x, 2, 3}\n> {1, 1, 2, 3}\n~~~\n\n字面模式：\n\n~~~mathematica\n{f[2], f[a], f[x_], f[y_]} /. f[x_] :> x^2\n{f[2], f[a], f[x_], f[y_]} /. f[Verbatim[x_]] :> x^2\n> {4, a^2, x_^2, y_^2}\n> {f[2], f[a], x^2, f[y_]}\n~~~\n\n","tags":["编程","Mathematica"],"categories":["课程笔记"]},{"title":"Statistical Rethinking:Chapter3","url":"/2022/06/29/rt3/","content":"\n## Sampling the Imaginary\n\n![](https://raw.githubusercontent.com/HFC666/image/master/img/rt3_1.png)\n\n<!--more-->\n\n### Sampling from a grid-approximate\n\n我们先用第二章的例子生成后验分布：\n\n~~~R\np_grid <- seq(from=0, to=1, length.out=1000)\nprior <- rep(1, 1000)\nlikelihood <- dbinom(6, size = 9, prob = p_grid)\nposterior <- likelihood * prior\nposterior <- posterior / sum(posterior)\n~~~\n\n下面我们对后验进行抽样，抽取$10000$个样本：\n\n~~~R\n# sample\nsamples <- sample(p_grid, prob = posterior, size = 1e4, replace = TRUE)\n# plot(samples)\ndens(samples)\n~~~\n\n![](https://raw.githubusercontent.com/HFC666/image/master/img/rt3_2.jpeg)\n\n### Sampling to summarize\n\n对样本进行总结主要包含以下几个方面：\n\n1. 定义边界的区间\n2. 定义概率质量区间的问题\n3. 点估计的问题\n\n#### Intervals of defined boundaries\n\n~~~R\n# add up posterior probability where p < 0.5\nsum(posterior[p_grid < 0.5])\n> 0.1718746\n\n# use samples\nsum(samples < 0.5) / 1e4\n> 0.1731\n\nsum(samples > 0.5 & samples < 0.75) / 1e4\n> 0.6037\n~~~\n\n#### Intervals of defined mass\n\n~~~R\nquantile(samples, 0.8)\n>       80% \n0.7597598 \n\nquantile(samples, c(0.1, 0.9))\n>       10%       90% \n0.4504505 0.8118118 \n~~~\n\n我们也可以通过`PI`和`HDPI`（highest posterior density interval ）函数计算给定概率质量的区间：\n\n~~~R\np_grid <- seq( from=0 , to=1 , length.out=1000 )\nprior <- rep(1,1000)\nlikelihood <- dbinom( 3 , size=3 , prob=p_grid ) \nposterior <- likelihood * prior\nposterior <- posterior / sum(posterior)\nsamples <- sample( p_grid , size=1e4 , replace=TRUE , prob=posterior )\n# center\nPI(samples, prob = 0.5)\n>       25%       75% \n0.7067067 0.9321822 \n\nHPDI(samples, prob = 0.5)\n>   |0.5      0.5| \n0.8428428 0.9979980 \n~~~\n\n一般情况下`PI`和`HDPI`的结果相差不大，但是当概率密度函数高度倾斜时其值会大不相同。\n\n#### Point estimates\n\n最大后验估计：\n\n~~~R\np_grid[which.max(posterior)]\n> 1\n~~~\n\n也可以根据样本来计算众数：\n\n~~~R\nchainmode(samples, adj=0.01)\n> 0.9901787\n~~~\n\n或是均值和中位数\n\n~~~R\nmean(samples)\nmedian(samples)\n> 0.8015405\n> 0.8448448\n~~~\n\n那么描述后验估计用哪一个点呢？有时候我们采用损失函数还衡量点的好坏，但是损失函数的选择也会影响点的选择，如我们选择两个点之间距离的绝对值的差值作为损失函数，那么答案就是中位数。\n\n~~~R\nloss <- sapply(p_grid, function(d) sum(posterior*abs(d-p_grid)))\np_grid[which.min(loss)]\n> 0.8408408\n~~~\n\n可以看到与中位数非常接近。\n\n### Sampling to simulate prediction\n\n#### Dummy data\n\n似然函数也可以用来产产生数据，例如我们之间的例子，我们的似然函数为：\n$$\n\\Pr(w\\mid n,p) = \\frac{n!}{w!(n-w)!}p^w(1-p)^{n-w}\n$$\n例如计算投掷两次，分别有$0,1,2$次是水的概率，$p=0.7$。\n\n~~~R\ndbinom(0:2, size = 2, prob = 0.7)\n> 0.09 0.42 0.49\n~~~\n\n产生数据：\n\n~~~R\nrbinom(10, size = 2, prob = 0.7)\n> 0 2 2 0 1 2 1 2 1 1\n~~~\n\n让我们以$p=0.7$的概率投掷$9$次产生$100000$个数据：\n\n~~~R\ndummy_w <- rbinom(1e5, size = 9, prob = 0.7)\nsimplehist(dummy_w, xlab = \"dummy water count\")\n~~~\n\n![](https://raw.githubusercontent.com/HFC666/image/master/img/rt3_3.jpeg)\n\n\n\n#### Model checking\n\n现在我们的模型有两个不确定性，一个是$p$的不确定性，因为$p$存在一个后验分布，另一个是样本生成的不确定性，即使对于固定的$p$，样本的生成也是不确定的。\n\n~~~R\nw <- rbinom(1e4, size = 9, prob = samples)\nsimplehist(w)\n~~~\n\n相当于我们先从$p$的后验分布中采样$p$，之后根据采样得到的$p$生成数据，得到的就是$p$的**后验预测分布**。\n\n![](https://raw.githubusercontent.com/HFC666/image/master/img/rt3_4.jpeg)\n\n","tags":["概率编程","贝叶斯统计","Statistical Rethinking"],"categories":["书籍阅读"]},{"title":"Statistical Rethinking:Chapter2","url":"/2022/06/29/rt2/","content":"\n## Small Worlds and Large Worlds\n\n![](https://raw.githubusercontent.com/HFC666/image/master/img/rt2_1.png)\n\n当初麦哲伦进行环球航行的时候，错误地认为地球比实际要小，他认为地球的周长只有$30000$km而不是$40000$km。麦哲伦的大小世界提供了模型与现实的对比。小世界是模型自成一体的逻辑世界，在这个小世界里我们根据自己的认知对模型做出一系列假设并且能够验证模型的逻辑。假设小世界是对现实世界的准确描述，没有替代模型可以更好地利用数据中的信息并支持更好的决策。\n\n大世界是现实世界，可能存在很多我们小世界中没有考虑的事件，模型是对现实世界的不完整表示。\n\n<!--more-->\n\n### The garden of forking data\n\n给定数据，能够用更多的方式产生数据的解释更可靠(概率更大)。\n\n#### Counting possibilities\n\n考虑下面的例子，假设有个袋子里有$4$个球，球的颜色为蓝色和白色，我们有放回地取三个球，得到的结果为**蓝白蓝**。袋子里的球和对应于产生该结果的方式如下表所示：\n\n![](https://raw.githubusercontent.com/HFC666/image/master/img/rt2_2.jpg)\n\n所以我们推测袋子里的球为第四种的可能性较大。\n\n#### Using prior information\n\n我们也可以结合先验信息。假设我们又从袋子里抽出来一个球为蓝色，那么我们上面得到的结果就可以作为先验来进行推测，与现在的结果进行相乘，得到的结果如下图：\n\n![](https://raw.githubusercontent.com/HFC666/image/master/img/rt2_3.jpg)\n\n> 第一列每中假设为产生新数据的方式，第二列为之前的结果作为先验。\n\n#### From counts to probability\n\n我们可以将之间的计算转为概率，还是之前的例子，变为：\n\n![](https://raw.githubusercontent.com/HFC666/image/master/img/rt2_4.jpg)\n\n> $p$为蓝色的比例。\n\n计算方法为：\n\n~~~R\nways <- c(0, 3, 8, 9, 0)\nways/sum(ways)\n> 0.00 0.15 0.40 0.45 0.00\n~~~\n\n### Building a  model\n\n假设我们要估计地球上海洋所占的比例，假设我们在地球上随机抽样得到的结果为`WLWWWLWLW`。我们利用上面提到的方法对其进行推断：\n\n![](https://raw.githubusercontent.com/HFC666/image/master/img/rt2_5.jpg)\n\n### Components of the model\n\n#### Likelihood\n\n首先是似然，似然指的是在给定参数情况下数据的合理性(发生的可能性)。\n\n#### Prior\n\n对于我们想要贝叶斯估计的每个参数，我们需要为其提供先验。先验也为一个概率分布，可以是之前数据得到的参数的概率分布或是我们自己根据经验的概率分布。\n\n#### Posterior\n\n再有了先验和似然后我们就可以计算后验：\n$$\n\\text { Posterior }=\\frac{\\text { Likelihood } \\times \\text { Prior }}{\\text { Average Likelihood }}\n$$\n\n### Making the model go\n\n由于后验分布存在积分，我们有时无法直接对其进行计算，这时候就需要数值方法，我们主要介绍三种方法：\n\n1. 网格近似\n2. 二次逼近\n3. MCMC\n\n#### Grid approximation\n\n最简单的调节技术之一是网格近似。虽然大多数参数是连续的，能够取无限数量的值，但事实证明，我们可以通过仅考虑参数值的有限网格来实现对连续后验分布的极好近似。\n\n但是在大多数真实建模中，网格近似是不切实际的。原因是随着参数数量的增加，它的扩展性很差。所以在后面的章节中，网格近似将逐渐消失，取而代之的是其他更有效的技术。\n\n~~~R\n# define grid\np_grid <- seq(from=0, to=1, length.out=20)\n\n# define prior\nprior = rep(1, 20)\n\n# compute likelihood at each value in grid\nlikelihood <- dbinom(6, size = 9, prob = p_grid)\n\n\n# compute product of likelihood and prior\nunstd.posterior <- likelihood * prior\n\n# standardize the posterior, so it sums to 1\nposterior <- unstd.posterior / sum(unstd.posterior)\n~~~\n\n~~~R\nplot(p_grid, posterior, type = \"b\", xlab = \"probability of water\", ylab = \"posterior probability\")\nmtext(\"20 points\")\n~~~\n\n![](https://raw.githubusercontent.com/HFC666/image/master/img/rt2_6.jpeg)\n\n#### Quadratic approximation\n\n在一般的条件下，后验分布峰值附近的区域在形状上将接近高斯或正态分布。这意味着后验分布可以有用地近似为高斯分布。高斯分布很方便，因为它可以完全用两个数字来描述：中心的位置（均值）和分布（方差）。该方法分为两个步骤：\n\n1. 找到后验分布的众数\n2. 一旦找到后验的峰值，就必须估计峰值附近的曲率。该曲率足以计算整个后验分布的二次近似\n\n~~~R\nlibrary(rethinking)\nglobe.qa <- map(\n  alist(\n    w ~ dbinom(9, p), #likelihood\n    p ~ dunif(0,1)\n  ),\n  data = list(w=6)\n)\n# display summary of quadratic approximation\nprecis(globe.qa)\n>  mean   sd 5.5% 94.5%\np 0.67 0.16 0.42  0.92\n~~~\n\n我们与真实的后验进行比较：\n\n~~~R\n# analytical calculation\nw <- 6\nn <- 9\ncurve(dbeta(x, w+1, n-w+1), from = 0, to=1)\n# quadratic approximation\ncurve(dnorm(x, 0.67, 0.16), lty=2, add = TRUE)\n~~~\n\n![](https://raw.githubusercontent.com/HFC666/image/master/img/rt2_7.jpeg)\n\n","tags":["概率编程","贝叶斯统计","Statistical Rethinking"],"categories":["书籍阅读"]},{"title":"Statistical Rethinking:Chapter1","url":"/2022/06/28/rt1/","content":"\n## The Golem of Prague\n\n![](https://raw.githubusercontent.com/HFC666/image/master/img/rt1_1.png)\n\n`golem`为魔像，为一个非常强大的机器人，但是它只会听从人的命令，没有自主思考的能力。因此人类必须给他设置非常具体的命令，否则可能会对人类造成伤害。\n\n<!--more-->\n\n### Statistical golems\n\n统计家也制造魔像。\n\n![](https://raw.githubusercontent.com/HFC666/image/master/img/rt1_2.jpg)\n\n> 统计家制造的魔像，非常强大，但也同样需要人类的指导。缺乏灵活性，在需要创造性的区域无法应用。\n\n### Statistical rethinking\n\n很多人认为统计推断的目标是检验无效假设。但这是不正确的，我们有以下两个理由：\n\n1. 假设不是模型。假设和不同种类的模型之间的关系是复杂的。许多模型对应同一个假设，许多假设对应一个模型。这使得严格的证伪变得不可能。\n2. 测量很重要。即使我们认为数据证伪了模型，另一位观察者也会争论我们的方法和措施。他们不相信数据。有时他们是对的。\n\n#### Hypotheses are not models\n\n当我们试图证伪一个假设时，我们必须使用某种模型，但是我们不能仅仅通过一个模型来证明假设是错误的。\n\n我们看一个关于进化的例子，有人认为进化是中性的，而有人不这么认为，所有存在两个假设。同一个假设可能导致不同的过程的模型，而同一个过程模型会引出不同的统计模型，同一个统计模型也可能对应于不同的过程模型和假设，因此证伪非常复杂。\n\n![](https://raw.githubusercontent.com/HFC666/image/master/img/rt1_3.jpg)\n\n\n\n#### Measurement matters\n\n首先观测是存在误差的，而且有的测量非常复杂容易出现误差。\n\n其次假设并不一定是离散的，现实生活中的很多假设都是连续的，例如$80\\%$的天鹅都是白色的，对我们证伪来说非常困难。\n\n### Three tools for golem engineering\n\n#### Bayesian data analysis\n\n贝叶斯统计用随机型来描述不确定性，更详细的将在第二章讲述。\n\n#### Multilevel models\n\n使用多级模型有四个典型且互补的原因：\n\n1. 调整重复抽样的估计值。当不止一个观察来自同一个人、地点或时间时，传统的单级模型可能会误导我们。\n2. 调整抽样不平衡的估计值。当某些个体、地点或时间的采样次数多于其他人时，我们也可能会被单级模型误导。\n3. 研究变异。如果我们的研究问题包括数据中个人或其他群体之间的变化，那么多层次模型将有很大帮助，因为它们明确地模拟了变化。\n4. 避免平均。学者们经常对一些数据进行预平均，以构建用于回归分析的变量。这可能很危险，因为平均会消除变化。因此，它制造了虚假的信念。多级模型允许我们保留原始预平均值中的不确定性，同时仍使用平均值进行预测。\n\n#### Model comparison and information criteria\n\n最著名的信息准则是 AIC，即 Akaike (ah-kah-ee-kay) 信息准则。AIC 及其同类被称为“信息”标准，因为它们从信息论中发展出对模型准确性的度量。我们可以用起来比较模型的好坏。\n\n","tags":["概率编程","贝叶斯统计","Statistical Rethinking"],"categories":["书籍阅读"]},{"title":"Turing:a language for flexible probabilistic inference","url":"/2022/06/28/Turing/","content":"\n## Turing: a language for flexible probabilistic inference\n\n> 文章链接：http://proceedings.mlr.press/v84/ge18b.html?ref=https://githubhelp.com\n\n![](https://raw.githubusercontent.com/HFC666/image/master/img/Tur1.png)\n\n<!--more-->\n\n### Background\n\n在概率模型中，我们关注的一般是$p(\\theta\\mid y,\\gamma)$，其中$\\theta$为参数，$y$为观测数据，$\\gamma$为一些确定了的超参数。\n\n#### Models as computer programs\n\n最早的概率编程语言为BUGS，可以追溯到20世纪90年代。下面展示了概率程序的一般结构。\n\n输入：数据$y$和超参数$\\gamma$\n\n步骤1：定义全局参数：\n$$\n\\theta^{\\text{global}}\\sim p(\\cdot\\mid \\gamma)\n$$\n步骤2：对于每一个观测$y_n$，定义(局部)隐变量并计算似然：\n$$\n\\begin{aligned}\n\\theta_n^{\\text{local}}&\\sim p\\left(\\cdot\\mid\\theta_{1:n-1}^{\\text{local}},\\theta^{\\text{global}},\\gamma\\right)\\\\\ny_n&\\sim p\\left(\\cdot\\mid \\theta_{1:n}^{\\text{local}},\\theta^{\\text{global}},\\gamma\\right)\n\\end{aligned}\n$$\n其中$n=1,2,\\cdots,N$。\n\n参数分为两类：$\\theta_n^{\\text{local}}$表示对于观测$y_n$的模型参数，如混合高斯模型中$y_n$属于哪个高斯分布的参数，而$\\theta^{\\text{global}}$表示全局变量。\n\n#### Inference for probabilistic programs\n\n概率程序只有在与高效的推理引擎相结合时才能发挥其灵活性潜力。为了解释概率编程中推理如何工作，我们考虑以下具有$K$个状态的HMM例子：\n$$\n\\begin{aligned}\n\\pi_k&\\sim \\text{Dir}(\\theta)\\\\\n\\phi_k&\\sim p(\\gamma)\\\\\nz_t\\mid z_{t-1}&\\sim \\text{Cat}(\\cdot\\mid \\pi_{z_{t-1}})\\\\\ny_t\\mid z_t&\\sim h(\\cdot\\mid \\phi_{z_t})\n\\end{aligned}\n$$\n其中$k = 1,2,\\cdots,K$，$t = 1,\\cdots,N$。\n\n具有以下三个步骤的高效 Gibbs 采样器通常用于贝叶斯推理：\n\n+ Step 1: Sample $z_{1: T} \\sim z_{1: T} \\mid \\phi_{1: K}, \\pi_{1: K}, y_{1: T} ;$\n+ Step 2: Sample $\\phi_{k} \\sim \\phi_{k} \\mid z_{1: T}, y_{1: T}, \\gamma$;\n+ Step 3: Sample $\\pi_{k} \\sim \\pi_{k} \\mid z_{1: T}, \\theta(k=1, \\ldots, K)$.\n\n\n\n#### Computation graph based inference\n\n对概率程序进行建模的一大挑战是构建模型变量之间的计算图。对于一些编程语言，在推理之前概率图模型就已经生成，但是当程序中存在随机分支时就会出现问题，在这种情况下，我们不得不求助于其他推理方法。\n\n### Composable MCMC inference\n\n我们提出的可组合推理方法利用了HMC算法和粒子吉布斯(PG)算法。为了描述所提出的概率程序方法，我们利用潜在狄利克雷分配(LDA)的例子。\n\n~~~julia\n@model lda(K ,M, N, w, d, beta, alpha) = begin\n    theta = Vector{Vector{Real}}(M)\n    for m = 1:M\n        theta[m] ~ Dirichlet(alpha)\n    end\n    phi = Vector{Vector{Real}}(K)\n    for k = 1:K\n        phi[k] ~ Dirichlet(beta)\n    end\n    \n    z = tzeros(Int, N)\n    for n = 1:N\n        z[n] ~ Categorical(theta[d[n]])\n        w[n] ~ Categorical(phi[z[n]])\n    end\nend\n~~~\n\n其中变量$\\phi,\\theta,z$表示模型参数，变量$K,M,N,d,\\beta,\\alpha$表示超参数，$w$表示观测数据。\n\n一旦定义了模型，提供数据和执行推理就很直观了。\n\n~~~julia\nmodel = lda(K, V, M, N, w, d, beta, alpha)\nsample(model, engine)\n~~~\n\n`engine`是我们想要使用的MCMC引擎。例如，如果要应用例子吉布斯采样，我们可以：\n\n~~~julia\nspl = PG(n, m)\nsample(model, spl)\n~~~\n\n这将会用含有$m$个粒子的PG进行$n$次迭代。\n\n我们也可以对不同的参数采用不同的采样器：\n\n~~~julia\nspl2 = Gibbs(1000, PG(10,2,:z), HMC(2, 0.1, 1, 5, :phi,:theta))\n~~~\n\n上述采样引擎`spl2`将参数分割为两部分，每个部分采用不同的采样方法，值得注意的是，分布的两个部分不需要是互斥的。\n\n#### A family of MCMC operators\n\n![](https://raw.githubusercontent.com/HFC666/image/master/img/tur2.jpg)\n\n> Supported Monte Carlo algorithms in Turing\n\n### Implementation and Experiments\n\n#### The Turing library\n\nTuring为Julia的一个包。因为Turing为一般的Julia程序，因此它可以利用Julia中丰富的数值和统计库。\n\n##### Efficient particle Gibbs implementation\n\n我们使用协程来实现粒子 Gibbs。协程可以看作是函数的泛化，具有可以在多个点暂停和恢复的特性。\n\n##### Automatic differentiation\n\nHMC在采样的过程需要梯度，当给定定义$\\log p(\\theta\\mid z_{1:N},\\gamma)$的计算程序时，这些梯度可以通过自动微分(AD)自动获得。为了简便和高效，我们率先使用了一种称为向量模式的前向微分技术。向量模式前向微分背后的主要概念是多维对偶数，其在标量函数上的行为定义为：\n$$\nf\\left(\\theta+\\sum_{i=1}^D y_i\\epsilon_i\\right) = f(\\theta) + f^{\\prime}(\\theta)\\sum_{i=1}^Dy_i\\epsilon_i\n$$\n其中$\\epsilon_i\\epsilon_j=0,\\text{for }i\\neq j$。\n\n对于小模型，向量前向AD非常高效。但是对于大模型逆向模式的AD较为高效，因此Turing两种模式都存在。\n\n##### Vectorized random variables\n\nTuring支持利用以下语法对独立同分布的变量进行矢量化采样：\n\n~~~julia\nrv = Vector(10)\nrv ~ [Normal(0, 1)]\n~~~\n\n##### Constrained random variables\n\nTuring支持约束的变量。主要由三种类型的约束：\n\n1. 有界的单变量。\n2. 有简单约束的多维变量，如相加和为$1$。\n3. 矩阵约束：例如协方差矩阵为半正定矩阵。\n\n##### MCMC output analysis\n\n在Turing中我们可以使用`describe`函数计算：\n\n1. 均值\n2. 标准差\n3. naive standard error\n4. 蒙特卡洛标准误差\n5. 有效样本数\n6. 分位数\n\n也可以使用`hpd`函数计算高后验概率区间，互相关`cor`，自相关`autocor`，状态空间变化率`changerate`和偏差信息准则`dic`等等。\n\n#### Finding the right inference engine\n\n下面我们将比较`NUTS`和`Gibbs(PG,HMC)`在不同的概率模型上。\n\n##### Models and inference engine setup\n\n**Stochastic Volatility Model**：参数的集合为$\\{\\phi,\\sigma,\\mu,h_{1:N}\\}$。所有这些参数对于目标分布来说都是可导的，因此NUTS算法是可用的：\n$$\n\\begin{aligned}\n\\mu &\\sim \\mathcal{C} \\mathrm{a}(0,10)), \\phi \\sim \\mathcal{U} \\mathrm{n}(-1,1), \\sigma \\sim \\mathcal{C} \\mathrm{a}(0,5), \\quad(\\sigma>0) \\\\\nh_{1} & \\sim \\mathcal{N}\\left(\\mu, \\sigma / \\sqrt{1-\\phi^{2}}\\right), h_{n} \\sim \\mathcal{N}\\left(\\mu+\\phi\\left(h_{n-1}-\\mu\\right), \\sigma\\right) \\\\\ny_{n} & \\sim \\mathcal{N}\\left(0, \\exp \\left(h_{n} / 2\\right)\\right) \\quad(n=2,3, \\ldots, N) .\n\\end{aligned}\n$$\n其中$\\mathcal{C}\\mathrm{a}$表示柯西分布。\n\n~~~julia\nspl1 = NUTS(1e4, 1e3, 0.65)\nspl2 = Gibbs(1e4, PG(5, 1, :h), NUTS(1, 1e3, 0.65, :mu, :phi, :sigma))\n~~~\n\n**Gaussian Mixture Model**：参数的集合为$\\{z,\\theta\\}$，其中参数$\\theta$是可导的，参数$z$不可以。为了运行NUTS算法，我们积分积掉$z$只对$\\theta$采样：\n$$\n\\begin{array}{r}\n\\mu=\\left(\\mu_{1: K}\\right), \\quad \\sigma=\\left(\\sigma_{1: K}\\right), \\quad \\pi=\\left(p_{1: K}\\right) \\\\\nz \\sim \\operatorname{Cat}(\\pi), \\quad \\theta \\sim \\mathcal{N}\\left(\\mu_{z}, \\sigma_{z}\\right)\n\\end{array}\n$$\n\n~~~julia\nspl3 = NUTS(5e4, 1000, 0.65)\nspl4 = Gibbs(5e4, PG(5, 1, :z), NUTS(5e2, 1e3, 0.65, :theta))\n~~~\n\n##### Results\n\n![](https://raw.githubusercontent.com/HFC666/image/master/img/tur3.jpg)\n\n> 上图为在GMM模型上trace plot，下图为联合分布的概率的对数的图，可以看到两个算法都达到了收敛，但是NUTS算法在某些对方被\"困住了\"。在下图更明显。\n\n![](https://raw.githubusercontent.com/HFC666/image/master/img/tur4.jpg)\n\n> 对具有$5$个混合的GMM采样的结果，可以更明显地看到NUTS算法被困住了，在图的上半部分只探索到了两个混合成分。\n\n\n\n","tags":["概率编程","Julia"],"categories":["文献阅读"]},{"title":"文章A Conceptual Introduction to Hamiltonian Monte Carlo阅读笔记","url":"/2022/06/22/HMC/","content":"\n## A Conceptual Introduction to Hamiltonian Monte Carlo\n\n> 文章链接：https://arxiv.org/abs/1701.02434\n\n![](https://raw.githubusercontent.com/HFC666/image/master/img/HMC1.png)\n\n<!--more-->\n\n### 期望的计算\n\n对于给定函数$f(q)$，我们在给定的$q$的分布$\\pi (q)$上计算其期望：\n$$\n\\mathbb{E}_{\\pi}[f] = \\int_{\\mathcal{Q}}\\pi(q)f(q)dq\n$$\n一般情况下此积分的原函数得不到，因此我们采用蒙特卡洛的方法，在$\\pi(q)$上对$q$进行采样，用下式计算期望：\n$$\n\\mathbb{E}_{\\pi}[f]\\approx \\frac{1}{n}\\sum_{i=1}^n f(q_i)\\quad q_i\\sim \\pi(q)\n$$\n但是我们如何在$\\pi(q)$上进行采样呢？为了节省时间，我们一般选择在概率密度较高的位置进行采样，即在概率密度最高的邻域内进行采样。但是在高维情况下存在问题，假设我们在概率密度最高的$1/3$邻域内进行采样，当维度为$n$时，积分区域的体积为$(1/3)^n$，当$n$很大时趋近于$0$，因此对积分的贡献很小。而概率密度较小的地方由于概率密度趋近于$0$，对积分的贡献也不大。我们着重关注的应该是介于两者之间的区域，其对积分的贡献较大，成为典型集(typical set)。我们研究的重点在于如何在**典型集上采样**。\n\n### 马尔可夫链蒙特卡洛方法\n\n#### 理想状态\n\n利用马尔可夫链蒙特卡洛(MCMC)方法可以在典型集上进行采样。在理想状态下，MCMC的采样过程可以分为三个阶段：\n\n1. 从初始位置到典型集，此时偏差(bias)较大。\n2. 进入典型集后，在典型集上进行探索，准确度迅速上升。\n3. 继续在典型集上进行探索，准确度上升缓慢。\n\n如下图所示：\n\n![](https://raw.githubusercontent.com/HFC666/image/master/img/HMC2.jpg)\n\n> 图(a)表示阶段1，图(b)表示阶段2，图(c)表示阶段3。\n\n当达到阶段3时，估计的结果符合大数定律：\n$$\n\\hat{f}^{\\text{MCMC}}_N\\sim \\mathcal{N}(\\mathbb{E}_{\\pi}[f],\\text{MCMC-SE})\n$$\n其中蒙特卡洛误差为：\n$$\n\\text{MCMC-SE}\\equiv \\sqrt{\\frac{\\text{Var}_{\\pi}[f]}{\\text{ESS}}}\n$$\n其中ESS为有效样本量，定义为：\n$$\n\\text{ESS} = \\frac{N}{1+2\\sum_{l=1}^\\infty \\rho_l}\n$$\n其中$\\rho_l$为之后$l$的自相关系数。\n\n#### 病态情况\n\n当典型集内存在高曲率区域时，会导致此区域无法被探索，造成偏差。\n\n![](https://raw.githubusercontent.com/HFC666/image/master/img/HMC3.jpg)\n\n> 病态情况：其中绿色区域表示高曲率区域。存在三种情况：\n>\n> 1. 无法跨过此高曲率区域，仅在一侧进行采样。\n> 2. 在高曲率区域周围震荡。\n> 3. 可以跨过高区域区域，在整个典型集上进行采样。\n\n#### Metropolis-Hastings采样\n\n一个较为简单的MCMC方法为M-H采样(Metropolis-Hastings采样)，在局部利用建议分布对目标分布进行近似，其分为两个步骤：\n\n1. 在提议分布$\\mathbb{Q}(q^\\prime\\mid q)$进行采样\n2. 计算接受率$$a(q^\\prime\\mid q) = \\min\\left(1,\\frac{\\mathbb{Q}(q\\mid q^\\prime)\\pi(q^\\prime)}{\\mathbb{Q}(q^\\prime\\mid q)\\pi(q)}\\right)$$，如果$a$大于生成的$0\\sim1$之间的随机数，接受样本$q^\\prime$，否则继续接受样本$q$。\n但是M-H采样在高维情况下存在接受率过低的问题。\n\n### Hamiltonian Monte Carlo\n\n汉密尔顿蒙特卡洛(HMC)方法：我们可以利用典型集的形状的特征来进行采样。我们不再在典型集上随机移动，而是通过向量场的形式来指示移动的方向，使其高效地在典型集上移动。\n\n我们将概率系统类比于物理系统，典型集类似于行星绕地球旋转地轨道。对于行星，我们需要添加动量来抵消重力使行星正常围绕地球运动；类比于概率空间，我们需要添加动量来抵消梯度使马尔可夫链在典型集上采样。\n\n#### 相空间和汉密尔顿方程\n\n我们需要引入动量参数来补充目标参数空间的每个维度：\n$$\nq_n \\rightarrow (q_n,p_n)\n$$\n这样将$D$维空间拓展为了$2D$维的空间，我们就将目标参数空间拓展为了相空间。相空间上的联合分布成为典型分布(canonical distribution)：\n$$\n\\pi(q,p) = \\pi(p\\mid q)\\pi(q)\n$$\n这样我们对动量参数进行积分后很容易得到我们要采样的目标参数。\n\n我们将典型分布写为不变的汉密尔顿函数的形式：\n$$\n\\pi(q,p) = \\exp^{-H(q,p)}\n$$\n所以：\n$$\n\\begin{aligned}\nH(q,p) &= -\\log\\pi(p\\mid q) - \\log\\pi(q)\\\\\n&\\equiv K(p,q) + V(q)\n\\end{aligned}\n$$\n其中$K(p,q)$被称为动能，$V(q)$被称为势能。\n\n我们利用汉密尔顿方程来生成向量场：\n$$\n\\begin{aligned}\n\\frac{dq}{dt} &= + \\frac{\\partial H}{\\partial p} = \\frac{\\partial K}{\\partial p}\\\\\n\\frac{dp}{dt} &= -\\frac{\\partial H}{\\partial q} = -\\frac{\\partial K}{\\partial q} - \\frac{\\partial V}{\\partial q}\n\\end{aligned}\n$$\n所以汉密尔顿方程是不随时间发生改变的，因为：\n$$\n\\begin{aligned}\n\\frac{dH}{dt} &= \\frac{\\partial H}{\\partial p}\\frac{d p}{dt} + \\frac{\\partial H}{\\partial q}\\frac{d q}{dt}\\\\\n&= -\\frac{\\partial H}{\\partial p}\\frac{\\partial H}{\\partial q} + \\frac{\\partial H}{\\partial q}\\frac{\\partial H}{\\partial p}\\\\\n&=0\n\\end{aligned}\n$$\n\n#### 理想条件下的汉密尔顿转移\n\n理想条件下的HMC可以分为3个步骤：\n\n1. 从初始位置产生初始动量\n2. 以此类推产生轨迹\n3. 从相空间投影到参数空间\n\n\n\n### 高效的HMC\n\n#### 相空间的几何形状\n\n汉密尔顿公式的性质使汉密尔顿方程的值始终保持不变。话句话说，每一个汉密尔顿轨迹都使一个能级：\n$$\nH^{-1}(E) = \\{q,p\\mid H(q,p)=E\\}\n$$\n如下图所示，相空间可以被分解维汉密尔顿能级。\n\n![](https://raw.githubusercontent.com/HFC666/image/master/img/HMC4.jpg)\n\n所以我们的采样过程可以分解为两个步骤，一个是在相同的能级上进行采样，一个是在不同的能级上进行跃迁，如下图：\n\n![](https://raw.githubusercontent.com/HFC666/image/master/img/HMC5.jpg)\n\n> 深红色表示在相同的能级上进行采样，浅红色的表示在不同能级上进行跃迁。\n\n#### 对动能的优化\n\n欧几里得-高斯动能：\n$$\n\\begin{aligned}\n\\Delta(q,q^\\prime) &= (q-q^\\prime)^\\top\\cdot M\\cdot(q-q^\\prime)\\\\\n\\Delta(p,p^\\prime) &= (p-p^\\prime)^\\top\\cdot M^{-1}\\cdot(p-p^\\prime)\n\\end{aligned}\n$$\n我们一般定义条件分布为：\n$$\n\\pi(p\\mid q) = \\mathcal{N}(p\\mid 0,M)\n$$\n\n\n这种特殊选择定义了欧几里得-高斯动能：\n$$\nK(q,p) = \\frac{1}{2}P^\\top \\cdot M^{-1}\\cdot p + \\frac{1}{2}\\log|M|+\\text{const}\n$$\n\n\n黎曼-高斯动能函数：与欧几里得-高斯动能函数不同之处为协方差与位置有关：\n$$\n\\pi(p\\mid q) = \\mathcal{N}(p\\mid 0,\\Sigma(q))\n$$\n定义了黎曼-高斯动能：\n$$\nK(q,p) = \\frac{1}{2}p^\\top\\cdot\\Sigma^{-1}(q)\\cdot p+\\frac{1}{2}\\log|\\Sigma(q)| + \\text{const}\n$$\n\n\n#### 对积分时间的优化\n\n这里的积分时间指的是在某个特定能级上的探索时间(步数)。随着积分时间的增加，时间期望会收敛到空间期望。\n\n![](https://raw.githubusercontent.com/HFC666/image/master/img/hmc6.jpg)\n\n> 图(a)：时间期望与空间期望的差值的绝对值随着积分时间的变化，可以看到到积分时间到达一定的程度后，增加积分时间对结果产生的影响并不大；图(b)：有效样本数随着积分时间的变化，与图(a)变化类似；图(c)：有效样本数/积分时间随着积分时间的变化，先增加后减小，存在最大值。\n\n当目标概率密度为：\n$$\n\\pi_\\beta(q)\\propto \\exp(-|q|^\\beta)\n$$\n动能函数为欧几里得动能：\n$$\n\\pi(p\\mid q) = \\mathcal{N}(0,1)\n$$\n最优积分时间与包含轨迹的能级的能量成比例：\n$$\nT_{\\text{optimal}}(q,p)\\propto (H(q,p))^{\\frac{2-\\beta}{2\\beta}}\n$$\n\n### 在实践中实现HMC\n\n由于在绝大数情况下我们不能准确地求解哈密顿方程，必须采用数值求解的方法，但是数值求解的过程会累积误差，对我们的结果产生影响。\n\n#### Symplectic Integrators\n\nSymplectic Integrators(辛积分器)是一个强大的积分器，它产生的数值轨迹不会偏离精确的能级，而是在其附近震荡，即使在很长的积分时间内也是如此。\n$$\n\\begin{aligned}\n&q_{0} \\leftarrow q, p_{0} \\leftarrow p \\\\\n&\\text {for } 0 \\leq n<\\llcorner T / \\epsilon\\lrcorner \\text { do } \\\\\n&\\quad p_{n+\\frac{1}{2}}  \\leftarrow p_{n}-\\frac{\\epsilon}{2} \\frac{\\partial V}{\\partial q}\\left(q_{n}\\right) \\\\\n&\\quad q_{n+1}  \\leftarrow q_{n}+\\epsilon p_{n+\\frac{1}{2}} \\\\\n&\\quad p_{n+1} \\leftarrow p_{n+\\frac{1}{2}}-\\frac{\\epsilon}{2} \\frac{\\partial V}{\\partial q}\\left(q_{n+1}\\right)\\\\\n&\\text {end for. }\n\\end{aligned}\n$$\n\n#### 纠正辛积分器\n\n我们在每个能级上运行$L$步，取最后一个样本$(q_L,p_L)$，之后进行能级跃迁。因为我们是使用数值的方法，因此在同一个能级上采样上可能不能保持能量不变。因此我们借用M-H采样的思想来对样本进行进行接受-拒绝，因为在同一个能级上采样当确定初始点时采到的样本是固定的，所以：\n$$\n\\mathbb{Q}(q_0,p_0\\mid q_L,p_L) = \\mathbb{Q}(q_L,p_L\\mid q_0,p_0)=1\n$$\n其接受概率为：\n$$\n\\begin{aligned}\na\\left(q_{L},p_{L} \\mid q_{0}, p_{0}\\right) &=\\min \\left(1, \\frac{\\mathbb{Q}\\left(q_{0}, p_{0} \\mid q_{L},p_{L}\\right) \\pi\\left(q_{L},p_{L}\\right)}{\\mathbb{Q}\\left(q_{L},p_{L} \\mid q_{0}, p_{0}\\right) \\pi\\left(q_{0}, p_{0}\\right)}\\right) \\\\\n\n&=\\min \\left(1, \\frac{\\pi\\left(q_{L},p_{L}\\right)}{\\pi\\left(q_{0}, p_{0}\\right)}\\right) \\\\\n&=\\min \\left(1, \\frac{\\exp \\left(-H\\left(q_{L},p_{L}\\right)\\right)}{\\exp \\left(-H\\left(q_{0}, p_{0}\\right)\\right)}\\right) \\\\\n&=\\min \\left(1, \\exp \\left(-H\\left(q_{L},p_{L}\\right)+H\\left(q_{0}, p_{0}\\right)\\right)\\right)\n\\end{aligned}\n$$\n\n\n\n\n","tags":["算法"],"categories":["文献阅读"]},{"title":"只争朝夕，不负韶华","url":"/2021/07/13/index/","content":"\n<h1 align=\"center\">\n    凡是过往皆为序章，所有将来皆为可盼。\n</h1>\n<p align=\"center\">\n    <img src=\"https://raw.githubusercontent.com/HFC666/image/master/img/head.jpg\" style=\"zoom: 100%;\" />\n</p>\n\n\n\n\n\n\n\n"}]