[{"title":"Statistical Rethinking:Chapter1","url":"/2022/06/28/rt1/","content":"\n## The Golem of Prague\n\n![](https://raw.githubusercontent.com/HFC666/image/master/img/rt1_1.png)\n\n`golem`为魔像，为一个非常强大的机器人，但是它只会听从人的命令，没有自主思考的能力。因此人类必须给他设置非常具体的命令，否则可能会对人类造成伤害。\n\n<!--more-->\n\n### Statistical golems\n\n统计家也制造魔像。\n\n![](https://raw.githubusercontent.com/HFC666/image/master/img/rt1_2.jpg)\n\n> 统计家制造的魔像，非常强大，但也同样需要人类的指导。缺乏灵活性，在需要创造性的区域无法应用。\n\n### Statistical rethinking\n\n很多人认为统计推断的目标是检验无效假设。但这是不正确的，我们有以下两个理由：\n\n1. 假设不是模型。假设和不同种类的模型之间的关系是复杂的。许多模型对应同一个假设，许多假设对应一个模型。这使得严格的证伪变得不可能。\n2. 测量很重要。即使我们认为数据证伪了模型，另一位观察者也会争论我们的方法和措施。他们不相信数据。有时他们是对的。\n\n#### Hypotheses are not models\n\n当我们试图证伪一个假设时，我们必须使用某种模型，但是我们不能仅仅通过一个模型来证明假设是错误的。\n\n我们看一个关于进化的例子，有人认为进化是中性的，而有人不这么认为，所有存在两个假设。同一个假设可能导致不同的过程的模型，而同一个过程模型会引出不同的统计模型，同一个统计模型也可能对应于不同的过程模型和假设，因此证伪非常复杂。\n\n![](https://raw.githubusercontent.com/HFC666/image/master/img/rt1_3.jpg)\n\n\n\n#### Measurement matters\n\n首先观测是存在误差的，而且有的测量非常复杂容易出现误差。\n\n其次假设并不一定是离散的，现实生活中的很多假设都是连续的，例如$80\\%$的天鹅都是白色的，对我们证伪来说非常困难。\n\n### Three tools for golem engineering\n\n#### Bayesian data analysis\n\n贝叶斯统计用随机型来描述不确定性，更详细的将在第二章讲述。\n\n#### Multilevel models\n\n使用多级模型有四个典型且互补的原因：\n\n1. 调整重复抽样的估计值。当不止一个观察来自同一个人、地点或时间时，传统的单级模型可能会误导我们。\n2. 调整抽样不平衡的估计值。当某些个体、地点或时间的采样次数多于其他人时，我们也可能会被单级模型误导。\n3. 研究变异。如果我们的研究问题包括数据中个人或其他群体之间的变化，那么多层次模型将有很大帮助，因为它们明确地模拟了变化。\n4. 避免平均。学者们经常对一些数据进行预平均，以构建用于回归分析的变量。这可能很危险，因为平均会消除变化。因此，它制造了虚假的信念。多级模型允许我们保留原始预平均值中的不确定性，同时仍使用平均值进行预测。\n\n#### Model comparison and information criteria\n\n最著名的信息准则是 AIC，即 Akaike (ah-kah-ee-kay) 信息准则。AIC 及其同类被称为“信息”标准，因为它们从信息论中发展出对模型准确性的度量。我们可以用起来比较模型的好坏。\n\n","tags":["概率编程","贝叶斯统计","Statistical Rethinking"],"categories":["书籍阅读"]},{"title":"Turing:a language for flexible probabilistic inference","url":"/2022/06/28/Turing/","content":"\n## Turing: a language for flexible probabilistic inference\n\n> 文章链接：http://proceedings.mlr.press/v84/ge18b.html?ref=https://githubhelp.com\n\n![](http://static01.imgkr.com/temp/9ce5f3b2581549b8b89e6c9cdf7ebea5.png)\n\n<!--more-->\n\n### Background\n\n在概率模型中，我们关注的一般是$p(\\theta\\mid y,\\gamma)$，其中$\\theta$为参数，$y$为观测数据，$\\gamma$为一些确定了的超参数。\n\n#### Models as computer programs\n\n最早的概率编程语言为BUGS，可以追溯到20世纪90年代。下面展示了概率程序的一般结构。\n\n输入：数据$y$和超参数$\\gamma$\n\n步骤1：定义全局参数：\n$$\n\\theta^{\\text{global}}\\sim p(\\cdot\\mid \\gamma)\n$$\n步骤2：对于每一个观测$y_n$，定义(局部)隐变量并计算似然：\n$$\n\\begin{aligned}\n\\theta_n^{\\text{local}}&\\sim p\\left(\\cdot\\mid\\theta_{1:n-1}^{\\text{local}},\\theta^{\\text{global}},\\gamma\\right)\\\\\ny_n&\\sim p\\left(\\cdot\\mid \\theta_{1:n}^{\\text{local}},\\theta^{\\text{global}},\\gamma\\right)\n\\end{aligned}\n$$\n其中$n=1,2,\\cdots,N$。\n\n参数分为两类：$\\theta_n^{\\text{local}}$表示对于观测$y_n$的模型参数，如混合高斯模型中$y_n$属于哪个高斯分布的参数，而$\\theta^{\\text{global}}$表示全局变量。\n\n#### Inference for probabilistic programs\n\n概率程序只有在与高效的推理引擎相结合时才能发挥其灵活性潜力。为了解释概率编程中推理如何工作，我们考虑以下具有$K$个状态的HMM例子：\n$$\n\\begin{aligned}\n\\pi_k&\\sim \\text{Dir}(\\theta)\\\\\n\\phi_k&\\sim p(\\gamma)\\\\\nz_t\\mid z_{t-1}&\\sim \\text{Cat}(\\cdot\\mid \\pi_{z_{t-1}})\\\\\ny_t\\mid z_t&\\sim h(\\cdot\\mid \\phi_{z_t})\n\\end{aligned}\n$$\n其中$k = 1,2,\\cdots,K$，$t = 1,\\cdots,N$。\n\n具有以下三个步骤的高效 Gibbs 采样器通常用于贝叶斯推理：\n\n+ Step 1: Sample $z_{1: T} \\sim z_{1: T} \\mid \\phi_{1: K}, \\pi_{1: K}, y_{1: T} ;$\n+ Step 2: Sample $\\phi_{k} \\sim \\phi_{k} \\mid z_{1: T}, y_{1: T}, \\gamma$;\n+ Step 3: Sample $\\pi_{k} \\sim \\pi_{k} \\mid z_{1: T}, \\theta(k=1, \\ldots, K)$.\n\n\n\n#### Computation graph based inference\n\n对概率程序进行建模的一大挑战是构建模型变量之间的计算图。对于一些编程语言，在推理之前概率图模型就已经生成，但是当程序中存在随机分支时就会出现问题，在这种情况下，我们不得不求助于其他推理方法。\n\n### Composable MCMC inference\n\n我们提出的可组合推理方法利用了HMC算法和粒子吉布斯(PG)算法。为了描述所提出的概率程序方法，我们利用潜在狄利克雷分配(LDA)的例子。\n\n~~~julia\n@model lda(K ,M, N, w, d, beta, alpha) = begin\n    theta = Vector{Vector{Real}}(M)\n    for m = 1:M\n        theta[m] ~ Dirichlet(alpha)\n    end\n    phi = Vector{Vector{Real}}(K)\n    for k = 1:K\n        phi[k] ~ Dirichlet(beta)\n    end\n    \n    z = tzeros(Int, N)\n    for n = 1:N\n        z[n] ~ Categorical(theta[d[n]])\n        w[n] ~ Categorical(phi[z[n]])\n    end\nend\n~~~\n\n其中变量$\\phi,\\theta,z$表示模型参数，变量$K,M,N,d,\\beta,\\alpha$表示超参数，$w$表示观测数据。\n\n一旦定义了模型，提供数据和执行推理就很直观了。\n\n~~~julia\nmodel = lda(K, V, M, N, w, d, beta, alpha)\nsample(model, engine)\n~~~\n\n`engine`是我们想要使用的MCMC引擎。例如，如果要应用例子吉布斯采样，我们可以：\n\n~~~julia\nspl = PG(n, m)\nsample(model, spl)\n~~~\n\n这将会用含有$m$个粒子的PG进行$n$次迭代。\n\n我们也可以对不同的参数采用不同的采样器：\n\n~~~julia\nspl2 = Gibbs(1000, PG(10,2,:z), HMC(2, 0.1, 1, 5, :phi,:theta))\n~~~\n\n上述采样引擎`spl2`将参数分割为两部分，每个部分采用不同的采样方法，值得注意的是，分布的两个部分不需要是互斥的。\n\n#### A family of MCMC operators\n\n![](http://static01.imgkr.com/temp/679a0f1dc05a4456ae7ee80183c63582.jpg)\n\n> Supported Monte Carlo algorithms in Turing\n\n### Implementation and Experiments\n\n#### The Turing library\n\nTuring为Julia的一个包。因为Turing为一般的Julia程序，因此它可以利用Julia中丰富的数值和统计库。\n\n##### Efficient particle Gibbs implementation\n\n我们使用协程来实现粒子 Gibbs。协程可以看作是函数的泛化，具有可以在多个点暂停和恢复的特性。\n\n##### Automatic differentiation\n\nHMC在采样的过程需要梯度，当给定定义$\\log p(\\theta\\mid z_{1:N},\\gamma)$的计算程序时，这些梯度可以通过自动微分(AD)自动获得。为了简便和高效，我们率先使用了一种称为向量模式的前向微分技术。向量模式前向微分背后的主要概念是多维对偶数，其在标量函数上的行为定义为：\n$$\nf\\left(\\theta+\\sum_{i=1}^D y_i\\epsilon_i\\right) = f(\\theta) + f^{\\prime}(\\theta)\\sum_{i=1}^Dy_i\\epsilon_i\n$$\n其中$\\epsilon_i\\epsilon_j=0,\\text{for }i\\neq j$。\n\n对于小模型，向量前向AD非常高效。但是对于大模型逆向模式的AD较为高效，因此Turing两种模式都存在。\n\n##### Vectorized random variables\n\nTuring支持利用以下语法对独立同分布的变量进行矢量化采样：\n\n~~~julia\nrv = Vector(10)\nrv ~ [Normal(0, 1)]\n~~~\n\n##### Constrained random variables\n\nTuring支持约束的变量。主要由三种类型的约束：\n\n1. 有界的单变量。\n2. 有简单约束的多维变量，如相加和为$1$。\n3. 矩阵约束：例如协方差矩阵为半正定矩阵。\n\n##### MCMC output analysis\n\n在Turing中我们可以使用`describe`函数计算：\n\n1. 均值\n2. 标准差\n3. naive standard error\n4. 蒙特卡洛标准误差\n5. 有效样本数\n6. 分位数\n\n也可以使用`hpd`函数计算高后验概率区间，互相关`cor`，自相关`autocor`，状态空间变化率`changerate`和偏差信息准则`dic`等等。\n\n#### Finding the right inference engine\n\n下面我们将比较`NUTS`和`Gibbs(PG,HMC)`在不同的概率模型上。\n\n##### Models and inference engine setup\n\n**Stochastic Volatility Model**：参数的集合为$\\{\\phi,\\sigma,\\mu,h_{1:N}\\}$。所有这些参数对于目标分布来说都是可导的，因此NUTS算法是可用的：\n$$\n\\begin{aligned}\n\\mu &\\sim \\mathcal{C} \\mathrm{a}(0,10)), \\phi \\sim \\mathcal{U} \\mathrm{n}(-1,1), \\sigma \\sim \\mathcal{C} \\mathrm{a}(0,5), \\quad(\\sigma>0) \\\\\nh_{1} & \\sim \\mathcal{N}\\left(\\mu, \\sigma / \\sqrt{1-\\phi^{2}}\\right), h_{n} \\sim \\mathcal{N}\\left(\\mu+\\phi\\left(h_{n-1}-\\mu\\right), \\sigma\\right) \\\\\ny_{n} & \\sim \\mathcal{N}\\left(0, \\exp \\left(h_{n} / 2\\right)\\right) \\quad(n=2,3, \\ldots, N) .\n\\end{aligned}\n$$\n其中$\\mathcal{C}\\mathrm{a}$表示柯西分布。\n\n~~~julia\nspl1 = NUTS(1e4, 1e3, 0.65)\nspl2 = Gibbs(1e4, PG(5, 1, :h), NUTS(1, 1e3, 0.65, :mu, :phi, :sigma))\n~~~\n\n**Gaussian Mixture Model**：参数的集合为$\\{z,\\theta\\}$，其中参数$\\theta$是可导的，参数$z$不可以。为了运行NUTS算法，我们积分积掉$z$只对$\\theta$采样：\n$$\n\\begin{array}{r}\n\\mu=\\left(\\mu_{1: K}\\right), \\quad \\sigma=\\left(\\sigma_{1: K}\\right), \\quad \\pi=\\left(p_{1: K}\\right) \\\\\nz \\sim \\operatorname{Cat}(\\pi), \\quad \\theta \\sim \\mathcal{N}\\left(\\mu_{z}, \\sigma_{z}\\right)\n\\end{array}\n$$\n\n~~~julia\nspl3 = NUTS(5e4, 1000, 0.65)\nspl4 = Gibbs(5e4, PG(5, 1, :z), NUTS(5e2, 1e3, 0.65, :theta))\n~~~\n\n##### Results\n\n![](http://static01.imgkr.com/temp/99d803e553514e089858f61d913babd1.jpg)\n\n> 上图为在GMM模型上trace plot，下图为联合分布的概率的对数的图，可以看到两个算法都达到了收敛，但是NUTS算法在某些对方被\"困住了\"。在下图更明显。\n\n![](http://static01.imgkr.com/temp/095ae7d0d5a54a33960b6cd2162133bc.jpg)\n\n> 对具有$5$个混合的GMM采样的结果，可以更明显地看到NUTS算法被困住了，在图的上半部分只探索到了两个混合成分。\n\n\n\n","tags":["概率编程","Julia"],"categories":["文献阅读"]},{"title":"文章A Conceptual Introduction to Hamiltonian Monte Carlo阅读笔记","url":"/2022/06/22/HMC/","content":"\n## A Conceptual Introduction to Hamiltonian Monte Carlo\n\n> 文章链接：https://arxiv.org/abs/1701.02434\n\n![](http://static01.imgkr.com/temp/8a010064bcee4b0eac9e9c05caa3f1ee.png)\n\n<!--more-->\n\n### 期望的计算\n\n对于给定函数$f(q)$，我们在给定的$q$的分布$\\pi (q)$上计算其期望：\n$$\n\\mathbb{E}_{\\pi}[f] = \\int_{\\mathcal{Q}}\\pi(q)f(q)dq\n$$\n一般情况下此积分的原函数得不到，因此我们采用蒙特卡洛的方法，在$\\pi(q)$上对$q$进行采样，用下式计算期望：\n$$\n\\mathbb{E}_{\\pi}[f]\\approx \\frac{1}{n}\\sum_{i=1}^n f(q_i)\\quad q_i\\sim \\pi(q)\n$$\n但是我们如何在$\\pi(q)$上进行采样呢？为了节省时间，我们一般选择在概率密度较高的位置进行采样，即在概率密度最高的邻域内进行采样。但是在高维情况下存在问题，假设我们在概率密度最高的$1/3$邻域内进行采样，当维度为$n$时，积分区域的体积为$(1/3)^n$，当$n$很大时趋近于$0$，因此对积分的贡献很小。而概率密度较小的地方由于概率密度趋近于$0$，对积分的贡献也不大。我们着重关注的应该是介于两者之间的区域，其对积分的贡献较大，成为典型集(typical set)。我们研究的重点在于如何在**典型集上采样**。\n\n### 马尔可夫链蒙特卡洛方法\n\n#### 理想状态\n\n利用马尔可夫链蒙特卡洛(MCMC)方法可以在典型集上进行采样。在理想状态下，MCMC的采样过程可以分为三个阶段：\n\n1. 从初始位置到典型集，此时偏差(bias)较大。\n2. 进入典型集后，在典型集上进行探索，准确度迅速上升。\n3. 继续在典型集上进行探索，准确度上升缓慢。\n\n如下图所示：\n\n![](http://static01.imgkr.com/temp/1a1df50a72d54d1998ae2b266f3e3aed.jpg)\n\n> 图(a)表示阶段1，图(b)表示阶段2，图(c)表示阶段3。\n\n当达到阶段3时，估计的结果符合大数定律：\n$$\n\\hat{f}^{\\text{MCMC}}_N\\sim \\mathcal{N}(\\mathbb{E}_{\\pi}[f],\\text{MCMC-SE})\n$$\n其中蒙特卡洛误差为：\n$$\n\\text{MCMC-SE}\\equiv \\sqrt{\\frac{\\text{Var}_{\\pi}[f]}{\\text{ESS}}}\n$$\n其中ESS为有效样本量，定义为：\n$$\n\\text{ESS} = \\frac{N}{1+2\\sum_{l=1}^\\infty \\rho_l}\n$$\n其中$\\rho_l$为之后$l$的自相关系数。\n\n#### 病态情况\n\n当典型集内存在高曲率区域时，会导致此区域无法被探索，造成偏差。\n\n![](http://static01.imgkr.com/temp/ef84320549ab469690733e68a944729c.jpg)\n\n> 病态情况：其中绿色区域表示高曲率区域。存在三种情况：\n>\n> 1. 无法跨过此高曲率区域，仅在一侧进行采样。\n> 2. 在高曲率区域周围震荡。\n> 3. 可以跨过高区域区域，在整个典型集上进行采样。\n\n#### Metropolis-Hastings采样\n\n一个较为简单的MCMC方法为M-H采样(Metropolis-Hastings采样)，在局部利用建议分布对目标分布进行近似，其分为两个步骤：\n\n1. 在提议分布$\\mathbb{Q}(q^\\prime\\mid q)$进行采样\n2. 计算接受率$$a(q^\\prime\\mid q) = \\min\\left(1,\\frac{\\mathbb{Q}(q\\mid q^\\prime)\\pi(q^\\prime)}{\\mathbb{Q}(q^\\prime\\mid q)\\pi(q)}\\right)$$，如果$a$大于生成的$0\\sim1$之间的随机数，接受样本$q^\\prime$，否则继续接受样本$q$。\n但是M-H采样在高维情况下存在接受率过低的问题。\n\n### Hamiltonian Monte Carlo\n\n汉密尔顿蒙特卡洛(HMC)方法：我们可以利用典型集的形状的特征来进行采样。我们不再在典型集上随机移动，而是通过向量场的形式来指示移动的方向，使其高效地在典型集上移动。\n\n我们将概率系统类比于物理系统，典型集类似于行星绕地球旋转地轨道。对于行星，我们需要添加动量来抵消重力使行星正常围绕地球运动；类比于概率空间，我们需要添加动量来抵消梯度使马尔可夫链在典型集上采样。\n\n#### 相空间和汉密尔顿方程\n\n我们需要引入动量参数来补充目标参数空间的每个维度：\n$$\nq_n \\rightarrow (q_n,p_n)\n$$\n这样将$D$维空间拓展为了$2D$维的空间，我们就将目标参数空间拓展为了相空间。相空间上的联合分布成为典型分布(canonical distribution)：\n$$\n\\pi(q,p) = \\pi(p\\mid q)\\pi(q)\n$$\n这样我们对动量参数进行积分后很容易得到我们要采样的目标参数。\n\n我们将典型分布写为不变的汉密尔顿函数的形式：\n$$\n\\pi(q,p) = \\exp^{-H(q,p)}\n$$\n所以：\n$$\n\\begin{aligned}\nH(q,p) &= -\\log\\pi(p\\mid q) - \\log\\pi(q)\\\\\n&\\equiv K(p,q) + V(q)\n\\end{aligned}\n$$\n其中$K(p,q)$被称为动能，$V(q)$被称为势能。\n\n我们利用汉密尔顿方程来生成向量场：\n$$\n\\begin{aligned}\n\\frac{dq}{dt} &= + \\frac{\\partial H}{\\partial p} = \\frac{\\partial K}{\\partial p}\\\\\n\\frac{dp}{dt} &= -\\frac{\\partial H}{\\partial q} = -\\frac{\\partial K}{\\partial q} - \\frac{\\partial V}{\\partial q}\n\\end{aligned}\n$$\n所以汉密尔顿方程是不随时间发生改变的，因为：\n$$\n\\begin{aligned}\n\\frac{dH}{dt} &= \\frac{\\partial H}{\\partial p}\\frac{d p}{dt} + \\frac{\\partial H}{\\partial q}\\frac{d q}{dt}\\\\\n&= -\\frac{\\partial H}{\\partial p}\\frac{\\partial H}{\\partial q} + \\frac{\\partial H}{\\partial q}\\frac{\\partial H}{\\partial p}\\\\\n&=0\n\\end{aligned}\n$$\n\n#### 理想条件下的汉密尔顿转移\n\n理想条件下的HMC可以分为3个步骤：\n\n1. 从初始位置产生初始动量\n2. 以此类推产生轨迹\n3. 从相空间投影到参数空间\n\n\n\n### 高效的HMC\n\n#### 相空间的几何形状\n\n汉密尔顿公式的性质使汉密尔顿方程的值始终保持不变。话句话说，每一个汉密尔顿轨迹都使一个能级：\n$$\nH^{-1}(E) = \\{q,p\\mid H(q,p)=E\\}\n$$\n如下图所示，相空间可以被分解维汉密尔顿能级。\n\n![](http://static01.imgkr.com/temp/028f6ae9a0074f3cb6047d241f6e3945.jpg)\n\n所以我们的采样过程可以分解为两个步骤，一个是在相同的能级上进行采样，一个是在不同的能级上进行跃迁，如下图：\n\n![](http://static01.imgkr.com/temp/72b6558918b7415589144699fd37eb61.jpg)\n\n> 深红色表示在相同的能级上进行采样，浅红色的表示在不同能级上进行跃迁。\n\n#### 对动能的优化\n\n欧几里得-高斯动能：\n$$\n\\begin{aligned}\n\\Delta(q,q^\\prime) &= (q-q^\\prime)^\\top\\cdot M\\cdot(q-q^\\prime)\\\\\n\\Delta(p,p^\\prime) &= (p-p^\\prime)^\\top\\cdot M^{-1}\\cdot(p-p^\\prime)\n\\end{aligned}\n$$\n我们一般定义条件分布为：\n$$\n\\pi(p\\mid q) = \\mathcal{N}(p\\mid 0,M)\n$$\n\n\n这种特殊选择定义了欧几里得-高斯动能：\n$$\nK(q,p) = \\frac{1}{2}P^\\top \\cdot M^{-1}\\cdot p + \\frac{1}{2}\\log|M|+\\text{const}\n$$\n\n\n黎曼-高斯动能函数：与欧几里得-高斯动能函数不同之处为协方差与位置有关：\n$$\n\\pi(p\\mid q) = \\mathcal{N}(p\\mid 0,\\Sigma(q))\n$$\n定义了黎曼-高斯动能：\n$$\nK(q,p) = \\frac{1}{2}p^\\top\\cdot\\Sigma^{-1}(q)\\cdot p+\\frac{1}{2}\\log|\\Sigma(q)| + \\text{const}\n$$\n\n\n#### 对积分时间的优化\n\n这里的积分时间指的是在某个特定能级上的探索时间(步数)。随着积分时间的增加，时间期望会收敛到空间期望。\n\n![](http://static01.imgkr.com/temp/cca5d76d452f47f695f3a308dfe6602f.jpg)\n\n> 图(a)：时间期望与空间期望的差值的绝对值随着积分时间的变化，可以看到到积分时间到达一定的程度后，增加积分时间对结果产生的影响并不大；图(b)：有效样本数随着积分时间的变化，与图(a)变化类似；图(c)：有效样本数/积分时间随着积分时间的变化，先增加后减小，存在最大值。\n\n当目标概率密度为：\n$$\n\\pi_\\beta(q)\\propto \\exp(-|q|^\\beta)\n$$\n动能函数为欧几里得动能：\n$$\n\\pi(p\\mid q) = \\mathcal{N}(0,1)\n$$\n最优积分时间与包含轨迹的能级的能量成比例：\n$$\nT_{\\text{optimal}}(q,p)\\propto (H(q,p))^{\\frac{2-\\beta}{2\\beta}}\n$$\n\n### 在实践中实现HMC\n\n由于在绝大数情况下我们不能准确地求解哈密顿方程，必须采用数值求解的方法，但是数值求解的过程会累积误差，对我们的结果产生影响。\n\n#### Symplectic Integrators\n\nSymplectic Integrators(辛积分器)是一个强大的积分器，它产生的数值轨迹不会偏离精确的能级，而是在其附近震荡，即使在很长的积分时间内也是如此。\n$$\n\\begin{aligned}\n&q_{0} \\leftarrow q, p_{0} \\leftarrow p \\\\\n&\\text {for } 0 \\leq n<\\llcorner T / \\epsilon\\lrcorner \\text { do } \\\\\n&\\quad p_{n+\\frac{1}{2}}  \\leftarrow p_{n}-\\frac{\\epsilon}{2} \\frac{\\partial V}{\\partial q}\\left(q_{n}\\right) \\\\\n&\\quad q_{n+1}  \\leftarrow q_{n}+\\epsilon p_{n+\\frac{1}{2}} \\\\\n&\\quad p_{n+1} \\leftarrow p_{n+\\frac{1}{2}}-\\frac{\\epsilon}{2} \\frac{\\partial V}{\\partial q}\\left(q_{n+1}\\right)\\\\\n&\\text {end for. }\n\\end{aligned}\n$$\n\n#### 纠正辛积分器\n\n我们在每个能级上运行$L$步，取最后一个样本$(q_L,p_L)$，之后进行能级跃迁。因为我们是使用数值的方法，因此在同一个能级上采样上可能不能保持能量不变。因此我们借用M-H采样的思想来对样本进行进行接受-拒绝，因为在同一个能级上采样当确定初始点时采到的样本是固定的，所以：\n$$\n\\mathbb{Q}(q_0,p_0\\mid q_L,p_L) = \\mathbb{Q}(q_L,p_L\\mid q_0,p_0)=1\n$$\n其接受概率为：\n$$\n\\begin{aligned}\na\\left(q_{L},p_{L} \\mid q_{0}, p_{0}\\right) &=\\min \\left(1, \\frac{\\mathbb{Q}\\left(q_{0}, p_{0} \\mid q_{L},p_{L}\\right) \\pi\\left(q_{L},p_{L}\\right)}{\\mathbb{Q}\\left(q_{L},p_{L} \\mid q_{0}, p_{0}\\right) \\pi\\left(q_{0}, p_{0}\\right)}\\right) \\\\\n\n&=\\min \\left(1, \\frac{\\pi\\left(q_{L},p_{L}\\right)}{\\pi\\left(q_{0}, p_{0}\\right)}\\right) \\\\\n&=\\min \\left(1, \\frac{\\exp \\left(-H\\left(q_{L},p_{L}\\right)\\right)}{\\exp \\left(-H\\left(q_{0}, p_{0}\\right)\\right)}\\right) \\\\\n&=\\min \\left(1, \\exp \\left(-H\\left(q_{L},p_{L}\\right)+H\\left(q_{0}, p_{0}\\right)\\right)\\right)\n\\end{aligned}\n$$\n\n\n\n\n","tags":["算法"],"categories":["文献阅读"]},{"title":"只争朝夕，不负韶华","url":"/2021/07/13/index/","content":"\n<h1 align=\"center\">\n    凡是过往皆为序章，所有将来皆可盼。\n</h1>\n\n<p align=\"center\">\n    <img src=\"https://pic3.zhimg.com/v2-526e03d542420a445ea3113417d592be_r.jpg\" style=\"zoom: 100%;\" />\n</p>\n\n\n\n\n"}]