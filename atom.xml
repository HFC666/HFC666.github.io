<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>独自赏晴雨</title>
  
  
  <link href="https://www.hfcouc.work/atom.xml" rel="self"/>
  
  <link href="https://www.hfcouc.work/"/>
  <updated>2022-06-29T01:33:09.911Z</updated>
  <id>https://www.hfcouc.work/</id>
  
  <author>
    <name>HFC</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Statistical Rethinking:Chapter2</title>
    <link href="https://www.hfcouc.work/2022/06/29/rt2/"/>
    <id>https://www.hfcouc.work/2022/06/29/rt2/</id>
    <published>2022-06-29T01:30:27.000Z</published>
    <updated>2022-06-29T01:33:09.911Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="\assets\js\Meting.min.js"></script><h2 id="Small-Worlds-and-Large-Worlds"><a href="#Small-Worlds-and-Large-Worlds" class="headerlink" title="Small Worlds and Large Worlds"></a>Small Worlds and Large Worlds</h2><p><img src="https://raw.githubusercontent.com/HFC666/image/master/img/rt2_1.png" alt=""></p><p>当初麦哲伦进行环球航行的时候，错误地认为地球比实际要小，他认为地球的周长只有$30000$km而不是$40000$km。麦哲伦的大小世界提供了模型与现实的对比。小世界是模型自成一体的逻辑世界，在这个小世界里我们根据自己的认知对模型做出一系列假设并且能够验证模型的逻辑。假设小世界是对现实世界的准确描述，没有替代模型可以更好地利用数据中的信息并支持更好的决策。</p><p>大世界是现实世界，可能存在很多我们小世界中没有考虑的事件，模型是对现实世界的不完整表示。</p><span id="more"></span><h3 id="The-garden-of-forking-data"><a href="#The-garden-of-forking-data" class="headerlink" title="The garden of forking data"></a>The garden of forking data</h3><p>给定数据，能够用更多的方式产生数据的解释更可靠(概率更大)。</p><h4 id="Counting-possibilities"><a href="#Counting-possibilities" class="headerlink" title="Counting possibilities"></a>Counting possibilities</h4><p>考虑下面的例子，假设有个袋子里有$4$个球，球的颜色为蓝色和白色，我们有放回地取三个球，得到的结果为<strong>蓝白蓝</strong>。袋子里的球和对应于产生该结果的方式如下表所示：</p><p><img src="https://raw.githubusercontent.com/HFC666/image/master/img/rt2_2.jpg" alt=""></p><p>所以我们推测袋子里的球为第四种的可能性较大。</p><h4 id="Using-prior-information"><a href="#Using-prior-information" class="headerlink" title="Using prior information"></a>Using prior information</h4><p>我们也可以结合先验信息。假设我们又从袋子里抽出来一个球为蓝色，那么我们上面得到的结果就可以作为先验来进行推测，与现在的结果进行相乘，得到的结果如下图：</p><p><img src="https://raw.githubusercontent.com/HFC666/image/master/img/rt2_3.jpg" alt=""></p><blockquote><p>第一列每中假设为产生新数据的方式，第二列为之前的结果作为先验。</p></blockquote><h4 id="From-counts-to-probability"><a href="#From-counts-to-probability" class="headerlink" title="From counts to probability"></a>From counts to probability</h4><p>我们可以将之间的计算转为概率，还是之前的例子，变为：</p><p><img src="https://raw.githubusercontent.com/HFC666/image/master/img/rt2_4.jpg" alt=""></p><blockquote><p>$p$为蓝色的比例。</p></blockquote><p>计算方法为：</p><figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">ways &lt;- <span class="built_in">c</span>(<span class="number">0</span>, <span class="number">3</span>, <span class="number">8</span>, <span class="number">9</span>, <span class="number">0</span>)</span><br><span class="line">ways/<span class="built_in">sum</span>(ways)</span><br><span class="line">&gt; <span class="number">0.00</span> <span class="number">0.15</span> <span class="number">0.40</span> <span class="number">0.45</span> <span class="number">0.00</span></span><br></pre></td></tr></table></figure><h3 id="Building-a-model"><a href="#Building-a-model" class="headerlink" title="Building a  model"></a>Building a  model</h3><p>假设我们要估计地球上海洋所占的比例，假设我们在地球上随机抽样得到的结果为<code>WLWWWLWLW</code>。我们利用上面提到的方法对其进行推断：</p><p><img src="https://raw.githubusercontent.com/HFC666/image/master/img/rt2_5.jpg" alt=""></p><h3 id="Components-of-the-model"><a href="#Components-of-the-model" class="headerlink" title="Components of the model"></a>Components of the model</h3><h4 id="Likelihood"><a href="#Likelihood" class="headerlink" title="Likelihood"></a>Likelihood</h4><p>首先是似然，似然指的是在给定参数情况下数据的合理性(发生的可能性)。</p><h4 id="Prior"><a href="#Prior" class="headerlink" title="Prior"></a>Prior</h4><p>对于我们想要贝叶斯估计的每个参数，我们需要为其提供先验。先验也为一个概率分布，可以是之前数据得到的参数的概率分布或是我们自己根据经验的概率分布。</p><h4 id="Posterior"><a href="#Posterior" class="headerlink" title="Posterior"></a>Posterior</h4><p>再有了先验和似然后我们就可以计算后验：</p><script type="math/tex; mode=display">\text { Posterior }=\frac{\text { Likelihood } \times \text { Prior }}{\text { Average Likelihood }}</script><h3 id="Making-the-model-go"><a href="#Making-the-model-go" class="headerlink" title="Making the model go"></a>Making the model go</h3><p>由于后验分布存在积分，我们有时无法直接对其进行计算，这时候就需要数值方法，我们主要介绍三种方法：</p><ol><li>网格近似</li><li>二次逼近</li><li>MCMC</li></ol><h4 id="Grid-approximation"><a href="#Grid-approximation" class="headerlink" title="Grid approximation"></a>Grid approximation</h4><p>最简单的调节技术之一是网格近似。虽然大多数参数是连续的，能够取无限数量的值，但事实证明，我们可以通过仅考虑参数值的有限网格来实现对连续后验分布的极好近似。</p><p>但是在大多数真实建模中，网格近似是不切实际的。原因是随着参数数量的增加，它的扩展性很差。所以在后面的章节中，网格近似将逐渐消失，取而代之的是其他更有效的技术。</p><figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># define grid</span></span><br><span class="line">p_grid &lt;- seq(from=<span class="number">0</span>, to=<span class="number">1</span>, length.out=<span class="number">20</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># define prior</span></span><br><span class="line">prior = <span class="built_in">rep</span>(<span class="number">1</span>, <span class="number">20</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># compute likelihood at each value in grid</span></span><br><span class="line">likelihood &lt;- dbinom(<span class="number">6</span>, size = <span class="number">9</span>, prob = p_grid)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># compute product of likelihood and prior</span></span><br><span class="line">unstd.posterior &lt;- likelihood * prior</span><br><span class="line"></span><br><span class="line"><span class="comment"># standardize the posterior, so it sums to 1</span></span><br><span class="line">posterior &lt;- unstd.posterior / <span class="built_in">sum</span>(unstd.posterior)</span><br></pre></td></tr></table></figure><figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">plot(p_grid, posterior, type = <span class="string">&quot;b&quot;</span>, xlab = <span class="string">&quot;probability of water&quot;</span>, ylab = <span class="string">&quot;posterior probability&quot;</span>)</span><br><span class="line">mtext(<span class="string">&quot;20 points&quot;</span>)</span><br></pre></td></tr></table></figure><p><img src="https://raw.githubusercontent.com/HFC666/image/master/img/rt2_6.jpeg" alt=""></p><h4 id="Quadratic-approximation"><a href="#Quadratic-approximation" class="headerlink" title="Quadratic approximation"></a>Quadratic approximation</h4><p>在一般的条件下，后验分布峰值附近的区域在形状上将接近高斯或正态分布。这意味着后验分布可以有用地近似为高斯分布。高斯分布很方便，因为它可以完全用两个数字来描述：中心的位置（均值）和分布（方差）。该方法分为两个步骤：</p><ol><li>找到后验分布的众数</li><li>一旦找到后验的峰值，就必须估计峰值附近的曲率。该曲率足以计算整个后验分布的二次近似</li></ol><figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">library(rethinking)</span><br><span class="line">globe.qa &lt;- map(</span><br><span class="line">  alist(</span><br><span class="line">    w ~ dbinom(<span class="number">9</span>, p), <span class="comment">#likelihood</span></span><br><span class="line">    p ~ dunif(<span class="number">0</span>,<span class="number">1</span>)</span><br><span class="line">  ),</span><br><span class="line">  data = <span class="built_in">list</span>(w=<span class="number">6</span>)</span><br><span class="line">)</span><br><span class="line"><span class="comment"># display summary of quadratic approximation</span></span><br><span class="line">precis(globe.qa)</span><br><span class="line">&gt;  mean   sd <span class="number">5.5</span>% 94.5%</span><br><span class="line">p <span class="number">0.67</span> <span class="number">0.16</span> <span class="number">0.42</span>  <span class="number">0.92</span></span><br></pre></td></tr></table></figure><p>我们与真实的后验进行比较：</p><figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># analytical calculation</span></span><br><span class="line">w &lt;- 6</span><br><span class="line">n &lt;- 9</span><br><span class="line">curve(dbeta(x, w+<span class="number">1</span>, n-w+<span class="number">1</span>), from = <span class="number">0</span>, to=<span class="number">1</span>)</span><br><span class="line"><span class="comment"># quadratic approximation</span></span><br><span class="line">curve(dnorm(x, <span class="number">0.67</span>, <span class="number">0.16</span>), lty=<span class="number">2</span>, add = <span class="literal">TRUE</span>)</span><br></pre></td></tr></table></figure><p><img src="https://raw.githubusercontent.com/HFC666/image/master/img/rt2_7.jpeg" alt=""></p>]]></content>
    
    
    <summary type="html">&lt;h2 id=&quot;Small-Worlds-and-Large-Worlds&quot;&gt;&lt;a href=&quot;#Small-Worlds-and-Large-Worlds&quot; class=&quot;headerlink&quot; title=&quot;Small Worlds and Large Worlds&quot;&gt;&lt;/a&gt;Small Worlds and Large Worlds&lt;/h2&gt;&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/HFC666/image/master/img/rt2_1.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;当初麦哲伦进行环球航行的时候，错误地认为地球比实际要小，他认为地球的周长只有$30000$km而不是$40000$km。麦哲伦的大小世界提供了模型与现实的对比。小世界是模型自成一体的逻辑世界，在这个小世界里我们根据自己的认知对模型做出一系列假设并且能够验证模型的逻辑。假设小世界是对现实世界的准确描述，没有替代模型可以更好地利用数据中的信息并支持更好的决策。&lt;/p&gt;
&lt;p&gt;大世界是现实世界，可能存在很多我们小世界中没有考虑的事件，模型是对现实世界的不完整表示。&lt;/p&gt;</summary>
    
    
    
    <category term="书籍阅读" scheme="https://www.hfcouc.work/categories/%E4%B9%A6%E7%B1%8D%E9%98%85%E8%AF%BB/"/>
    
    
    <category term="概率编程" scheme="https://www.hfcouc.work/tags/%E6%A6%82%E7%8E%87%E7%BC%96%E7%A8%8B/"/>
    
    <category term="贝叶斯统计" scheme="https://www.hfcouc.work/tags/%E8%B4%9D%E5%8F%B6%E6%96%AF%E7%BB%9F%E8%AE%A1/"/>
    
    <category term="Statistical Rethinking" scheme="https://www.hfcouc.work/tags/Statistical-Rethinking/"/>
    
  </entry>
  
  <entry>
    <title>Statistical Rethinking:Chapter1</title>
    <link href="https://www.hfcouc.work/2022/06/28/rt1/"/>
    <id>https://www.hfcouc.work/2022/06/28/rt1/</id>
    <published>2022-06-28T09:47:59.000Z</published>
    <updated>2022-06-29T01:30:33.136Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="\assets\js\Meting.min.js"></script><h2 id="The-Golem-of-Prague"><a href="#The-Golem-of-Prague" class="headerlink" title="The Golem of Prague"></a>The Golem of Prague</h2><p><img src="https://raw.githubusercontent.com/HFC666/image/master/img/rt1_1.png" alt=""></p><p><code>golem</code>为魔像，为一个非常强大的机器人，但是它只会听从人的命令，没有自主思考的能力。因此人类必须给他设置非常具体的命令，否则可能会对人类造成伤害。</p><span id="more"></span><h3 id="Statistical-golems"><a href="#Statistical-golems" class="headerlink" title="Statistical golems"></a>Statistical golems</h3><p>统计家也制造魔像。</p><p><img src="https://raw.githubusercontent.com/HFC666/image/master/img/rt1_2.jpg" alt=""></p><blockquote><p>统计家制造的魔像，非常强大，但也同样需要人类的指导。缺乏灵活性，在需要创造性的区域无法应用。</p></blockquote><h3 id="Statistical-rethinking"><a href="#Statistical-rethinking" class="headerlink" title="Statistical rethinking"></a>Statistical rethinking</h3><p>很多人认为统计推断的目标是检验无效假设。但这是不正确的，我们有以下两个理由：</p><ol><li>假设不是模型。假设和不同种类的模型之间的关系是复杂的。许多模型对应同一个假设，许多假设对应一个模型。这使得严格的证伪变得不可能。</li><li>测量很重要。即使我们认为数据证伪了模型，另一位观察者也会争论我们的方法和措施。他们不相信数据。有时他们是对的。</li></ol><h4 id="Hypotheses-are-not-models"><a href="#Hypotheses-are-not-models" class="headerlink" title="Hypotheses are not models"></a>Hypotheses are not models</h4><p>当我们试图证伪一个假设时，我们必须使用某种模型，但是我们不能仅仅通过一个模型来证明假设是错误的。</p><p>我们看一个关于进化的例子，有人认为进化是中性的，而有人不这么认为，所有存在两个假设。同一个假设可能导致不同的过程的模型，而同一个过程模型会引出不同的统计模型，同一个统计模型也可能对应于不同的过程模型和假设，因此证伪非常复杂。</p><p><img src="https://raw.githubusercontent.com/HFC666/image/master/img/rt1_3.jpg" alt=""></p><h4 id="Measurement-matters"><a href="#Measurement-matters" class="headerlink" title="Measurement matters"></a>Measurement matters</h4><p>首先观测是存在误差的，而且有的测量非常复杂容易出现误差。</p><p>其次假设并不一定是离散的，现实生活中的很多假设都是连续的，例如$80\%$的天鹅都是白色的，对我们证伪来说非常困难。</p><h3 id="Three-tools-for-golem-engineering"><a href="#Three-tools-for-golem-engineering" class="headerlink" title="Three tools for golem engineering"></a>Three tools for golem engineering</h3><h4 id="Bayesian-data-analysis"><a href="#Bayesian-data-analysis" class="headerlink" title="Bayesian data analysis"></a>Bayesian data analysis</h4><p>贝叶斯统计用随机型来描述不确定性，更详细的将在第二章讲述。</p><h4 id="Multilevel-models"><a href="#Multilevel-models" class="headerlink" title="Multilevel models"></a>Multilevel models</h4><p>使用多级模型有四个典型且互补的原因：</p><ol><li>调整重复抽样的估计值。当不止一个观察来自同一个人、地点或时间时，传统的单级模型可能会误导我们。</li><li>调整抽样不平衡的估计值。当某些个体、地点或时间的采样次数多于其他人时，我们也可能会被单级模型误导。</li><li>研究变异。如果我们的研究问题包括数据中个人或其他群体之间的变化，那么多层次模型将有很大帮助，因为它们明确地模拟了变化。</li><li>避免平均。学者们经常对一些数据进行预平均，以构建用于回归分析的变量。这可能很危险，因为平均会消除变化。因此，它制造了虚假的信念。多级模型允许我们保留原始预平均值中的不确定性，同时仍使用平均值进行预测。</li></ol><h4 id="Model-comparison-and-information-criteria"><a href="#Model-comparison-and-information-criteria" class="headerlink" title="Model comparison and information criteria"></a>Model comparison and information criteria</h4><p>最著名的信息准则是 AIC，即 Akaike (ah-kah-ee-kay) 信息准则。AIC 及其同类被称为“信息”标准，因为它们从信息论中发展出对模型准确性的度量。我们可以用起来比较模型的好坏。</p>]]></content>
    
    
    <summary type="html">&lt;h2 id=&quot;The-Golem-of-Prague&quot;&gt;&lt;a href=&quot;#The-Golem-of-Prague&quot; class=&quot;headerlink&quot; title=&quot;The Golem of Prague&quot;&gt;&lt;/a&gt;The Golem of Prague&lt;/h2&gt;&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/HFC666/image/master/img/rt1_1.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;&lt;code&gt;golem&lt;/code&gt;为魔像，为一个非常强大的机器人，但是它只会听从人的命令，没有自主思考的能力。因此人类必须给他设置非常具体的命令，否则可能会对人类造成伤害。&lt;/p&gt;</summary>
    
    
    
    <category term="书籍阅读" scheme="https://www.hfcouc.work/categories/%E4%B9%A6%E7%B1%8D%E9%98%85%E8%AF%BB/"/>
    
    
    <category term="概率编程" scheme="https://www.hfcouc.work/tags/%E6%A6%82%E7%8E%87%E7%BC%96%E7%A8%8B/"/>
    
    <category term="贝叶斯统计" scheme="https://www.hfcouc.work/tags/%E8%B4%9D%E5%8F%B6%E6%96%AF%E7%BB%9F%E8%AE%A1/"/>
    
    <category term="Statistical Rethinking" scheme="https://www.hfcouc.work/tags/Statistical-Rethinking/"/>
    
  </entry>
  
  <entry>
    <title>Turing:a language for flexible probabilistic inference</title>
    <link href="https://www.hfcouc.work/2022/06/28/Turing/"/>
    <id>https://www.hfcouc.work/2022/06/28/Turing/</id>
    <published>2022-06-28T02:32:40.000Z</published>
    <updated>2022-06-28T02:40:23.041Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="\assets\js\Meting.min.js"></script><h2 id="Turing-a-language-for-flexible-probabilistic-inference"><a href="#Turing-a-language-for-flexible-probabilistic-inference" class="headerlink" title="Turing: a language for flexible probabilistic inference"></a>Turing: a language for flexible probabilistic inference</h2><blockquote><p>文章链接：<a href="http://proceedings.mlr.press/v84/ge18b.html?ref=https://githubhelp.com">http://proceedings.mlr.press/v84/ge18b.html?ref=https://githubhelp.com</a></p></blockquote><p><img src="http://static01.imgkr.com/temp/9ce5f3b2581549b8b89e6c9cdf7ebea5.png" alt=""></p><span id="more"></span><h3 id="Background"><a href="#Background" class="headerlink" title="Background"></a>Background</h3><p>在概率模型中，我们关注的一般是$p(\theta\mid y,\gamma)$，其中$\theta$为参数，$y$为观测数据，$\gamma$为一些确定了的超参数。</p><h4 id="Models-as-computer-programs"><a href="#Models-as-computer-programs" class="headerlink" title="Models as computer programs"></a>Models as computer programs</h4><p>最早的概率编程语言为BUGS，可以追溯到20世纪90年代。下面展示了概率程序的一般结构。</p><p>输入：数据$y$和超参数$\gamma$</p><p>步骤1：定义全局参数：</p><script type="math/tex; mode=display">\theta^{\text{global}}\sim p(\cdot\mid \gamma)</script><p>步骤2：对于每一个观测$y_n$，定义(局部)隐变量并计算似然：</p><script type="math/tex; mode=display">\begin{aligned}\theta_n^{\text{local}}&\sim p\left(\cdot\mid\theta_{1:n-1}^{\text{local}},\theta^{\text{global}},\gamma\right)\\y_n&\sim p\left(\cdot\mid \theta_{1:n}^{\text{local}},\theta^{\text{global}},\gamma\right)\end{aligned}</script><p>其中$n=1,2,\cdots,N$。</p><p>参数分为两类：$\theta_n^{\text{local}}$表示对于观测$y_n$的模型参数，如混合高斯模型中$y_n$属于哪个高斯分布的参数，而$\theta^{\text{global}}$表示全局变量。</p><h4 id="Inference-for-probabilistic-programs"><a href="#Inference-for-probabilistic-programs" class="headerlink" title="Inference for probabilistic programs"></a>Inference for probabilistic programs</h4><p>概率程序只有在与高效的推理引擎相结合时才能发挥其灵活性潜力。为了解释概率编程中推理如何工作，我们考虑以下具有$K$个状态的HMM例子：</p><script type="math/tex; mode=display">\begin{aligned}\pi_k&\sim \text{Dir}(\theta)\\\phi_k&\sim p(\gamma)\\z_t\mid z_{t-1}&\sim \text{Cat}(\cdot\mid \pi_{z_{t-1}})\\y_t\mid z_t&\sim h(\cdot\mid \phi_{z_t})\end{aligned}</script><p>其中$k = 1,2,\cdots,K$，$t = 1,\cdots,N$。</p><p>具有以下三个步骤的高效 Gibbs 采样器通常用于贝叶斯推理：</p><ul><li>Step 1: Sample $z_{1: T} \sim z_{1: T} \mid \phi_{1: K}, \pi_{1: K}, y_{1: T} ;$</li><li>Step 2: Sample $\phi_{k} \sim \phi_{k} \mid z_{1: T}, y_{1: T}, \gamma$;</li><li>Step 3: Sample $\pi_{k} \sim \pi_{k} \mid z_{1: T}, \theta(k=1, \ldots, K)$.</li></ul><h4 id="Computation-graph-based-inference"><a href="#Computation-graph-based-inference" class="headerlink" title="Computation graph based inference"></a>Computation graph based inference</h4><p>对概率程序进行建模的一大挑战是构建模型变量之间的计算图。对于一些编程语言，在推理之前概率图模型就已经生成，但是当程序中存在随机分支时就会出现问题，在这种情况下，我们不得不求助于其他推理方法。</p><h3 id="Composable-MCMC-inference"><a href="#Composable-MCMC-inference" class="headerlink" title="Composable MCMC inference"></a>Composable MCMC inference</h3><p>我们提出的可组合推理方法利用了HMC算法和粒子吉布斯(PG)算法。为了描述所提出的概率程序方法，我们利用潜在狄利克雷分配(LDA)的例子。</p><figure class="highlight julia"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@model</span> lda(K ,M, N, w, d, beta, alpha) = <span class="keyword">begin</span></span><br><span class="line">    theta = <span class="built_in">Vector</span>&#123;<span class="built_in">Vector</span>&#123;<span class="built_in">Real</span>&#125;&#125;(M)</span><br><span class="line">    <span class="keyword">for</span> m = <span class="number">1</span>:M</span><br><span class="line">        theta[m] ~ Dirichlet(alpha)</span><br><span class="line">    <span class="keyword">end</span></span><br><span class="line">    phi = <span class="built_in">Vector</span>&#123;<span class="built_in">Vector</span>&#123;<span class="built_in">Real</span>&#125;&#125;(K)</span><br><span class="line">    <span class="keyword">for</span> k = <span class="number">1</span>:K</span><br><span class="line">        phi[k] ~ Dirichlet(beta)</span><br><span class="line">    <span class="keyword">end</span></span><br><span class="line">    </span><br><span class="line">    z = tzeros(<span class="built_in">Int</span>, N)</span><br><span class="line">    <span class="keyword">for</span> n = <span class="number">1</span>:N</span><br><span class="line">        z[n] ~ Categorical(theta[d[n]])</span><br><span class="line">        w[n] ~ Categorical(phi[z[n]])</span><br><span class="line">    <span class="keyword">end</span></span><br><span class="line"><span class="keyword">end</span></span><br></pre></td></tr></table></figure><p>其中变量$\phi,\theta,z$表示模型参数，变量$K,M,N,d,\beta,\alpha$表示超参数，$w$表示观测数据。</p><p>一旦定义了模型，提供数据和执行推理就很直观了。</p><figure class="highlight julia"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">model = lda(K, V, M, N, w, d, beta, alpha)</span><br><span class="line">sample(model, engine)</span><br></pre></td></tr></table></figure><p><code>engine</code>是我们想要使用的MCMC引擎。例如，如果要应用例子吉布斯采样，我们可以：</p><figure class="highlight julia"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">spl = PG(n, m)</span><br><span class="line">sample(model, spl)</span><br></pre></td></tr></table></figure><p>这将会用含有$m$个粒子的PG进行$n$次迭代。</p><p>我们也可以对不同的参数采用不同的采样器：</p><figure class="highlight julia"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">spl2 = Gibbs(<span class="number">1000</span>, PG(<span class="number">10</span>,<span class="number">2</span>,:z), HMC(<span class="number">2</span>, <span class="number">0.1</span>, <span class="number">1</span>, <span class="number">5</span>, :phi,:theta))</span><br></pre></td></tr></table></figure><p>上述采样引擎<code>spl2</code>将参数分割为两部分，每个部分采用不同的采样方法，值得注意的是，分布的两个部分不需要是互斥的。</p><h4 id="A-family-of-MCMC-operators"><a href="#A-family-of-MCMC-operators" class="headerlink" title="A family of MCMC operators"></a>A family of MCMC operators</h4><p><img src="http://static01.imgkr.com/temp/679a0f1dc05a4456ae7ee80183c63582.jpg" alt=""></p><blockquote><p>Supported Monte Carlo algorithms in Turing</p></blockquote><h3 id="Implementation-and-Experiments"><a href="#Implementation-and-Experiments" class="headerlink" title="Implementation and Experiments"></a>Implementation and Experiments</h3><h4 id="The-Turing-library"><a href="#The-Turing-library" class="headerlink" title="The Turing library"></a>The Turing library</h4><p>Turing为Julia的一个包。因为Turing为一般的Julia程序，因此它可以利用Julia中丰富的数值和统计库。</p><h5 id="Efficient-particle-Gibbs-implementation"><a href="#Efficient-particle-Gibbs-implementation" class="headerlink" title="Efficient particle Gibbs implementation"></a>Efficient particle Gibbs implementation</h5><p>我们使用协程来实现粒子 Gibbs。协程可以看作是函数的泛化，具有可以在多个点暂停和恢复的特性。</p><h5 id="Automatic-differentiation"><a href="#Automatic-differentiation" class="headerlink" title="Automatic differentiation"></a>Automatic differentiation</h5><p>HMC在采样的过程需要梯度，当给定定义$\log p(\theta\mid z_{1:N},\gamma)$的计算程序时，这些梯度可以通过自动微分(AD)自动获得。为了简便和高效，我们率先使用了一种称为向量模式的前向微分技术。向量模式前向微分背后的主要概念是多维对偶数，其在标量函数上的行为定义为：</p><script type="math/tex; mode=display">f\left(\theta+\sum_{i=1}^D y_i\epsilon_i\right) = f(\theta) + f^{\prime}(\theta)\sum_{i=1}^Dy_i\epsilon_i</script><p>其中$\epsilon_i\epsilon_j=0,\text{for }i\neq j$。</p><p>对于小模型，向量前向AD非常高效。但是对于大模型逆向模式的AD较为高效，因此Turing两种模式都存在。</p><h5 id="Vectorized-random-variables"><a href="#Vectorized-random-variables" class="headerlink" title="Vectorized random variables"></a>Vectorized random variables</h5><p>Turing支持利用以下语法对独立同分布的变量进行矢量化采样：</p><figure class="highlight julia"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">rv = <span class="built_in">Vector</span>(<span class="number">10</span>)</span><br><span class="line">rv ~ [Normal(<span class="number">0</span>, <span class="number">1</span>)]</span><br></pre></td></tr></table></figure><h5 id="Constrained-random-variables"><a href="#Constrained-random-variables" class="headerlink" title="Constrained random variables"></a>Constrained random variables</h5><p>Turing支持约束的变量。主要由三种类型的约束：</p><ol><li>有界的单变量。</li><li>有简单约束的多维变量，如相加和为$1$。</li><li>矩阵约束：例如协方差矩阵为半正定矩阵。</li></ol><h5 id="MCMC-output-analysis"><a href="#MCMC-output-analysis" class="headerlink" title="MCMC output analysis"></a>MCMC output analysis</h5><p>在Turing中我们可以使用<code>describe</code>函数计算：</p><ol><li>均值</li><li>标准差</li><li>naive standard error</li><li>蒙特卡洛标准误差</li><li>有效样本数</li><li>分位数</li></ol><p>也可以使用<code>hpd</code>函数计算高后验概率区间，互相关<code>cor</code>，自相关<code>autocor</code>，状态空间变化率<code>changerate</code>和偏差信息准则<code>dic</code>等等。</p><h4 id="Finding-the-right-inference-engine"><a href="#Finding-the-right-inference-engine" class="headerlink" title="Finding the right inference engine"></a>Finding the right inference engine</h4><p>下面我们将比较<code>NUTS</code>和<code>Gibbs(PG,HMC)</code>在不同的概率模型上。</p><h5 id="Models-and-inference-engine-setup"><a href="#Models-and-inference-engine-setup" class="headerlink" title="Models and inference engine setup"></a>Models and inference engine setup</h5><p><strong>Stochastic Volatility Model</strong>：参数的集合为$\{\phi,\sigma,\mu,h_{1:N}\}$。所有这些参数对于目标分布来说都是可导的，因此NUTS算法是可用的：</p><script type="math/tex; mode=display">\begin{aligned}\mu &\sim \mathcal{C} \mathrm{a}(0,10)), \phi \sim \mathcal{U} \mathrm{n}(-1,1), \sigma \sim \mathcal{C} \mathrm{a}(0,5), \quad(\sigma>0) \\h_{1} & \sim \mathcal{N}\left(\mu, \sigma / \sqrt{1-\phi^{2}}\right), h_{n} \sim \mathcal{N}\left(\mu+\phi\left(h_{n-1}-\mu\right), \sigma\right) \\y_{n} & \sim \mathcal{N}\left(0, \exp \left(h_{n} / 2\right)\right) \quad(n=2,3, \ldots, N) .\end{aligned}</script><p>其中$\mathcal{C}\mathrm{a}$表示柯西分布。</p><figure class="highlight julia"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">spl1 = NUTS(<span class="number">1e4</span>, <span class="number">1e3</span>, <span class="number">0.65</span>)</span><br><span class="line">spl2 = Gibbs(<span class="number">1e4</span>, PG(<span class="number">5</span>, <span class="number">1</span>, :h), NUTS(<span class="number">1</span>, <span class="number">1e3</span>, <span class="number">0.65</span>, :mu, :phi, :sigma))</span><br></pre></td></tr></table></figure><p><strong>Gaussian Mixture Model</strong>：参数的集合为$\{z,\theta\}$，其中参数$\theta$是可导的，参数$z$不可以。为了运行NUTS算法，我们积分积掉$z$只对$\theta$采样：</p><script type="math/tex; mode=display">\begin{array}{r}\mu=\left(\mu_{1: K}\right), \quad \sigma=\left(\sigma_{1: K}\right), \quad \pi=\left(p_{1: K}\right) \\z \sim \operatorname{Cat}(\pi), \quad \theta \sim \mathcal{N}\left(\mu_{z}, \sigma_{z}\right)\end{array}</script><figure class="highlight julia"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">spl3 = NUTS(<span class="number">5e4</span>, <span class="number">1000</span>, <span class="number">0.65</span>)</span><br><span class="line">spl4 = Gibbs(<span class="number">5e4</span>, PG(<span class="number">5</span>, <span class="number">1</span>, :z), NUTS(<span class="number">5e2</span>, <span class="number">1e3</span>, <span class="number">0.65</span>, :theta))</span><br></pre></td></tr></table></figure><h5 id="Results"><a href="#Results" class="headerlink" title="Results"></a>Results</h5><p><img src="http://static01.imgkr.com/temp/99d803e553514e089858f61d913babd1.jpg" alt=""></p><blockquote><p>上图为在GMM模型上trace plot，下图为联合分布的概率的对数的图，可以看到两个算法都达到了收敛，但是NUTS算法在某些对方被”困住了”。在下图更明显。</p></blockquote><p><img src="http://static01.imgkr.com/temp/095ae7d0d5a54a33960b6cd2162133bc.jpg" alt=""></p><blockquote><p>对具有$5$个混合的GMM采样的结果，可以更明显地看到NUTS算法被困住了，在图的上半部分只探索到了两个混合成分。</p></blockquote>]]></content>
    
    
    <summary type="html">&lt;h2 id=&quot;Turing-a-language-for-flexible-probabilistic-inference&quot;&gt;&lt;a href=&quot;#Turing-a-language-for-flexible-probabilistic-inference&quot; class=&quot;headerlink&quot; title=&quot;Turing: a language for flexible probabilistic inference&quot;&gt;&lt;/a&gt;Turing: a language for flexible probabilistic inference&lt;/h2&gt;&lt;blockquote&gt;
&lt;p&gt;文章链接：&lt;a href=&quot;http://proceedings.mlr.press/v84/ge18b.html?ref=https://githubhelp.com&quot;&gt;http://proceedings.mlr.press/v84/ge18b.html?ref=https://githubhelp.com&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;img src=&quot;http://static01.imgkr.com/temp/9ce5f3b2581549b8b89e6c9cdf7ebea5.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;</summary>
    
    
    
    <category term="文献阅读" scheme="https://www.hfcouc.work/categories/%E6%96%87%E7%8C%AE%E9%98%85%E8%AF%BB/"/>
    
    
    <category term="概率编程" scheme="https://www.hfcouc.work/tags/%E6%A6%82%E7%8E%87%E7%BC%96%E7%A8%8B/"/>
    
    <category term="Julia" scheme="https://www.hfcouc.work/tags/Julia/"/>
    
  </entry>
  
  <entry>
    <title>文章A Conceptual Introduction to Hamiltonian Monte Carlo阅读笔记</title>
    <link href="https://www.hfcouc.work/2022/06/22/HMC/"/>
    <id>https://www.hfcouc.work/2022/06/22/HMC/</id>
    <published>2022-06-22T09:51:00.000Z</published>
    <updated>2022-06-28T02:34:43.829Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="\assets\js\Meting.min.js"></script><h2 id="A-Conceptual-Introduction-to-Hamiltonian-Monte-Carlo"><a href="#A-Conceptual-Introduction-to-Hamiltonian-Monte-Carlo" class="headerlink" title="A Conceptual Introduction to Hamiltonian Monte Carlo"></a>A Conceptual Introduction to Hamiltonian Monte Carlo</h2><blockquote><p>文章链接：<a href="https://arxiv.org/abs/1701.02434">https://arxiv.org/abs/1701.02434</a></p></blockquote><p><img src="http://static01.imgkr.com/temp/8a010064bcee4b0eac9e9c05caa3f1ee.png" alt=""></p><span id="more"></span><h3 id="期望的计算"><a href="#期望的计算" class="headerlink" title="期望的计算"></a>期望的计算</h3><p>对于给定函数$f(q)$，我们在给定的$q$的分布$\pi (q)$上计算其期望：</p><script type="math/tex; mode=display">\mathbb{E}_{\pi}[f] = \int_{\mathcal{Q}}\pi(q)f(q)dq</script><p>一般情况下此积分的原函数得不到，因此我们采用蒙特卡洛的方法，在$\pi(q)$上对$q$进行采样，用下式计算期望：</p><script type="math/tex; mode=display">\mathbb{E}_{\pi}[f]\approx \frac{1}{n}\sum_{i=1}^n f(q_i)\quad q_i\sim \pi(q)</script><p>但是我们如何在$\pi(q)$上进行采样呢？为了节省时间，我们一般选择在概率密度较高的位置进行采样，即在概率密度最高的邻域内进行采样。但是在高维情况下存在问题，假设我们在概率密度最高的$1/3$邻域内进行采样，当维度为$n$时，积分区域的体积为$(1/3)^n$，当$n$很大时趋近于$0$，因此对积分的贡献很小。而概率密度较小的地方由于概率密度趋近于$0$，对积分的贡献也不大。我们着重关注的应该是介于两者之间的区域，其对积分的贡献较大，成为典型集(typical set)。我们研究的重点在于如何在<strong>典型集上采样</strong>。</p><h3 id="马尔可夫链蒙特卡洛方法"><a href="#马尔可夫链蒙特卡洛方法" class="headerlink" title="马尔可夫链蒙特卡洛方法"></a>马尔可夫链蒙特卡洛方法</h3><h4 id="理想状态"><a href="#理想状态" class="headerlink" title="理想状态"></a>理想状态</h4><p>利用马尔可夫链蒙特卡洛(MCMC)方法可以在典型集上进行采样。在理想状态下，MCMC的采样过程可以分为三个阶段：</p><ol><li>从初始位置到典型集，此时偏差(bias)较大。</li><li>进入典型集后，在典型集上进行探索，准确度迅速上升。</li><li>继续在典型集上进行探索，准确度上升缓慢。</li></ol><p>如下图所示：</p><p><img src="http://static01.imgkr.com/temp/1a1df50a72d54d1998ae2b266f3e3aed.jpg" alt=""></p><blockquote><p>图(a)表示阶段1，图(b)表示阶段2，图(c)表示阶段3。</p></blockquote><p>当达到阶段3时，估计的结果符合大数定律：</p><script type="math/tex; mode=display">\hat{f}^{\text{MCMC}}_N\sim \mathcal{N}(\mathbb{E}_{\pi}[f],\text{MCMC-SE})</script><p>其中蒙特卡洛误差为：</p><script type="math/tex; mode=display">\text{MCMC-SE}\equiv \sqrt{\frac{\text{Var}_{\pi}[f]}{\text{ESS}}}</script><p>其中ESS为有效样本量，定义为：</p><script type="math/tex; mode=display">\text{ESS} = \frac{N}{1+2\sum_{l=1}^\infty \rho_l}</script><p>其中$\rho_l$为之后$l$的自相关系数。</p><h4 id="病态情况"><a href="#病态情况" class="headerlink" title="病态情况"></a>病态情况</h4><p>当典型集内存在高曲率区域时，会导致此区域无法被探索，造成偏差。</p><p><img src="http://static01.imgkr.com/temp/ef84320549ab469690733e68a944729c.jpg" alt=""></p><blockquote><p>病态情况：其中绿色区域表示高曲率区域。存在三种情况：</p><ol><li>无法跨过此高曲率区域，仅在一侧进行采样。</li><li>在高曲率区域周围震荡。</li><li>可以跨过高区域区域，在整个典型集上进行采样。</li></ol></blockquote><h4 id="Metropolis-Hastings采样"><a href="#Metropolis-Hastings采样" class="headerlink" title="Metropolis-Hastings采样"></a>Metropolis-Hastings采样</h4><p>一个较为简单的MCMC方法为M-H采样(Metropolis-Hastings采样)，在局部利用建议分布对目标分布进行近似，其分为两个步骤：</p><ol><li>在提议分布$\mathbb{Q}(q^\prime\mid q)$进行采样</li><li>计算接受率<script type="math/tex">a(q^\prime\mid q) = \min\left(1,\frac{\mathbb{Q}(q\mid q^\prime)\pi(q^\prime)}{\mathbb{Q}(q^\prime\mid q)\pi(q)}\right)</script>，如果$a$大于生成的$0\sim1$之间的随机数，接受样本$q^\prime$，否则继续接受样本$q$。<br>但是M-H采样在高维情况下存在接受率过低的问题。</li></ol><h3 id="Hamiltonian-Monte-Carlo"><a href="#Hamiltonian-Monte-Carlo" class="headerlink" title="Hamiltonian Monte Carlo"></a>Hamiltonian Monte Carlo</h3><p>汉密尔顿蒙特卡洛(HMC)方法：我们可以利用典型集的形状的特征来进行采样。我们不再在典型集上随机移动，而是通过向量场的形式来指示移动的方向，使其高效地在典型集上移动。</p><p>我们将概率系统类比于物理系统，典型集类似于行星绕地球旋转地轨道。对于行星，我们需要添加动量来抵消重力使行星正常围绕地球运动；类比于概率空间，我们需要添加动量来抵消梯度使马尔可夫链在典型集上采样。</p><h4 id="相空间和汉密尔顿方程"><a href="#相空间和汉密尔顿方程" class="headerlink" title="相空间和汉密尔顿方程"></a>相空间和汉密尔顿方程</h4><p>我们需要引入动量参数来补充目标参数空间的每个维度：</p><script type="math/tex; mode=display">q_n \rightarrow (q_n,p_n)</script><p>这样将$D$维空间拓展为了$2D$维的空间，我们就将目标参数空间拓展为了相空间。相空间上的联合分布成为典型分布(canonical distribution)：</p><script type="math/tex; mode=display">\pi(q,p) = \pi(p\mid q)\pi(q)</script><p>这样我们对动量参数进行积分后很容易得到我们要采样的目标参数。</p><p>我们将典型分布写为不变的汉密尔顿函数的形式：</p><script type="math/tex; mode=display">\pi(q,p) = \exp^{-H(q,p)}</script><p>所以：</p><script type="math/tex; mode=display">\begin{aligned}H(q,p) &= -\log\pi(p\mid q) - \log\pi(q)\\&\equiv K(p,q) + V(q)\end{aligned}</script><p>其中$K(p,q)$被称为动能，$V(q)$被称为势能。</p><p>我们利用汉密尔顿方程来生成向量场：</p><script type="math/tex; mode=display">\begin{aligned}\frac{dq}{dt} &= + \frac{\partial H}{\partial p} = \frac{\partial K}{\partial p}\\\frac{dp}{dt} &= -\frac{\partial H}{\partial q} = -\frac{\partial K}{\partial q} - \frac{\partial V}{\partial q}\end{aligned}</script><p>所以汉密尔顿方程是不随时间发生改变的，因为：</p><script type="math/tex; mode=display">\begin{aligned}\frac{dH}{dt} &= \frac{\partial H}{\partial p}\frac{d p}{dt} + \frac{\partial H}{\partial q}\frac{d q}{dt}\\&= -\frac{\partial H}{\partial p}\frac{\partial H}{\partial q} + \frac{\partial H}{\partial q}\frac{\partial H}{\partial p}\\&=0\end{aligned}</script><h4 id="理想条件下的汉密尔顿转移"><a href="#理想条件下的汉密尔顿转移" class="headerlink" title="理想条件下的汉密尔顿转移"></a>理想条件下的汉密尔顿转移</h4><p>理想条件下的HMC可以分为3个步骤：</p><ol><li>从初始位置产生初始动量</li><li>以此类推产生轨迹</li><li>从相空间投影到参数空间</li></ol><h3 id="高效的HMC"><a href="#高效的HMC" class="headerlink" title="高效的HMC"></a>高效的HMC</h3><h4 id="相空间的几何形状"><a href="#相空间的几何形状" class="headerlink" title="相空间的几何形状"></a>相空间的几何形状</h4><p>汉密尔顿公式的性质使汉密尔顿方程的值始终保持不变。话句话说，每一个汉密尔顿轨迹都使一个能级：</p><script type="math/tex; mode=display">H^{-1}(E) = \{q,p\mid H(q,p)=E\}</script><p>如下图所示，相空间可以被分解维汉密尔顿能级。</p><p><img src="http://static01.imgkr.com/temp/028f6ae9a0074f3cb6047d241f6e3945.jpg" alt=""></p><p>所以我们的采样过程可以分解为两个步骤，一个是在相同的能级上进行采样，一个是在不同的能级上进行跃迁，如下图：</p><p><img src="http://static01.imgkr.com/temp/72b6558918b7415589144699fd37eb61.jpg" alt=""></p><blockquote><p>深红色表示在相同的能级上进行采样，浅红色的表示在不同能级上进行跃迁。</p></blockquote><h4 id="对动能的优化"><a href="#对动能的优化" class="headerlink" title="对动能的优化"></a>对动能的优化</h4><p>欧几里得-高斯动能：</p><script type="math/tex; mode=display">\begin{aligned}\Delta(q,q^\prime) &= (q-q^\prime)^\top\cdot M\cdot(q-q^\prime)\\\Delta(p,p^\prime) &= (p-p^\prime)^\top\cdot M^{-1}\cdot(p-p^\prime)\end{aligned}</script><p>我们一般定义条件分布为：</p><script type="math/tex; mode=display">\pi(p\mid q) = \mathcal{N}(p\mid 0,M)</script><p>这种特殊选择定义了欧几里得-高斯动能：</p><script type="math/tex; mode=display">K(q,p) = \frac{1}{2}P^\top \cdot M^{-1}\cdot p + \frac{1}{2}\log|M|+\text{const}</script><p>黎曼-高斯动能函数：与欧几里得-高斯动能函数不同之处为协方差与位置有关：</p><script type="math/tex; mode=display">\pi(p\mid q) = \mathcal{N}(p\mid 0,\Sigma(q))</script><p>定义了黎曼-高斯动能：</p><script type="math/tex; mode=display">K(q,p) = \frac{1}{2}p^\top\cdot\Sigma^{-1}(q)\cdot p+\frac{1}{2}\log|\Sigma(q)| + \text{const}</script><h4 id="对积分时间的优化"><a href="#对积分时间的优化" class="headerlink" title="对积分时间的优化"></a>对积分时间的优化</h4><p>这里的积分时间指的是在某个特定能级上的探索时间(步数)。随着积分时间的增加，时间期望会收敛到空间期望。</p><p><img src="http://static01.imgkr.com/temp/cca5d76d452f47f695f3a308dfe6602f.jpg" alt=""></p><blockquote><p>图(a)：时间期望与空间期望的差值的绝对值随着积分时间的变化，可以看到到积分时间到达一定的程度后，增加积分时间对结果产生的影响并不大；图(b)：有效样本数随着积分时间的变化，与图(a)变化类似；图(c)：有效样本数/积分时间随着积分时间的变化，先增加后减小，存在最大值。</p></blockquote><p>当目标概率密度为：</p><script type="math/tex; mode=display">\pi_\beta(q)\propto \exp(-|q|^\beta)</script><p>动能函数为欧几里得动能：</p><script type="math/tex; mode=display">\pi(p\mid q) = \mathcal{N}(0,1)</script><p>最优积分时间与包含轨迹的能级的能量成比例：</p><script type="math/tex; mode=display">T_{\text{optimal}}(q,p)\propto (H(q,p))^{\frac{2-\beta}{2\beta}}</script><h3 id="在实践中实现HMC"><a href="#在实践中实现HMC" class="headerlink" title="在实践中实现HMC"></a>在实践中实现HMC</h3><p>由于在绝大数情况下我们不能准确地求解哈密顿方程，必须采用数值求解的方法，但是数值求解的过程会累积误差，对我们的结果产生影响。</p><h4 id="Symplectic-Integrators"><a href="#Symplectic-Integrators" class="headerlink" title="Symplectic Integrators"></a>Symplectic Integrators</h4><p>Symplectic Integrators(辛积分器)是一个强大的积分器，它产生的数值轨迹不会偏离精确的能级，而是在其附近震荡，即使在很长的积分时间内也是如此。</p><script type="math/tex; mode=display">\begin{aligned}&q_{0} \leftarrow q, p_{0} \leftarrow p \\&\text {for } 0 \leq n<\llcorner T / \epsilon\lrcorner \text { do } \\&\quad p_{n+\frac{1}{2}}  \leftarrow p_{n}-\frac{\epsilon}{2} \frac{\partial V}{\partial q}\left(q_{n}\right) \\&\quad q_{n+1}  \leftarrow q_{n}+\epsilon p_{n+\frac{1}{2}} \\&\quad p_{n+1} \leftarrow p_{n+\frac{1}{2}}-\frac{\epsilon}{2} \frac{\partial V}{\partial q}\left(q_{n+1}\right)\\&\text {end for. }\end{aligned}</script><h4 id="纠正辛积分器"><a href="#纠正辛积分器" class="headerlink" title="纠正辛积分器"></a>纠正辛积分器</h4><p>我们在每个能级上运行$L$步，取最后一个样本$(q_L,p_L)$，之后进行能级跃迁。因为我们是使用数值的方法，因此在同一个能级上采样上可能不能保持能量不变。因此我们借用M-H采样的思想来对样本进行进行接受-拒绝，因为在同一个能级上采样当确定初始点时采到的样本是固定的，所以：</p><script type="math/tex; mode=display">\mathbb{Q}(q_0,p_0\mid q_L,p_L) = \mathbb{Q}(q_L,p_L\mid q_0,p_0)=1</script><p>其接受概率为：</p><script type="math/tex; mode=display">\begin{aligned}a\left(q_{L},p_{L} \mid q_{0}, p_{0}\right) &=\min \left(1, \frac{\mathbb{Q}\left(q_{0}, p_{0} \mid q_{L},p_{L}\right) \pi\left(q_{L},p_{L}\right)}{\mathbb{Q}\left(q_{L},p_{L} \mid q_{0}, p_{0}\right) \pi\left(q_{0}, p_{0}\right)}\right) \\&=\min \left(1, \frac{\pi\left(q_{L},p_{L}\right)}{\pi\left(q_{0}, p_{0}\right)}\right) \\&=\min \left(1, \frac{\exp \left(-H\left(q_{L},p_{L}\right)\right)}{\exp \left(-H\left(q_{0}, p_{0}\right)\right)}\right) \\&=\min \left(1, \exp \left(-H\left(q_{L},p_{L}\right)+H\left(q_{0}, p_{0}\right)\right)\right)\end{aligned}</script>]]></content>
    
    
    <summary type="html">&lt;h2 id=&quot;A-Conceptual-Introduction-to-Hamiltonian-Monte-Carlo&quot;&gt;&lt;a href=&quot;#A-Conceptual-Introduction-to-Hamiltonian-Monte-Carlo&quot; class=&quot;headerlink&quot; title=&quot;A Conceptual Introduction to Hamiltonian Monte Carlo&quot;&gt;&lt;/a&gt;A Conceptual Introduction to Hamiltonian Monte Carlo&lt;/h2&gt;&lt;blockquote&gt;
&lt;p&gt;文章链接：&lt;a href=&quot;https://arxiv.org/abs/1701.02434&quot;&gt;https://arxiv.org/abs/1701.02434&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;img src=&quot;http://static01.imgkr.com/temp/8a010064bcee4b0eac9e9c05caa3f1ee.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;</summary>
    
    
    
    <category term="文献阅读" scheme="https://www.hfcouc.work/categories/%E6%96%87%E7%8C%AE%E9%98%85%E8%AF%BB/"/>
    
    
    <category term="算法" scheme="https://www.hfcouc.work/tags/%E7%AE%97%E6%B3%95/"/>
    
  </entry>
  
  <entry>
    <title>只争朝夕，不负韶华</title>
    <link href="https://www.hfcouc.work/2021/07/13/index/"/>
    <id>https://www.hfcouc.work/2021/07/13/index/</id>
    <published>2021-07-13T12:13:25.471Z</published>
    <updated>2022-06-28T08:27:11.493Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="\assets\css\APlayer.min.css"><script src="\assets\js\APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="\assets\js\Meting.min.js"></script><h1 align="center">    凡是过往皆为序章，所有将来皆可盼。</h1><p align="center">    <img src="https://pic3.zhimg.com/v2-526e03d542420a445ea3113417d592be_r.jpg" style="zoom: 100%;" /></p>]]></content>
    
    
      
      
    <summary type="html">&lt;link rel=&quot;stylesheet&quot; class=&quot;aplayer-secondary-style-marker&quot; href=&quot;\assets\css\APlayer.min.css&quot;&gt;&lt;script src=&quot;\assets\js\APlayer.min.js&quot; cla</summary>
      
    
    
    
    
  </entry>
  
</feed>
